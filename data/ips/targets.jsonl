{"id": "flask:src/flask/logging.py", "language": "python", "code": "from __future__ import annotations\n\nimport logging\nimport sys\nimport typing as t\n\nfrom werkzeug.local import LocalProxy\n\nfrom .globals import request\n\nif t.TYPE_CHECKING:  # pragma: no cover\n    from .sansio.app import App\n\n\n@LocalProxy\ndef wsgi_errors_stream() -> t.TextIO:\n    \"\"\"Find the most appropriate error stream for the application. If a request\n    is active, log to ``wsgi.errors``, otherwise use ``sys.stderr``.\n\n    If you configure your own :class:`logging.StreamHandler`, you may want to\n    use this for the stream. If you are using file or dict configuration and\n    can't import this directly, you can refer to it as\n    ``ext://flask.logging.wsgi_errors_stream``.\n    \"\"\"\n    if request:\n        return request.environ[\"wsgi.errors\"]  # type: ignore[no-any-return]\n\n    return sys.stderr\n\n\ndef has_level_handler(logger: logging.Logger) -> bool:\n    \"\"\"Check if there is a handler in the logging chain that will handle the\n    given logger's :meth:`effective level <~logging.Logger.getEffectiveLevel>`.\n    \"\"\"\n    level = logger.getEffectiveLevel()\n    current = logger\n\n    while current:\n        if any(handler.level <= level for handler in current.handlers):\n            return True\n\n        if not current.propagate:\n            break\n\n        current = current.parent  # type: ignore\n\n    return False\n\n\n#: Log messages to :func:`~flask.logging.wsgi_errors_stream` with the format\n#: ``[%(asctime)s] %(levelname)s in %(module)s: %(message)s``.\ndefault_handler = logging.StreamHandler(wsgi_errors_stream)  # type: ignore\ndefault_handler.setFormatter(\n    logging.Formatter(\"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\")\n)\n\n\ndef create_logger(app: App) -> logging.Logger:\n    \"\"\"Get the Flask app's logger and configure it if needed.\n\n    The logger name will be the same as\n    :attr:`app.import_name <flask.Flask.name>`.\n\n    When :attr:`~flask.Flask.debug` is enabled, set the logger level to\n    :data:`logging.DEBUG` if it is not set.\n\n    If there is no handler for the logger's effective level, add a\n    :class:`~logging.StreamHandler` for\n    :func:`~flask.logging.wsgi_errors_stream` with a basic format.\n    \"\"\"\n    logger = logging.getLogger(app.name)\n\n    if app.debug and not logger.level:\n        logger.setLevel(logging.DEBUG)\n\n    if not has_level_handler(logger):\n        logger.addHandler(default_handler)\n\n    return logger\n", "metadata": {"license": "BSD-3-Clause", "len_tokens": 566}}
{"id": "flask:src/flask/sessions.py", "language": "python", "code": "class SessionMixin(MutableMapping[str, t.Any]):\n    \"\"\"Expands a basic dictionary with session attributes.\"\"\"\n\n    @property\n    def permanent(self) -> bool:\n        \"\"\"This reflects the ``'_permanent'`` key in the dict.\"\"\"\n        return self.get(\"_permanent\", False)  # type: ignore[no-any-return]\n\n    @permanent.setter\n    def permanent(self, value: bool) -> None:\n        self[\"_permanent\"] = bool(value)\n\n    #: Some implementations can detect whether a session is newly\n    #: created, but that is not guaranteed. Use with caution. The mixin\n    # default is hard-coded ``False``.\n    new = False\n\n    #: Some implementations can detect changes to the session and set\n    #: this when that happens. The mixin default is hard coded to\n    #: ``True``.\n    modified = True\n\n    #: Some implementations can detect when session data is read or\n    #: written and set this when that happens. The mixin default is hard\n    #: coded to ``True``.\n    accessed = True", "metadata": {"license": "BSD-3-Clause", "len_tokens": 225}}
{"id": "flask:src/flask/sessions.py", "language": "python", "code": "class SecureCookieSession(CallbackDict[str, t.Any], SessionMixin):\n    \"\"\"Base class for sessions based on signed cookies.\n\n    This session backend will set the :attr:`modified` and\n    :attr:`accessed` attributes. It cannot reliably track whether a\n    session is new (vs. empty), so :attr:`new` remains hard coded to\n    ``False``.\n    \"\"\"\n\n    #: When data is changed, this is set to ``True``. Only the session\n    #: dictionary itself is tracked; if the session contains mutable\n    #: data (for example a nested dict) then this must be set to\n    #: ``True`` manually when modifying that data. The session cookie\n    #: will only be written to the response if this is ``True``.\n    modified = False\n\n    #: When data is read or written, this is set to ``True``. Used by\n    # :class:`.SecureCookieSessionInterface` to add a ``Vary: Cookie``\n    #: header, which allows caching proxies to cache different pages for\n    #: different users.\n    accessed = False\n\n    def __init__(\n        self,\n        initial: c.Mapping[str, t.Any] | c.Iterable[tuple[str, t.Any]] | None = None,\n    ) -> None:\n        def on_update(self: te.Self) -> None:\n            self.modified = True\n            self.accessed = True\n\n        super().__init__(initial, on_update)\n\n    def __getitem__(self, key: str) -> t.Any:\n        self.accessed = True\n        return super().__getitem__(key)\n\n    def get(self, key: str, default: t.Any = None) -> t.Any:\n        self.accessed = True\n        return super().get(key, default)\n\n    def setdefault(self, key: str, default: t.Any = None) -> t.Any:\n        self.accessed = True\n        return super().setdefault(key, default)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 412}}
{"id": "flask:src/flask/sessions.py", "language": "python", "code": "class SecureCookieSessionInterface(SessionInterface):\n    \"\"\"The default session interface that stores sessions in signed cookies\n    through the :mod:`itsdangerous` module.\n    \"\"\"\n\n    #: the salt that should be applied on top of the secret key for the\n    #: signing of cookie based sessions.\n    salt = \"cookie-session\"\n    #: the hash function to use for the signature.  The default is sha1\n    digest_method = staticmethod(_lazy_sha1)\n    #: the name of the itsdangerous supported key derivation.  The default\n    #: is hmac.\n    key_derivation = \"hmac\"\n    #: A python serializer for the payload.  The default is a compact\n    #: JSON derived serializer with support for some extra Python types\n    #: such as datetime objects or tuples.\n    serializer = session_json_serializer\n    session_class = SecureCookieSession\n\n    def get_signing_serializer(self, app: Flask) -> URLSafeTimedSerializer | None:\n        if not app.secret_key:\n            return None\n\n        keys: list[str | bytes] = []\n\n        if fallbacks := app.config[\"SECRET_KEY_FALLBACKS\"]:\n            keys.extend(fallbacks)\n\n        keys.append(app.secret_key)  # itsdangerous expects current key at top\n        return URLSafeTimedSerializer(\n            keys,  # type: ignore[arg-type]\n            salt=self.salt,\n            serializer=self.serializer,\n            signer_kwargs={\n                \"key_derivation\": self.key_derivation,\n                \"digest_method\": self.digest_method,\n            },\n        )\n\n    def open_session(self, app: Flask, request: Request) -> SecureCookieSession | None:\n        s = self.get_signing_serializer(app)\n        if s is None:\n            return None\n        val = request.cookies.get(self.get_cookie_name(app))\n        if not val:\n            return self.session_class()\n        max_age = int(app.permanent_session_lifetime.total_seconds())\n        try:\n            data = s.loads(val, max_age=max_age)\n            return self.session_class(data)\n        except BadSignature:\n            return self.session_class()\n\n    def save_session(\n        self, app: Flask, session: SessionMixin, response: Response\n    ) -> None:\n        name = self.get_cookie_name(app)\n        domain = self.get_cookie_domain(app)\n        path = self.get_cookie_path(app)\n        secure = self.get_cookie_secure(app)\n        partitioned = self.get_cookie_partitioned(app)\n        samesite = self.get_cookie_samesite(app)\n        httponly = self.get_cookie_httponly(app)\n\n        # Add a \"Vary: Cookie\" header if the session was accessed at all.\n        if session.accessed:\n            response.vary.add(\"Cookie\")\n\n        # If the session is modified to be empty, remove the cookie.\n        # If the session is empty, return without setting the cookie.\n        if not session:\n            if session.modified:\n                response.delete_cookie(\n                    name,\n                    domain=domain,\n                    path=path,\n                    secure=secure,\n                    partitioned=partitioned,\n                    samesite=samesite,\n                    httponly=httponly,\n                )\n                response.vary.add(\"Cookie\")\n\n            return\n\n        if not self.should_set_cookie(app, session):\n            return\n\n        expires = self.get_expiration_time(app, session)\n        val = self.get_signing_serializer(app).dumps(dict(session))  # type: ignore[union-attr]\n        response.set_cookie(\n            name,\n            val,\n            expires=expires,\n            httponly=httponly,\n            domain=domain,\n            path=path,\n            secure=secure,\n            partitioned=partitioned,\n            samesite=samesite,\n        )\n        response.vary.add(\"Cookie\")", "metadata": {"license": "BSD-3-Clause", "len_tokens": 779}}
{"id": "flask:src/flask/sessions.py", "language": "python", "code": "def save_session(\n        self, app: Flask, session: SessionMixin, response: Response\n    ) -> None:\n        name = self.get_cookie_name(app)\n        domain = self.get_cookie_domain(app)\n        path = self.get_cookie_path(app)\n        secure = self.get_cookie_secure(app)\n        partitioned = self.get_cookie_partitioned(app)\n        samesite = self.get_cookie_samesite(app)\n        httponly = self.get_cookie_httponly(app)\n\n        # Add a \"Vary: Cookie\" header if the session was accessed at all.\n        if session.accessed:\n            response.vary.add(\"Cookie\")\n\n        # If the session is modified to be empty, remove the cookie.\n        # If the session is empty, return without setting the cookie.\n        if not session:\n            if session.modified:\n                response.delete_cookie(\n                    name,\n                    domain=domain,\n                    path=path,\n                    secure=secure,\n                    partitioned=partitioned,\n                    samesite=samesite,\n                    httponly=httponly,\n                )\n                response.vary.add(\"Cookie\")\n\n            return\n\n        if not self.should_set_cookie(app, session):\n            return\n\n        expires = self.get_expiration_time(app, session)\n        val = self.get_signing_serializer(app).dumps(dict(session))  # type: ignore[union-attr]\n        response.set_cookie(\n            name,\n            val,\n            expires=expires,\n            httponly=httponly,\n            domain=domain,\n            path=path,\n            secure=secure,\n            partitioned=partitioned,\n            samesite=samesite,\n        )\n        response.vary.add(\"Cookie\")", "metadata": {"license": "BSD-3-Clause", "len_tokens": 347}}
{"id": "flask:src/flask/config.py", "language": "python", "code": "class ConfigAttribute(t.Generic[T]):\n    \"\"\"Makes an attribute forward to the config\"\"\"\n\n    def __init__(\n        self, name: str, get_converter: t.Callable[[t.Any], T] | None = None\n    ) -> None:\n        self.__name__ = name\n        self.get_converter = get_converter\n\n    @t.overload\n    def __get__(self, obj: None, owner: None) -> te.Self: ...\n\n    @t.overload\n    def __get__(self, obj: App, owner: type[App]) -> T: ...\n\n    def __get__(self, obj: App | None, owner: type[App] | None = None) -> T | te.Self:\n        if obj is None:\n            return self\n\n        rv = obj.config[self.__name__]\n\n        if self.get_converter is not None:\n            rv = self.get_converter(rv)\n\n        return rv  # type: ignore[no-any-return]\n\n    def __set__(self, obj: App, value: t.Any) -> None:\n        obj.config[self.__name__] = value", "metadata": {"license": "BSD-3-Clause", "len_tokens": 231}}
{"id": "flask:src/flask/config.py", "language": "python", "code": "def from_envvar(self, variable_name: str, silent: bool = False) -> bool:\n        \"\"\"Loads a configuration from an environment variable pointing to\n        a configuration file.  This is basically just a shortcut with nicer\n        error messages for this line of code::\n\n            app.config.from_pyfile(os.environ['YOURAPPLICATION_SETTINGS'])\n\n        :param variable_name: name of the environment variable\n        :param silent: set to ``True`` if you want silent failure for missing\n                       files.\n        :return: ``True`` if the file was loaded successfully.\n        \"\"\"\n        rv = os.environ.get(variable_name)\n        if not rv:\n            if silent:\n                return False\n            raise RuntimeError(\n                f\"The environment variable {variable_name!r} is not set\"\n                \" and as such configuration could not be loaded. Set\"\n                \" this variable and make it point to a configuration\"\n                \" file\"\n            )\n        return self.from_pyfile(rv, silent=silent)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 203}}
{"id": "flask:src/flask/config.py", "language": "python", "code": "def from_prefixed_env(\n        self, prefix: str = \"FLASK\", *, loads: t.Callable[[str], t.Any] = json.loads\n    ) -> bool:\n        \"\"\"Load any environment variables that start with ``FLASK_``,\n        dropping the prefix from the env key for the config key. Values\n        are passed through a loading function to attempt to convert them\n        to more specific types than strings.\n\n        Keys are loaded in :func:`sorted` order.\n\n        The default loading function attempts to parse values as any\n        valid JSON type, including dicts and lists.\n\n        Specific items in nested dicts can be set by separating the\n        keys with double underscores (``__``). If an intermediate key\n        doesn't exist, it will be initialized to an empty dict.\n\n        :param prefix: Load env vars that start with this prefix,\n            separated with an underscore (``_``).\n        :param loads: Pass each string value to this function and use\n            the returned value as the config value. If any error is\n            raised it is ignored and the value remains a string. The\n            default is :func:`json.loads`.\n\n        .. versionadded:: 2.1\n        \"\"\"\n        prefix = f\"{prefix}_\"\n\n        for key in sorted(os.environ):\n            if not key.startswith(prefix):\n                continue\n\n            value = os.environ[key]\n            key = key.removeprefix(prefix)\n\n            try:\n                value = loads(value)\n            except Exception:\n                # Keep the value as a string if loading failed.\n                pass\n\n            if \"__\" not in key:\n                # A non-nested key, set directly.\n                self[key] = value\n                continue\n\n            # Traverse nested dictionaries with keys separated by \"__\".\n            current = self\n            *parts, tail = key.split(\"__\")\n\n            for part in parts:\n                # If an intermediate dict does not exist, create it.\n                if part not in current:\n                    current[part] = {}\n\n                current = current[part]\n\n            current[tail] = value\n\n        return True", "metadata": {"license": "BSD-3-Clause", "len_tokens": 431}}
{"id": "flask:src/flask/config.py", "language": "python", "code": "def from_pyfile(\n        self, filename: str | os.PathLike[str], silent: bool = False\n    ) -> bool:\n        \"\"\"Updates the values in the config from a Python file.  This function\n        behaves as if the file was imported as module with the\n        :meth:`from_object` function.\n\n        :param filename: the filename of the config.  This can either be an\n                         absolute filename or a filename relative to the\n                         root path.\n        :param silent: set to ``True`` if you want silent failure for missing\n                       files.\n        :return: ``True`` if the file was loaded successfully.\n\n        .. versionadded:: 0.7\n           `silent` parameter.\n        \"\"\"\n        filename = os.path.join(self.root_path, filename)\n        d = types.ModuleType(\"config\")\n        d.__file__ = filename\n        try:\n            with open(filename, mode=\"rb\") as config_file:\n                exec(compile(config_file.read(), filename, \"exec\"), d.__dict__)\n        except OSError as e:\n            if silent and e.errno in (errno.ENOENT, errno.EISDIR, errno.ENOTDIR):\n                return False\n            e.strerror = f\"Unable to load configuration file ({e.strerror})\"\n            raise\n        self.from_object(d)\n        return True", "metadata": {"license": "BSD-3-Clause", "len_tokens": 279}}
{"id": "flask:src/flask/config.py", "language": "python", "code": "def from_object(self, obj: object | str) -> None:\n        \"\"\"Updates the values from the given object.  An object can be of one\n        of the following two types:\n\n        -   a string: in this case the object with that name will be imported\n        -   an actual object reference: that object is used directly\n\n        Objects are usually either modules or classes. :meth:`from_object`\n        loads only the uppercase attributes of the module/class. A ``dict``\n        object will not work with :meth:`from_object` because the keys of a\n        ``dict`` are not attributes of the ``dict`` class.\n\n        Example of module-based configuration::\n\n            app.config.from_object('yourapplication.default_config')\n            from yourapplication import default_config\n            app.config.from_object(default_config)\n\n        Nothing is done to the object before loading. If the object is a\n        class and has ``@property`` attributes, it needs to be\n        instantiated before being passed to this method.\n\n        You should not use this function to load the actual configuration but\n        rather configuration defaults.  The actual config should be loaded\n        with :meth:`from_pyfile` and ideally from a location not within the\n        package because the package might be installed system wide.\n\n        See :ref:`config-dev-prod` for an example of class-based configuration\n        using :meth:`from_object`.\n\n        :param obj: an import name or object\n        \"\"\"\n        if isinstance(obj, str):\n            obj = import_string(obj)\n        for key in dir(obj):\n            if key.isupper():\n                self[key] = getattr(obj, key)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 339}}
{"id": "flask:src/flask/config.py", "language": "python", "code": "def from_file(\n        self,\n        filename: str | os.PathLike[str],\n        load: t.Callable[[t.IO[t.Any]], t.Mapping[str, t.Any]],\n        silent: bool = False,\n        text: bool = True,\n    ) -> bool:\n        \"\"\"Update the values in the config from a file that is loaded\n        using the ``load`` parameter. The loaded data is passed to the\n        :meth:`from_mapping` method.\n\n        .. code-block:: python\n\n            import json\n            app.config.from_file(\"config.json\", load=json.load)\n\n            import tomllib\n            app.config.from_file(\"config.toml\", load=tomllib.load, text=False)\n\n        :param filename: The path to the data file. This can be an\n            absolute path or relative to the config root path.\n        :param load: A callable that takes a file handle and returns a\n            mapping of loaded data from the file.\n        :type load: ``Callable[[Reader], Mapping]`` where ``Reader``\n            implements a ``read`` method.\n        :param silent: Ignore the file if it doesn't exist.\n        :param text: Open the file in text or binary mode.\n        :return: ``True`` if the file was loaded successfully.\n\n        .. versionchanged:: 2.3\n            The ``text`` parameter was added.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        filename = os.path.join(self.root_path, filename)\n\n        try:\n            with open(filename, \"r\" if text else \"rb\") as f:\n                obj = load(f)\n        except OSError as e:\n            if silent and e.errno in (errno.ENOENT, errno.EISDIR):\n                return False\n\n            e.strerror = f\"Unable to load configuration file ({e.strerror})\"\n            raise\n\n        return self.from_mapping(obj)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 390}}
{"id": "flask:src/flask/config.py", "language": "python", "code": "def get_namespace(\n        self, namespace: str, lowercase: bool = True, trim_namespace: bool = True\n    ) -> dict[str, t.Any]:\n        \"\"\"Returns a dictionary containing a subset of configuration options\n        that match the specified namespace/prefix. Example usage::\n\n            app.config['IMAGE_STORE_TYPE'] = 'fs'\n            app.config['IMAGE_STORE_PATH'] = '/var/app/images'\n            app.config['IMAGE_STORE_BASE_URL'] = 'http://img.website.com'\n            image_store_config = app.config.get_namespace('IMAGE_STORE_')\n\n        The resulting dictionary `image_store_config` would look like::\n\n            {\n                'type': 'fs',\n                'path': '/var/app/images',\n                'base_url': 'http://img.website.com'\n            }\n\n        This is often useful when configuration options map directly to\n        keyword arguments in functions or class constructors.\n\n        :param namespace: a configuration namespace\n        :param lowercase: a flag indicating if the keys of the resulting\n                          dictionary should be lowercase\n        :param trim_namespace: a flag indicating if the keys of the resulting\n                          dictionary should not include the namespace\n\n        .. versionadded:: 0.11\n        \"\"\"\n        rv = {}\n        for k, v in self.items():\n            if not k.startswith(namespace):\n                continue\n            if trim_namespace:\n                key = k[len(namespace) :]\n            else:\n                key = k\n            if lowercase:\n                key = key.lower()\n            rv[key] = v\n        return rv", "metadata": {"license": "BSD-3-Clause", "len_tokens": 310}}
{"id": "flask:src/flask/templating.py", "language": "python", "code": "class DispatchingJinjaLoader(BaseLoader):\n    \"\"\"A loader that looks for templates in the application and all\n    the blueprint folders.\n    \"\"\"\n\n    def __init__(self, app: App) -> None:\n        self.app = app\n\n    def get_source(\n        self, environment: BaseEnvironment, template: str\n    ) -> tuple[str, str | None, t.Callable[[], bool] | None]:\n        if self.app.config[\"EXPLAIN_TEMPLATE_LOADING\"]:\n            return self._get_source_explained(environment, template)\n        return self._get_source_fast(environment, template)\n\n    def _get_source_explained(\n        self, environment: BaseEnvironment, template: str\n    ) -> tuple[str, str | None, t.Callable[[], bool] | None]:\n        attempts = []\n        rv: tuple[str, str | None, t.Callable[[], bool] | None] | None\n        trv: None | (tuple[str, str | None, t.Callable[[], bool] | None]) = None\n\n        for srcobj, loader in self._iter_loaders(template):\n            try:\n                rv = loader.get_source(environment, template)\n                if trv is None:\n                    trv = rv\n            except TemplateNotFound:\n                rv = None\n            attempts.append((loader, srcobj, rv))\n\n        from .debughelpers import explain_template_loading_attempts\n\n        explain_template_loading_attempts(self.app, template, attempts)\n\n        if trv is not None:\n            return trv\n        raise TemplateNotFound(template)\n\n    def _get_source_fast(\n        self, environment: BaseEnvironment, template: str\n    ) -> tuple[str, str | None, t.Callable[[], bool] | None]:\n        for _srcobj, loader in self._iter_loaders(template):\n            try:\n                return loader.get_source(environment, template)\n            except TemplateNotFound:\n                continue\n        raise TemplateNotFound(template)\n\n    def _iter_loaders(self, template: str) -> t.Iterator[tuple[Scaffold, BaseLoader]]:\n        loader = self.app.jinja_loader\n        if loader is not None:\n            yield self.app, loader\n\n        for blueprint in self.app.iter_blueprints():\n            loader = blueprint.jinja_loader\n            if loader is not None:\n                yield blueprint, loader\n\n    def list_templates(self) -> list[str]:\n        result = set()\n        loader = self.app.jinja_loader\n        if loader is not None:\n            result.update(loader.list_templates())\n\n        for blueprint in self.app.iter_blueprints():\n            loader = blueprint.jinja_loader\n            if loader is not None:\n                for template in loader.list_templates():\n                    result.add(template)\n\n        return list(result)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 566}}
{"id": "flask:src/flask/templating.py", "language": "python", "code": "def _get_source_explained(\n        self, environment: BaseEnvironment, template: str\n    ) -> tuple[str, str | None, t.Callable[[], bool] | None]:\n        attempts = []\n        rv: tuple[str, str | None, t.Callable[[], bool] | None] | None\n        trv: None | (tuple[str, str | None, t.Callable[[], bool] | None]) = None\n\n        for srcobj, loader in self._iter_loaders(template):\n            try:\n                rv = loader.get_source(environment, template)\n                if trv is None:\n                    trv = rv\n            except TemplateNotFound:\n                rv = None\n            attempts.append((loader, srcobj, rv))\n\n        from .debughelpers import explain_template_loading_attempts\n\n        explain_template_loading_attempts(self.app, template, attempts)\n\n        if trv is not None:\n            return trv\n        raise TemplateNotFound(template)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 201}}
{"id": "flask:src/flask/globals.py", "language": "python", "code": "from __future__ import annotations\n\nimport typing as t\nfrom contextvars import ContextVar\n\nfrom werkzeug.local import LocalProxy\n\nif t.TYPE_CHECKING:  # pragma: no cover\n    from .app import Flask\n    from .ctx import _AppCtxGlobals\n    from .ctx import AppContext\n    from .sessions import SessionMixin\n    from .wrappers import Request\n\n    T = t.TypeVar(\"T\", covariant=True)\n\n    class ProxyMixin(t.Protocol[T]):\n        def _get_current_object(self) -> T: ...\n\n    # These subclasses inform type checkers that the proxy objects look like the\n    # proxied type along with the _get_current_object method.\n    class FlaskProxy(ProxyMixin[Flask], Flask): ...\n\n    class AppContextProxy(ProxyMixin[AppContext], AppContext): ...\n\n    class _AppCtxGlobalsProxy(ProxyMixin[_AppCtxGlobals], _AppCtxGlobals): ...\n\n    class RequestProxy(ProxyMixin[Request], Request): ...\n\n    class SessionMixinProxy(ProxyMixin[SessionMixin], SessionMixin): ...\n\n\n_no_app_msg = \"\"\"\\\nWorking outside of application context.\n\nAttempted to use functionality that expected a current application to be set. To\nsolve this, set up an app context using 'with app.app_context()'. See the\ndocumentation on app context for more information.\\\n\"\"\"\n_cv_app: ContextVar[AppContext] = ContextVar(\"flask.app_ctx\")\napp_ctx: AppContextProxy = LocalProxy(  # type: ignore[assignment]\n    _cv_app, unbound_message=_no_app_msg\n)\ncurrent_app: FlaskProxy = LocalProxy(  # type: ignore[assignment]\n    _cv_app, \"app\", unbound_message=_no_app_msg\n)\ng: _AppCtxGlobalsProxy = LocalProxy(  # type: ignore[assignment]\n    _cv_app, \"g\", unbound_message=_no_app_msg\n)\n\n_no_req_msg = \"\"\"\\\nWorking outside of request context.\n\nAttempted to use functionality that expected an active HTTP request. See the\ndocumentation on request context for more information.\\\n\"\"\"\nrequest: RequestProxy = LocalProxy(  # type: ignore[assignment]\n    _cv_app, \"request\", unbound_message=_no_req_msg\n)\nsession: SessionMixinProxy = LocalProxy(  # type: ignore[assignment]\n    _cv_app, \"session\", unbound_message=_no_req_msg\n)\n\n\ndef __getattr__(name: str) -> t.Any:\n    import warnings\n\n    if name == \"request_ctx\":\n        warnings.warn(\n            \"'request_ctx' has merged with 'app_ctx', and will be removed\"\n            \" in Flask 4.0. Use 'app_ctx' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return app_ctx\n\n    raise AttributeError(name)\n", "metadata": {"license": "BSD-3-Clause", "len_tokens": 592}}
{"id": "flask:src/flask/__init__.py", "language": "python", "code": "from . import json as json\nfrom .app import Flask as Flask\nfrom .blueprints import Blueprint as Blueprint\nfrom .config import Config as Config\nfrom .ctx import after_this_request as after_this_request\nfrom .ctx import copy_current_request_context as copy_current_request_context\nfrom .ctx import has_app_context as has_app_context\nfrom .ctx import has_request_context as has_request_context\nfrom .globals import current_app as current_app\nfrom .globals import g as g\nfrom .globals import request as request\nfrom .globals import session as session\nfrom .helpers import abort as abort\nfrom .helpers import flash as flash\nfrom .helpers import get_flashed_messages as get_flashed_messages\nfrom .helpers import get_template_attribute as get_template_attribute\nfrom .helpers import make_response as make_response\nfrom .helpers import redirect as redirect\nfrom .helpers import send_file as send_file\nfrom .helpers import send_from_directory as send_from_directory\nfrom .helpers import stream_with_context as stream_with_context\nfrom .helpers import url_for as url_for\nfrom .json import jsonify as jsonify\nfrom .signals import appcontext_popped as appcontext_popped\nfrom .signals import appcontext_pushed as appcontext_pushed\nfrom .signals import appcontext_tearing_down as appcontext_tearing_down\nfrom .signals import before_render_template as before_render_template\nfrom .signals import got_request_exception as got_request_exception\nfrom .signals import message_flashed as message_flashed\nfrom .signals import request_finished as request_finished\nfrom .signals import request_started as request_started\nfrom .signals import request_tearing_down as request_tearing_down\nfrom .signals import template_rendered as template_rendered\nfrom .templating import render_template as render_template\nfrom .templating import render_template_string as render_template_string\nfrom .templating import stream_template as stream_template\nfrom .templating import stream_template_string as stream_template_string\nfrom .wrappers import Request as Request\nfrom .wrappers import Response as Response\n", "metadata": {"license": "BSD-3-Clause", "len_tokens": 424}}
{"id": "flask:src/flask/blueprints.py", "language": "python", "code": "def __init__(\n        self,\n        name: str,\n        import_name: str,\n        static_folder: str | os.PathLike[str] | None = None,\n        static_url_path: str | None = None,\n        template_folder: str | os.PathLike[str] | None = None,\n        url_prefix: str | None = None,\n        subdomain: str | None = None,\n        url_defaults: dict[str, t.Any] | None = None,\n        root_path: str | None = None,\n        cli_group: str | None = _sentinel,  # type: ignore\n    ) -> None:\n        super().__init__(\n            name,\n            import_name,\n            static_folder,\n            static_url_path,\n            template_folder,\n            url_prefix,\n            subdomain,\n            url_defaults,\n            root_path,\n            cli_group,\n        )\n\n        #: The Click command group for registering CLI commands for this\n        #: object. The commands are available from the ``flask`` command\n        #: once the application has been discovered and blueprints have\n        #: been registered.\n        self.cli = AppGroup()\n\n        # Set the name of the Click group in case someone wants to add\n        # the app's commands to another CLI tool.\n        self.cli.name = self.name", "metadata": {"license": "BSD-3-Clause", "len_tokens": 264}}
{"id": "flask:src/flask/blueprints.py", "language": "python", "code": "def get_send_file_max_age(self, filename: str | None) -> int | None:\n        \"\"\"Used by :func:`send_file` to determine the ``max_age`` cache\n        value for a given file path if it wasn't passed.\n\n        By default, this returns :data:`SEND_FILE_MAX_AGE_DEFAULT` from\n        the configuration of :data:`~flask.current_app`. This defaults\n        to ``None``, which tells the browser to use conditional requests\n        instead of a timed cache, which is usually preferable.\n\n        Note this is a duplicate of the same method in the Flask\n        class.\n\n        .. versionchanged:: 2.0\n            The default configuration is ``None`` instead of 12 hours.\n\n        .. versionadded:: 0.9\n        \"\"\"\n        value = current_app.config[\"SEND_FILE_MAX_AGE_DEFAULT\"]\n\n        if value is None:\n            return None\n\n        if isinstance(value, timedelta):\n            return int(value.total_seconds())\n\n        return value", "metadata": {"license": "BSD-3-Clause", "len_tokens": 205}}
{"id": "flask:src/flask/blueprints.py", "language": "python", "code": "def open_resource(\n        self, resource: str, mode: str = \"rb\", encoding: str | None = \"utf-8\"\n    ) -> t.IO[t.AnyStr]:\n        \"\"\"Open a resource file relative to :attr:`root_path` for reading. The\n        blueprint-relative equivalent of the app's :meth:`~.Flask.open_resource`\n        method.\n\n        :param resource: Path to the resource relative to :attr:`root_path`.\n        :param mode: Open the file in this mode. Only reading is supported,\n            valid values are ``\"r\"`` (or ``\"rt\"``) and ``\"rb\"``.\n        :param encoding: Open the file with this encoding when opening in text\n            mode. This is ignored when opening in binary mode.\n\n        .. versionchanged:: 3.1\n            Added the ``encoding`` parameter.\n        \"\"\"\n        if mode not in {\"r\", \"rt\", \"rb\"}:\n            raise ValueError(\"Resources can only be opened for reading.\")\n\n        path = os.path.join(self.root_path, resource)\n\n        if mode == \"rb\":\n            return open(path, mode)  # pyright: ignore\n\n        return open(path, mode, encoding=encoding)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 256}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "def find_best_app(module: ModuleType) -> Flask:\n    \"\"\"Given a module instance this tries to find the best possible\n    application in the module or raises an exception.\n    \"\"\"\n    from . import Flask\n\n    # Search for the most common names first.\n    for attr_name in (\"app\", \"application\"):\n        app = getattr(module, attr_name, None)\n\n        if isinstance(app, Flask):\n            return app\n\n    # Otherwise find the only object that is a Flask instance.\n    matches = [v for v in module.__dict__.values() if isinstance(v, Flask)]\n\n    if len(matches) == 1:\n        return matches[0]\n    elif len(matches) > 1:\n        raise NoAppException(\n            \"Detected multiple Flask applications in module\"\n            f\" '{module.__name__}'. Use '{module.__name__}:name'\"\n            \" to specify the correct one.\"\n        )\n\n    # Search for app factory functions.\n    for attr_name in (\"create_app\", \"make_app\"):\n        app_factory = getattr(module, attr_name, None)\n\n        if inspect.isfunction(app_factory):\n            try:\n                app = app_factory()\n\n                if isinstance(app, Flask):\n                    return app\n            except TypeError as e:\n                if not _called_with_wrong_args(app_factory):\n                    raise\n\n                raise NoAppException(\n                    f\"Detected factory '{attr_name}' in module '{module.__name__}',\"\n                    \" but could not call it without arguments. Use\"\n                    f\" '{module.__name__}:{attr_name}(args)'\"\n                    \" to specify arguments.\"\n                ) from e\n\n    raise NoAppException(\n        \"Failed to find Flask application or factory in module\"\n        f\" '{module.__name__}'. Use '{module.__name__}:name'\"\n        \" to specify one.\"\n    )", "metadata": {"license": "BSD-3-Clause", "len_tokens": 375}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "def find_app_by_string(module: ModuleType, app_name: str) -> Flask:\n    \"\"\"Check if the given string is a variable name or a function. Call\n    a function to get the app instance, or return the variable directly.\n    \"\"\"\n    from . import Flask\n\n    # Parse app_name as a single expression to determine if it's a valid\n    # attribute name or function call.\n    try:\n        expr = ast.parse(app_name.strip(), mode=\"eval\").body\n    except SyntaxError:\n        raise NoAppException(\n            f\"Failed to parse {app_name!r} as an attribute name or function call.\"\n        ) from None\n\n    if isinstance(expr, ast.Name):\n        name = expr.id\n        args = []\n        kwargs = {}\n    elif isinstance(expr, ast.Call):\n        # Ensure the function name is an attribute name only.\n        if not isinstance(expr.func, ast.Name):\n            raise NoAppException(\n                f\"Function reference must be a simple name: {app_name!r}.\"\n            )\n\n        name = expr.func.id\n\n        # Parse the positional and keyword arguments as literals.\n        try:\n            args = [ast.literal_eval(arg) for arg in expr.args]\n            kwargs = {\n                kw.arg: ast.literal_eval(kw.value)\n                for kw in expr.keywords\n                if kw.arg is not None\n            }\n        except ValueError:\n            # literal_eval gives cryptic error messages, show a generic\n            # message with the full expression instead.\n            raise NoAppException(\n                f\"Failed to parse arguments as literal values: {app_name!r}.\"\n            ) from None\n    else:\n        raise NoAppException(\n            f\"Failed to parse {app_name!r} as an attribute name or function call.\"\n        )\n\n    try:\n        attr = getattr(module, name)\n    except AttributeError as e:\n        raise NoAppException(\n            f\"Failed to find attribute {name!r} in {module.__name__!r}.\"\n        ) from e\n\n    # If the attribute is a function, call it with any args and kwargs\n    # to get the real application.\n    if inspect.isfunction(attr):\n        try:\n            app = attr(*args, **kwargs)\n        except TypeError as e:\n            if not _called_with_wrong_args(attr):\n                raise\n\n            raise NoAppException(\n                f\"The factory {app_name!r} in module\"\n                f\" {module.__name__!r} could not be called with the\"\n                \" specified arguments.\"\n            ) from e\n    else:\n        app = attr\n\n    if isinstance(app, Flask):\n        return app\n\n    raise NoAppException(\n        \"A valid Flask application was not obtained from\"\n        f\" '{module.__name__}:{app_name}'.\"\n    )", "metadata": {"license": "BSD-3-Clause", "len_tokens": 585}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "def locate_app(\n    module_name: str, app_name: str | None, raise_if_not_found: bool = True\n) -> Flask | None:\n    try:\n        __import__(module_name)\n    except ImportError:\n        # Reraise the ImportError if it occurred within the imported module.\n        # Determine this by checking whether the trace has a depth > 1.\n        if sys.exc_info()[2].tb_next:  # type: ignore[union-attr]\n            raise NoAppException(\n                f\"While importing {module_name!r}, an ImportError was\"\n                f\" raised:\\n\\n{traceback.format_exc()}\"\n            ) from None\n        elif raise_if_not_found:\n            raise NoAppException(f\"Could not import {module_name!r}.\") from None\n        else:\n            return None\n\n    module = sys.modules[module_name]\n\n    if app_name is None:\n        return find_best_app(module)\n    else:\n        return find_app_by_string(module, app_name)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 211}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "class ScriptInfo:\n    \"\"\"Helper object to deal with Flask applications.  This is usually not\n    necessary to interface with as it's used internally in the dispatching\n    to click.  In future versions of Flask this object will most likely play\n    a bigger role.  Typically it's created automatically by the\n    :class:`FlaskGroup` but you can also manually create it and pass it\n    onwards as click object.\n\n    .. versionchanged:: 3.1\n        Added the ``load_dotenv_defaults`` parameter and attribute.\n    \"\"\"\n\n    def __init__(\n        self,\n        app_import_path: str | None = None,\n        create_app: t.Callable[..., Flask] | None = None,\n        set_debug_flag: bool = True,\n        load_dotenv_defaults: bool = True,\n    ) -> None:\n        #: Optionally the import path for the Flask application.\n        self.app_import_path = app_import_path\n        #: Optionally a function that is passed the script info to create\n        #: the instance of the application.\n        self.create_app = create_app\n        #: A dictionary with arbitrary data that can be associated with\n        #: this script info.\n        self.data: dict[t.Any, t.Any] = {}\n        self.set_debug_flag = set_debug_flag\n\n        self.load_dotenv_defaults = get_load_dotenv(load_dotenv_defaults)\n        \"\"\"Whether default ``.flaskenv`` and ``.env`` files should be loaded.\n\n        ``ScriptInfo`` doesn't load anything, this is for reference when doing\n        the load elsewhere during processing.\n\n        .. versionadded:: 3.1\n        \"\"\"\n\n        self._loaded_app: Flask | None = None\n\n    def load_app(self) -> Flask:\n        \"\"\"Loads the Flask app (if not yet loaded) and returns it.  Calling\n        this multiple times will just result in the already loaded app to\n        be returned.\n        \"\"\"\n        if self._loaded_app is not None:\n            return self._loaded_app\n        app: Flask | None = None\n        if self.create_app is not None:\n            app = self.create_app()\n        else:\n            if self.app_import_path:\n                path, name = (\n                    re.split(r\":(?![\\\\/])\", self.app_import_path, maxsplit=1) + [None]\n                )[:2]\n                import_name = prepare_import(path)\n                app = locate_app(import_name, name)\n            else:\n                for path in (\"wsgi.py\", \"app.py\"):\n                    import_name = prepare_import(path)\n                    app = locate_app(import_name, None, raise_if_not_found=False)\n\n                    if app is not None:\n                        break\n\n        if app is None:\n            raise NoAppException(\n                \"Could not locate a Flask application. Use the\"\n                \" 'flask --app' option, 'FLASK_APP' environment\"\n                \" variable, or a 'wsgi.py' or 'app.py' file in the\"\n                \" current directory.\"\n            )\n\n        if self.set_debug_flag:\n            # Update the app's debug flag through the descriptor so that\n            # other values repopulate as well.\n            app.debug = get_debug_flag()\n\n        self._loaded_app = app\n        return app", "metadata": {"license": "BSD-3-Clause", "len_tokens": 672}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "def with_appcontext(f: F) -> F:\n    \"\"\"Wraps a callback so that it's guaranteed to be executed with the\n    script's application context.\n\n    Custom commands (and their options) registered under ``app.cli`` or\n    ``blueprint.cli`` will always have an app context available, this\n    decorator is not required in that case.\n\n    .. versionchanged:: 2.2\n        The app context is active for subcommands as well as the\n        decorated callback. The app context is always available to\n        ``app.cli`` command and parameter callbacks.\n    \"\"\"\n\n    @click.pass_context\n    def decorator(ctx: click.Context, /, *args: t.Any, **kwargs: t.Any) -> t.Any:\n        if not current_app:\n            app = ctx.ensure_object(ScriptInfo).load_app()\n            ctx.with_resource(app.app_context())\n\n        return ctx.invoke(f, *args, **kwargs)\n\n    return update_wrapper(decorator, f)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 202}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "class AppGroup(click.Group):\n    \"\"\"This works similar to a regular click :class:`~click.Group` but it\n    changes the behavior of the :meth:`command` decorator so that it\n    automatically wraps the functions in :func:`with_appcontext`.\n\n    Not to be confused with :class:`FlaskGroup`.\n    \"\"\"\n\n    def command(  # type: ignore[override]\n        self, *args: t.Any, **kwargs: t.Any\n    ) -> t.Callable[[t.Callable[..., t.Any]], click.Command]:\n        \"\"\"This works exactly like the method of the same name on a regular\n        :class:`click.Group` but it wraps callbacks in :func:`with_appcontext`\n        unless it's disabled by passing ``with_appcontext=False``.\n        \"\"\"\n        wrap_for_ctx = kwargs.pop(\"with_appcontext\", True)\n\n        def decorator(f: t.Callable[..., t.Any]) -> click.Command:\n            if wrap_for_ctx:\n                f = with_appcontext(f)\n            return super(AppGroup, self).command(*args, **kwargs)(f)  # type: ignore[no-any-return]\n\n        return decorator\n\n    def group(  # type: ignore[override]\n        self, *args: t.Any, **kwargs: t.Any\n    ) -> t.Callable[[t.Callable[..., t.Any]], click.Group]:\n        \"\"\"This works exactly like the method of the same name on a regular\n        :class:`click.Group` but it defaults the group class to\n        :class:`AppGroup`.\n        \"\"\"\n        kwargs.setdefault(\"cls\", AppGroup)\n        return super().group(*args, **kwargs)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 342}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "def load_dotenv(\n    path: str | os.PathLike[str] | None = None, load_defaults: bool = True\n) -> bool:\n    \"\"\"Load \"dotenv\" files to set environment variables. A given path takes\n    precedence over ``.env``, which takes precedence over ``.flaskenv``. After\n    loading and combining these files, values are only set if the key is not\n    already set in ``os.environ``.\n\n    This is a no-op if `python-dotenv`_ is not installed.\n\n    .. _python-dotenv: https://github.com/theskumar/python-dotenv#readme\n\n    :param path: Load the file at this location.\n    :param load_defaults: Search for and load the default ``.flaskenv`` and\n        ``.env`` files.\n    :return: ``True`` if at least one env var was loaded.\n\n    .. versionchanged:: 3.1\n        Added the ``load_defaults`` parameter. A given path takes precedence\n        over default files.\n\n    .. versionchanged:: 2.0\n        The current directory is not changed to the location of the\n        loaded file.\n\n    .. versionchanged:: 2.0\n        When loading the env files, set the default encoding to UTF-8.\n\n    .. versionchanged:: 1.1.0\n        Returns ``False`` when python-dotenv is not installed, or when\n        the given path isn't a file.\n\n    .. versionadded:: 1.0\n    \"\"\"\n    try:\n        import dotenv\n    except ImportError:\n        if path or os.path.isfile(\".env\") or os.path.isfile(\".flaskenv\"):\n            click.secho(\n                \" * Tip: There are .env files present. Install python-dotenv\"\n                \" to use them.\",\n                fg=\"yellow\",\n                err=True,\n            )\n\n        return False\n\n    data: dict[str, str | None] = {}\n\n    if load_defaults:\n        for default_name in (\".flaskenv\", \".env\"):\n            if not (default_path := dotenv.find_dotenv(default_name, usecwd=True)):\n                continue\n\n            data |= dotenv.dotenv_values(default_path, encoding=\"utf-8\")\n\n    if path is not None and os.path.isfile(path):\n        data |= dotenv.dotenv_values(path, encoding=\"utf-8\")\n\n    for key, value in data.items():\n        if key in os.environ or value is None:\n            continue\n\n        os.environ[key] = value\n\n    return bool(data)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 523}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "class CertParamType(click.ParamType):\n    \"\"\"Click option type for the ``--cert`` option. Allows either an\n    existing file, the string ``'adhoc'``, or an import for a\n    :class:`~ssl.SSLContext` object.\n    \"\"\"\n\n    name = \"path\"\n\n    def __init__(self) -> None:\n        self.path_type = click.Path(exists=True, dir_okay=False, resolve_path=True)\n\n    def convert(\n        self, value: t.Any, param: click.Parameter | None, ctx: click.Context | None\n    ) -> t.Any:\n        try:\n            import ssl\n        except ImportError:\n            raise click.BadParameter(\n                'Using \"--cert\" requires Python to be compiled with SSL support.',\n                ctx,\n                param,\n            ) from None\n\n        try:\n            return self.path_type(value, param, ctx)\n        except click.BadParameter:\n            value = click.STRING(value, param, ctx).lower()\n\n            if value == \"adhoc\":\n                try:\n                    import cryptography  # noqa: F401\n                except ImportError:\n                    raise click.BadParameter(\n                        \"Using ad-hoc certificates requires the cryptography library.\",\n                        ctx,\n                        param,\n                    ) from None\n\n                return value\n\n            obj = import_string(value, silent=True)\n\n            if isinstance(obj, ssl.SSLContext):\n                return obj\n\n            raise", "metadata": {"license": "BSD-3-Clause", "len_tokens": 290}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "def _validate_key(ctx: click.Context, param: click.Parameter, value: t.Any) -> t.Any:\n    \"\"\"The ``--key`` option must be specified when ``--cert`` is a file.\n    Modifies the ``cert`` param to be a ``(cert, key)`` pair if needed.\n    \"\"\"\n    cert = ctx.params.get(\"cert\")\n    is_adhoc = cert == \"adhoc\"\n\n    try:\n        import ssl\n    except ImportError:\n        is_context = False\n    else:\n        is_context = isinstance(cert, ssl.SSLContext)\n\n    if value is not None:\n        if is_adhoc:\n            raise click.BadParameter(\n                'When \"--cert\" is \"adhoc\", \"--key\" is not used.', ctx, param\n            )\n\n        if is_context:\n            raise click.BadParameter(\n                'When \"--cert\" is an SSLContext object, \"--key\" is not used.',\n                ctx,\n                param,\n            )\n\n        if not cert:\n            raise click.BadParameter('\"--cert\" must also be specified.', ctx, param)\n\n        ctx.params[\"cert\"] = cert, value\n\n    else:\n        if cert and not (is_adhoc or is_context):\n            raise click.BadParameter('Required when using \"--cert\".', ctx, param)\n\n    return value", "metadata": {"license": "BSD-3-Clause", "len_tokens": 274}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "def run_command(\n    info: ScriptInfo,\n    host: str,\n    port: int,\n    reload: bool,\n    debugger: bool,\n    with_threads: bool,\n    cert: ssl.SSLContext | tuple[str, str | None] | t.Literal[\"adhoc\"] | None,\n    extra_files: list[str] | None,\n    exclude_patterns: list[str] | None,\n) -> None:\n    \"\"\"Run a local development server.\n\n    This server is for development purposes only. It does not provide\n    the stability, security, or performance of production WSGI servers.\n\n    The reloader and debugger are enabled by default with the '--debug'\n    option.\n    \"\"\"\n    try:\n        app: WSGIApplication = info.load_app()  # pyright: ignore\n    except Exception as e:\n        if is_running_from_reloader():\n            # When reloading, print out the error immediately, but raise\n            # it later so the debugger or server can handle it.\n            traceback.print_exc()\n            err = e\n\n            def app(\n                environ: WSGIEnvironment, start_response: StartResponse\n            ) -> cabc.Iterable[bytes]:\n                raise err from None\n\n        else:\n            # When not reloading, raise the error immediately so the\n            # command fails.\n            raise e from None\n\n    debug = get_debug_flag()\n\n    if reload is None:\n        reload = debug\n\n    if debugger is None:\n        debugger = debug\n\n    show_server_banner(debug, info.app_import_path)\n\n    run_simple(\n        host,\n        port,\n        app,\n        use_reloader=reload,\n        use_debugger=debugger,\n        threaded=with_threads,\n        ssl_context=cert,\n        extra_files=extra_files,\n        exclude_patterns=exclude_patterns,\n    )", "metadata": {"license": "BSD-3-Clause", "len_tokens": 375}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "def shell_command() -> None:\n    \"\"\"Run an interactive Python shell in the context of a given\n    Flask application.  The application will populate the default\n    namespace of this shell according to its configuration.\n\n    This is useful for executing small snippets of management code\n    without having to manually configure the application.\n    \"\"\"\n    import code\n\n    banner = (\n        f\"Python {sys.version} on {sys.platform}\\n\"\n        f\"App: {current_app.import_name}\\n\"\n        f\"Instance: {current_app.instance_path}\"\n    )\n    ctx: dict[str, t.Any] = {}\n\n    # Support the regular Python interpreter startup script if someone\n    # is using it.\n    startup = os.environ.get(\"PYTHONSTARTUP\")\n    if startup and os.path.isfile(startup):\n        with open(startup) as f:\n            eval(compile(f.read(), startup, \"exec\"), ctx)\n\n    ctx.update(current_app.make_shell_context())\n\n    # Site, customize, or startup script can set a hook to call when\n    # entering interactive mode. The default one sets up readline with\n    # tab and history completion.\n    interactive_hook = getattr(sys, \"__interactivehook__\", None)\n\n    if interactive_hook is not None:\n        try:\n            import readline\n            from rlcompleter import Completer\n        except ImportError:\n            pass\n        else:\n            # rlcompleter uses __main__.__dict__ by default, which is\n            # flask.__main__. Use the shell context instead.\n            readline.set_completer(Completer(ctx).complete)\n\n        interactive_hook()\n\n    code.interact(banner=banner, local=ctx)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 345}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "def routes_command(sort: str, all_methods: bool) -> None:\n    \"\"\"Show all registered routes with endpoints and methods.\"\"\"\n    rules = list(current_app.url_map.iter_rules())\n\n    if not rules:\n        click.echo(\"No routes were registered.\")\n        return\n\n    ignored_methods = set() if all_methods else {\"HEAD\", \"OPTIONS\"}\n    host_matching = current_app.url_map.host_matching\n    has_domain = any(rule.host if host_matching else rule.subdomain for rule in rules)\n    rows = []\n\n    for rule in rules:\n        row = [\n            rule.endpoint,\n            \", \".join(sorted((rule.methods or set()) - ignored_methods)),\n        ]\n\n        if has_domain:\n            row.append((rule.host if host_matching else rule.subdomain) or \"\")\n\n        row.append(rule.rule)\n        rows.append(row)\n\n    headers = [\"Endpoint\", \"Methods\"]\n    sorts = [\"endpoint\", \"methods\"]\n\n    if has_domain:\n        headers.append(\"Host\" if host_matching else \"Subdomain\")\n        sorts.append(\"domain\")\n\n    headers.append(\"Rule\")\n    sorts.append(\"rule\")\n\n    try:\n        rows.sort(key=itemgetter(sorts.index(sort)))\n    except ValueError:\n        pass\n\n    rows.insert(0, headers)\n    widths = [max(len(row[i]) for row in rows) for i in range(len(headers))]\n    rows.insert(1, [\"-\" * w for w in widths])\n    template = \"  \".join(f\"{{{i}:<{w}}}\" for i, w in enumerate(widths))\n\n    for row in rows:\n        click.echo(template.format(*row))", "metadata": {"license": "BSD-3-Clause", "len_tokens": 329}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "def __init__(\n        self,\n        app_import_path: str | None = None,\n        create_app: t.Callable[..., Flask] | None = None,\n        set_debug_flag: bool = True,\n        load_dotenv_defaults: bool = True,\n    ) -> None:\n        #: Optionally the import path for the Flask application.\n        self.app_import_path = app_import_path\n        #: Optionally a function that is passed the script info to create\n        #: the instance of the application.\n        self.create_app = create_app\n        #: A dictionary with arbitrary data that can be associated with\n        #: this script info.\n        self.data: dict[t.Any, t.Any] = {}\n        self.set_debug_flag = set_debug_flag\n\n        self.load_dotenv_defaults = get_load_dotenv(load_dotenv_defaults)\n        \"\"\"Whether default ``.flaskenv`` and ``.env`` files should be loaded.\n\n        ``ScriptInfo`` doesn't load anything, this is for reference when doing\n        the load elsewhere during processing.\n\n        .. versionadded:: 3.1\n        \"\"\"\n\n        self._loaded_app: Flask | None = None", "metadata": {"license": "BSD-3-Clause", "len_tokens": 234}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "def load_app(self) -> Flask:\n        \"\"\"Loads the Flask app (if not yet loaded) and returns it.  Calling\n        this multiple times will just result in the already loaded app to\n        be returned.\n        \"\"\"\n        if self._loaded_app is not None:\n            return self._loaded_app\n        app: Flask | None = None\n        if self.create_app is not None:\n            app = self.create_app()\n        else:\n            if self.app_import_path:\n                path, name = (\n                    re.split(r\":(?![\\\\/])\", self.app_import_path, maxsplit=1) + [None]\n                )[:2]\n                import_name = prepare_import(path)\n                app = locate_app(import_name, name)\n            else:\n                for path in (\"wsgi.py\", \"app.py\"):\n                    import_name = prepare_import(path)\n                    app = locate_app(import_name, None, raise_if_not_found=False)\n\n                    if app is not None:\n                        break\n\n        if app is None:\n            raise NoAppException(\n                \"Could not locate a Flask application. Use the\"\n                \" 'flask --app' option, 'FLASK_APP' environment\"\n                \" variable, or a 'wsgi.py' or 'app.py' file in the\"\n                \" current directory.\"\n            )\n\n        if self.set_debug_flag:\n            # Update the app's debug flag through the descriptor so that\n            # other values repopulate as well.\n            app.debug = get_debug_flag()\n\n        self._loaded_app = app\n        return app", "metadata": {"license": "BSD-3-Clause", "len_tokens": 321}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "def __init__(\n        self,\n        add_default_commands: bool = True,\n        create_app: t.Callable[..., Flask] | None = None,\n        add_version_option: bool = True,\n        load_dotenv: bool = True,\n        set_debug_flag: bool = True,\n        **extra: t.Any,\n    ) -> None:\n        params: list[click.Parameter] = list(extra.pop(\"params\", None) or ())\n        # Processing is done with option callbacks instead of a group\n        # callback. This allows users to make a custom group callback\n        # without losing the behavior. --env-file must come first so\n        # that it is eagerly evaluated before --app.\n        params.extend((_env_file_option, _app_option, _debug_option))\n\n        if add_version_option:\n            params.append(version_option)\n\n        if \"context_settings\" not in extra:\n            extra[\"context_settings\"] = {}\n\n        extra[\"context_settings\"].setdefault(\"auto_envvar_prefix\", \"FLASK\")\n\n        super().__init__(params=params, **extra)\n\n        self.create_app = create_app\n        self.load_dotenv = load_dotenv\n        self.set_debug_flag = set_debug_flag\n\n        if add_default_commands:\n            self.add_command(run_command)\n            self.add_command(shell_command)\n            self.add_command(routes_command)\n\n        self._loaded_plugin_commands = False", "metadata": {"license": "BSD-3-Clause", "len_tokens": 280}}
{"id": "flask:src/flask/cli.py", "language": "python", "code": "def get_command(self, ctx: click.Context, name: str) -> click.Command | None:\n        self._load_plugin_commands()\n        # Look up built-in and plugin commands, which should be\n        # available even if the app fails to load.\n        rv = super().get_command(ctx, name)\n\n        if rv is not None:\n            return rv\n\n        info = ctx.ensure_object(ScriptInfo)\n\n        # Look up commands provided by the app, showing an error and\n        # continuing if the app couldn't be loaded.\n        try:\n            app = info.load_app()\n        except NoAppException as e:\n            click.secho(f\"Error: {e.format_message()}\\n\", err=True, fg=\"red\")\n            return None\n\n        # Push an app context for the loaded app unless it is already\n        # active somehow. This makes the context available to parameter\n        # and command callbacks without needing @with_appcontext.\n        if not current_app or current_app._get_current_object() is not app:\n            ctx.with_resource(app.app_context())\n\n        return app.cli.get_command(ctx, name)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 229}}
{"id": "flask:src/flask/wrappers.py", "language": "python", "code": "class Response(ResponseBase):\n    \"\"\"The response object that is used by default in Flask.  Works like the\n    response object from Werkzeug but is set to have an HTML mimetype by\n    default.  Quite often you don't have to create this object yourself because\n    :meth:`~flask.Flask.make_response` will take care of that for you.\n\n    If you want to replace the response object used you can subclass this and\n    set :attr:`~flask.Flask.response_class` to your subclass.\n\n    .. versionchanged:: 1.0\n        JSON support is added to the response, like the request. This is useful\n        when testing to get the test client response data as JSON.\n\n    .. versionchanged:: 1.0\n\n        Added :attr:`max_cookie_size`.\n    \"\"\"\n\n    default_mimetype: str | None = \"text/html\"\n\n    json_module = json\n\n    autocorrect_location_header = False\n\n    @property\n    def max_cookie_size(self) -> int:  # type: ignore\n        \"\"\"Read-only view of the :data:`MAX_COOKIE_SIZE` config key.\n\n        See :attr:`~werkzeug.wrappers.Response.max_cookie_size` in\n        Werkzeug's docs.\n        \"\"\"\n        if current_app:\n            return current_app.config[\"MAX_COOKIE_SIZE\"]  # type: ignore[no-any-return]\n\n        # return Werkzeug's default when not in an app context\n        return super().max_cookie_size", "metadata": {"license": "BSD-3-Clause", "len_tokens": 303}}
{"id": "flask:src/flask/wrappers.py", "language": "python", "code": "def max_content_length(self) -> int | None:\n        \"\"\"The maximum number of bytes that will be read during this request. If\n        this limit is exceeded, a 413 :exc:`~werkzeug.exceptions.RequestEntityTooLarge`\n        error is raised. If it is set to ``None``, no limit is enforced at the\n        Flask application level. However, if it is ``None`` and the request has\n        no ``Content-Length`` header and the WSGI server does not indicate that\n        it terminates the stream, then no data is read to avoid an infinite\n        stream.\n\n        Each request defaults to the :data:`MAX_CONTENT_LENGTH` config, which\n        defaults to ``None``. It can be set on a specific ``request`` to apply\n        the limit to that specific view. This should be set appropriately based\n        on an application's or view's specific needs.\n\n        .. versionchanged:: 3.1\n            This can be set per-request.\n\n        .. versionchanged:: 0.6\n            This is configurable through Flask config.\n        \"\"\"\n        if self._max_content_length is not None:\n            return self._max_content_length\n\n        if not current_app:\n            return super().max_content_length\n\n        return current_app.config[\"MAX_CONTENT_LENGTH\"]", "metadata": {"license": "BSD-3-Clause", "len_tokens": 269}}
{"id": "flask:src/flask/wrappers.py", "language": "python", "code": "def max_form_memory_size(self) -> int | None:\n        \"\"\"The maximum size in bytes any non-file form field may be in a\n        ``multipart/form-data`` body. If this limit is exceeded, a 413\n        :exc:`~werkzeug.exceptions.RequestEntityTooLarge` error is raised. If it\n        is set to ``None``, no limit is enforced at the Flask application level.\n\n        Each request defaults to the :data:`MAX_FORM_MEMORY_SIZE` config, which\n        defaults to ``500_000``. It can be set on a specific ``request`` to\n        apply the limit to that specific view. This should be set appropriately\n        based on an application's or view's specific needs.\n\n        .. versionchanged:: 3.1\n            This is configurable through Flask config.\n        \"\"\"\n        if self._max_form_memory_size is not None:\n            return self._max_form_memory_size\n\n        if not current_app:\n            return super().max_form_memory_size\n\n        return current_app.config[\"MAX_FORM_MEMORY_SIZE\"]", "metadata": {"license": "BSD-3-Clause", "len_tokens": 217}}
{"id": "flask:src/flask/wrappers.py", "language": "python", "code": "def max_form_parts(self) -> int | None:\n        \"\"\"The maximum number of fields that may be present in a\n        ``multipart/form-data`` body. If this limit is exceeded, a 413\n        :exc:`~werkzeug.exceptions.RequestEntityTooLarge` error is raised. If it\n        is set to ``None``, no limit is enforced at the Flask application level.\n\n        Each request defaults to the :data:`MAX_FORM_PARTS` config, which\n        defaults to ``1_000``. It can be set on a specific ``request`` to apply\n        the limit to that specific view. This should be set appropriately based\n        on an application's or view's specific needs.\n\n        .. versionchanged:: 3.1\n            This is configurable through Flask config.\n        \"\"\"\n        if self._max_form_parts is not None:\n            return self._max_form_parts\n\n        if not current_app:\n            return super().max_form_parts\n\n        return current_app.config[\"MAX_FORM_PARTS\"]", "metadata": {"license": "BSD-3-Clause", "len_tokens": 210}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def __init_subclass__(cls, **kwargs: t.Any) -> None:\n        import warnings\n\n        # These method signatures were updated to take a ctx param. Detect\n        # overridden methods in subclasses that still have the old signature.\n        # Show a deprecation warning and wrap to call with correct args.\n        for method in (\n            cls.handle_http_exception,\n            cls.handle_user_exception,\n            cls.handle_exception,\n            cls.log_exception,\n            cls.dispatch_request,\n            cls.full_dispatch_request,\n            cls.finalize_request,\n            cls.make_default_options_response,\n            cls.preprocess_request,\n            cls.process_response,\n            cls.do_teardown_request,\n            cls.do_teardown_appcontext,\n        ):\n            base_method = getattr(Flask, method.__name__)\n\n            if method is base_method:\n                # not overridden\n                continue\n\n            # get the second parameter (first is self)\n            iter_params = iter(inspect.signature(method).parameters.values())\n            next(iter_params)\n            param = next(iter_params, None)\n\n            # must have second parameter named ctx or annotated AppContext\n            if param is None or not (\n                # no annotation, match name\n                (param.annotation is inspect.Parameter.empty and param.name == \"ctx\")\n                or (\n                    # string annotation, access path ends with AppContext\n                    isinstance(param.annotation, str)\n                    and param.annotation.rpartition(\".\")[2] == \"AppContext\"\n                )\n                or (\n                    # class annotation\n                    inspect.isclass(param.annotation)\n                    and issubclass(param.annotation, AppContext)\n                )\n            ):\n                warnings.warn(\n                    f\"The '{method.__name__}' method now takes 'ctx: AppContext'\"\n                    \" as the first parameter. The old signature is deprecated\"\n                    \" and will not be supported in Flask 4.0.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n                setattr(cls, method.__name__, remove_ctx(method))\n                setattr(Flask, method.__name__, add_ctx(base_method))", "metadata": {"license": "BSD-3-Clause", "len_tokens": 412}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def __init__(\n        self,\n        import_name: str,\n        static_url_path: str | None = None,\n        static_folder: str | os.PathLike[str] | None = \"static\",\n        static_host: str | None = None,\n        host_matching: bool = False,\n        subdomain_matching: bool = False,\n        template_folder: str | os.PathLike[str] | None = \"templates\",\n        instance_path: str | None = None,\n        instance_relative_config: bool = False,\n        root_path: str | None = None,\n    ):\n        super().__init__(\n            import_name=import_name,\n            static_url_path=static_url_path,\n            static_folder=static_folder,\n            static_host=static_host,\n            host_matching=host_matching,\n            subdomain_matching=subdomain_matching,\n            template_folder=template_folder,\n            instance_path=instance_path,\n            instance_relative_config=instance_relative_config,\n            root_path=root_path,\n        )\n\n        #: The Click command group for registering CLI commands for this\n        #: object. The commands are available from the ``flask`` command\n        #: once the application has been discovered and blueprints have\n        #: been registered.\n        self.cli = cli.AppGroup()\n\n        # Set the name of the Click group in case someone wants to add\n        # the app's commands to another CLI tool.\n        self.cli.name = self.name\n\n        # Add a static route using the provided static_url_path, static_host,\n        # and static_folder if there is a configured static_folder.\n        # Note we do this without checking if static_folder exists.\n        # For one, it might be created while the server is running (e.g. during\n        # development). Also, Google App Engine stores static files somewhere\n        if self.has_static_folder:\n            assert bool(static_host) == host_matching, (\n                \"Invalid static_host/host_matching combination\"\n            )\n            # Use a weakref to avoid creating a reference cycle between the app\n            # and the view function (see #3761).\n            self_ref = weakref.ref(self)\n            self.add_url_rule(\n                f\"{self.static_url_path}/<path:filename>\",\n                endpoint=\"static\",\n                host=static_host,\n                view_func=lambda **kw: self_ref().send_static_file(**kw),  # type: ignore # noqa: B950\n            )", "metadata": {"license": "BSD-3-Clause", "len_tokens": 489}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def get_send_file_max_age(self, filename: str | None) -> int | None:\n        \"\"\"Used by :func:`send_file` to determine the ``max_age`` cache\n        value for a given file path if it wasn't passed.\n\n        By default, this returns :data:`SEND_FILE_MAX_AGE_DEFAULT` from\n        the configuration of :data:`~flask.current_app`. This defaults\n        to ``None``, which tells the browser to use conditional requests\n        instead of a timed cache, which is usually preferable.\n\n        Note this is a duplicate of the same method in the Flask\n        class.\n\n        .. versionchanged:: 2.0\n            The default configuration is ``None`` instead of 12 hours.\n\n        .. versionadded:: 0.9\n        \"\"\"\n        value = self.config[\"SEND_FILE_MAX_AGE_DEFAULT\"]\n\n        if value is None:\n            return None\n\n        if isinstance(value, timedelta):\n            return int(value.total_seconds())\n\n        return value", "metadata": {"license": "BSD-3-Clause", "len_tokens": 204}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def open_resource(\n        self, resource: str, mode: str = \"rb\", encoding: str | None = None\n    ) -> t.IO[t.AnyStr]:\n        \"\"\"Open a resource file relative to :attr:`root_path` for reading.\n\n        For example, if the file ``schema.sql`` is next to the file\n        ``app.py`` where the ``Flask`` app is defined, it can be opened\n        with:\n\n        .. code-block:: python\n\n            with app.open_resource(\"schema.sql\") as f:\n                conn.executescript(f.read())\n\n        :param resource: Path to the resource relative to :attr:`root_path`.\n        :param mode: Open the file in this mode. Only reading is supported,\n            valid values are ``\"r\"`` (or ``\"rt\"``) and ``\"rb\"``.\n        :param encoding: Open the file with this encoding when opening in text\n            mode. This is ignored when opening in binary mode.\n\n        .. versionchanged:: 3.1\n            Added the ``encoding`` parameter.\n        \"\"\"\n        if mode not in {\"r\", \"rt\", \"rb\"}:\n            raise ValueError(\"Resources can only be opened for reading.\")\n\n        path = os.path.join(self.root_path, resource)\n\n        if mode == \"rb\":\n            return open(path, mode)  # pyright: ignore\n\n        return open(path, mode, encoding=encoding)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 297}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def create_jinja_environment(self) -> Environment:\n        \"\"\"Create the Jinja environment based on :attr:`jinja_options`\n        and the various Jinja-related methods of the app. Changing\n        :attr:`jinja_options` after this will have no effect. Also adds\n        Flask-related globals and filters to the environment.\n\n        .. versionchanged:: 0.11\n           ``Environment.auto_reload`` set in accordance with\n           ``TEMPLATES_AUTO_RELOAD`` configuration option.\n\n        .. versionadded:: 0.5\n        \"\"\"\n        options = dict(self.jinja_options)\n\n        if \"autoescape\" not in options:\n            options[\"autoescape\"] = self.select_jinja_autoescape\n\n        if \"auto_reload\" not in options:\n            auto_reload = self.config[\"TEMPLATES_AUTO_RELOAD\"]\n\n            if auto_reload is None:\n                auto_reload = self.debug\n\n            options[\"auto_reload\"] = auto_reload\n\n        rv = self.jinja_environment(self, **options)\n        rv.globals.update(\n            url_for=self.url_for,\n            get_flashed_messages=get_flashed_messages,\n            config=self.config,\n            # request, session and g are normally added with the\n            # context processor for efficiency reasons but for imported\n            # templates we also want the proxies in there.\n            request=request,\n            session=session,\n            g=g,\n        )\n        rv.policies[\"json.dumps_function\"] = self.json.dumps\n        return rv", "metadata": {"license": "BSD-3-Clause", "len_tokens": 302}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def create_url_adapter(self, request: Request | None) -> MapAdapter | None:\n        \"\"\"Creates a URL adapter for the given request. The URL adapter\n        is created at a point where the request context is not yet set\n        up so the request is passed explicitly.\n\n        .. versionchanged:: 3.1\n            If :data:`SERVER_NAME` is set, it does not restrict requests to\n            only that domain, for both ``subdomain_matching`` and\n            ``host_matching``.\n\n        .. versionchanged:: 1.0\n            :data:`SERVER_NAME` no longer implicitly enables subdomain\n            matching. Use :attr:`subdomain_matching` instead.\n\n        .. versionchanged:: 0.9\n           This can be called outside a request when the URL adapter is created\n           for an application context.\n\n        .. versionadded:: 0.6\n        \"\"\"\n        if request is not None:\n            if (trusted_hosts := self.config[\"TRUSTED_HOSTS\"]) is not None:\n                request.trusted_hosts = trusted_hosts\n\n            # Check trusted_hosts here until bind_to_environ does.\n            request.host = get_host(request.environ, request.trusted_hosts)  # pyright: ignore\n            subdomain = None\n            server_name = self.config[\"SERVER_NAME\"]\n\n            if self.url_map.host_matching:\n                # Don't pass SERVER_NAME, otherwise it's used and the actual\n                # host is ignored, which breaks host matching.\n                server_name = None\n            elif not self.subdomain_matching:\n                # Werkzeug doesn't implement subdomain matching yet. Until then,\n                # disable it by forcing the current subdomain to the default, or\n                # the empty string.\n                subdomain = self.url_map.default_subdomain or \"\"\n\n            return self.url_map.bind_to_environ(\n                request.environ, server_name=server_name, subdomain=subdomain\n            )\n\n        # Need at least SERVER_NAME to match/build outside a request.\n        if self.config[\"SERVER_NAME\"] is not None:\n            return self.url_map.bind(\n                self.config[\"SERVER_NAME\"],\n                script_name=self.config[\"APPLICATION_ROOT\"],\n                url_scheme=self.config[\"PREFERRED_URL_SCHEME\"],\n            )\n\n        return None", "metadata": {"license": "BSD-3-Clause", "len_tokens": 456}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def update_template_context(\n        self, ctx: AppContext, context: dict[str, t.Any]\n    ) -> None:\n        \"\"\"Update the template context with some commonly used variables.\n        This injects request, session, config and g into the template\n        context as well as everything template context processors want\n        to inject.  Note that the as of Flask 0.6, the original values\n        in the context will not be overridden if a context processor\n        decides to return a value with the same key.\n\n        :param context: the context as a dictionary that is updated in place\n                        to add extra variables.\n        \"\"\"\n        names: t.Iterable[str | None] = (None,)\n\n        # A template may be rendered outside a request context.\n        if ctx.has_request:\n            names = chain(names, reversed(ctx.request.blueprints))\n\n        # The values passed to render_template take precedence. Keep a\n        # copy to re-apply after all context functions.\n        orig_ctx = context.copy()\n\n        for name in names:\n            if name in self.template_context_processors:\n                for func in self.template_context_processors[name]:\n                    context.update(self.ensure_sync(func)())\n\n        context.update(orig_ctx)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 250}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def test_client(self, use_cookies: bool = True, **kwargs: t.Any) -> FlaskClient:\n        \"\"\"Creates a test client for this application.  For information\n        about unit testing head over to :doc:`/testing`.\n\n        Note that if you are testing for assertions or exceptions in your\n        application code, you must set ``app.testing = True`` in order for the\n        exceptions to propagate to the test client.  Otherwise, the exception\n        will be handled by the application (not visible to the test client) and\n        the only indication of an AssertionError or other exception will be a\n        500 status code response to the test client.  See the :attr:`testing`\n        attribute.  For example::\n\n            app.testing = True\n            client = app.test_client()\n\n        The test client can be used in a ``with`` block to defer the closing down\n        of the context until the end of the ``with`` block.  This is useful if\n        you want to access the context locals for testing::\n\n            with app.test_client() as c:\n                rv = c.get('/?vodka=42')\n                assert request.args['vodka'] == '42'\n\n        Additionally, you may pass optional keyword arguments that will then\n        be passed to the application's :attr:`test_client_class` constructor.\n        For example::\n\n            from flask.testing import FlaskClient\n\n            class CustomClient(FlaskClient):\n                def __init__(self, *args, **kwargs):\n                    self._authentication = kwargs.pop(\"authentication\")\n                    super(CustomClient,self).__init__( *args, **kwargs)\n\n            app.test_client_class = CustomClient\n            client = app.test_client(authentication='Basic ....')\n\n        See :class:`~flask.testing.FlaskClient` for more information.\n\n        .. versionchanged:: 0.4\n           added support for ``with`` block usage for the client.\n\n        .. versionadded:: 0.7\n           The `use_cookies` parameter was added as well as the ability\n           to override the client to be used by setting the\n           :attr:`test_client_class` attribute.\n\n        .. versionchanged:: 0.11\n           Added `**kwargs` to support passing additional keyword arguments to\n           the constructor of :attr:`test_client_class`.\n        \"\"\"\n        cls = self.test_client_class\n        if cls is None:\n            from .testing import FlaskClient as cls\n        return cls(  # type: ignore\n            self, self.response_class, use_cookies=use_cookies, **kwargs\n        )", "metadata": {"license": "BSD-3-Clause", "len_tokens": 536}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def handle_http_exception(\n        self, ctx: AppContext, e: HTTPException\n    ) -> HTTPException | ft.ResponseReturnValue:\n        \"\"\"Handles an HTTP exception.  By default this will invoke the\n        registered error handlers and fall back to returning the\n        exception as response.\n\n        .. versionchanged:: 1.0.3\n            ``RoutingException``, used internally for actions such as\n             slash redirects during routing, is not passed to error\n             handlers.\n\n        .. versionchanged:: 1.0\n            Exceptions are looked up by code *and* by MRO, so\n            ``HTTPException`` subclasses can be handled with a catch-all\n            handler for the base ``HTTPException``.\n\n        .. versionadded:: 0.3\n        \"\"\"\n        # Proxy exceptions don't have error codes.  We want to always return\n        # those unchanged as errors\n        if e.code is None:\n            return e\n\n        # RoutingExceptions are used internally to trigger routing\n        # actions, such as slash redirects raising RequestRedirect. They\n        # are not raised or handled in user code.\n        if isinstance(e, RoutingException):\n            return e\n\n        handler = self._find_error_handler(e, ctx.request.blueprints)\n        if handler is None:\n            return e\n        return self.ensure_sync(handler)(e)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 277}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def handle_user_exception(\n        self, ctx: AppContext, e: Exception\n    ) -> HTTPException | ft.ResponseReturnValue:\n        \"\"\"This method is called whenever an exception occurs that\n        should be handled. A special case is :class:`~werkzeug\n        .exceptions.HTTPException` which is forwarded to the\n        :meth:`handle_http_exception` method. This function will either\n        return a response value or reraise the exception with the same\n        traceback.\n\n        .. versionchanged:: 1.0\n            Key errors raised from request data like ``form`` show the\n            bad key in debug mode rather than a generic bad request\n            message.\n\n        .. versionadded:: 0.7\n        \"\"\"\n        if isinstance(e, BadRequestKeyError) and (\n            self.debug or self.config[\"TRAP_BAD_REQUEST_ERRORS\"]\n        ):\n            e.show_exception = True\n\n        if isinstance(e, HTTPException) and not self.trap_http_exception(e):\n            return self.handle_http_exception(ctx, e)\n\n        handler = self._find_error_handler(e, ctx.request.blueprints)\n\n        if handler is None:\n            raise\n\n        return self.ensure_sync(handler)(e)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 247}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def handle_exception(self, ctx: AppContext, e: Exception) -> Response:\n        \"\"\"Handle an exception that did not have an error handler\n        associated with it, or that was raised from an error handler.\n        This always causes a 500 ``InternalServerError``.\n\n        Always sends the :data:`got_request_exception` signal.\n\n        If :data:`PROPAGATE_EXCEPTIONS` is ``True``, such as in debug\n        mode, the error will be re-raised so that the debugger can\n        display it. Otherwise, the original exception is logged, and\n        an :exc:`~werkzeug.exceptions.InternalServerError` is returned.\n\n        If an error handler is registered for ``InternalServerError`` or\n        ``500``, it will be used. For consistency, the handler will\n        always receive the ``InternalServerError``. The original\n        unhandled exception is available as ``e.original_exception``.\n\n        .. versionchanged:: 1.1.0\n            Always passes the ``InternalServerError`` instance to the\n            handler, setting ``original_exception`` to the unhandled\n            error.\n\n        .. versionchanged:: 1.1.0\n            ``after_request`` functions and other finalization is done\n            even for the default 500 response when there is no handler.\n\n        .. versionadded:: 0.3\n        \"\"\"\n        exc_info = sys.exc_info()\n        got_request_exception.send(self, _async_wrapper=self.ensure_sync, exception=e)\n        propagate = self.config[\"PROPAGATE_EXCEPTIONS\"]\n\n        if propagate is None:\n            propagate = self.testing or self.debug\n\n        if propagate:\n            # Re-raise if called with an active exception, otherwise\n            # raise the passed in exception.\n            if exc_info[1] is e:\n                raise\n\n            raise e\n\n        self.log_exception(ctx, exc_info)\n        server_error: InternalServerError | ft.ResponseReturnValue\n        server_error = InternalServerError(original_exception=e)\n        handler = self._find_error_handler(server_error, ctx.request.blueprints)\n\n        if handler is not None:\n            server_error = self.ensure_sync(handler)(server_error)\n\n        return self.finalize_request(ctx, server_error, from_error_handler=True)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 466}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def dispatch_request(self, ctx: AppContext) -> ft.ResponseReturnValue:\n        \"\"\"Does the request dispatching.  Matches the URL and returns the\n        return value of the view or error handler.  This does not have to\n        be a response object.  In order to convert the return value to a\n        proper response object, call :func:`make_response`.\n\n        .. versionchanged:: 0.7\n           This no longer does the exception handling, this code was\n           moved to the new :meth:`full_dispatch_request`.\n        \"\"\"\n        req = ctx.request\n\n        if req.routing_exception is not None:\n            self.raise_routing_exception(req)\n        rule: Rule = req.url_rule  # type: ignore[assignment]\n        # if we provide automatic options for this URL and the\n        # request came with the OPTIONS method, reply automatically\n        if (\n            getattr(rule, \"provide_automatic_options\", False)\n            and req.method == \"OPTIONS\"\n        ):\n            return self.make_default_options_response(ctx)\n        # otherwise dispatch to the handler for that endpoint\n        view_args: dict[str, t.Any] = req.view_args  # type: ignore[assignment]\n        return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 262}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def finalize_request(\n        self,\n        ctx: AppContext,\n        rv: ft.ResponseReturnValue | HTTPException,\n        from_error_handler: bool = False,\n    ) -> Response:\n        \"\"\"Given the return value from a view function this finalizes\n        the request by converting it into a response and invoking the\n        postprocessing functions.  This is invoked for both normal\n        request dispatching as well as error handlers.\n\n        Because this means that it might be called as a result of a\n        failure a special safe mode is available which can be enabled\n        with the `from_error_handler` flag.  If enabled, failures in\n        response processing will be logged and otherwise ignored.\n\n        :internal:\n        \"\"\"\n        response = self.make_response(rv)\n        try:\n            response = self.process_response(ctx, response)\n            request_finished.send(\n                self, _async_wrapper=self.ensure_sync, response=response\n            )\n        except Exception:\n            if not from_error_handler:\n                raise\n            self.logger.exception(\n                \"Request finalizing failed with an error while handling an error\"\n            )\n        return response", "metadata": {"license": "BSD-3-Clause", "len_tokens": 228}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def preprocess_request(self, ctx: AppContext) -> ft.ResponseReturnValue | None:\n        \"\"\"Called before the request is dispatched. Calls\n        :attr:`url_value_preprocessors` registered with the app and the\n        current blueprint (if any). Then calls :attr:`before_request_funcs`\n        registered with the app and the blueprint.\n\n        If any :meth:`before_request` handler returns a non-None value, the\n        value is handled as if it was the return value from the view, and\n        further request handling is stopped.\n        \"\"\"\n        req = ctx.request\n        names = (None, *reversed(req.blueprints))\n\n        for name in names:\n            if name in self.url_value_preprocessors:\n                for url_func in self.url_value_preprocessors[name]:\n                    url_func(req.endpoint, req.view_args)\n\n        for name in names:\n            if name in self.before_request_funcs:\n                for before_func in self.before_request_funcs[name]:\n                    rv = self.ensure_sync(before_func)()\n\n                    if rv is not None:\n                        return rv  # type: ignore[no-any-return]\n\n        return None", "metadata": {"license": "BSD-3-Clause", "len_tokens": 231}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def process_response(self, ctx: AppContext, response: Response) -> Response:\n        \"\"\"Can be overridden in order to modify the response object\n        before it's sent to the WSGI server.  By default this will\n        call all the :meth:`after_request` decorated functions.\n\n        .. versionchanged:: 0.5\n           As of Flask 0.5 the functions registered for after request\n           execution are called in reverse order of registration.\n\n        :param response: a :attr:`response_class` object.\n        :return: a new response object or the same, has to be an\n                 instance of :attr:`response_class`.\n        \"\"\"\n        for func in ctx._after_request_functions:\n            response = self.ensure_sync(func)(response)\n\n        for name in chain(ctx.request.blueprints, (None,)):\n            if name in self.after_request_funcs:\n                for func in reversed(self.after_request_funcs[name]):\n                    response = self.ensure_sync(func)(response)\n\n        if not self.session_interface.is_null_session(ctx.session):\n            self.session_interface.save_session(self, ctx.session, response)\n\n        return response", "metadata": {"license": "BSD-3-Clause", "len_tokens": 229}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def do_teardown_request(\n        self, ctx: AppContext, exc: BaseException | None = None\n    ) -> None:\n        \"\"\"Called after the request is dispatched and the response is finalized,\n        right before the request context is popped. Called by\n        :meth:`.AppContext.pop`.\n\n        This calls all functions decorated with :meth:`teardown_request`, and\n        :meth:`Blueprint.teardown_request` if a blueprint handled the request.\n        Finally, the :data:`request_tearing_down` signal is sent.\n\n        :param exc: An unhandled exception raised while dispatching the request.\n            Passed to each teardown function.\n\n        .. versionchanged:: 0.9\n            Added the ``exc`` argument.\n        \"\"\"\n        for name in chain(ctx.request.blueprints, (None,)):\n            if name in self.teardown_request_funcs:\n                for func in reversed(self.teardown_request_funcs[name]):\n                    self.ensure_sync(func)(exc)\n\n        request_tearing_down.send(self, _async_wrapper=self.ensure_sync, exc=exc)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 217}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def test_request_context(self, *args: t.Any, **kwargs: t.Any) -> AppContext:\n        \"\"\"Create an :class:`.AppContext` with request information created from\n        the given arguments. When the context is pushed, :data:`.request`,\n        :data:`.session`, :data:`g:, and :data:`.current_app` become available.\n\n        This is useful during testing to run a function that uses request data\n        without dispatching a full request. Use this as a ``with`` block to push\n        a context.\n\n        .. code-block:: python\n\n            with app.test_request_context(...):\n                generate_report()\n\n        See :doc:`/appcontext`.\n\n        Takes the same arguments as Werkzeug's\n        :class:`~werkzeug.test.EnvironBuilder`, with some defaults from\n        the application. See the linked Werkzeug docs for most of the\n        available arguments. Flask-specific behavior is listed here.\n\n        :param path: URL path being requested.\n        :param base_url: Base URL where the app is being served, which\n            ``path`` is relative to. If not given, built from\n            :data:`PREFERRED_URL_SCHEME`, ``subdomain``, :data:`SERVER_NAME`,\n            and :data:`APPLICATION_ROOT`.\n        :param subdomain: Subdomain name to prepend to :data:`SERVER_NAME`.\n        :param url_scheme: Scheme to use instead of\n            :data:`PREFERRED_URL_SCHEME`.\n        :param data: The request body text or bytes,or a dict of form data.\n        :param json: If given, this is serialized as JSON and passed as\n            ``data``. Also defaults ``content_type`` to\n            ``application/json``.\n        :param args: Other positional arguments passed to\n            :class:`~werkzeug.test.EnvironBuilder`.\n        :param kwargs: Other keyword arguments passed to\n            :class:`~werkzeug.test.EnvironBuilder`.\n        \"\"\"\n        from .testing import EnvironBuilder\n\n        builder = EnvironBuilder(self, *args, **kwargs)\n\n        try:\n            environ = builder.get_environ()\n        finally:\n            builder.close()\n\n        return self.request_context(environ)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 459}}
{"id": "flask:src/flask/app.py", "language": "python", "code": "def wsgi_app(\n        self, environ: WSGIEnvironment, start_response: StartResponse\n    ) -> cabc.Iterable[bytes]:\n        \"\"\"The actual WSGI application. This is not implemented in\n        :meth:`__call__` so that middlewares can be applied without\n        losing a reference to the app object. Instead of doing this::\n\n            app = MyMiddleware(app)\n\n        It's a better idea to do this instead::\n\n            app.wsgi_app = MyMiddleware(app.wsgi_app)\n\n        Then you still have the original application object around and\n        can continue to call methods on it.\n\n        .. versionchanged:: 0.7\n            Teardown events for the request and app contexts are called\n            even if an unhandled error occurs. Other events may not be\n            called depending on when an error occurs during dispatch.\n\n        :param environ: A WSGI environment.\n        :param start_response: A callable accepting a status code,\n            a list of headers, and an optional exception context to\n            start the response.\n        \"\"\"\n        ctx = self.request_context(environ)\n        error: BaseException | None = None\n        try:\n            try:\n                ctx.push()\n                response = self.full_dispatch_request(ctx)\n            except Exception as e:\n                error = e\n                response = self.handle_exception(ctx, e)\n            except:  # noqa: B001\n                error = sys.exc_info()[1]\n                raise\n            return response(environ, start_response)\n        finally:\n            if \"werkzeug.debug.preserve_context\" in environ:\n                environ[\"werkzeug.debug.preserve_context\"](ctx)\n\n            if error is not None and self.should_ignore_error(error):\n                error = None\n\n            ctx.pop(error)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 362}}
{"id": "flask:src/flask/debughelpers.py", "language": "python", "code": "class DebugFilesKeyError(KeyError, AssertionError):\n    \"\"\"Raised from request.files during debugging.  The idea is that it can\n    provide a better error message than just a generic KeyError/BadRequest.\n    \"\"\"\n\n    def __init__(self, request: Request, key: str) -> None:\n        form_matches = request.form.getlist(key)\n        buf = [\n            f\"You tried to access the file {key!r} in the request.files\"\n            \" dictionary but it does not exist. The mimetype for the\"\n            f\" request is {request.mimetype!r} instead of\"\n            \" 'multipart/form-data' which means that no file contents\"\n            \" were transmitted. To fix this error you should provide\"\n            ' enctype=\"multipart/form-data\" in your form.'\n        ]\n        if form_matches:\n            names = \", \".join(repr(x) for x in form_matches)\n            buf.append(\n                \"\\n\\nThe browser instead transmitted some file names. \"\n                f\"This was submitted: {names}\"\n            )\n        self.msg = \"\".join(buf)\n\n    def __str__(self) -> str:\n        return self.msg", "metadata": {"license": "BSD-3-Clause", "len_tokens": 238}}
{"id": "flask:src/flask/debughelpers.py", "language": "python", "code": "class FormDataRoutingRedirect(AssertionError):\n    \"\"\"This exception is raised in debug mode if a routing redirect\n    would cause the browser to drop the method or body. This happens\n    when method is not GET, HEAD or OPTIONS and the status code is not\n    307 or 308.\n    \"\"\"\n\n    def __init__(self, request: Request) -> None:\n        exc = request.routing_exception\n        assert isinstance(exc, RequestRedirect)\n        buf = [\n            f\"A request was sent to '{request.url}', but routing issued\"\n            f\" a redirect to the canonical URL '{exc.new_url}'.\"\n        ]\n\n        if f\"{request.base_url}/\" == exc.new_url.partition(\"?\")[0]:\n            buf.append(\n                \" The URL was defined with a trailing slash. Flask\"\n                \" will redirect to the URL with a trailing slash if it\"\n                \" was accessed without one.\"\n            )\n\n        buf.append(\n            \" Send requests to the canonical URL, or use 307 or 308 for\"\n            \" routing redirects. Otherwise, browsers will drop form\"\n            \" data.\\n\\n\"\n            \"This exception is only raised in debug mode.\"\n        )\n        super().__init__(\"\".join(buf))", "metadata": {"license": "BSD-3-Clause", "len_tokens": 250}}
{"id": "flask:src/flask/debughelpers.py", "language": "python", "code": "def explain_template_loading_attempts(\n    app: App,\n    template: str,\n    attempts: list[\n        tuple[\n            BaseLoader,\n            Scaffold,\n            tuple[str, str | None, t.Callable[[], bool] | None] | None,\n        ]\n    ],\n) -> None:\n    \"\"\"This should help developers understand what failed\"\"\"\n    info = [f\"Locating template {template!r}:\"]\n    total_found = 0\n    blueprint = None\n\n    if (ctx := _cv_app.get(None)) is not None and ctx.has_request:\n        blueprint = ctx.request.blueprint\n\n    for idx, (loader, srcobj, triple) in enumerate(attempts):\n        if isinstance(srcobj, App):\n            src_info = f\"application {srcobj.import_name!r}\"\n        elif isinstance(srcobj, Blueprint):\n            src_info = f\"blueprint {srcobj.name!r} ({srcobj.import_name})\"\n        else:\n            src_info = repr(srcobj)\n\n        info.append(f\"{idx + 1:5}: trying loader of {src_info}\")\n\n        for line in _dump_loader_info(loader):\n            info.append(f\"       {line}\")\n\n        if triple is None:\n            detail = \"no match\"\n        else:\n            detail = f\"found ({triple[1] or '<string>'!r})\"\n            total_found += 1\n        info.append(f\"       -> {detail}\")\n\n    seems_fishy = False\n    if total_found == 0:\n        info.append(\"Error: the template could not be found.\")\n        seems_fishy = True\n    elif total_found > 1:\n        info.append(\"Warning: multiple loaders returned a match for the template.\")\n        seems_fishy = True\n\n    if blueprint is not None and seems_fishy:\n        info.append(\n            \"  The template was looked up from an endpoint that belongs\"\n            f\" to the blueprint {blueprint!r}.\"\n        )\n        info.append(\"  Maybe you did not place a template in the right folder?\")\n        info.append(\"  See https://flask.palletsprojects.com/blueprints/#templates\")\n\n    app.logger.info(\"\\n\".join(info))", "metadata": {"license": "BSD-3-Clause", "len_tokens": 456}}
{"id": "flask:src/flask/ctx.py", "language": "python", "code": "class _AppCtxGlobals:\n    \"\"\"A plain object. Used as a namespace for storing data during an\n    application context.\n\n    Creating an app context automatically creates this object, which is\n    made available as the :data:`.g` proxy.\n\n    .. describe:: 'key' in g\n\n        Check whether an attribute is present.\n\n        .. versionadded:: 0.10\n\n    .. describe:: iter(g)\n\n        Return an iterator over the attribute names.\n\n        .. versionadded:: 0.10\n    \"\"\"\n\n    # Define attr methods to let mypy know this is a namespace object\n    # that has arbitrary attributes.\n\n    def __getattr__(self, name: str) -> t.Any:\n        try:\n            return self.__dict__[name]\n        except KeyError:\n            raise AttributeError(name) from None\n\n    def __setattr__(self, name: str, value: t.Any) -> None:\n        self.__dict__[name] = value\n\n    def __delattr__(self, name: str) -> None:\n        try:\n            del self.__dict__[name]\n        except KeyError:\n            raise AttributeError(name) from None\n\n    def get(self, name: str, default: t.Any | None = None) -> t.Any:\n        \"\"\"Get an attribute by name, or a default value. Like\n        :meth:`dict.get`.\n\n        :param name: Name of attribute to get.\n        :param default: Value to return if the attribute is not present.\n\n        .. versionadded:: 0.10\n        \"\"\"\n        return self.__dict__.get(name, default)\n\n    def pop(self, name: str, default: t.Any = _sentinel) -> t.Any:\n        \"\"\"Get and remove an attribute by name. Like :meth:`dict.pop`.\n\n        :param name: Name of attribute to pop.\n        :param default: Value to return if the attribute is not present,\n            instead of raising a ``KeyError``.\n\n        .. versionadded:: 0.11\n        \"\"\"\n        if default is _sentinel:\n            return self.__dict__.pop(name)\n        else:\n            return self.__dict__.pop(name, default)\n\n    def setdefault(self, name: str, default: t.Any = None) -> t.Any:\n        \"\"\"Get the value of an attribute if it is present, otherwise\n        set and return a default value. Like :meth:`dict.setdefault`.\n\n        :param name: Name of attribute to get.\n        :param default: Value to set and return if the attribute is not\n            present.\n\n        .. versionadded:: 0.11\n        \"\"\"\n        return self.__dict__.setdefault(name, default)\n\n    def __contains__(self, item: str) -> bool:\n        return item in self.__dict__\n\n    def __iter__(self) -> t.Iterator[str]:\n        return iter(self.__dict__)\n\n    def __repr__(self) -> str:\n        ctx = _cv_app.get(None)\n        if ctx is not None:\n            return f\"<flask.g of '{ctx.app.name}'>\"\n        return object.__repr__(self)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 647}}
{"id": "flask:src/flask/ctx.py", "language": "python", "code": "def after_this_request(\n    f: ft.AfterRequestCallable[t.Any],\n) -> ft.AfterRequestCallable[t.Any]:\n    \"\"\"Decorate a function to run after the current request. The behavior is the\n    same as :meth:`.Flask.after_request`, except it only applies to the current\n    request, rather than every request. Therefore, it must be used within a\n    request context, rather than during setup.\n\n    .. code-block:: python\n\n        @app.route(\"/\")\n        def index():\n            @after_this_request\n            def add_header(response):\n                response.headers[\"X-Foo\"] = \"Parachute\"\n                return response\n\n            return \"Hello, World!\"\n\n    .. versionadded:: 0.9\n    \"\"\"\n    ctx = _cv_app.get(None)\n\n    if ctx is None or not ctx.has_request:\n        raise RuntimeError(\n            \"'after_this_request' can only be used when a request\"\n            \" context is active, such as in a view function.\"\n        )\n\n    ctx._after_request_functions.append(f)\n    return f", "metadata": {"license": "BSD-3-Clause", "len_tokens": 219}}
{"id": "flask:src/flask/ctx.py", "language": "python", "code": "def copy_current_request_context(f: F) -> F:\n    \"\"\"Decorate a function to run inside the current request context. This can\n    be used when starting a background task, otherwise it will not see the app\n    and request objects that were active in the parent.\n\n    .. warning::\n\n        Due to the following caveats, it is often safer (and simpler) to pass\n        the data you need when starting the task, rather than using this and\n        relying on the context objects.\n\n    In order to avoid execution switching partially though reading data, either\n    read the request body (access ``form``, ``json``, ``data``, etc) before\n    starting the task, or use a lock. This can be an issue when using threading,\n    but shouldn't be an issue when using greenlet/gevent or asyncio.\n\n    If the task will access ``session``, be sure to do so in the parent as well\n    so that the ``Vary: cookie`` header will be set. Modifying ``session`` in\n    the task should be avoided, as it may execute after the response cookie has\n    already been written.\n\n    .. code-block:: python\n\n        import gevent\n        from flask import copy_current_request_context\n\n        @app.route('/')\n        def index():\n            @copy_current_request_context\n            def do_some_work():\n                # do some work here, it can access flask.request or\n                # flask.session like you would otherwise in the view function.\n                ...\n            gevent.spawn(do_some_work)\n            return 'Regular response'\n\n    .. versionadded:: 0.10\n    \"\"\"\n    ctx = _cv_app.get(None)\n\n    if ctx is None:\n        raise RuntimeError(\n            \"'copy_current_request_context' can only be used when a\"\n            \" request context is active, such as in a view function.\"\n        )\n\n    ctx = ctx.copy()\n\n    def wrapper(*args: t.Any, **kwargs: t.Any) -> t.Any:\n        with ctx:\n            return ctx.app.ensure_sync(f)(*args, **kwargs)\n\n    return update_wrapper(wrapper, f)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 438}}
{"id": "flask:src/flask/ctx.py", "language": "python", "code": "def __init__(\n        self,\n        app: Flask,\n        *,\n        request: Request | None = None,\n        session: SessionMixin | None = None,\n    ) -> None:\n        self.app = app\n        \"\"\"The application represented by this context. Accessed through\n        :data:`.current_app`.\n        \"\"\"\n\n        self.g: _AppCtxGlobals = app.app_ctx_globals_class()\n        \"\"\"The global data for this context. Accessed through :data:`.g`.\"\"\"\n\n        self.url_adapter: MapAdapter | None = None\n        \"\"\"The URL adapter bound to the request, or the app if not in a request.\n        May be ``None`` if binding failed.\n        \"\"\"\n\n        self._request: Request | None = request\n        self._session: SessionMixin | None = session\n        self._flashes: list[tuple[str, str]] | None = None\n        self._after_request_functions: list[ft.AfterRequestCallable[t.Any]] = []\n\n        try:\n            self.url_adapter = app.create_url_adapter(self._request)\n        except HTTPException as e:\n            if self._request is not None:\n                self._request.routing_exception = e\n\n        self._cv_token: contextvars.Token[AppContext] | None = None\n        \"\"\"The previous state to restore when popping.\"\"\"\n\n        self._push_count: int = 0\n        \"\"\"Track nested pushes of this context. Cleanup will only run once the\n        original push has been popped.\n        \"\"\"", "metadata": {"license": "BSD-3-Clause", "len_tokens": 308}}
{"id": "flask:src/flask/ctx.py", "language": "python", "code": "def pop(self, exc: BaseException | None = None) -> None:\n        \"\"\"Pop this context so that it is no longer the active context. Then\n        call teardown functions and signals.\n\n        Typically, this is not used directly. Instead, use a ``with`` block\n        to manage the context.\n\n        This context must currently be the active context, otherwise a\n        :exc:`RuntimeError` is raised. In some situations, such as streaming or\n        testing, the context may have been pushed multiple times. It will only\n        trigger cleanup once it has been popped as many times as it was pushed.\n        Until then, it will remain the active context.\n\n        :param exc: An unhandled exception that was raised while the context was\n            active. Passed to teardown functions.\n\n        .. versionchanged:: 0.9\n            Added the ``exc`` argument.\n        \"\"\"\n        if self._cv_token is None:\n            raise RuntimeError(f\"Cannot pop this context ({self!r}), it is not pushed.\")\n\n        ctx = _cv_app.get(None)\n\n        if ctx is None or self._cv_token is None:\n            raise RuntimeError(\n                f\"Cannot pop this context ({self!r}), there is no active context.\"\n            )\n\n        if ctx is not self:\n            raise RuntimeError(\n                f\"Cannot pop this context ({self!r}), it is not the active\"\n                f\" context ({ctx!r}).\"\n            )\n\n        self._push_count -= 1\n\n        if self._push_count > 0:\n            return\n\n        try:\n            if self._request is not None:\n                self.app.do_teardown_request(self, exc)\n                self._request.close()\n        finally:\n            self.app.do_teardown_appcontext(self, exc)\n            _cv_app.reset(self._cv_token)\n            self._cv_token = None\n            appcontext_popped.send(self.app, _async_wrapper=self.app.ensure_sync)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 403}}
{"id": "flask:src/flask/typing.py", "language": "python", "code": "from __future__ import annotations\n\nimport collections.abc as cabc\nimport typing as t\n\nif t.TYPE_CHECKING:  # pragma: no cover\n    from _typeshed.wsgi import WSGIApplication  # noqa: F401\n    from werkzeug.datastructures import Headers  # noqa: F401\n    from werkzeug.sansio.response import Response  # noqa: F401\n\n# The possible types that are directly convertible or are a Response object.\nResponseValue = t.Union[\n    \"Response\",\n    str,\n    bytes,\n    list[t.Any],\n    # Only dict is actually accepted, but Mapping allows for TypedDict.\n    t.Mapping[str, t.Any],\n    t.Iterator[str],\n    t.Iterator[bytes],\n    cabc.AsyncIterable[str],  # for Quart, until App is generic.\n    cabc.AsyncIterable[bytes],\n]\n\n# the possible types for an individual HTTP header\nHeaderValue = str | list[str] | tuple[str, ...]\n\n# the possible types for HTTP headers\nHeadersValue = t.Union[\n    \"Headers\",\n    t.Mapping[str, HeaderValue],\n    t.Sequence[tuple[str, HeaderValue]],\n]\n\n# The possible types returned by a route function.\nResponseReturnValue = t.Union[\n    ResponseValue,\n    tuple[ResponseValue, HeadersValue],\n    tuple[ResponseValue, int],\n    tuple[ResponseValue, int, HeadersValue],\n    \"WSGIApplication\",\n]\n\n# Allow any subclass of werkzeug.Response, such as the one from Flask,\n# as a callback argument. Using werkzeug.Response directly makes a\n# callback annotated with flask.Response fail type checking.\nResponseClass = t.TypeVar(\"ResponseClass\", bound=\"Response\")\n\nAppOrBlueprintKey = str | None  # The App key is None, whereas blueprints are named\nAfterRequestCallable = (\n    t.Callable[[ResponseClass], ResponseClass]\n    | t.Callable[[ResponseClass], t.Awaitable[ResponseClass]]\n)\nBeforeFirstRequestCallable = t.Callable[[], None] | t.Callable[[], t.Awaitable[None]]\nBeforeRequestCallable = (\n    t.Callable[[], ResponseReturnValue | None]\n    | t.Callable[[], t.Awaitable[ResponseReturnValue | None]]\n)\nShellContextProcessorCallable = t.Callable[[], dict[str, t.Any]]\nTeardownCallable = (\n    t.Callable[[BaseException | None], None]\n    | t.Callable[[BaseException | None], t.Awaitable[None]]\n)\nTemplateContextProcessorCallable = (\n    t.Callable[[], dict[str, t.Any]] | t.Callable[[], t.Awaitable[dict[str, t.Any]]]\n)\nTemplateFilterCallable = t.Callable[..., t.Any]\nTemplateGlobalCallable = t.Callable[..., t.Any]\nTemplateTestCallable = t.Callable[..., bool]\nURLDefaultCallable = t.Callable[[str, dict[str, t.Any]], None]\nURLValuePreprocessorCallable = t.Callable[[str | None, dict[str, t.Any] | None], None]\n\n# This should take Exception, but that either breaks typing the argument\n# with a specific exception, or decorating multiple times with different\n# exceptions (and using a union type on the argument).\n# https://github.com/pallets/flask/issues/4095\n# https://github.com/pallets/flask/issues/4295\n# https://github.com/pallets/flask/issues/4297\nErrorHandlerCallable = (\n    t.Callable[[t.Any], ResponseReturnValue]\n    | t.Callable[[t.Any], t.Awaitable[ResponseReturnValue]]\n)\n\nRouteCallable = (\n    t.Callable[..., ResponseReturnValue]\n    | t.Callable[..., t.Awaitable[ResponseReturnValue]]\n)\n", "metadata": {"license": "BSD-3-Clause", "len_tokens": 783}}
{"id": "flask:src/flask/helpers.py", "language": "python", "code": "def stream_with_context(\n    generator_or_function: t.Iterator[t.AnyStr] | t.Callable[..., t.Iterator[t.AnyStr]],\n) -> t.Iterator[t.AnyStr] | t.Callable[[t.Iterator[t.AnyStr]], t.Iterator[t.AnyStr]]:\n    \"\"\"Wrap a response generator function so that it runs inside the current\n    request context. This keeps :data:`.request`, :data:`.session`, and :data:`.g`\n    available, even though at the point the generator runs the request context\n    will typically have ended.\n\n    Use it as a decorator on a generator function:\n\n    .. code-block:: python\n\n        from flask import stream_with_context, request, Response\n\n        @app.get(\"/stream\")\n        def streamed_response():\n            @stream_with_context\n            def generate():\n                yield \"Hello \"\n                yield request.args[\"name\"]\n                yield \"!\"\n\n            return Response(generate())\n\n    Or use it as a wrapper around a created generator:\n\n    .. code-block:: python\n\n        from flask import stream_with_context, request, Response\n\n        @app.get(\"/stream\")\n        def streamed_response():\n            def generate():\n                yield \"Hello \"\n                yield request.args[\"name\"]\n                yield \"!\"\n\n            return Response(stream_with_context(generate()))\n\n    .. versionadded:: 0.9\n    \"\"\"\n    try:\n        gen = iter(generator_or_function)  # type: ignore[arg-type]\n    except TypeError:\n\n        def decorator(*args: t.Any, **kwargs: t.Any) -> t.Any:\n            gen = generator_or_function(*args, **kwargs)  # type: ignore[operator]\n            return stream_with_context(gen)\n\n        return update_wrapper(decorator, generator_or_function)  # type: ignore[arg-type]\n\n    def generator() -> t.Iterator[t.AnyStr]:\n        if (ctx := _cv_app.get(None)) is None:\n            raise RuntimeError(\n                \"'stream_with_context' can only be used when a request\"\n                \" context is active, such as in a view function.\"\n            )\n\n        with ctx:\n            yield None  # type: ignore[misc]\n\n            try:\n                yield from gen\n            finally:\n                # Clean up in case the user wrapped a WSGI iterator.\n                if hasattr(gen, \"close\"):\n                    gen.close()\n\n    # Execute the generator to the sentinel value. This captures the current\n    # context and pushes it to preserve it. Further iteration will yield from\n    # the original iterator.\n    wrapped_g = generator()\n    next(wrapped_g)\n    return wrapped_g", "metadata": {"license": "BSD-3-Clause", "len_tokens": 523}}
{"id": "flask:src/flask/helpers.py", "language": "python", "code": "def make_response(*args: t.Any) -> Response:\n    \"\"\"Sometimes it is necessary to set additional headers in a view.  Because\n    views do not have to return response objects but can return a value that\n    is converted into a response object by Flask itself, it becomes tricky to\n    add headers to it.  This function can be called instead of using a return\n    and you will get a response object which you can use to attach headers.\n\n    If view looked like this and you want to add a new header::\n\n        def index():\n            return render_template('index.html', foo=42)\n\n    You can now do something like this::\n\n        def index():\n            response = make_response(render_template('index.html', foo=42))\n            response.headers['X-Parachutes'] = 'parachutes are cool'\n            return response\n\n    This function accepts the very same arguments you can return from a\n    view function.  This for example creates a response with a 404 error\n    code::\n\n        response = make_response(render_template('not_found.html'), 404)\n\n    The other use case of this function is to force the return value of a\n    view function into a response which is helpful with view\n    decorators::\n\n        response = make_response(view_function())\n        response.headers['X-Parachutes'] = 'parachutes are cool'\n\n    Internally this function does the following things:\n\n    -   if no arguments are passed, it creates a new response argument\n    -   if one argument is passed, :meth:`flask.Flask.make_response`\n        is invoked with it.\n    -   if more than one argument is passed, the arguments are passed\n        to the :meth:`flask.Flask.make_response` function as tuple.\n\n    .. versionadded:: 0.6\n    \"\"\"\n    if not args:\n        return current_app.response_class()\n    if len(args) == 1:\n        args = args[0]\n    return current_app.make_response(args)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 413}}
{"id": "flask:src/flask/helpers.py", "language": "python", "code": "def url_for(\n    endpoint: str,\n    *,\n    _anchor: str | None = None,\n    _method: str | None = None,\n    _scheme: str | None = None,\n    _external: bool | None = None,\n    **values: t.Any,\n) -> str:\n    \"\"\"Generate a URL to the given endpoint with the given values.\n\n    This requires an active request or application context, and calls\n    :meth:`current_app.url_for() <flask.Flask.url_for>`. See that method\n    for full documentation.\n\n    :param endpoint: The endpoint name associated with the URL to\n        generate. If this starts with a ``.``, the current blueprint\n        name (if any) will be used.\n    :param _anchor: If given, append this as ``#anchor`` to the URL.\n    :param _method: If given, generate the URL associated with this\n        method for the endpoint.\n    :param _scheme: If given, the URL will have this scheme if it is\n        external.\n    :param _external: If given, prefer the URL to be internal (False) or\n        require it to be external (True). External URLs include the\n        scheme and domain. When not in an active request, URLs are\n        external by default.\n    :param values: Values to use for the variable parts of the URL rule.\n        Unknown keys are appended as query string arguments, like\n        ``?a=b&c=d``.\n\n    .. versionchanged:: 2.2\n        Calls ``current_app.url_for``, allowing an app to override the\n        behavior.\n\n    .. versionchanged:: 0.10\n       The ``_scheme`` parameter was added.\n\n    .. versionchanged:: 0.9\n       The ``_anchor`` and ``_method`` parameters were added.\n\n    .. versionchanged:: 0.9\n       Calls ``app.handle_url_build_error`` on build errors.\n    \"\"\"\n    return current_app.url_for(\n        endpoint,\n        _anchor=_anchor,\n        _method=_method,\n        _scheme=_scheme,\n        _external=_external,\n        **values,\n    )", "metadata": {"license": "BSD-3-Clause", "len_tokens": 450}}
{"id": "flask:src/flask/helpers.py", "language": "python", "code": "def redirect(\n    location: str, code: int = 302, Response: type[BaseResponse] | None = None\n) -> BaseResponse:\n    \"\"\"Create a redirect response object.\n\n    If :data:`~flask.current_app` is available, it will use its\n    :meth:`~flask.Flask.redirect` method, otherwise it will use\n    :func:`werkzeug.utils.redirect`.\n\n    :param location: The URL to redirect to.\n    :param code: The status code for the redirect.\n    :param Response: The response class to use. Not used when\n        ``current_app`` is active, which uses ``app.response_class``.\n\n    .. versionadded:: 2.2\n        Calls ``current_app.redirect`` if available instead of always\n        using Werkzeug's default ``redirect``.\n    \"\"\"\n    if (ctx := _cv_app.get(None)) is not None:\n        return ctx.app.redirect(location, code=code)\n\n    return _wz_redirect(location, code=code, Response=Response)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 215}}
{"id": "flask:src/flask/helpers.py", "language": "python", "code": "def abort(code: int | BaseResponse, *args: t.Any, **kwargs: t.Any) -> t.NoReturn:\n    \"\"\"Raise an :exc:`~werkzeug.exceptions.HTTPException` for the given\n    status code.\n\n    If :data:`~flask.current_app` is available, it will call its\n    :attr:`~flask.Flask.aborter` object, otherwise it will use\n    :func:`werkzeug.exceptions.abort`.\n\n    :param code: The status code for the exception, which must be\n        registered in ``app.aborter``.\n    :param args: Passed to the exception.\n    :param kwargs: Passed to the exception.\n\n    .. versionadded:: 2.2\n        Calls ``current_app.aborter`` if available instead of always\n        using Werkzeug's default ``abort``.\n    \"\"\"\n    if (ctx := _cv_app.get(None)) is not None:\n        ctx.app.aborter(code, *args, **kwargs)\n\n    _wz_abort(code, *args, **kwargs)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 216}}
{"id": "flask:src/flask/helpers.py", "language": "python", "code": "def flash(message: str, category: str = \"message\") -> None:\n    \"\"\"Flashes a message to the next request.  In order to remove the\n    flashed message from the session and to display it to the user,\n    the template has to call :func:`get_flashed_messages`.\n\n    .. versionchanged:: 0.3\n       `category` parameter added.\n\n    :param message: the message to be flashed.\n    :param category: the category for the message.  The following values\n                     are recommended: ``'message'`` for any kind of message,\n                     ``'error'`` for errors, ``'info'`` for information\n                     messages and ``'warning'`` for warnings.  However any\n                     kind of string can be used as category.\n    \"\"\"\n    # Original implementation:\n    #\n    #     session.setdefault('_flashes', []).append((category, message))\n    #\n    # This assumed that changes made to mutable structures in the session are\n    # always in sync with the session object, which is not true for session\n    # implementations that use external storage for keeping their keys/values.\n    flashes = session.get(\"_flashes\", [])\n    flashes.append((category, message))\n    session[\"_flashes\"] = flashes\n    app = current_app._get_current_object()\n    message_flashed.send(\n        app,\n        _async_wrapper=app.ensure_sync,\n        message=message,\n        category=category,\n    )", "metadata": {"license": "BSD-3-Clause", "len_tokens": 305}}
{"id": "flask:src/flask/helpers.py", "language": "python", "code": "def get_flashed_messages(\n    with_categories: bool = False, category_filter: t.Iterable[str] = ()\n) -> list[str] | list[tuple[str, str]]:\n    \"\"\"Pulls all flashed messages from the session and returns them.\n    Further calls in the same request to the function will return\n    the same messages.  By default just the messages are returned,\n    but when `with_categories` is set to ``True``, the return value will\n    be a list of tuples in the form ``(category, message)`` instead.\n\n    Filter the flashed messages to one or more categories by providing those\n    categories in `category_filter`.  This allows rendering categories in\n    separate html blocks.  The `with_categories` and `category_filter`\n    arguments are distinct:\n\n    * `with_categories` controls whether categories are returned with message\n      text (``True`` gives a tuple, where ``False`` gives just the message text).\n    * `category_filter` filters the messages down to only those matching the\n      provided categories.\n\n    See :doc:`/patterns/flashing` for examples.\n\n    .. versionchanged:: 0.3\n       `with_categories` parameter added.\n\n    .. versionchanged:: 0.9\n        `category_filter` parameter added.\n\n    :param with_categories: set to ``True`` to also receive categories.\n    :param category_filter: filter of categories to limit return values.  Only\n                            categories in the list will be returned.\n    \"\"\"\n    flashes = app_ctx._flashes\n    if flashes is None:\n        flashes = session.pop(\"_flashes\") if \"_flashes\" in session else []\n        app_ctx._flashes = flashes\n    if category_filter:\n        flashes = list(filter(lambda f: f[0] in category_filter, flashes))\n    if not with_categories:\n        return [x[1] for x in flashes]\n    return flashes", "metadata": {"license": "BSD-3-Clause", "len_tokens": 398}}
{"id": "flask:src/flask/helpers.py", "language": "python", "code": "def send_from_directory(\n    directory: os.PathLike[str] | str,\n    path: os.PathLike[str] | str,\n    **kwargs: t.Any,\n) -> Response:\n    \"\"\"Send a file from within a directory using :func:`send_file`.\n\n    .. code-block:: python\n\n        @app.route(\"/uploads/<path:name>\")\n        def download_file(name):\n            return send_from_directory(\n                app.config['UPLOAD_FOLDER'], name, as_attachment=True\n            )\n\n    This is a secure way to serve files from a folder, such as static\n    files or uploads. Uses :func:`~werkzeug.security.safe_join` to\n    ensure the path coming from the client is not maliciously crafted to\n    point outside the specified directory.\n\n    If the final path does not point to an existing regular file,\n    raises a 404 :exc:`~werkzeug.exceptions.NotFound` error.\n\n    :param directory: The directory that ``path`` must be located under,\n        relative to the current application's root path. This *must not*\n        be a value provided by the client, otherwise it becomes insecure.\n    :param path: The path to the file to send, relative to\n        ``directory``.\n    :param kwargs: Arguments to pass to :func:`send_file`.\n\n    .. versionchanged:: 2.0\n        ``path`` replaces the ``filename`` parameter.\n\n    .. versionadded:: 2.0\n        Moved the implementation to Werkzeug. This is now a wrapper to\n        pass some Flask-specific arguments.\n\n    .. versionadded:: 0.5\n    \"\"\"\n    return werkzeug.utils.send_from_directory(  # type: ignore[return-value]\n        directory, path, **_prepare_send_file_kwargs(**kwargs)\n    )", "metadata": {"license": "BSD-3-Clause", "len_tokens": 365}}
{"id": "flask:src/flask/helpers.py", "language": "python", "code": "def get_root_path(import_name: str) -> str:\n    \"\"\"Find the root path of a package, or the path that contains a\n    module. If it cannot be found, returns the current working\n    directory.\n\n    Not to be confused with the value returned by :func:`find_package`.\n\n    :meta private:\n    \"\"\"\n    # Module already imported and has a file attribute. Use that first.\n    mod = sys.modules.get(import_name)\n\n    if mod is not None and hasattr(mod, \"__file__\") and mod.__file__ is not None:\n        return os.path.dirname(os.path.abspath(mod.__file__))\n\n    # Next attempt: check the loader.\n    try:\n        spec = importlib.util.find_spec(import_name)\n\n        if spec is None:\n            raise ValueError\n    except (ImportError, ValueError):\n        loader = None\n    else:\n        loader = spec.loader\n\n    # Loader does not exist or we're referring to an unloaded main\n    # module or a main module without path (interactive sessions), go\n    # with the current working directory.\n    if loader is None:\n        return os.getcwd()\n\n    if hasattr(loader, \"get_filename\"):\n        filepath = loader.get_filename(import_name)  # pyright: ignore\n    else:\n        # Fall back to imports.\n        __import__(import_name)\n        mod = sys.modules[import_name]\n        filepath = getattr(mod, \"__file__\", None)\n\n        # If we don't have a file path it might be because it is a\n        # namespace package. In this case pick the root path from the\n        # first module that is contained in the package.\n        if filepath is None:\n            raise RuntimeError(\n                \"No root path can be found for the provided module\"\n                f\" {import_name!r}. This can happen because the module\"\n                \" came from an import hook that does not provide file\"\n                \" name information or because it's a namespace package.\"\n                \" In this case the root path needs to be explicitly\"\n                \" provided.\"\n            )\n\n    # filepath is import_name.py for a module, or __init__.py for a package.\n    return os.path.dirname(os.path.abspath(filepath))", "metadata": {"license": "BSD-3-Clause", "len_tokens": 452}}
{"id": "flask:src/flask/views.py", "language": "python", "code": "class MethodView(View):\n    \"\"\"Dispatches request methods to the corresponding instance methods.\n    For example, if you implement a ``get`` method, it will be used to\n    handle ``GET`` requests.\n\n    This can be useful for defining a REST API.\n\n    :attr:`methods` is automatically set based on the methods defined on\n    the class.\n\n    See :doc:`views` for a detailed guide.\n\n    .. code-block:: python\n\n        class CounterAPI(MethodView):\n            def get(self):\n                return str(session.get(\"counter\", 0))\n\n            def post(self):\n                session[\"counter\"] = session.get(\"counter\", 0) + 1\n                return redirect(url_for(\"counter\"))\n\n        app.add_url_rule(\n            \"/counter\", view_func=CounterAPI.as_view(\"counter\")\n        )\n    \"\"\"\n\n    def __init_subclass__(cls, **kwargs: t.Any) -> None:\n        super().__init_subclass__(**kwargs)\n\n        if \"methods\" not in cls.__dict__:\n            methods = set()\n\n            for base in cls.__bases__:\n                if getattr(base, \"methods\", None):\n                    methods.update(base.methods)  # type: ignore[attr-defined]\n\n            for key in http_method_funcs:\n                if hasattr(cls, key):\n                    methods.add(key.upper())\n\n            if methods:\n                cls.methods = methods\n\n    def dispatch_request(self, **kwargs: t.Any) -> ft.ResponseReturnValue:\n        meth = getattr(self, request.method.lower(), None)\n\n        # If the request method is HEAD and we don't have a handler for it\n        # retry with GET.\n        if meth is None and request.method == \"HEAD\":\n            meth = getattr(self, \"get\", None)\n\n        assert meth is not None, f\"Unimplemented method {request.method!r}\"\n        return current_app.ensure_sync(meth)(**kwargs)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 386}}
{"id": "flask:src/flask/views.py", "language": "python", "code": "def as_view(\n        cls, name: str, *class_args: t.Any, **class_kwargs: t.Any\n    ) -> ft.RouteCallable:\n        \"\"\"Convert the class into a view function that can be registered\n        for a route.\n\n        By default, the generated view will create a new instance of the\n        view class for every request and call its\n        :meth:`dispatch_request` method. If the view class sets\n        :attr:`init_every_request` to ``False``, the same instance will\n        be used for every request.\n\n        Except for ``name``, all other arguments passed to this method\n        are forwarded to the view class ``__init__`` method.\n\n        .. versionchanged:: 2.2\n            Added the ``init_every_request`` class attribute.\n        \"\"\"\n        if cls.init_every_request:\n\n            def view(**kwargs: t.Any) -> ft.ResponseReturnValue:\n                self = view.view_class(  # type: ignore[attr-defined]\n                    *class_args, **class_kwargs\n                )\n                return current_app.ensure_sync(self.dispatch_request)(**kwargs)  # type: ignore[no-any-return]\n\n        else:\n            self = cls(*class_args, **class_kwargs)  # pyright: ignore\n\n            def view(**kwargs: t.Any) -> ft.ResponseReturnValue:\n                return current_app.ensure_sync(self.dispatch_request)(**kwargs)  # type: ignore[no-any-return]\n\n        if cls.decorators:\n            view.__name__ = name\n            view.__module__ = cls.__module__\n            for decorator in cls.decorators:\n                view = decorator(view)\n\n        # We attach the view class to the view function for two reasons:\n        # first of all it allows us to easily figure out what class-based\n        # view this thing came from, secondly it's also used for instantiating\n        # the view class so you can actually replace it with something else\n        # for testing purposes and debugging.\n        view.view_class = cls  # type: ignore\n        view.__name__ = name\n        view.__doc__ = cls.__doc__\n        view.__module__ = cls.__module__\n        view.methods = cls.methods  # type: ignore\n        view.provide_automatic_options = cls.provide_automatic_options  # type: ignore\n        return view", "metadata": {"license": "BSD-3-Clause", "len_tokens": 481}}
{"id": "flask:src/flask/sansio/blueprints.py", "language": "python", "code": "class BlueprintSetupState:\n    \"\"\"Temporary holder object for registering a blueprint with the\n    application.  An instance of this class is created by the\n    :meth:`~flask.Blueprint.make_setup_state` method and later passed\n    to all register callback functions.\n    \"\"\"\n\n    def __init__(\n        self,\n        blueprint: Blueprint,\n        app: App,\n        options: t.Any,\n        first_registration: bool,\n    ) -> None:\n        #: a reference to the current application\n        self.app = app\n\n        #: a reference to the blueprint that created this setup state.\n        self.blueprint = blueprint\n\n        #: a dictionary with all options that were passed to the\n        #: :meth:`~flask.Flask.register_blueprint` method.\n        self.options = options\n\n        #: as blueprints can be registered multiple times with the\n        #: application and not everything wants to be registered\n        #: multiple times on it, this attribute can be used to figure\n        #: out if the blueprint was registered in the past already.\n        self.first_registration = first_registration\n\n        subdomain = self.options.get(\"subdomain\")\n        if subdomain is None:\n            subdomain = self.blueprint.subdomain\n\n        #: The subdomain that the blueprint should be active for, ``None``\n        #: otherwise.\n        self.subdomain = subdomain\n\n        url_prefix = self.options.get(\"url_prefix\")\n        if url_prefix is None:\n            url_prefix = self.blueprint.url_prefix\n        #: The prefix that should be used for all URLs defined on the\n        #: blueprint.\n        self.url_prefix = url_prefix\n\n        self.name = self.options.get(\"name\", blueprint.name)\n        self.name_prefix = self.options.get(\"name_prefix\", \"\")\n\n        #: A dictionary with URL defaults that is added to each and every\n        #: URL that was defined with the blueprint.\n        self.url_defaults = dict(self.blueprint.url_values_defaults)\n        self.url_defaults.update(self.options.get(\"url_defaults\", ()))\n\n    def add_url_rule(\n        self,\n        rule: str,\n        endpoint: str | None = None,\n        view_func: ft.RouteCallable | None = None,\n        **options: t.Any,\n    ) -> None:\n        \"\"\"A helper method to register a rule (and optionally a view function)\n        to the application.  The endpoint is automatically prefixed with the\n        blueprint's name.\n        \"\"\"\n        if self.url_prefix is not None:\n            if rule:\n                rule = \"/\".join((self.url_prefix.rstrip(\"/\"), rule.lstrip(\"/\")))\n            else:\n                rule = self.url_prefix\n        options.setdefault(\"subdomain\", self.subdomain)\n        if endpoint is None:\n            endpoint = _endpoint_from_view_func(view_func)  # type: ignore\n        defaults = self.url_defaults\n        if \"defaults\" in options:\n            defaults = dict(defaults, **options.pop(\"defaults\"))\n\n        self.app.add_url_rule(\n            rule,\n            f\"{self.name_prefix}.{self.name}.{endpoint}\".lstrip(\".\"),\n            view_func,\n            defaults=defaults,\n            **options,\n        )", "metadata": {"license": "BSD-3-Clause", "len_tokens": 643}}
{"id": "flask:src/flask/sansio/blueprints.py", "language": "python", "code": "def __init__(\n        self,\n        blueprint: Blueprint,\n        app: App,\n        options: t.Any,\n        first_registration: bool,\n    ) -> None:\n        #: a reference to the current application\n        self.app = app\n\n        #: a reference to the blueprint that created this setup state.\n        self.blueprint = blueprint\n\n        #: a dictionary with all options that were passed to the\n        #: :meth:`~flask.Flask.register_blueprint` method.\n        self.options = options\n\n        #: as blueprints can be registered multiple times with the\n        #: application and not everything wants to be registered\n        #: multiple times on it, this attribute can be used to figure\n        #: out if the blueprint was registered in the past already.\n        self.first_registration = first_registration\n\n        subdomain = self.options.get(\"subdomain\")\n        if subdomain is None:\n            subdomain = self.blueprint.subdomain\n\n        #: The subdomain that the blueprint should be active for, ``None``\n        #: otherwise.\n        self.subdomain = subdomain\n\n        url_prefix = self.options.get(\"url_prefix\")\n        if url_prefix is None:\n            url_prefix = self.blueprint.url_prefix\n        #: The prefix that should be used for all URLs defined on the\n        #: blueprint.\n        self.url_prefix = url_prefix\n\n        self.name = self.options.get(\"name\", blueprint.name)\n        self.name_prefix = self.options.get(\"name_prefix\", \"\")\n\n        #: A dictionary with URL defaults that is added to each and every\n        #: URL that was defined with the blueprint.\n        self.url_defaults = dict(self.blueprint.url_values_defaults)\n        self.url_defaults.update(self.options.get(\"url_defaults\", ()))", "metadata": {"license": "BSD-3-Clause", "len_tokens": 355}}
{"id": "flask:src/flask/sansio/blueprints.py", "language": "python", "code": "def add_url_rule(\n        self,\n        rule: str,\n        endpoint: str | None = None,\n        view_func: ft.RouteCallable | None = None,\n        **options: t.Any,\n    ) -> None:\n        \"\"\"A helper method to register a rule (and optionally a view function)\n        to the application.  The endpoint is automatically prefixed with the\n        blueprint's name.\n        \"\"\"\n        if self.url_prefix is not None:\n            if rule:\n                rule = \"/\".join((self.url_prefix.rstrip(\"/\"), rule.lstrip(\"/\")))\n            else:\n                rule = self.url_prefix\n        options.setdefault(\"subdomain\", self.subdomain)\n        if endpoint is None:\n            endpoint = _endpoint_from_view_func(view_func)  # type: ignore\n        defaults = self.url_defaults\n        if \"defaults\" in options:\n            defaults = dict(defaults, **options.pop(\"defaults\"))\n\n        self.app.add_url_rule(\n            rule,\n            f\"{self.name_prefix}.{self.name}.{endpoint}\".lstrip(\".\"),\n            view_func,\n            defaults=defaults,\n            **options,\n        )", "metadata": {"license": "BSD-3-Clause", "len_tokens": 228}}
{"id": "flask:src/flask/sansio/blueprints.py", "language": "python", "code": "def __init__(\n        self,\n        name: str,\n        import_name: str,\n        static_folder: str | os.PathLike[str] | None = None,\n        static_url_path: str | None = None,\n        template_folder: str | os.PathLike[str] | None = None,\n        url_prefix: str | None = None,\n        subdomain: str | None = None,\n        url_defaults: dict[str, t.Any] | None = None,\n        root_path: str | None = None,\n        cli_group: str | None = _sentinel,  # type: ignore[assignment]\n    ):\n        super().__init__(\n            import_name=import_name,\n            static_folder=static_folder,\n            static_url_path=static_url_path,\n            template_folder=template_folder,\n            root_path=root_path,\n        )\n\n        if not name:\n            raise ValueError(\"'name' may not be empty.\")\n\n        if \".\" in name:\n            raise ValueError(\"'name' may not contain a dot '.' character.\")\n\n        self.name = name\n        self.url_prefix = url_prefix\n        self.subdomain = subdomain\n        self.deferred_functions: list[DeferredSetupFunction] = []\n\n        if url_defaults is None:\n            url_defaults = {}\n\n        self.url_values_defaults = url_defaults\n        self.cli_group = cli_group\n        self._blueprints: list[tuple[Blueprint, dict[str, t.Any]]] = []", "metadata": {"license": "BSD-3-Clause", "len_tokens": 294}}
{"id": "flask:src/flask/sansio/blueprints.py", "language": "python", "code": "def _merge_blueprint_funcs(self, app: App, name: str) -> None:\n        def extend(\n            bp_dict: dict[ft.AppOrBlueprintKey, list[t.Any]],\n            parent_dict: dict[ft.AppOrBlueprintKey, list[t.Any]],\n        ) -> None:\n            for key, values in bp_dict.items():\n                key = name if key is None else f\"{name}.{key}\"\n                parent_dict[key].extend(values)\n\n        for key, value in self.error_handler_spec.items():\n            key = name if key is None else f\"{name}.{key}\"\n            value = defaultdict(\n                dict,\n                {\n                    code: {exc_class: func for exc_class, func in code_values.items()}\n                    for code, code_values in value.items()\n                },\n            )\n            app.error_handler_spec[key] = value\n\n        for endpoint, func in self.view_functions.items():\n            app.view_functions[endpoint] = func\n\n        extend(self.before_request_funcs, app.before_request_funcs)\n        extend(self.after_request_funcs, app.after_request_funcs)\n        extend(\n            self.teardown_request_funcs,\n            app.teardown_request_funcs,\n        )\n        extend(self.url_default_functions, app.url_default_functions)\n        extend(self.url_value_preprocessors, app.url_value_preprocessors)\n        extend(self.template_context_processors, app.template_context_processors)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 274}}
{"id": "flask:src/flask/sansio/blueprints.py", "language": "python", "code": "def add_url_rule(\n        self,\n        rule: str,\n        endpoint: str | None = None,\n        view_func: ft.RouteCallable | None = None,\n        provide_automatic_options: bool | None = None,\n        **options: t.Any,\n    ) -> None:\n        \"\"\"Register a URL rule with the blueprint. See :meth:`.Flask.add_url_rule` for\n        full documentation.\n\n        The URL rule is prefixed with the blueprint's URL prefix. The endpoint name,\n        used with :func:`url_for`, is prefixed with the blueprint's name.\n        \"\"\"\n        if endpoint and \".\" in endpoint:\n            raise ValueError(\"'endpoint' may not contain a dot '.' character.\")\n\n        if view_func and hasattr(view_func, \"__name__\") and \".\" in view_func.__name__:\n            raise ValueError(\"'view_func' name may not contain a dot '.' character.\")\n\n        self.record(\n            lambda s: s.add_url_rule(\n                rule,\n                endpoint,\n                view_func,\n                provide_automatic_options=provide_automatic_options,\n                **options,\n            )\n        )", "metadata": {"license": "BSD-3-Clause", "len_tokens": 227}}
{"id": "flask:src/flask/sansio/blueprints.py", "language": "python", "code": "def app_template_filter(\n        self, name: T_template_filter | str | None = None\n    ) -> T_template_filter | t.Callable[[T_template_filter], T_template_filter]:\n        \"\"\"Decorate a function to register it as a custom Jinja filter. The name\n        is optional. The decorator may be used without parentheses.\n\n        The :meth:`add_app_template_filter` method may be used to register a\n        function later rather than decorating.\n\n        The filter is available in all templates, not only those under this\n        blueprint. Equivalent to :meth:`.Flask.template_filter`.\n\n        :param name: The name to register the filter as. If not given, uses the\n            function's name.\n        \"\"\"\n        if callable(name):\n            self.add_app_template_filter(name)\n            return name\n\n        def decorator(f: T_template_filter) -> T_template_filter:\n            self.add_app_template_filter(f, name=name)\n            return f\n\n        return decorator", "metadata": {"license": "BSD-3-Clause", "len_tokens": 200}}
{"id": "flask:src/flask/sansio/blueprints.py", "language": "python", "code": "def app_template_test(\n        self, name: T_template_test | str | None = None\n    ) -> T_template_test | t.Callable[[T_template_test], T_template_test]:\n        \"\"\"Decorate a function to register it as a custom Jinja test. The name\n        is optional. The decorator may be used without parentheses.\n\n        The :meth:`add_app_template_test` method may be used to register a\n        function later rather than decorating.\n\n        The test is available in all templates, not only those under this\n        blueprint. Equivalent to :meth:`.Flask.template_test`.\n\n        :param name: The name to register the filter as. If not given, uses the\n            function's name.\n\n        .. versionadded:: 0.10\n        \"\"\"\n        if callable(name):\n            self.add_app_template_test(name)\n            return name\n\n        def decorator(f: T_template_test) -> T_template_test:\n            self.add_app_template_test(f, name=name)\n            return f\n\n        return decorator", "metadata": {"license": "BSD-3-Clause", "len_tokens": 210}}
{"id": "flask:src/flask/sansio/blueprints.py", "language": "python", "code": "def app_template_global(\n        self, name: T_template_global | str | None = None\n    ) -> T_template_global | t.Callable[[T_template_global], T_template_global]:\n        \"\"\"Decorate a function to register it as a custom Jinja global. The name\n        is optional. The decorator may be used without parentheses.\n\n        The :meth:`add_app_template_global` method may be used to register a\n        function later rather than decorating.\n\n        The global is available in all templates, not only those under this\n        blueprint. Equivalent to :meth:`.Flask.template_global`.\n\n        :param name: The name to register the global as. If not given, uses the\n            function's name.\n\n        .. versionadded:: 0.10\n        \"\"\"\n        if callable(name):\n            self.add_app_template_global(name)\n            return name\n\n        def decorator(f: T_template_global) -> T_template_global:\n            self.add_app_template_global(f, name=name)\n            return f\n\n        return decorator", "metadata": {"license": "BSD-3-Clause", "len_tokens": 210}}
{"id": "flask:src/flask/sansio/app.py", "language": "python", "code": "def logger(self) -> logging.Logger:\n        \"\"\"A standard Python :class:`~logging.Logger` for the app, with\n        the same name as :attr:`name`.\n\n        In debug mode, the logger's :attr:`~logging.Logger.level` will\n        be set to :data:`~logging.DEBUG`.\n\n        If there are no handlers configured, a default handler will be\n        added. See :doc:`/logging` for more information.\n\n        .. versionchanged:: 1.1.0\n            The logger takes the same name as :attr:`name` rather than\n            hard-coding ``\"flask.app\"``.\n\n        .. versionchanged:: 1.0.0\n            Behavior was simplified. The logger is always named\n            ``\"flask.app\"``. The level is only set during configuration,\n            it doesn't check ``app.debug`` each time. Only one format is\n            used, not different ones depending on ``app.debug``. No\n            handlers are removed, and a handler is only added if no\n            handlers are already configured.\n\n        .. versionadded:: 0.3\n        \"\"\"\n        return create_logger(self)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 241}}
{"id": "flask:src/flask/sansio/app.py", "language": "python", "code": "def register_blueprint(self, blueprint: Blueprint, **options: t.Any) -> None:\n        \"\"\"Register a :class:`~flask.Blueprint` on the application. Keyword\n        arguments passed to this method will override the defaults set on the\n        blueprint.\n\n        Calls the blueprint's :meth:`~flask.Blueprint.register` method after\n        recording the blueprint in the application's :attr:`blueprints`.\n\n        :param blueprint: The blueprint to register.\n        :param url_prefix: Blueprint routes will be prefixed with this.\n        :param subdomain: Blueprint routes will match on this subdomain.\n        :param url_defaults: Blueprint routes will use these default values for\n            view arguments.\n        :param options: Additional keyword arguments are passed to\n            :class:`~flask.blueprints.BlueprintSetupState`. They can be\n            accessed in :meth:`~flask.Blueprint.record` callbacks.\n\n        .. versionchanged:: 2.0.1\n            The ``name`` option can be used to change the (pre-dotted)\n            name the blueprint is registered with. This allows the same\n            blueprint to be registered multiple times with unique names\n            for ``url_for``.\n\n        .. versionadded:: 0.7\n        \"\"\"\n        blueprint.register(self, options)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 268}}
{"id": "flask:src/flask/sansio/app.py", "language": "python", "code": "def add_url_rule(\n        self,\n        rule: str,\n        endpoint: str | None = None,\n        view_func: ft.RouteCallable | None = None,\n        provide_automatic_options: bool | None = None,\n        **options: t.Any,\n    ) -> None:\n        if endpoint is None:\n            endpoint = _endpoint_from_view_func(view_func)  # type: ignore\n        options[\"endpoint\"] = endpoint\n        methods = options.pop(\"methods\", None)\n\n        # if the methods are not given and the view_func object knows its\n        # methods we can use that instead.  If neither exists, we go with\n        # a tuple of only ``GET`` as default.\n        if methods is None:\n            methods = getattr(view_func, \"methods\", None) or (\"GET\",)\n        if isinstance(methods, str):\n            raise TypeError(\n                \"Allowed methods must be a list of strings, for\"\n                ' example: @app.route(..., methods=[\"POST\"])'\n            )\n        methods = {item.upper() for item in methods}\n\n        # Methods that should always be added\n        required_methods: set[str] = set(getattr(view_func, \"required_methods\", ()))\n\n        # starting with Flask 0.8 the view_func object can disable and\n        # force-enable the automatic options handling.\n        if provide_automatic_options is None:\n            provide_automatic_options = getattr(\n                view_func, \"provide_automatic_options\", None\n            )\n\n        if provide_automatic_options is None:\n            if \"OPTIONS\" not in methods and self.config[\"PROVIDE_AUTOMATIC_OPTIONS\"]:\n                provide_automatic_options = True\n                required_methods.add(\"OPTIONS\")\n            else:\n                provide_automatic_options = False\n\n        # Add the required methods now.\n        methods |= required_methods\n\n        rule_obj = self.url_rule_class(rule, methods=methods, **options)\n        rule_obj.provide_automatic_options = provide_automatic_options  # type: ignore[attr-defined]\n\n        self.url_map.add(rule_obj)\n        if view_func is not None:\n            old_func = self.view_functions.get(endpoint)\n            if old_func is not None and old_func != view_func:\n                raise AssertionError(\n                    \"View function mapping is overwriting an existing\"\n                    f\" endpoint function: {endpoint}\"\n                )\n            self.view_functions[endpoint] = view_func", "metadata": {"license": "BSD-3-Clause", "len_tokens": 492}}
{"id": "flask:src/flask/sansio/app.py", "language": "python", "code": "def template_test(\n        self, name: T_template_test | str | None = None\n    ) -> T_template_test | t.Callable[[T_template_test], T_template_test]:\n        \"\"\"Decorate a function to register it as a custom Jinja test. The name\n        is optional. The decorator may be used without parentheses.\n\n        .. code-block:: python\n\n            @app.template_test(\"prime\")\n            def is_prime_test(n):\n                if n == 2:\n                    return True\n                for i in range(2, int(math.ceil(math.sqrt(n))) + 1):\n                    if n % i == 0:\n                        return False\n              return True\n\n        The :meth:`add_template_test` method may be used to register a function\n        later rather than decorating.\n\n        :param name: The name to register the filter as. If not given, uses the\n            function's name.\n\n        .. versionadded:: 0.10\n        \"\"\"\n        if callable(name):\n            self.add_template_test(name)\n            return name\n\n        def decorator(f: T_template_test) -> T_template_test:\n            self.add_template_test(f, name=name)\n            return f\n\n        return decorator", "metadata": {"license": "BSD-3-Clause", "len_tokens": 246}}
{"id": "flask:src/flask/sansio/app.py", "language": "python", "code": "def template_global(\n        self, name: T_template_global | str | None = None\n    ) -> T_template_global | t.Callable[[T_template_global], T_template_global]:\n        \"\"\"Decorate a function to register it as a custom Jinja global. The name\n        is optional. The decorator may be used without parentheses.\n\n        .. code-block:: python\n\n            @app.template_global\n            def double(n):\n                return 2 * n\n\n        The :meth:`add_template_global` method may be used to register a\n        function later rather than decorating.\n\n        :param name: The name to register the global as. If not given, uses the\n            function's name.\n\n        .. versionadded:: 0.10\n        \"\"\"\n        if callable(name):\n            self.add_template_global(name)\n            return name\n\n        def decorator(f: T_template_global) -> T_template_global:\n            self.add_template_global(f, name=name)\n            return f\n\n        return decorator", "metadata": {"license": "BSD-3-Clause", "len_tokens": 202}}
{"id": "flask:src/flask/sansio/app.py", "language": "python", "code": "def teardown_appcontext(self, f: T_teardown) -> T_teardown:\n        \"\"\"Registers a function to be called when the app context is popped. The\n        context is popped at the end of a request, CLI command, or manual ``with``\n        block.\n\n        .. code-block:: python\n\n            with app.app_context():\n                ...\n\n        When the ``with`` block exits (or ``ctx.pop()`` is called), the\n        teardown functions are called just before the app context is\n        made inactive.\n\n        When a teardown function was called because of an unhandled\n        exception it will be passed an error object. If an\n        :meth:`errorhandler` is registered, it will handle the exception\n        and the teardown will not receive it.\n\n        Teardown functions must avoid raising exceptions. If they\n        execute code that might fail they must surround that code with a\n        ``try``/``except`` block and log any errors.\n\n        The return values of teardown functions are ignored.\n\n        .. versionadded:: 0.9\n        \"\"\"\n        self.teardown_appcontext_funcs.append(f)\n        return f", "metadata": {"license": "BSD-3-Clause", "len_tokens": 234}}
{"id": "flask:src/flask/sansio/app.py", "language": "python", "code": "def trap_http_exception(self, e: Exception) -> bool:\n        \"\"\"Checks if an HTTP exception should be trapped or not.  By default\n        this will return ``False`` for all exceptions except for a bad request\n        key error if ``TRAP_BAD_REQUEST_ERRORS`` is set to ``True``.  It\n        also returns ``True`` if ``TRAP_HTTP_EXCEPTIONS`` is set to ``True``.\n\n        This is called for all HTTP exceptions raised by a view function.\n        If it returns ``True`` for any exception the error handler for this\n        exception is not called and it shows up as regular exception in the\n        traceback.  This is helpful for debugging implicitly raised HTTP\n        exceptions.\n\n        .. versionchanged:: 1.0\n            Bad request errors are not trapped by default in debug mode.\n\n        .. versionadded:: 0.8\n        \"\"\"\n        if self.config[\"TRAP_HTTP_EXCEPTIONS\"]:\n            return True\n\n        trap_bad_request = self.config[\"TRAP_BAD_REQUEST_ERRORS\"]\n\n        # if unset, trap key errors in debug mode\n        if (\n            trap_bad_request is None\n            and self.debug\n            and isinstance(e, BadRequestKeyError)\n        ):\n            return True\n\n        if trap_bad_request:\n            return isinstance(e, BadRequest)\n\n        return False", "metadata": {"license": "BSD-3-Clause", "len_tokens": 274}}
{"id": "flask:src/flask/sansio/app.py", "language": "python", "code": "def handle_url_build_error(\n        self, error: BuildError, endpoint: str, values: dict[str, t.Any]\n    ) -> str:\n        \"\"\"Called by :meth:`.url_for` if a\n        :exc:`~werkzeug.routing.BuildError` was raised. If this returns\n        a value, it will be returned by ``url_for``, otherwise the error\n        will be re-raised.\n\n        Each function in :attr:`url_build_error_handlers` is called with\n        ``error``, ``endpoint`` and ``values``. If a function returns\n        ``None`` or raises a ``BuildError``, it is skipped. Otherwise,\n        its return value is returned by ``url_for``.\n\n        :param error: The active ``BuildError`` being handled.\n        :param endpoint: The endpoint being built.\n        :param values: The keyword arguments passed to ``url_for``.\n        \"\"\"\n        for handler in self.url_build_error_handlers:\n            try:\n                rv = handler(error, endpoint, values)\n            except BuildError as e:\n                # make error available outside except block\n                error = e\n            else:\n                if rv is not None:\n                    return rv\n\n        # Re-raise if called with an active exception, otherwise raise\n        # the passed in exception.\n        if error is sys.exc_info()[1]:\n            raise\n\n        raise error", "metadata": {"license": "BSD-3-Clause", "len_tokens": 288}}
{"id": "flask:src/flask/sansio/scaffold.py", "language": "python", "code": "def _find_package_path(import_name: str) -> str:\n    \"\"\"Find the path that contains the package or module.\"\"\"\n    root_mod_name, _, _ = import_name.partition(\".\")\n\n    try:\n        root_spec = importlib.util.find_spec(root_mod_name)\n\n        if root_spec is None:\n            raise ValueError(\"not found\")\n    except (ImportError, ValueError):\n        # ImportError: the machinery told us it does not exist\n        # ValueError:\n        #    - the module name was invalid\n        #    - the module name is __main__\n        #    - we raised `ValueError` due to `root_spec` being `None`\n        return os.getcwd()\n\n    if root_spec.submodule_search_locations:\n        if root_spec.origin is None or root_spec.origin == \"namespace\":\n            # namespace package\n            package_spec = importlib.util.find_spec(import_name)\n\n            if package_spec is not None and package_spec.submodule_search_locations:\n                # Pick the path in the namespace that contains the submodule.\n                package_path = pathlib.Path(\n                    os.path.commonpath(package_spec.submodule_search_locations)\n                )\n                search_location = next(\n                    location\n                    for location in root_spec.submodule_search_locations\n                    if package_path.is_relative_to(location)\n                )\n            else:\n                # Pick the first path.\n                search_location = root_spec.submodule_search_locations[0]\n\n            return os.path.dirname(search_location)\n        else:\n            # package with __init__.py\n            return os.path.dirname(os.path.dirname(root_spec.origin))\n    else:\n        # module\n        return os.path.dirname(root_spec.origin)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 336}}
{"id": "flask:src/flask/sansio/scaffold.py", "language": "python", "code": "def find_package(import_name: str) -> tuple[str | None, str]:\n    \"\"\"Find the prefix that a package is installed under, and the path\n    that it would be imported from.\n\n    The prefix is the directory containing the standard directory\n    hierarchy (lib, bin, etc.). If the package is not installed to the\n    system (:attr:`sys.prefix`) or a virtualenv (``site-packages``),\n    ``None`` is returned.\n\n    The path is the entry in :attr:`sys.path` that contains the package\n    for import. If the package is not installed, it's assumed that the\n    package was imported from the current working directory.\n    \"\"\"\n    package_path = _find_package_path(import_name)\n    py_prefix = os.path.abspath(sys.prefix)\n\n    # installed to the system\n    if pathlib.PurePath(package_path).is_relative_to(py_prefix):\n        return py_prefix, package_path\n\n    site_parent, site_folder = os.path.split(package_path)\n\n    # installed to a virtualenv\n    if site_folder.lower() == \"site-packages\":\n        parent, folder = os.path.split(site_parent)\n\n        # Windows (prefix/lib/site-packages)\n        if folder.lower() == \"lib\":\n            return parent, package_path\n\n        # Unix (prefix/lib/pythonX.Y/site-packages)\n        if os.path.basename(parent).lower() == \"lib\":\n            return os.path.dirname(parent), package_path\n\n        # something else (prefix/site-packages)\n        return site_parent, package_path\n\n    # not installed\n    return None, package_path", "metadata": {"license": "BSD-3-Clause", "len_tokens": 327}}
{"id": "flask:src/flask/sansio/scaffold.py", "language": "python", "code": "def route(self, rule: str, **options: t.Any) -> t.Callable[[T_route], T_route]:\n        \"\"\"Decorate a view function to register it with the given URL\n        rule and options. Calls :meth:`add_url_rule`, which has more\n        details about the implementation.\n\n        .. code-block:: python\n\n            @app.route(\"/\")\n            def index():\n                return \"Hello, World!\"\n\n        See :ref:`url-route-registrations`.\n\n        The endpoint name for the route defaults to the name of the view\n        function if the ``endpoint`` parameter isn't passed.\n\n        The ``methods`` parameter defaults to ``[\"GET\"]``. ``HEAD`` and\n        ``OPTIONS`` are added automatically.\n\n        :param rule: The URL rule string.\n        :param options: Extra options passed to the\n            :class:`~werkzeug.routing.Rule` object.\n        \"\"\"\n\n        def decorator(f: T_route) -> T_route:\n            endpoint = options.pop(\"endpoint\", None)\n            self.add_url_rule(rule, endpoint, f, **options)\n            return f\n\n        return decorator", "metadata": {"license": "BSD-3-Clause", "len_tokens": 228}}
{"id": "flask:src/flask/sansio/scaffold.py", "language": "python", "code": "def add_url_rule(\n        self,\n        rule: str,\n        endpoint: str | None = None,\n        view_func: ft.RouteCallable | None = None,\n        provide_automatic_options: bool | None = None,\n        **options: t.Any,\n    ) -> None:\n        \"\"\"Register a rule for routing incoming requests and building\n        URLs. The :meth:`route` decorator is a shortcut to call this\n        with the ``view_func`` argument. These are equivalent:\n\n        .. code-block:: python\n\n            @app.route(\"/\")\n            def index():\n                ...\n\n        .. code-block:: python\n\n            def index():\n                ...\n\n            app.add_url_rule(\"/\", view_func=index)\n\n        See :ref:`url-route-registrations`.\n\n        The endpoint name for the route defaults to the name of the view\n        function if the ``endpoint`` parameter isn't passed. An error\n        will be raised if a function has already been registered for the\n        endpoint.\n\n        The ``methods`` parameter defaults to ``[\"GET\"]``. ``HEAD`` is\n        always added automatically, and ``OPTIONS`` is added\n        automatically by default.\n\n        ``view_func`` does not necessarily need to be passed, but if the\n        rule should participate in routing an endpoint name must be\n        associated with a view function at some point with the\n        :meth:`endpoint` decorator.\n\n        .. code-block:: python\n\n            app.add_url_rule(\"/\", endpoint=\"index\")\n\n            @app.endpoint(\"index\")\n            def index():\n                ...\n\n        If ``view_func`` has a ``required_methods`` attribute, those\n        methods are added to the passed and automatic methods. If it\n        has a ``provide_automatic_methods`` attribute, it is used as the\n        default if the parameter is not passed.\n\n        :param rule: The URL rule string.\n        :param endpoint: The endpoint name to associate with the rule\n            and view function. Used when routing and building URLs.\n            Defaults to ``view_func.__name__``.\n        :param view_func: The view function to associate with the\n            endpoint name.\n        :param provide_automatic_options: Add the ``OPTIONS`` method and\n            respond to ``OPTIONS`` requests automatically.\n        :param options: Extra options passed to the\n            :class:`~werkzeug.routing.Rule` object.\n        \"\"\"\n        raise NotImplementedError", "metadata": {"license": "BSD-3-Clause", "len_tokens": 489}}
{"id": "flask:src/flask/sansio/scaffold.py", "language": "python", "code": "def before_request(self, f: T_before_request) -> T_before_request:\n        \"\"\"Register a function to run before each request.\n\n        For example, this can be used to open a database connection, or\n        to load the logged in user from the session.\n\n        .. code-block:: python\n\n            @app.before_request\n            def load_user():\n                if \"user_id\" in session:\n                    g.user = db.session.get(session[\"user_id\"])\n\n        The function will be called without any arguments. If it returns\n        a non-``None`` value, the value is handled as if it was the\n        return value from the view, and further request handling is\n        stopped.\n\n        This is available on both app and blueprint objects. When used on an app, this\n        executes before every request. When used on a blueprint, this executes before\n        every request that the blueprint handles. To register with a blueprint and\n        execute before every request, use :meth:`.Blueprint.before_app_request`.\n        \"\"\"\n        self.before_request_funcs.setdefault(None, []).append(f)\n        return f", "metadata": {"license": "BSD-3-Clause", "len_tokens": 225}}
{"id": "flask:src/flask/sansio/scaffold.py", "language": "python", "code": "def after_request(self, f: T_after_request) -> T_after_request:\n        \"\"\"Register a function to run after each request to this object.\n\n        The function is called with the response object, and must return\n        a response object. This allows the functions to modify or\n        replace the response before it is sent.\n\n        If a function raises an exception, any remaining\n        ``after_request`` functions will not be called. Therefore, this\n        should not be used for actions that must execute, such as to\n        close resources. Use :meth:`teardown_request` for that.\n\n        This is available on both app and blueprint objects. When used on an app, this\n        executes after every request. When used on a blueprint, this executes after\n        every request that the blueprint handles. To register with a blueprint and\n        execute after every request, use :meth:`.Blueprint.after_app_request`.\n        \"\"\"\n        self.after_request_funcs.setdefault(None, []).append(f)\n        return f", "metadata": {"license": "BSD-3-Clause", "len_tokens": 205}}
{"id": "flask:src/flask/sansio/scaffold.py", "language": "python", "code": "def teardown_request(self, f: T_teardown) -> T_teardown:\n        \"\"\"Register a function to be called when the request context is\n        popped. Typically, this happens at the end of each request, but\n        contexts may be pushed manually during testing.\n\n        .. code-block:: python\n\n            with app.test_request_context():\n                ...\n\n        When the ``with`` block exits (or ``ctx.pop()`` is called), the\n        teardown functions are called just before the request context is\n        made inactive.\n\n        When a teardown function was called because of an unhandled\n        exception it will be passed an error object. If an\n        :meth:`errorhandler` is registered, it will handle the exception\n        and the teardown will not receive it.\n\n        Teardown functions must avoid raising exceptions. If they\n        execute code that might fail they must surround that code with a\n        ``try``/``except`` block and log any errors.\n\n        The return values of teardown functions are ignored.\n\n        This is available on both app and blueprint objects. When used on an app, this\n        executes after every request. When used on a blueprint, this executes after\n        every request that the blueprint handles. To register with a blueprint and\n        execute after every request, use :meth:`.Blueprint.teardown_app_request`.\n        \"\"\"\n        self.teardown_request_funcs.setdefault(None, []).append(f)\n        return f", "metadata": {"license": "BSD-3-Clause", "len_tokens": 293}}
{"id": "flask:src/flask/sansio/scaffold.py", "language": "python", "code": "def url_value_preprocessor(\n        self,\n        f: T_url_value_preprocessor,\n    ) -> T_url_value_preprocessor:\n        \"\"\"Register a URL value preprocessor function for all view\n        functions in the application. These functions will be called before the\n        :meth:`before_request` functions.\n\n        The function can modify the values captured from the matched url before\n        they are passed to the view. For example, this can be used to pop a\n        common language code value and place it in ``g`` rather than pass it to\n        every view.\n\n        The function is passed the endpoint name and values dict. The return\n        value is ignored.\n\n        This is available on both app and blueprint objects. When used on an app, this\n        is called for every request. When used on a blueprint, this is called for\n        requests that the blueprint handles. To register with a blueprint and affect\n        every request, use :meth:`.Blueprint.app_url_value_preprocessor`.\n        \"\"\"\n        self.url_value_preprocessors[None].append(f)\n        return f", "metadata": {"license": "BSD-3-Clause", "len_tokens": 223}}
{"id": "flask:src/flask/sansio/scaffold.py", "language": "python", "code": "def errorhandler(\n        self, code_or_exception: type[Exception] | int\n    ) -> t.Callable[[T_error_handler], T_error_handler]:\n        \"\"\"Register a function to handle errors by code or exception class.\n\n        A decorator that is used to register a function given an\n        error code.  Example::\n\n            @app.errorhandler(404)\n            def page_not_found(error):\n                return 'This page does not exist', 404\n\n        You can also register handlers for arbitrary exceptions::\n\n            @app.errorhandler(DatabaseError)\n            def special_exception_handler(error):\n                return 'Database connection failed', 500\n\n        This is available on both app and blueprint objects. When used on an app, this\n        can handle errors from every request. When used on a blueprint, this can handle\n        errors from requests that the blueprint handles. To register with a blueprint\n        and affect every request, use :meth:`.Blueprint.app_errorhandler`.\n\n        .. versionadded:: 0.7\n            Use :meth:`register_error_handler` instead of modifying\n            :attr:`error_handler_spec` directly, for application wide error\n            handlers.\n\n        .. versionadded:: 0.7\n           One can now additionally also register custom exception types\n           that do not necessarily have to be a subclass of the\n           :class:`~werkzeug.exceptions.HTTPException` class.\n\n        :param code_or_exception: the code as integer for the handler, or\n                                  an arbitrary exception\n        \"\"\"\n\n        def decorator(f: T_error_handler) -> T_error_handler:\n            self.register_error_handler(code_or_exception, f)\n            return f\n\n        return decorator", "metadata": {"license": "BSD-3-Clause", "len_tokens": 341}}
{"id": "flask:src/flask/sansio/scaffold.py", "language": "python", "code": "def _get_exc_class_and_code(\n        exc_class_or_code: type[Exception] | int,\n    ) -> tuple[type[Exception], int | None]:\n        \"\"\"Get the exception class being handled. For HTTP status codes\n        or ``HTTPException`` subclasses, return both the exception and\n        status code.\n\n        :param exc_class_or_code: Any exception class, or an HTTP status\n            code as an integer.\n        \"\"\"\n        exc_class: type[Exception]\n\n        if isinstance(exc_class_or_code, int):\n            try:\n                exc_class = default_exceptions[exc_class_or_code]\n            except KeyError:\n                raise ValueError(\n                    f\"'{exc_class_or_code}' is not a recognized HTTP\"\n                    \" error code. Use a subclass of HTTPException with\"\n                    \" that code instead.\"\n                ) from None\n        else:\n            exc_class = exc_class_or_code\n\n        if isinstance(exc_class, Exception):\n            raise TypeError(\n                f\"{exc_class!r} is an instance, not a class. Handlers\"\n                \" can only be registered for Exception classes or HTTP\"\n                \" error codes.\"\n            )\n\n        if not issubclass(exc_class, Exception):\n            raise ValueError(\n                f\"'{exc_class.__name__}' is not a subclass of Exception.\"\n                \" Handlers can only be registered for Exception classes\"\n                \" or HTTP error codes.\"\n            )\n\n        if issubclass(exc_class, HTTPException):\n            return exc_class, exc_class.code\n        else:\n            return exc_class, None", "metadata": {"license": "BSD-3-Clause", "len_tokens": 319}}
{"id": "flask:src/flask/json/provider.py", "language": "python", "code": "class JSONProvider:\n    \"\"\"A standard set of JSON operations for an application. Subclasses\n    of this can be used to customize JSON behavior or use different\n    JSON libraries.\n\n    To implement a provider for a specific library, subclass this base\n    class and implement at least :meth:`dumps` and :meth:`loads`. All\n    other methods have default implementations.\n\n    To use a different provider, either subclass ``Flask`` and set\n    :attr:`~flask.Flask.json_provider_class` to a provider class, or set\n    :attr:`app.json <flask.Flask.json>` to an instance of the class.\n\n    :param app: An application instance. This will be stored as a\n        :class:`weakref.proxy` on the :attr:`_app` attribute.\n\n    .. versionadded:: 2.2\n    \"\"\"\n\n    def __init__(self, app: App) -> None:\n        self._app: App = weakref.proxy(app)\n\n    def dumps(self, obj: t.Any, **kwargs: t.Any) -> str:\n        \"\"\"Serialize data as JSON.\n\n        :param obj: The data to serialize.\n        :param kwargs: May be passed to the underlying JSON library.\n        \"\"\"\n        raise NotImplementedError\n\n    def dump(self, obj: t.Any, fp: t.IO[str], **kwargs: t.Any) -> None:\n        \"\"\"Serialize data as JSON and write to a file.\n\n        :param obj: The data to serialize.\n        :param fp: A file opened for writing text. Should use the UTF-8\n            encoding to be valid JSON.\n        :param kwargs: May be passed to the underlying JSON library.\n        \"\"\"\n        fp.write(self.dumps(obj, **kwargs))\n\n    def loads(self, s: str | bytes, **kwargs: t.Any) -> t.Any:\n        \"\"\"Deserialize data as JSON.\n\n        :param s: Text or UTF-8 bytes.\n        :param kwargs: May be passed to the underlying JSON library.\n        \"\"\"\n        raise NotImplementedError\n\n    def load(self, fp: t.IO[t.AnyStr], **kwargs: t.Any) -> t.Any:\n        \"\"\"Deserialize data as JSON read from a file.\n\n        :param fp: A file opened for reading text or UTF-8 bytes.\n        :param kwargs: May be passed to the underlying JSON library.\n        \"\"\"\n        return self.loads(fp.read(), **kwargs)\n\n    def _prepare_response_obj(\n        self, args: tuple[t.Any, ...], kwargs: dict[str, t.Any]\n    ) -> t.Any:\n        if args and kwargs:\n            raise TypeError(\"app.json.response() takes either args or kwargs, not both\")\n\n        if not args and not kwargs:\n            return None\n\n        if len(args) == 1:\n            return args[0]\n\n        return args or kwargs\n\n    def response(self, *args: t.Any, **kwargs: t.Any) -> Response:\n        \"\"\"Serialize the given arguments as JSON, and return a\n        :class:`~flask.Response` object with the ``application/json``\n        mimetype.\n\n        The :func:`~flask.json.jsonify` function calls this method for\n        the current application.\n\n        Either positional or keyword arguments can be given, not both.\n        If no arguments are given, ``None`` is serialized.\n\n        :param args: A single value to serialize, or multiple values to\n            treat as a list to serialize.\n        :param kwargs: Treat as a dict to serialize.\n        \"\"\"\n        obj = self._prepare_response_obj(args, kwargs)\n        return self._app.response_class(self.dumps(obj), mimetype=\"application/json\")", "metadata": {"license": "BSD-3-Clause", "len_tokens": 762}}
{"id": "flask:src/flask/json/provider.py", "language": "python", "code": "def response(self, *args: t.Any, **kwargs: t.Any) -> Response:\n        \"\"\"Serialize the given arguments as JSON, and return a\n        :class:`~flask.Response` object with it. The response mimetype\n        will be \"application/json\" and can be changed with\n        :attr:`mimetype`.\n\n        If :attr:`compact` is ``False`` or debug mode is enabled, the\n        output will be formatted to be easier to read.\n\n        Either positional or keyword arguments can be given, not both.\n        If no arguments are given, ``None`` is serialized.\n\n        :param args: A single value to serialize, or multiple values to\n            treat as a list to serialize.\n        :param kwargs: Treat as a dict to serialize.\n        \"\"\"\n        obj = self._prepare_response_obj(args, kwargs)\n        dump_args: dict[str, t.Any] = {}\n\n        if (self.compact is None and self._app.debug) or self.compact is False:\n            dump_args.setdefault(\"indent\", 2)\n        else:\n            dump_args.setdefault(\"separators\", (\",\", \":\"))\n\n        return self._app.response_class(\n            f\"{self.dumps(obj, **dump_args)}\\n\", mimetype=self.mimetype\n        )", "metadata": {"license": "BSD-3-Clause", "len_tokens": 266}}
{"id": "flask:src/flask/json/__init__.py", "language": "python", "code": "def dumps(obj: t.Any, **kwargs: t.Any) -> str:\n    \"\"\"Serialize data as JSON.\n\n    If :data:`~flask.current_app` is available, it will use its\n    :meth:`app.json.dumps() <flask.json.provider.JSONProvider.dumps>`\n    method, otherwise it will use :func:`json.dumps`.\n\n    :param obj: The data to serialize.\n    :param kwargs: Arguments passed to the ``dumps`` implementation.\n\n    .. versionchanged:: 2.3\n        The ``app`` parameter was removed.\n\n    .. versionchanged:: 2.2\n        Calls ``current_app.json.dumps``, allowing an app to override\n        the behavior.\n\n    .. versionchanged:: 2.0.2\n        :class:`decimal.Decimal` is supported by converting to a string.\n\n    .. versionchanged:: 2.0\n        ``encoding`` will be removed in Flask 2.1.\n\n    .. versionchanged:: 1.0.3\n        ``app`` can be passed directly, rather than requiring an app\n        context for configuration.\n    \"\"\"\n    if current_app:\n        return current_app.json.dumps(obj, **kwargs)\n\n    kwargs.setdefault(\"default\", _default)\n    return _json.dumps(obj, **kwargs)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 265}}
{"id": "flask:src/flask/json/__init__.py", "language": "python", "code": "def dump(obj: t.Any, fp: t.IO[str], **kwargs: t.Any) -> None:\n    \"\"\"Serialize data as JSON and write to a file.\n\n    If :data:`~flask.current_app` is available, it will use its\n    :meth:`app.json.dump() <flask.json.provider.JSONProvider.dump>`\n    method, otherwise it will use :func:`json.dump`.\n\n    :param obj: The data to serialize.\n    :param fp: A file opened for writing text. Should use the UTF-8\n        encoding to be valid JSON.\n    :param kwargs: Arguments passed to the ``dump`` implementation.\n\n    .. versionchanged:: 2.3\n        The ``app`` parameter was removed.\n\n    .. versionchanged:: 2.2\n        Calls ``current_app.json.dump``, allowing an app to override\n        the behavior.\n\n    .. versionchanged:: 2.0\n        Writing to a binary file, and the ``encoding`` argument, will be\n        removed in Flask 2.1.\n    \"\"\"\n    if current_app:\n        current_app.json.dump(obj, fp, **kwargs)\n    else:\n        kwargs.setdefault(\"default\", _default)\n        _json.dump(obj, fp, **kwargs)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 259}}
{"id": "flask:src/flask/json/__init__.py", "language": "python", "code": "def loads(s: str | bytes, **kwargs: t.Any) -> t.Any:\n    \"\"\"Deserialize data as JSON.\n\n    If :data:`~flask.current_app` is available, it will use its\n    :meth:`app.json.loads() <flask.json.provider.JSONProvider.loads>`\n    method, otherwise it will use :func:`json.loads`.\n\n    :param s: Text or UTF-8 bytes.\n    :param kwargs: Arguments passed to the ``loads`` implementation.\n\n    .. versionchanged:: 2.3\n        The ``app`` parameter was removed.\n\n    .. versionchanged:: 2.2\n        Calls ``current_app.json.loads``, allowing an app to override\n        the behavior.\n\n    .. versionchanged:: 2.0\n        ``encoding`` will be removed in Flask 2.1. The data must be a\n        string or UTF-8 bytes.\n\n    .. versionchanged:: 1.0.3\n        ``app`` can be passed directly, rather than requiring an app\n        context for configuration.\n    \"\"\"\n    if current_app:\n        return current_app.json.loads(s, **kwargs)\n\n    return _json.loads(s, **kwargs)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 246}}
{"id": "flask:src/flask/json/__init__.py", "language": "python", "code": "def load(fp: t.IO[t.AnyStr], **kwargs: t.Any) -> t.Any:\n    \"\"\"Deserialize data as JSON read from a file.\n\n    If :data:`~flask.current_app` is available, it will use its\n    :meth:`app.json.load() <flask.json.provider.JSONProvider.load>`\n    method, otherwise it will use :func:`json.load`.\n\n    :param fp: A file opened for reading text or UTF-8 bytes.\n    :param kwargs: Arguments passed to the ``load`` implementation.\n\n    .. versionchanged:: 2.3\n        The ``app`` parameter was removed.\n\n    .. versionchanged:: 2.2\n        Calls ``current_app.json.load``, allowing an app to override\n        the behavior.\n\n    .. versionchanged:: 2.2\n        The ``app`` parameter will be removed in Flask 2.3.\n\n    .. versionchanged:: 2.0\n        ``encoding`` will be removed in Flask 2.1. The file must be text\n        mode, or binary mode with UTF-8 bytes.\n    \"\"\"\n    if current_app:\n        return current_app.json.load(fp, **kwargs)\n\n    return _json.load(fp, **kwargs)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 255}}
{"id": "flask:src/flask/json/__init__.py", "language": "python", "code": "def jsonify(*args: t.Any, **kwargs: t.Any) -> Response:\n    \"\"\"Serialize the given arguments as JSON, and return a\n    :class:`~flask.Response` object with the ``application/json``\n    mimetype. A dict or list returned from a view will be converted to a\n    JSON response automatically without needing to call this.\n\n    This requires an active app context, and calls\n    :meth:`app.json.response() <flask.json.provider.JSONProvider.response>`.\n\n    In debug mode, the output is formatted with indentation to make it\n    easier to read. This may also be controlled by the provider.\n\n    Either positional or keyword arguments can be given, not both.\n    If no arguments are given, ``None`` is serialized.\n\n    :param args: A single value to serialize, or multiple values to\n        treat as a list to serialize.\n    :param kwargs: Treat as a dict to serialize.\n\n    .. versionchanged:: 2.2\n        Calls ``current_app.json.response``, allowing an app to override\n        the behavior.\n\n    .. versionchanged:: 2.0.2\n        :class:`decimal.Decimal` is supported by converting to a string.\n\n    .. versionchanged:: 0.11\n        Added support for serializing top-level arrays. This was a\n        security risk in ancient browsers. See :ref:`security-json`.\n\n    .. versionadded:: 0.2\n    \"\"\"\n    return current_app.json.response(*args, **kwargs)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 313}}
{"id": "flask:src/flask/json/tag.py", "language": "python", "code": "class JSONTag:\n    \"\"\"Base class for defining type tags for :class:`TaggedJSONSerializer`.\"\"\"\n\n    __slots__ = (\"serializer\",)\n\n    #: The tag to mark the serialized object with. If empty, this tag is\n    #: only used as an intermediate step during tagging.\n    key: str = \"\"\n\n    def __init__(self, serializer: TaggedJSONSerializer) -> None:\n        \"\"\"Create a tagger for the given serializer.\"\"\"\n        self.serializer = serializer\n\n    def check(self, value: t.Any) -> bool:\n        \"\"\"Check if the given value should be tagged by this tag.\"\"\"\n        raise NotImplementedError\n\n    def to_json(self, value: t.Any) -> t.Any:\n        \"\"\"Convert the Python object to an object that is a valid JSON type.\n        The tag will be added later.\"\"\"\n        raise NotImplementedError\n\n    def to_python(self, value: t.Any) -> t.Any:\n        \"\"\"Convert the JSON representation back to the correct type. The tag\n        will already be removed.\"\"\"\n        raise NotImplementedError\n\n    def tag(self, value: t.Any) -> dict[str, t.Any]:\n        \"\"\"Convert the value to a valid JSON type and add the tag structure\n        around it.\"\"\"\n        return {self.key: self.to_json(value)}", "metadata": {"license": "BSD-3-Clause", "len_tokens": 266}}
{"id": "flask:src/flask/json/tag.py", "language": "python", "code": "class TaggedJSONSerializer:\n    \"\"\"Serializer that uses a tag system to compactly represent objects that\n    are not JSON types. Passed as the intermediate serializer to\n    :class:`itsdangerous.Serializer`.\n\n    The following extra types are supported:\n\n    * :class:`dict`\n    * :class:`tuple`\n    * :class:`bytes`\n    * :class:`~markupsafe.Markup`\n    * :class:`~uuid.UUID`\n    * :class:`~datetime.datetime`\n    \"\"\"\n\n    __slots__ = (\"tags\", \"order\")\n\n    #: Tag classes to bind when creating the serializer. Other tags can be\n    #: added later using :meth:`~register`.\n    default_tags = [\n        TagDict,\n        PassDict,\n        TagTuple,\n        PassList,\n        TagBytes,\n        TagMarkup,\n        TagUUID,\n        TagDateTime,\n    ]\n\n    def __init__(self) -> None:\n        self.tags: dict[str, JSONTag] = {}\n        self.order: list[JSONTag] = []\n\n        for cls in self.default_tags:\n            self.register(cls)\n\n    def register(\n        self,\n        tag_class: type[JSONTag],\n        force: bool = False,\n        index: int | None = None,\n    ) -> None:\n        \"\"\"Register a new tag with this serializer.\n\n        :param tag_class: tag class to register. Will be instantiated with this\n            serializer instance.\n        :param force: overwrite an existing tag. If false (default), a\n            :exc:`KeyError` is raised.\n        :param index: index to insert the new tag in the tag order. Useful when\n            the new tag is a special case of an existing tag. If ``None``\n            (default), the tag is appended to the end of the order.\n\n        :raise KeyError: if the tag key is already registered and ``force`` is\n            not true.\n        \"\"\"\n        tag = tag_class(self)\n        key = tag.key\n\n        if key:\n            if not force and key in self.tags:\n                raise KeyError(f\"Tag '{key}' is already registered.\")\n\n            self.tags[key] = tag\n\n        if index is None:\n            self.order.append(tag)\n        else:\n            self.order.insert(index, tag)\n\n    def tag(self, value: t.Any) -> t.Any:\n        \"\"\"Convert a value to a tagged representation if necessary.\"\"\"\n        for tag in self.order:\n            if tag.check(value):\n                return tag.tag(value)\n\n        return value\n\n    def untag(self, value: dict[str, t.Any]) -> t.Any:\n        \"\"\"Convert a tagged representation back to the original type.\"\"\"\n        if len(value) != 1:\n            return value\n\n        key = next(iter(value))\n\n        if key not in self.tags:\n            return value\n\n        return self.tags[key].to_python(value[key])\n\n    def _untag_scan(self, value: t.Any) -> t.Any:\n        if isinstance(value, dict):\n            # untag each item recursively\n            value = {k: self._untag_scan(v) for k, v in value.items()}\n            # untag the dict itself\n            value = self.untag(value)\n        elif isinstance(value, list):\n            # untag each item recursively\n            value = [self._untag_scan(item) for item in value]\n\n        return value\n\n    def dumps(self, value: t.Any) -> str:\n        \"\"\"Tag the value and dump it to a compact JSON string.\"\"\"\n        return dumps(self.tag(value), separators=(\",\", \":\"))\n\n    def loads(self, value: str) -> t.Any:\n        \"\"\"Load data from a JSON string and deserialized any tagged objects.\"\"\"\n        return self._untag_scan(loads(value))", "metadata": {"license": "BSD-3-Clause", "len_tokens": 776}}
{"id": "flask:src/flask/json/tag.py", "language": "python", "code": "def register(\n        self,\n        tag_class: type[JSONTag],\n        force: bool = False,\n        index: int | None = None,\n    ) -> None:\n        \"\"\"Register a new tag with this serializer.\n\n        :param tag_class: tag class to register. Will be instantiated with this\n            serializer instance.\n        :param force: overwrite an existing tag. If false (default), a\n            :exc:`KeyError` is raised.\n        :param index: index to insert the new tag in the tag order. Useful when\n            the new tag is a special case of an existing tag. If ``None``\n            (default), the tag is appended to the end of the order.\n\n        :raise KeyError: if the tag key is already registered and ``force`` is\n            not true.\n        \"\"\"\n        tag = tag_class(self)\n        key = tag.key\n\n        if key:\n            if not force and key in self.tags:\n                raise KeyError(f\"Tag '{key}' is already registered.\")\n\n            self.tags[key] = tag\n\n        if index is None:\n            self.order.append(tag)\n        else:\n            self.order.insert(index, tag)", "metadata": {"license": "BSD-3-Clause", "len_tokens": 243}}
{"id": "requests:src/requests/cookies.py", "language": "python", "code": "class MockRequest:\n    \"\"\"Wraps a `requests.Request` to mimic a `urllib2.Request`.\n\n    The code in `http.cookiejar.CookieJar` expects this interface in order to correctly\n    manage cookie policies, i.e., determine whether a cookie can be set, given the\n    domains of the request and the cookie.\n\n    The original request object is read-only. The client is responsible for collecting\n    the new headers via `get_new_headers()` and interpreting them appropriately. You\n    probably want `get_cookie_header`, defined below.\n    \"\"\"\n\n    def __init__(self, request):\n        self._r = request\n        self._new_headers = {}\n        self.type = urlparse(self._r.url).scheme\n\n    def get_type(self):\n        return self.type\n\n    def get_host(self):\n        return urlparse(self._r.url).netloc\n\n    def get_origin_req_host(self):\n        return self.get_host()\n\n    def get_full_url(self):\n        # Only return the response's URL if the user hadn't set the Host\n        # header\n        if not self._r.headers.get(\"Host\"):\n            return self._r.url\n        # If they did set it, retrieve it and reconstruct the expected domain\n        host = to_native_string(self._r.headers[\"Host\"], encoding=\"utf-8\")\n        parsed = urlparse(self._r.url)\n        # Reconstruct the URL as we expect it\n        return urlunparse(\n            [\n                parsed.scheme,\n                host,\n                parsed.path,\n                parsed.params,\n                parsed.query,\n                parsed.fragment,\n            ]\n        )\n\n    def is_unverifiable(self):\n        return True\n\n    def has_header(self, name):\n        return name in self._r.headers or name in self._new_headers\n\n    def get_header(self, name, default=None):\n        return self._r.headers.get(name, self._new_headers.get(name, default))\n\n    def add_header(self, key, val):\n        \"\"\"cookiejar has no legitimate use for this method; add it back if you find one.\"\"\"\n        raise NotImplementedError(\n            \"Cookie headers should be added with add_unredirected_header()\"\n        )\n\n    def add_unredirected_header(self, name, value):\n        self._new_headers[name] = value\n\n    def get_new_headers(self):\n        return self._new_headers\n\n    @property\n    def unverifiable(self):\n        return self.is_unverifiable()\n\n    @property\n    def origin_req_host(self):\n        return self.get_origin_req_host()\n\n    @property\n    def host(self):\n        return self.get_host()", "metadata": {"license": "Apache-2.0", "len_tokens": 536}}
{"id": "requests:src/requests/cookies.py", "language": "python", "code": "def create_cookie(name, value, **kwargs):\n    \"\"\"Make a cookie from underspecified parameters.\n\n    By default, the pair of `name` and `value` will be set for the domain ''\n    and sent on every request (this is sometimes called a \"supercookie\").\n    \"\"\"\n    result = {\n        \"version\": 0,\n        \"name\": name,\n        \"value\": value,\n        \"port\": None,\n        \"domain\": \"\",\n        \"path\": \"/\",\n        \"secure\": False,\n        \"expires\": None,\n        \"discard\": True,\n        \"comment\": None,\n        \"comment_url\": None,\n        \"rest\": {\"HttpOnly\": None},\n        \"rfc2109\": False,\n    }\n\n    badargs = set(kwargs) - set(result)\n    if badargs:\n        raise TypeError(\n            f\"create_cookie() got unexpected keyword arguments: {list(badargs)}\"\n        )\n\n    result.update(kwargs)\n    result[\"port_specified\"] = bool(result[\"port\"])\n    result[\"domain_specified\"] = bool(result[\"domain\"])\n    result[\"domain_initial_dot\"] = result[\"domain\"].startswith(\".\")\n    result[\"path_specified\"] = bool(result[\"path\"])\n\n    return cookielib.Cookie(**result)", "metadata": {"license": "Apache-2.0", "len_tokens": 260}}
{"id": "requests:src/requests/cookies.py", "language": "python", "code": "def morsel_to_cookie(morsel):\n    \"\"\"Convert a Morsel object into a Cookie containing the one k/v pair.\"\"\"\n\n    expires = None\n    if morsel[\"max-age\"]:\n        try:\n            expires = int(time.time() + int(morsel[\"max-age\"]))\n        except ValueError:\n            raise TypeError(f\"max-age: {morsel['max-age']} must be integer\")\n    elif morsel[\"expires\"]:\n        time_template = \"%a, %d-%b-%Y %H:%M:%S GMT\"\n        expires = calendar.timegm(time.strptime(morsel[\"expires\"], time_template))\n    return create_cookie(\n        comment=morsel[\"comment\"],\n        comment_url=bool(morsel[\"comment\"]),\n        discard=False,\n        domain=morsel[\"domain\"],\n        expires=expires,\n        name=morsel.key,\n        path=morsel[\"path\"],\n        port=None,\n        rest={\"HttpOnly\": morsel[\"httponly\"]},\n        rfc2109=False,\n        secure=bool(morsel[\"secure\"]),\n        value=morsel.value,\n        version=morsel[\"version\"] or 0,\n    )", "metadata": {"license": "Apache-2.0", "len_tokens": 242}}
{"id": "requests:src/requests/cookies.py", "language": "python", "code": "def _find_no_duplicates(self, name, domain=None, path=None):\n        \"\"\"Both ``__get_item__`` and ``get`` call this function: it's never\n        used elsewhere in Requests.\n\n        :param name: a string containing name of cookie\n        :param domain: (optional) string containing domain of cookie\n        :param path: (optional) string containing path of cookie\n        :raises KeyError: if cookie is not found\n        :raises CookieConflictError: if there are multiple cookies\n            that match name and optionally domain and path\n        :return: cookie.value\n        \"\"\"\n        toReturn = None\n        for cookie in iter(self):\n            if cookie.name == name:\n                if domain is None or cookie.domain == domain:\n                    if path is None or cookie.path == path:\n                        if toReturn is not None:\n                            # if there are multiple cookies that meet passed in criteria\n                            raise CookieConflictError(\n                                f\"There are multiple cookies with name, {name!r}\"\n                            )\n                        # we will eventually return this as long as no cookie conflict\n                        toReturn = cookie.value\n\n        if toReturn:\n            return toReturn\n        raise KeyError(f\"name={name!r}, domain={domain!r}, path={path!r}\")", "metadata": {"license": "Apache-2.0", "len_tokens": 264}}
{"id": "requests:src/requests/auth.py", "language": "python", "code": "def _basic_auth_str(username, password):\n    \"\"\"Returns a Basic Auth string.\"\"\"\n\n    # \"I want us to put a big-ol' comment on top of it that\n    # says that this behaviour is dumb but we need to preserve\n    # it because people are relying on it.\"\n    #    - Lukasa\n    #\n    # These are here solely to maintain backwards compatibility\n    # for things like ints. This will be removed in 3.0.0.\n    if not isinstance(username, basestring):\n        warnings.warn(\n            \"Non-string usernames will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(username),\n            category=DeprecationWarning,\n        )\n        username = str(username)\n\n    if not isinstance(password, basestring):\n        warnings.warn(\n            \"Non-string passwords will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(type(password)),\n            category=DeprecationWarning,\n        )\n        password = str(password)\n    # -- End Removal --\n\n    if isinstance(username, str):\n        username = username.encode(\"latin1\")\n\n    if isinstance(password, str):\n        password = password.encode(\"latin1\")\n\n    authstr = \"Basic \" + to_native_string(\n        b64encode(b\":\".join((username, password))).strip()\n    )\n\n    return authstr", "metadata": {"license": "Apache-2.0", "len_tokens": 342}}
{"id": "requests:src/requests/auth.py", "language": "python", "code": "def handle_401(self, r, **kwargs):\n        \"\"\"\n        Takes the given response and tries digest-auth, if needed.\n\n        :rtype: requests.Response\n        \"\"\"\n\n        # If response is not 4xx, do not auth\n        # See https://github.com/psf/requests/issues/3772\n        if not 400 <= r.status_code < 500:\n            self._thread_local.num_401_calls = 1\n            return r\n\n        if self._thread_local.pos is not None:\n            # Rewind the file position indicator of the body to where\n            # it was to resend the request.\n            r.request.body.seek(self._thread_local.pos)\n        s_auth = r.headers.get(\"www-authenticate\", \"\")\n\n        if \"digest\" in s_auth.lower() and self._thread_local.num_401_calls < 2:\n            self._thread_local.num_401_calls += 1\n            pat = re.compile(r\"digest \", flags=re.IGNORECASE)\n            self._thread_local.chal = parse_dict_header(pat.sub(\"\", s_auth, count=1))\n\n            # Consume content and release the original connection\n            # to allow our new request to reuse the same one.\n            r.content\n            r.close()\n            prep = r.request.copy()\n            extract_cookies_to_jar(prep._cookies, r.request, r.raw)\n            prep.prepare_cookies(prep._cookies)\n\n            prep.headers[\"Authorization\"] = self.build_digest_header(\n                prep.method, prep.url\n            )\n            _r = r.connection.send(prep, **kwargs)\n            _r.history.append(r)\n            _r.request = prep\n\n            return _r\n\n        self._thread_local.num_401_calls = 1\n        return r", "metadata": {"license": "Apache-2.0", "len_tokens": 363}}
{"id": "requests:src/requests/sessions.py", "language": "python", "code": "def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n    \"\"\"Determines appropriate setting for a given request, taking into account\n    the explicit setting on that request, and the setting in the session. If a\n    setting is a dictionary, they will be merged together using `dict_class`\n    \"\"\"\n\n    if session_setting is None:\n        return request_setting\n\n    if request_setting is None:\n        return session_setting\n\n    # Bypass if not a dictionary (e.g. verify)\n    if not (\n        isinstance(session_setting, Mapping) and isinstance(request_setting, Mapping)\n    ):\n        return request_setting\n\n    merged_setting = dict_class(to_key_val_list(session_setting))\n    merged_setting.update(to_key_val_list(request_setting))\n\n    # Remove keys that are set to None. Extract keys first to avoid altering\n    # the dictionary during iteration.\n    none_keys = [k for (k, v) in merged_setting.items() if v is None]\n    for key in none_keys:\n        del merged_setting[key]\n\n    return merged_setting", "metadata": {"license": "Apache-2.0", "len_tokens": 216}}
{"id": "requests:src/requests/sessions.py", "language": "python", "code": "def get_redirect_target(self, resp):\n        \"\"\"Receives a Response. Returns a redirect URI or ``None``\"\"\"\n        # Due to the nature of how requests processes redirects this method will\n        # be called at least once upon the original response and at least twice\n        # on each subsequent redirect response (if any).\n        # If a custom mixin is used to handle this logic, it may be advantageous\n        # to cache the redirect location onto the response object as a private\n        # attribute.\n        if resp.is_redirect:\n            location = resp.headers[\"location\"]\n            # Currently the underlying http module on py3 decode headers\n            # in latin1, but empirical evidence suggests that latin1 is very\n            # rarely used with non-ASCII characters in HTTP headers.\n            # It is more likely to get UTF8 header rather than latin1.\n            # This causes incorrect handling of UTF8 encoded location headers.\n            # To solve this, we re-encode the location in latin1.\n            location = location.encode(\"latin1\")\n            return to_native_string(location, \"utf8\")\n        return None", "metadata": {"license": "Apache-2.0", "len_tokens": 228}}
{"id": "requests:src/requests/sessions.py", "language": "python", "code": "def should_strip_auth(self, old_url, new_url):\n        \"\"\"Decide whether Authorization header should be removed when redirecting\"\"\"\n        old_parsed = urlparse(old_url)\n        new_parsed = urlparse(new_url)\n        if old_parsed.hostname != new_parsed.hostname:\n            return True\n        # Special case: allow http -> https redirect when using the standard\n        # ports. This isn't specified by RFC 7235, but is kept to avoid\n        # breaking backwards compatibility with older versions of requests\n        # that allowed any redirects on the same host.\n        if (\n            old_parsed.scheme == \"http\"\n            and old_parsed.port in (80, None)\n            and new_parsed.scheme == \"https\"\n            and new_parsed.port in (443, None)\n        ):\n            return False\n\n        # Handle default port usage corresponding to scheme.\n        changed_port = old_parsed.port != new_parsed.port\n        changed_scheme = old_parsed.scheme != new_parsed.scheme\n        default_port = (DEFAULT_PORTS.get(old_parsed.scheme, None), None)\n        if (\n            not changed_scheme\n            and old_parsed.port in default_port\n            and new_parsed.port in default_port\n        ):\n            return False\n\n        # Standard case: root URI must match\n        return changed_port or changed_scheme", "metadata": {"license": "Apache-2.0", "len_tokens": 277}}
{"id": "requests:src/requests/sessions.py", "language": "python", "code": "def rebuild_proxies(self, prepared_request, proxies):\n        \"\"\"This method re-evaluates the proxy configuration by considering the\n        environment variables. If we are redirected to a URL covered by\n        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing\n        proxy keys for this URL (in case they were stripped by a previous\n        redirect).\n\n        This method also replaces the Proxy-Authorization header where\n        necessary.\n\n        :rtype: dict\n        \"\"\"\n        headers = prepared_request.headers\n        scheme = urlparse(prepared_request.url).scheme\n        new_proxies = resolve_proxies(prepared_request, proxies, self.trust_env)\n\n        if \"Proxy-Authorization\" in headers:\n            del headers[\"Proxy-Authorization\"]\n\n        try:\n            username, password = get_auth_from_url(new_proxies[scheme])\n        except KeyError:\n            username, password = None, None\n\n        # urllib3 handles proxy authorization for us in the standard adapter.\n        # Avoid appending this to TLS tunneled requests where it may be leaked.\n        if not scheme.startswith(\"https\") and username and password:\n            headers[\"Proxy-Authorization\"] = _basic_auth_str(username, password)\n\n        return new_proxies", "metadata": {"license": "Apache-2.0", "len_tokens": 251}}
{"id": "requests:src/requests/sessions.py", "language": "python", "code": "def __init__(self):\n        #: A case-insensitive dictionary of headers to be sent on each\n        #: :class:`Request <Request>` sent from this\n        #: :class:`Session <Session>`.\n        self.headers = default_headers()\n\n        #: Default Authentication tuple or object to attach to\n        #: :class:`Request <Request>`.\n        self.auth = None\n\n        #: Dictionary mapping protocol or protocol and host to the URL of the proxy\n        #: (e.g. {'http': 'foo.bar:3128', 'http://host.name': 'foo.bar:4012'}) to\n        #: be used on each :class:`Request <Request>`.\n        self.proxies = {}\n\n        #: Event-handling hooks.\n        self.hooks = default_hooks()\n\n        #: Dictionary of querystring data to attach to each\n        #: :class:`Request <Request>`. The dictionary values may be lists for\n        #: representing multivalued query parameters.\n        self.params = {}\n\n        #: Stream response content default.\n        self.stream = False\n\n        #: SSL Verification default.\n        #: Defaults to `True`, requiring requests to verify the TLS certificate at the\n        #: remote end.\n        #: If verify is set to `False`, requests will accept any TLS certificate\n        #: presented by the server, and will ignore hostname mismatches and/or\n        #: expired certificates, which will make your application vulnerable to\n        #: man-in-the-middle (MitM) attacks.\n        #: Only set this to `False` for testing.\n        self.verify = True\n\n        #: SSL client certificate default, if String, path to ssl client\n        #: cert file (.pem). If Tuple, ('cert', 'key') pair.\n        self.cert = None\n\n        #: Maximum number of redirects allowed. If the request exceeds this\n        #: limit, a :class:`TooManyRedirects` exception is raised.\n        #: This defaults to requests.models.DEFAULT_REDIRECT_LIMIT, which is\n        #: 30.\n        self.max_redirects = DEFAULT_REDIRECT_LIMIT\n\n        #: Trust environment settings for proxy configuration, default\n        #: authentication and similar.\n        self.trust_env = True\n\n        #: A CookieJar containing all currently outstanding cookies set on this\n        #: session. By default it is a\n        #: :class:`RequestsCookieJar <requests.cookies.RequestsCookieJar>`, but\n        #: may be any other ``cookielib.CookieJar`` compatible object.\n        self.cookies = cookiejar_from_dict({})\n\n        # Default connection adapters.\n        self.adapters = OrderedDict()\n        self.mount(\"https://\", HTTPAdapter())\n        self.mount(\"http://\", HTTPAdapter())", "metadata": {"license": "Apache-2.0", "len_tokens": 549}}
{"id": "requests:src/requests/sessions.py", "language": "python", "code": "def prepare_request(self, request):\n        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for\n        transmission and returns it. The :class:`PreparedRequest` has settings\n        merged from the :class:`Request <Request>` instance and those of the\n        :class:`Session`.\n\n        :param request: :class:`Request` instance to prepare with this\n            session's settings.\n        :rtype: requests.PreparedRequest\n        \"\"\"\n        cookies = request.cookies or {}\n\n        # Bootstrap CookieJar.\n        if not isinstance(cookies, cookielib.CookieJar):\n            cookies = cookiejar_from_dict(cookies)\n\n        # Merge with session cookies\n        merged_cookies = merge_cookies(\n            merge_cookies(RequestsCookieJar(), self.cookies), cookies\n        )\n\n        # Set environment's basic authentication if not explicitly set.\n        auth = request.auth\n        if self.trust_env and not auth and not self.auth:\n            auth = get_netrc_auth(request.url)\n\n        p = PreparedRequest()\n        p.prepare(\n            method=request.method.upper(),\n            url=request.url,\n            files=request.files,\n            data=request.data,\n            json=request.json,\n            headers=merge_setting(\n                request.headers, self.headers, dict_class=CaseInsensitiveDict\n            ),\n            params=merge_setting(request.params, self.params),\n            auth=merge_setting(auth, self.auth),\n            cookies=merged_cookies,\n            hooks=merge_hooks(request.hooks, self.hooks),\n        )\n        return p", "metadata": {"license": "Apache-2.0", "len_tokens": 307}}
{"id": "requests:src/requests/sessions.py", "language": "python", "code": "def request(\n        self,\n        method,\n        url,\n        params=None,\n        data=None,\n        headers=None,\n        cookies=None,\n        files=None,\n        auth=None,\n        timeout=None,\n        allow_redirects=True,\n        proxies=None,\n        hooks=None,\n        stream=None,\n        verify=None,\n        cert=None,\n        json=None,\n    ):\n        \"\"\"Constructs a :class:`Request <Request>`, prepares it and sends it.\n        Returns :class:`Response <Response>` object.\n\n        :param method: method for the new :class:`Request` object.\n        :param url: URL for the new :class:`Request` object.\n        :param params: (optional) Dictionary or bytes to be sent in the query\n            string for the :class:`Request`.\n        :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n            object to send in the body of the :class:`Request`.\n        :param json: (optional) json to send in the body of the\n            :class:`Request`.\n        :param headers: (optional) Dictionary of HTTP Headers to send with the\n            :class:`Request`.\n        :param cookies: (optional) Dict or CookieJar object to send with the\n            :class:`Request`.\n        :param files: (optional) Dictionary of ``'filename': file-like-objects``\n            for multipart encoding upload.\n        :param auth: (optional) Auth tuple or callable to enable\n            Basic/Digest/Custom HTTP Auth.\n        :param timeout: (optional) How many seconds to wait for the server to send\n            data before giving up, as a float, or a :ref:`(connect timeout,\n            read timeout) <timeouts>` tuple.\n        :type timeout: float or tuple\n        :param allow_redirects: (optional) Set to True by default.\n        :type allow_redirects: bool\n        :param proxies: (optional) Dictionary mapping protocol or protocol and\n            hostname to the URL of the proxy.\n        :param hooks: (optional) Dictionary mapping hook name to one event or\n            list of events, event must be callable.\n        :param stream: (optional) whether to immediately download the response\n            content. Defaults to ``False``.\n        :param verify: (optional) Either a boolean, in which case it controls whether we verify\n            the server's TLS certificate, or a string, in which case it must be a path\n            to a CA bundle to use. Defaults to ``True``. When set to\n            ``False``, requests will accept any TLS certificate presented by\n            the server, and will ignore hostname mismatches and/or expired\n            certificates, which will make your application vulnerable to\n            man-in-the-middle (MitM) attacks. Setting verify to ``False``\n            may be useful during local development or testing.\n        :param cert: (optional) if String, path to ssl client cert file (.pem).\n            If Tuple, ('cert', 'key') pair.\n        :rtype: requests.Response\n        \"\"\"\n        # Create the Request.\n        req = Request(\n            method=method.upper(),\n            url=url,\n            headers=headers,\n            files=files,\n            data=data or {},\n            json=json,\n            params=params or {},\n            auth=auth,\n            cookies=cookies,\n            hooks=hooks,\n        )\n        prep = self.prepare_request(req)\n\n        proxies = proxies or {}\n\n        settings = self.merge_environment_settings(\n            prep.url, proxies, stream, verify, cert\n        )\n\n        # Send the request.\n        send_kwargs = {\n            \"timeout\": timeout,\n            \"allow_redirects\": allow_redirects,\n        }\n        send_kwargs.update(settings)\n        resp = self.send(prep, **send_kwargs)\n\n        return resp", "metadata": {"license": "Apache-2.0", "len_tokens": 785}}
{"id": "requests:src/requests/sessions.py", "language": "python", "code": "def send(self, request, **kwargs):\n        \"\"\"Send a given PreparedRequest.\n\n        :rtype: requests.Response\n        \"\"\"\n        # Set defaults that the hooks can utilize to ensure they always have\n        # the correct parameters to reproduce the previous request.\n        kwargs.setdefault(\"stream\", self.stream)\n        kwargs.setdefault(\"verify\", self.verify)\n        kwargs.setdefault(\"cert\", self.cert)\n        if \"proxies\" not in kwargs:\n            kwargs[\"proxies\"] = resolve_proxies(request, self.proxies, self.trust_env)\n\n        # It's possible that users might accidentally send a Request object.\n        # Guard against that specific failure case.\n        if isinstance(request, Request):\n            raise ValueError(\"You can only send PreparedRequests.\")\n\n        # Set up variables needed for resolve_redirects and dispatching of hooks\n        allow_redirects = kwargs.pop(\"allow_redirects\", True)\n        stream = kwargs.get(\"stream\")\n        hooks = request.hooks\n\n        # Get the appropriate adapter to use\n        adapter = self.get_adapter(url=request.url)\n\n        # Start time (approximately) of the request\n        start = preferred_clock()\n\n        # Send the request\n        r = adapter.send(request, **kwargs)\n\n        # Total elapsed time of the request (approximately)\n        elapsed = preferred_clock() - start\n        r.elapsed = timedelta(seconds=elapsed)\n\n        # Response manipulation hooks\n        r = dispatch_hook(\"response\", hooks, r, **kwargs)\n\n        # Persist cookies\n        if r.history:\n            # If the hooks create history then we want those cookies too\n            for resp in r.history:\n                extract_cookies_to_jar(self.cookies, resp.request, resp.raw)\n\n        extract_cookies_to_jar(self.cookies, request, r.raw)\n\n        # Resolve redirects if allowed.\n        if allow_redirects:\n            # Redirect resolving generator.\n            gen = self.resolve_redirects(r, request, **kwargs)\n            history = [resp for resp in gen]\n        else:\n            history = []\n\n        # Shuffle things around if there's history.\n        if history:\n            # Insert the first (original) request at the start\n            history.insert(0, r)\n            # Get the last request made\n            r = history.pop()\n            r.history = history\n\n        # If redirects aren't being followed, store the response on the Request for Response.next().\n        if not allow_redirects:\n            try:\n                r._next = next(\n                    self.resolve_redirects(r, request, yield_requests=True, **kwargs)\n                )\n            except StopIteration:\n                pass\n\n        if not stream:\n            r.content\n\n        return r", "metadata": {"license": "Apache-2.0", "len_tokens": 537}}
{"id": "requests:src/requests/sessions.py", "language": "python", "code": "def merge_environment_settings(self, url, proxies, stream, verify, cert):\n        \"\"\"\n        Check the environment and merge it with some settings.\n\n        :rtype: dict\n        \"\"\"\n        # Gather clues from the surrounding environment.\n        if self.trust_env:\n            # Set environment's proxies.\n            no_proxy = proxies.get(\"no_proxy\") if proxies is not None else None\n            env_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n            for k, v in env_proxies.items():\n                proxies.setdefault(k, v)\n\n            # Look for requests environment configuration\n            # and be compatible with cURL.\n            if verify is True or verify is None:\n                verify = (\n                    os.environ.get(\"REQUESTS_CA_BUNDLE\")\n                    or os.environ.get(\"CURL_CA_BUNDLE\")\n                    or verify\n                )\n\n        # Merge all the kwargs.\n        proxies = merge_setting(proxies, self.proxies)\n        stream = merge_setting(stream, self.stream)\n        verify = merge_setting(verify, self.verify)\n        cert = merge_setting(cert, self.cert)\n\n        return {\"proxies\": proxies, \"stream\": stream, \"verify\": verify, \"cert\": cert}", "metadata": {"license": "Apache-2.0", "len_tokens": 246}}
{"id": "requests:src/requests/compat.py", "language": "python", "code": "\"\"\"\nrequests.compat\n~~~~~~~~~~~~~~~\n\nThis module previously handled import compatibility issues\nbetween Python 2 and Python 3. It remains for backwards\ncompatibility until the next major version.\n\"\"\"\n\nimport importlib\nimport sys\n\n# -------\n# urllib3\n# -------\nfrom urllib3 import __version__ as urllib3_version\n\n# Detect which major version of urllib3 is being used.\ntry:\n    is_urllib3_1 = int(urllib3_version.split(\".\")[0]) == 1\nexcept (TypeError, AttributeError):\n    # If we can't discern a version, prefer old functionality.\n    is_urllib3_1 = True\n\n# -------------------\n# Character Detection\n# -------------------\n\n\ndef _resolve_char_detection():\n    \"\"\"Find supported character detection libraries.\"\"\"\n    chardet = None\n    for lib in (\"chardet\", \"charset_normalizer\"):\n        if chardet is None:\n            try:\n                chardet = importlib.import_module(lib)\n            except ImportError:\n                pass\n    return chardet\n\n\nchardet = _resolve_char_detection()\n\n# -------\n# Pythons\n# -------\n\n# Syntax sugar.\n_ver = sys.version_info\n\n#: Python 2.x?\nis_py2 = _ver[0] == 2\n\n#: Python 3.x?\nis_py3 = _ver[0] == 3\n\n# json/simplejson module import resolution\nhas_simplejson = False\ntry:\n    import simplejson as json\n\n    has_simplejson = True\nexcept ImportError:\n    import json\n\nif has_simplejson:\n    from simplejson import JSONDecodeError\nelse:\n    from json import JSONDecodeError\n\n# Keep OrderedDict for backwards compatibility.\nfrom collections import OrderedDict\nfrom collections.abc import Callable, Mapping, MutableMapping\nfrom http import cookiejar as cookielib\nfrom http.cookies import Morsel\nfrom io import StringIO\n\n# --------------\n# Legacy Imports\n# --------------\nfrom urllib.parse import (\n    quote,\n    quote_plus,\n    unquote,\n    unquote_plus,\n    urldefrag,\n    urlencode,\n    urljoin,\n    urlparse,\n    urlsplit,\n    urlunparse,\n)\nfrom urllib.request import (\n    getproxies,\n    getproxies_environment,\n    parse_http_list,\n    proxy_bypass,\n    proxy_bypass_environment,\n)\n\nbuiltin_str = str\nstr = str\nbytes = bytes\nbasestring = (str, bytes)\nnumeric_types = (int, float)\ninteger_types = (int,)\n", "metadata": {"license": "Apache-2.0", "len_tokens": 520}}
{"id": "requests:src/requests/models.py", "language": "python", "code": "class Request(RequestHooksMixin):\n    \"\"\"A user-created :class:`Request <Request>` object.\n\n    Used to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.\n\n    :param method: HTTP method to use.\n    :param url: URL to send.\n    :param headers: dictionary of headers to send.\n    :param files: dictionary of {filename: fileobject} files to multipart upload.\n    :param data: the body to attach to the request. If a dictionary or\n        list of tuples ``[(key, value)]`` is provided, form-encoding will\n        take place.\n    :param json: json for the body to attach to the request (if files or data is not specified).\n    :param params: URL parameters to append to the URL. If a dictionary or\n        list of tuples ``[(key, value)]`` is provided, form-encoding will\n        take place.\n    :param auth: Auth handler or (user, pass) tuple.\n    :param cookies: dictionary or CookieJar of cookies to attach to this request.\n    :param hooks: dictionary of callback hooks, for internal usage.\n\n    Usage::\n\n      >>> import requests\n      >>> req = requests.Request('GET', 'https://httpbin.org/get')\n      >>> req.prepare()\n      <PreparedRequest [GET]>\n    \"\"\"\n\n    def __init__(\n        self,\n        method=None,\n        url=None,\n        headers=None,\n        files=None,\n        data=None,\n        params=None,\n        auth=None,\n        cookies=None,\n        hooks=None,\n        json=None,\n    ):\n        # Default empty dicts for dict params.\n        data = [] if data is None else data\n        files = [] if files is None else files\n        headers = {} if headers is None else headers\n        params = {} if params is None else params\n        hooks = {} if hooks is None else hooks\n\n        self.hooks = default_hooks()\n        for k, v in list(hooks.items()):\n            self.register_hook(event=k, hook=v)\n\n        self.method = method\n        self.url = url\n        self.headers = headers\n        self.files = files\n        self.data = data\n        self.json = json\n        self.params = params\n        self.auth = auth\n        self.cookies = cookies\n\n    def __repr__(self):\n        return f\"<Request [{self.method}]>\"\n\n    def prepare(self):\n        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it.\"\"\"\n        p = PreparedRequest()\n        p.prepare(\n            method=self.method,\n            url=self.url,\n            headers=self.headers,\n            files=self.files,\n            data=self.data,\n            json=self.json,\n            params=self.params,\n            auth=self.auth,\n            cookies=self.cookies,\n            hooks=self.hooks,\n        )\n        return p", "metadata": {"license": "Apache-2.0", "len_tokens": 587}}
{"id": "requests:src/requests/models.py", "language": "python", "code": "def _encode_params(data):\n        \"\"\"Encode parameters in a piece of data.\n\n        Will successfully encode parameters when passed as a dict or a list of\n        2-tuples. Order is retained if data is a list of 2-tuples but arbitrary\n        if parameters are supplied as a dict.\n        \"\"\"\n\n        if isinstance(data, (str, bytes)):\n            return data\n        elif hasattr(data, \"read\"):\n            return data\n        elif hasattr(data, \"__iter__\"):\n            result = []\n            for k, vs in to_key_val_list(data):\n                if isinstance(vs, basestring) or not hasattr(vs, \"__iter__\"):\n                    vs = [vs]\n                for v in vs:\n                    if v is not None:\n                        result.append(\n                            (\n                                k.encode(\"utf-8\") if isinstance(k, str) else k,\n                                v.encode(\"utf-8\") if isinstance(v, str) else v,\n                            )\n                        )\n            return urlencode(result, doseq=True)\n        else:\n            return data", "metadata": {"license": "Apache-2.0", "len_tokens": 212}}
{"id": "requests:src/requests/models.py", "language": "python", "code": "def _encode_files(files, data):\n        \"\"\"Build the body for a multipart/form-data request.\n\n        Will successfully encode files when passed as a dict or a list of\n        tuples. Order is retained if data is a list of tuples but arbitrary\n        if parameters are supplied as a dict.\n        The tuples may be 2-tuples (filename, fileobj), 3-tuples (filename, fileobj, contentype)\n        or 4-tuples (filename, fileobj, contentype, custom_headers).\n        \"\"\"\n        if not files:\n            raise ValueError(\"Files must be provided.\")\n        elif isinstance(data, basestring):\n            raise ValueError(\"Data must not be a string.\")\n\n        new_fields = []\n        fields = to_key_val_list(data or {})\n        files = to_key_val_list(files or {})\n\n        for field, val in fields:\n            if isinstance(val, basestring) or not hasattr(val, \"__iter__\"):\n                val = [val]\n            for v in val:\n                if v is not None:\n                    # Don't call str() on bytestrings: in Py3 it all goes wrong.\n                    if not isinstance(v, bytes):\n                        v = str(v)\n\n                    new_fields.append(\n                        (\n                            field.decode(\"utf-8\")\n                            if isinstance(field, bytes)\n                            else field,\n                            v.encode(\"utf-8\") if isinstance(v, str) else v,\n                        )\n                    )\n\n        for k, v in files:\n            # support for explicit filename\n            ft = None\n            fh = None\n            if isinstance(v, (tuple, list)):\n                if len(v) == 2:\n                    fn, fp = v\n                elif len(v) == 3:\n                    fn, fp, ft = v\n                else:\n                    fn, fp, ft, fh = v\n            else:\n                fn = guess_filename(v) or k\n                fp = v\n\n            if isinstance(fp, (str, bytes, bytearray)):\n                fdata = fp\n            elif hasattr(fp, \"read\"):\n                fdata = fp.read()\n            elif fp is None:\n                continue\n            else:\n                fdata = fp\n\n            rf = RequestField(name=k, data=fdata, filename=fn, headers=fh)\n            rf.make_multipart(content_type=ft)\n            new_fields.append(rf)\n\n        body, content_type = encode_multipart_formdata(new_fields)\n\n        return body, content_type", "metadata": {"license": "Apache-2.0", "len_tokens": 503}}
{"id": "requests:src/requests/models.py", "language": "python", "code": "def prepare_url(self, url, params):\n        \"\"\"Prepares the given HTTP URL.\"\"\"\n        #: Accept objects that have string representations.\n        #: We're unable to blindly call unicode/str functions\n        #: as this will include the bytestring indicator (b'')\n        #: on python 3.x.\n        #: https://github.com/psf/requests/pull/2238\n        if isinstance(url, bytes):\n            url = url.decode(\"utf8\")\n        else:\n            url = str(url)\n\n        # Remove leading whitespaces from url\n        url = url.lstrip()\n\n        # Don't do any URL preparation for non-HTTP schemes like `mailto`,\n        # `data` etc to work around exceptions from `url_parse`, which\n        # handles RFC 3986 only.\n        if \":\" in url and not url.lower().startswith(\"http\"):\n            self.url = url\n            return\n\n        # Support for unicode domain names and paths.\n        try:\n            scheme, auth, host, port, path, query, fragment = parse_url(url)\n        except LocationParseError as e:\n            raise InvalidURL(*e.args)\n\n        if not scheme:\n            raise MissingSchema(\n                f\"Invalid URL {url!r}: No scheme supplied. \"\n                f\"Perhaps you meant https://{url}?\"\n            )\n\n        if not host:\n            raise InvalidURL(f\"Invalid URL {url!r}: No host supplied\")\n\n        # In general, we want to try IDNA encoding the hostname if the string contains\n        # non-ASCII characters. This allows users to automatically get the correct IDNA\n        # behaviour. For strings containing only ASCII characters, we need to also verify\n        # it doesn't start with a wildcard (*), before allowing the unencoded hostname.\n        if not unicode_is_ascii(host):\n            try:\n                host = self._get_idna_encoded_host(host)\n            except UnicodeError:\n                raise InvalidURL(\"URL has an invalid label.\")\n        elif host.startswith((\"*\", \".\")):\n            raise InvalidURL(\"URL has an invalid label.\")\n\n        # Carefully reconstruct the network location\n        netloc = auth or \"\"\n        if netloc:\n            netloc += \"@\"\n        netloc += host\n        if port:\n            netloc += f\":{port}\"\n\n        # Bare domains aren't valid URLs.\n        if not path:\n            path = \"/\"\n\n        if isinstance(params, (str, bytes)):\n            params = to_native_string(params)\n\n        enc_params = self._encode_params(params)\n        if enc_params:\n            if query:\n                query = f\"{query}&{enc_params}\"\n            else:\n                query = enc_params\n\n        url = requote_uri(urlunparse([scheme, netloc, path, None, query, fragment]))\n        self.url = url", "metadata": {"license": "Apache-2.0", "len_tokens": 571}}
{"id": "requests:src/requests/models.py", "language": "python", "code": "def prepare_body(self, data, files, json=None):\n        \"\"\"Prepares the given HTTP body data.\"\"\"\n\n        # Check if file, fo, generator, iterator.\n        # If not, run through normal process.\n\n        # Nottin' on you.\n        body = None\n        content_type = None\n\n        if not data and json is not None:\n            # urllib3 requires a bytes-like body. Python 2's json.dumps\n            # provides this natively, but Python 3 gives a Unicode string.\n            content_type = \"application/json\"\n\n            try:\n                body = complexjson.dumps(json, allow_nan=False)\n            except ValueError as ve:\n                raise InvalidJSONError(ve, request=self)\n\n            if not isinstance(body, bytes):\n                body = body.encode(\"utf-8\")\n\n        is_stream = all(\n            [\n                hasattr(data, \"__iter__\"),\n                not isinstance(data, (basestring, list, tuple, Mapping)),\n            ]\n        )\n\n        if is_stream:\n            try:\n                length = super_len(data)\n            except (TypeError, AttributeError, UnsupportedOperation):\n                length = None\n\n            body = data\n\n            if getattr(body, \"tell\", None) is not None:\n                # Record the current file position before reading.\n                # This will allow us to rewind a file in the event\n                # of a redirect.\n                try:\n                    self._body_position = body.tell()\n                except OSError:\n                    # This differentiates from None, allowing us to catch\n                    # a failed `tell()` later when trying to rewind the body\n                    self._body_position = object()\n\n            if files:\n                raise NotImplementedError(\n                    \"Streamed bodies and files are mutually exclusive.\"\n                )\n\n            if length:\n                self.headers[\"Content-Length\"] = builtin_str(length)\n            else:\n                self.headers[\"Transfer-Encoding\"] = \"chunked\"\n        else:\n            # Multi-part file uploads.\n            if files:\n                (body, content_type) = self._encode_files(files, data)\n            else:\n                if data:\n                    body = self._encode_params(data)\n                    if isinstance(data, basestring) or hasattr(data, \"read\"):\n                        content_type = None\n                    else:\n                        content_type = \"application/x-www-form-urlencoded\"\n\n            self.prepare_content_length(body)\n\n            # Add content-type if it wasn't explicitly provided.\n            if content_type and (\"content-type\" not in self.headers):\n                self.headers[\"Content-Type\"] = content_type\n\n        self.body = body", "metadata": {"license": "Apache-2.0", "len_tokens": 518}}
{"id": "requests:src/requests/models.py", "language": "python", "code": "def __init__(self):\n        self._content = False\n        self._content_consumed = False\n        self._next = None\n\n        #: Integer Code of responded HTTP Status, e.g. 404 or 200.\n        self.status_code = None\n\n        #: Case-insensitive Dictionary of Response Headers.\n        #: For example, ``headers['content-encoding']`` will return the\n        #: value of a ``'Content-Encoding'`` response header.\n        self.headers = CaseInsensitiveDict()\n\n        #: File-like object representation of response (for advanced usage).\n        #: Use of ``raw`` requires that ``stream=True`` be set on the request.\n        #: This requirement does not apply for use internally to Requests.\n        self.raw = None\n\n        #: Final URL location of Response.\n        self.url = None\n\n        #: Encoding to decode with when accessing r.text.\n        self.encoding = None\n\n        #: A list of :class:`Response <Response>` objects from\n        #: the history of the Request. Any redirect responses will end\n        #: up here. The list is sorted from the oldest to the most recent request.\n        self.history = []\n\n        #: Textual reason of responded HTTP Status, e.g. \"Not Found\" or \"OK\".\n        self.reason = None\n\n        #: A CookieJar of Cookies the server sent back.\n        self.cookies = cookiejar_from_dict({})\n\n        #: The amount of time elapsed between sending the request\n        #: and the arrival of the response (as a timedelta).\n        #: This property specifically measures the time taken between sending\n        #: the first byte of the request and finishing parsing the headers. It\n        #: is therefore unaffected by consuming the response content or the\n        #: value of the ``stream`` keyword argument.\n        self.elapsed = datetime.timedelta(0)\n\n        #: The :class:`PreparedRequest <PreparedRequest>` object to which this\n        #: is a response.\n        self.request = None", "metadata": {"license": "Apache-2.0", "len_tokens": 400}}
{"id": "requests:src/requests/models.py", "language": "python", "code": "def iter_content(self, chunk_size=1, decode_unicode=False):\n        \"\"\"Iterates over the response data.  When stream=True is set on the\n        request, this avoids reading the content at once into memory for\n        large responses.  The chunk size is the number of bytes it should\n        read into memory.  This is not necessarily the length of each item\n        returned as decoding can take place.\n\n        chunk_size must be of type int or None. A value of None will\n        function differently depending on the value of `stream`.\n        stream=True will read data as it arrives in whatever size the\n        chunks are received. If stream=False, data is returned as\n        a single chunk.\n\n        If decode_unicode is True, content will be decoded using the best\n        available encoding based on the response.\n        \"\"\"\n\n        def generate():\n            # Special case for urllib3.\n            if hasattr(self.raw, \"stream\"):\n                try:\n                    yield from self.raw.stream(chunk_size, decode_content=True)\n                except ProtocolError as e:\n                    raise ChunkedEncodingError(e)\n                except DecodeError as e:\n                    raise ContentDecodingError(e)\n                except ReadTimeoutError as e:\n                    raise ConnectionError(e)\n                except SSLError as e:\n                    raise RequestsSSLError(e)\n            else:\n                # Standard file-like object.\n                while True:\n                    chunk = self.raw.read(chunk_size)\n                    if not chunk:\n                        break\n                    yield chunk\n\n            self._content_consumed = True\n\n        if self._content_consumed and isinstance(self._content, bool):\n            raise StreamConsumedError()\n        elif chunk_size is not None and not isinstance(chunk_size, int):\n            raise TypeError(\n                f\"chunk_size must be an int, it is instead a {type(chunk_size)}.\"\n            )\n        # simulate reading small chunks of the content\n        reused_chunks = iter_slices(self._content, chunk_size)\n\n        stream_chunks = generate()\n\n        chunks = reused_chunks if self._content_consumed else stream_chunks\n\n        if decode_unicode:\n            chunks = stream_decode_response_unicode(chunks, self)\n\n        return chunks", "metadata": {"license": "Apache-2.0", "len_tokens": 439}}
{"id": "requests:src/requests/models.py", "language": "python", "code": "def text(self):\n        \"\"\"Content of the response, in unicode.\n\n        If Response.encoding is None, encoding will be guessed using\n        ``charset_normalizer`` or ``chardet``.\n\n        The encoding of the response content is determined based solely on HTTP\n        headers, following RFC 2616 to the letter. If you can take advantage of\n        non-HTTP knowledge to make a better guess at the encoding, you should\n        set ``r.encoding`` appropriately before accessing this property.\n        \"\"\"\n\n        # Try charset from content-type\n        content = None\n        encoding = self.encoding\n\n        if not self.content:\n            return \"\"\n\n        # Fallback to auto-detected encoding.\n        if self.encoding is None:\n            encoding = self.apparent_encoding\n\n        # Decode unicode from given encoding.\n        try:\n            content = str(self.content, encoding, errors=\"replace\")\n        except (LookupError, TypeError):\n            # A LookupError is raised if the encoding was not found which could\n            # indicate a misspelling or similar mistake.\n            #\n            # A TypeError can be raised if encoding is None\n            #\n            # So we try blindly encoding.\n            content = str(self.content, errors=\"replace\")\n\n        return content", "metadata": {"license": "Apache-2.0", "len_tokens": 255}}
{"id": "requests:src/requests/models.py", "language": "python", "code": "def json(self, **kwargs):\n        r\"\"\"Decodes the JSON response body (if any) as a Python object.\n\n        This may return a dictionary, list, etc. depending on what is in the response.\n\n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n\n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n\n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n            raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)", "metadata": {"license": "Apache-2.0", "len_tokens": 351}}
{"id": "requests:src/requests/models.py", "language": "python", "code": "def raise_for_status(self):\n        \"\"\"Raises :class:`HTTPError`, if one occurred.\"\"\"\n\n        http_error_msg = \"\"\n        if isinstance(self.reason, bytes):\n            # We attempt to decode utf-8 first because some servers\n            # choose to localize their reason strings. If the string\n            # isn't utf-8, we fall back to iso-8859-1 for all other\n            # encodings. (See PR #3538)\n            try:\n                reason = self.reason.decode(\"utf-8\")\n            except UnicodeDecodeError:\n                reason = self.reason.decode(\"iso-8859-1\")\n        else:\n            reason = self.reason\n\n        if 400 <= self.status_code < 500:\n            http_error_msg = (\n                f\"{self.status_code} Client Error: {reason} for url: {self.url}\"\n            )\n\n        elif 500 <= self.status_code < 600:\n            http_error_msg = (\n                f\"{self.status_code} Server Error: {reason} for url: {self.url}\"\n            )\n\n        if http_error_msg:\n            raise HTTPError(http_error_msg, response=self)", "metadata": {"license": "Apache-2.0", "len_tokens": 234}}
{"id": "requests:src/requests/__init__.py", "language": "python", "code": "def check_compatibility(urllib3_version, chardet_version, charset_normalizer_version):\n    urllib3_version = urllib3_version.split(\".\")\n    assert urllib3_version != [\"dev\"]  # Verify urllib3 isn't installed from git.\n\n    # Sometimes, urllib3 only reports its version as 16.1.\n    if len(urllib3_version) == 2:\n        urllib3_version.append(\"0\")\n\n    # Check urllib3 for compatibility.\n    major, minor, patch = urllib3_version  # noqa: F811\n    major, minor, patch = int(major), int(minor), int(patch)\n    # urllib3 >= 1.21.1\n    assert major >= 1\n    if major == 1:\n        assert minor >= 21\n\n    # Check charset_normalizer for compatibility.\n    if chardet_version:\n        major, minor, patch = chardet_version.split(\".\")[:3]\n        major, minor, patch = int(major), int(minor), int(patch)\n        # chardet_version >= 3.0.2, < 6.0.0\n        assert (3, 0, 2) <= (major, minor, patch) < (6, 0, 0)\n    elif charset_normalizer_version:\n        major, minor, patch = charset_normalizer_version.split(\".\")[:3]\n        major, minor, patch = int(major), int(minor), int(patch)\n        # charset_normalizer >= 2.0.0 < 4.0.0\n        assert (2, 0, 0) <= (major, minor, patch) < (4, 0, 0)\n    else:\n        warnings.warn(\n            \"Unable to find acceptable character detection dependency \"\n            \"(chardet or charset_normalizer).\",\n            RequestsDependencyWarning,\n        )", "metadata": {"license": "Apache-2.0", "len_tokens": 393}}
{"id": "requests:src/requests/packages.py", "language": "python", "code": "import sys\n\nfrom .compat import chardet\n\n# This code exists for backwards compatibility reasons.\n# I don't like it either. Just look the other way. :)\n\nfor package in (\"urllib3\", \"idna\"):\n    locals()[package] = __import__(package)\n    # This traversal is apparently necessary such that the identities are\n    # preserved (requests.packages.urllib3.* is urllib3.*)\n    for mod in list(sys.modules):\n        if mod == package or mod.startswith(f\"{package}.\"):\n            sys.modules[f\"requests.packages.{mod}\"] = sys.modules[mod]\n\nif chardet is not None:\n    target = chardet.__name__\n    for mod in list(sys.modules):\n        if mod == target or mod.startswith(f\"{target}.\"):\n            imported_mod = sys.modules[mod]\n            sys.modules[f\"requests.packages.{mod}\"] = imported_mod\n            mod = mod.replace(target, \"chardet\")\n            sys.modules[f\"requests.packages.{mod}\"] = imported_mod\n", "metadata": {"license": "Apache-2.0", "len_tokens": 215}}
{"id": "requests:src/requests/api.py", "language": "python", "code": "def request(method, url, **kwargs):\n    \"\"\"Constructs and sends a :class:`Request <Request>`.\n\n    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.\n    :param url: URL for the new :class:`Request` object.\n    :param params: (optional) Dictionary, list of tuples or bytes to send\n        in the query string for the :class:`Request`.\n    :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n        object to send in the body of the :class:`Request`.\n    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.\n    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.\n    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.\n    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.\n        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``\n        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content_type'`` is a string\n        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers\n        to add for the file.\n    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.\n    :param timeout: (optional) How many seconds to wait for the server to send data\n        before giving up, as a float, or a :ref:`(connect timeout, read\n        timeout) <timeouts>` tuple.\n    :type timeout: float or tuple\n    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.\n    :type allow_redirects: bool\n    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.\n    :param verify: (optional) Either a boolean, in which case it controls whether we verify\n            the server's TLS certificate, or a string, in which case it must be a path\n            to a CA bundle to use. Defaults to ``True``.\n    :param stream: (optional) if ``False``, the response content will be immediately downloaded.\n    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n\n    Usage::\n\n      >>> import requests\n      >>> req = requests.request('GET', 'https://httpbin.org/get')\n      >>> req\n      <Response [200]>\n    \"\"\"\n\n    # By using the 'with' statement we are sure the session is closed, thus we\n    # avoid leaving sockets open which can trigger a ResourceWarning in some\n    # cases, and look like a memory leak in others.\n    with sessions.Session() as session:\n        return session.request(method=method, url=url, **kwargs)", "metadata": {"license": "Apache-2.0", "len_tokens": 728}}
{"id": "requests:src/requests/_internal_utils.py", "language": "python", "code": "\"\"\"\nrequests._internal_utils\n~~~~~~~~~~~~~~\n\nProvides utility functions that are consumed internally by Requests\nwhich depend on extremely few external helpers (such as compat)\n\"\"\"\nimport re\n\nfrom .compat import builtin_str\n\n_VALID_HEADER_NAME_RE_BYTE = re.compile(rb\"^[^:\\s][^:\\r\\n]*$\")\n_VALID_HEADER_NAME_RE_STR = re.compile(r\"^[^:\\s][^:\\r\\n]*$\")\n_VALID_HEADER_VALUE_RE_BYTE = re.compile(rb\"^\\S[^\\r\\n]*$|^$\")\n_VALID_HEADER_VALUE_RE_STR = re.compile(r\"^\\S[^\\r\\n]*$|^$\")\n\n_HEADER_VALIDATORS_STR = (_VALID_HEADER_NAME_RE_STR, _VALID_HEADER_VALUE_RE_STR)\n_HEADER_VALIDATORS_BYTE = (_VALID_HEADER_NAME_RE_BYTE, _VALID_HEADER_VALUE_RE_BYTE)\nHEADER_VALIDATORS = {\n    bytes: _HEADER_VALIDATORS_BYTE,\n    str: _HEADER_VALIDATORS_STR,\n}\n\n\ndef to_native_string(string, encoding=\"ascii\"):\n    \"\"\"Given a string object, regardless of type, returns a representation of\n    that string in the native string type, encoding and decoding where\n    necessary. This assumes ASCII unless told otherwise.\n    \"\"\"\n    if isinstance(string, builtin_str):\n        out = string\n    else:\n        out = string.decode(encoding)\n\n    return out\n\n\ndef unicode_is_ascii(u_string):\n    \"\"\"Determine if unicode string only contains ASCII characters.\n\n    :param str u_string: unicode string to check. Must be unicode\n        and not Python 2 `str`.\n    :rtype: bool\n    \"\"\"\n    assert isinstance(u_string, str)\n    try:\n        u_string.encode(\"ascii\")\n        return True\n    except UnicodeEncodeError:\n        return False\n", "metadata": {"license": "Apache-2.0", "len_tokens": 352}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def super_len(o):\n    total_length = None\n    current_position = 0\n\n    if not is_urllib3_1 and isinstance(o, str):\n        # urllib3 2.x+ treats all strings as utf-8 instead\n        # of latin-1 (iso-8859-1) like http.client.\n        o = o.encode(\"utf-8\")\n\n    if hasattr(o, \"__len__\"):\n        total_length = len(o)\n\n    elif hasattr(o, \"len\"):\n        total_length = o.len\n\n    elif hasattr(o, \"fileno\"):\n        try:\n            fileno = o.fileno()\n        except (io.UnsupportedOperation, AttributeError):\n            # AttributeError is a surprising exception, seeing as how we've just checked\n            # that `hasattr(o, 'fileno')`.  It happens for objects obtained via\n            # `Tarfile.extractfile()`, per issue 5229.\n            pass\n        else:\n            total_length = os.fstat(fileno).st_size\n\n            # Having used fstat to determine the file length, we need to\n            # confirm that this file was opened up in binary mode.\n            if \"b\" not in o.mode:\n                warnings.warn(\n                    (\n                        \"Requests has determined the content-length for this \"\n                        \"request using the binary size of the file: however, the \"\n                        \"file has been opened in text mode (i.e. without the 'b' \"\n                        \"flag in the mode). This may lead to an incorrect \"\n                        \"content-length. In Requests 3.0, support will be removed \"\n                        \"for files in text mode.\"\n                    ),\n                    FileModeWarning,\n                )\n\n    if hasattr(o, \"tell\"):\n        try:\n            current_position = o.tell()\n        except OSError:\n            # This can happen in some weird situations, such as when the file\n            # is actually a special file descriptor like stdin. In this\n            # instance, we don't know what the length is, so set it to zero and\n            # let requests chunk it instead.\n            if total_length is not None:\n                current_position = total_length\n        else:\n            if hasattr(o, \"seek\") and total_length is None:\n                # StringIO and BytesIO have seek but no usable fileno\n                try:\n                    # seek to end of file\n                    o.seek(0, 2)\n                    total_length = o.tell()\n\n                    # seek back to current position to support\n                    # partially read file-like objects\n                    o.seek(current_position or 0)\n                except OSError:\n                    total_length = 0\n\n    if total_length is None:\n        total_length = 0\n\n    return max(0, total_length - current_position)", "metadata": {"license": "Apache-2.0", "len_tokens": 567}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def get_netrc_auth(url, raise_errors=False):\n    \"\"\"Returns the Requests tuple auth for a given url from netrc.\"\"\"\n\n    netrc_file = os.environ.get(\"NETRC\")\n    if netrc_file is not None:\n        netrc_locations = (netrc_file,)\n    else:\n        netrc_locations = (f\"~/{f}\" for f in NETRC_FILES)\n\n    try:\n        from netrc import NetrcParseError, netrc\n\n        netrc_path = None\n\n        for f in netrc_locations:\n            loc = os.path.expanduser(f)\n            if os.path.exists(loc):\n                netrc_path = loc\n                break\n\n        # Abort early if there isn't one.\n        if netrc_path is None:\n            return\n\n        ri = urlparse(url)\n        host = ri.hostname\n\n        try:\n            _netrc = netrc(netrc_path).authenticators(host)\n            if _netrc:\n                # Return with login / password\n                login_i = 0 if _netrc[0] else 1\n                return (_netrc[login_i], _netrc[2])\n        except (NetrcParseError, OSError):\n            # If there was a parsing error or a permissions issue reading the file,\n            # we'll just skip netrc auth unless explicitly asked to raise errors.\n            if raise_errors:\n                raise\n\n    # App Engine hackiness.\n    except (ImportError, AttributeError):\n        pass", "metadata": {"license": "Apache-2.0", "len_tokens": 302}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def extract_zipped_paths(path):\n    \"\"\"Replace nonexistent paths that look like they refer to a member of a zip\n    archive with the location of an extracted copy of the target, or else\n    just return the provided path unchanged.\n    \"\"\"\n    if os.path.exists(path):\n        # this is already a valid path, no need to do anything further\n        return path\n\n    # find the first valid part of the provided path and treat that as a zip archive\n    # assume the rest of the path is the name of a member in the archive\n    archive, member = os.path.split(path)\n    while archive and not os.path.exists(archive):\n        archive, prefix = os.path.split(archive)\n        if not prefix:\n            # If we don't check for an empty prefix after the split (in other words, archive remains unchanged after the split),\n            # we _can_ end up in an infinite loop on a rare corner case affecting a small number of users\n            break\n        member = \"/\".join([prefix, member])\n\n    if not zipfile.is_zipfile(archive):\n        return path\n\n    zip_file = zipfile.ZipFile(archive)\n    if member not in zip_file.namelist():\n        return path\n\n    # we have a valid zip archive and a valid member of that archive\n    tmp = tempfile.gettempdir()\n    extracted_path = os.path.join(tmp, member.split(\"/\")[-1])\n    if not os.path.exists(extracted_path):\n        # use read + write to avoid the creating nested folders, we only want the file, avoids mkdir racing condition\n        with atomic_open(extracted_path) as file_handler:\n            file_handler.write(zip_file.read(member))\n    return extracted_path", "metadata": {"license": "Apache-2.0", "len_tokens": 353}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def parse_list_header(value):\n    \"\"\"Parse lists as described by RFC 2068 Section 2.\n\n    In particular, parse comma-separated lists where the elements of\n    the list may include quoted-strings.  A quoted-string could\n    contain a comma.  A non-quoted string could have quotes in the\n    middle.  Quotes are removed automatically after parsing.\n\n    It basically works like :func:`parse_set_header` just that items\n    may appear multiple times and case sensitivity is preserved.\n\n    The return value is a standard :class:`list`:\n\n    >>> parse_list_header('token, \"quoted value\"')\n    ['token', 'quoted value']\n\n    To create a header from the :class:`list` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a list header.\n    :return: :class:`list`\n    :rtype: list\n    \"\"\"\n    result = []\n    for item in _parse_list_header(value):\n        if item[:1] == item[-1:] == '\"':\n            item = unquote_header_value(item[1:-1])\n        result.append(item)\n    return result", "metadata": {"license": "Apache-2.0", "len_tokens": 242}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def parse_dict_header(value):\n    \"\"\"Parse lists of key, value pairs as described by RFC 2068 Section 2 and\n    convert them into a python dict:\n\n    >>> d = parse_dict_header('foo=\"is a fish\", bar=\"as well\"')\n    >>> type(d) is dict\n    True\n    >>> sorted(d.items())\n    [('bar', 'as well'), ('foo', 'is a fish')]\n\n    If there is no value for a key it will be `None`:\n\n    >>> parse_dict_header('key_without_value')\n    {'key_without_value': None}\n\n    To create a header from the :class:`dict` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a dict header.\n    :return: :class:`dict`\n    :rtype: dict\n    \"\"\"\n    result = {}\n    for item in _parse_list_header(value):\n        if \"=\" not in item:\n            result[item] = None\n            continue\n        name, value = item.split(\"=\", 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result", "metadata": {"license": "Apache-2.0", "len_tokens": 258}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def unquote_header_value(value, is_filename=False):\n    r\"\"\"Unquotes a header value.  (Reversal of :func:`quote_header_value`).\n    This does not use the real unquoting but what browsers are actually\n    using for quoting.\n\n    :param value: the header value to unquote.\n    :rtype: str\n    \"\"\"\n    if value and value[0] == value[-1] == '\"':\n        # this is not the real unquoting, but fixing this so that the\n        # RFC is met will result in bugs with internet explorer and\n        # probably some other browsers as well.  IE for example is\n        # uploading files with \"C:\\foo\\bar.txt\" as filename\n        value = value[1:-1]\n\n        # if this is a filename and the starting characters look like\n        # a UNC path, then just return the value without quotes.  Using the\n        # replace sequence below on a UNC path has the effect of turning\n        # the leading double slash into a single slash and then\n        # _fix_ie_filename() doesn't work correctly.  See #458.\n        if not is_filename or value[:2] != \"\\\\\\\\\":\n            return value.replace(\"\\\\\\\\\", \"\\\\\").replace('\\\\\"', '\"')\n    return value", "metadata": {"license": "Apache-2.0", "len_tokens": 270}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def get_encodings_from_content(content):\n    \"\"\"Returns encodings from given content string.\n\n    :param content: bytestring to extract encodings from.\n    \"\"\"\n    warnings.warn(\n        (\n            \"In requests 3.0, get_encodings_from_content will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n\n    charset_re = re.compile(r'<meta.*?charset=[\"\\']*(.+?)[\"\\'>]', flags=re.I)\n    pragma_re = re.compile(r'<meta.*?content=[\"\\']*;?charset=(.+?)[\"\\'>]', flags=re.I)\n    xml_re = re.compile(r'^<\\?xml.*?encoding=[\"\\']*(.+?)[\"\\'>]')\n\n    return (\n        charset_re.findall(content)\n        + pragma_re.findall(content)\n        + xml_re.findall(content)\n    )", "metadata": {"license": "Apache-2.0", "len_tokens": 203}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def get_unicode_from_response(r):\n    \"\"\"Returns the requested content back in unicode.\n\n    :param r: Response object to get unicode content from.\n\n    Tried:\n\n    1. charset from content-type\n    2. fall back and replace all unicode characters\n\n    :rtype: str\n    \"\"\"\n    warnings.warn(\n        (\n            \"In requests 3.0, get_unicode_from_response will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n\n    tried_encodings = []\n\n    # Try charset from content-type\n    encoding = get_encoding_from_headers(r.headers)\n\n    if encoding:\n        try:\n            return str(r.content, encoding)\n        except UnicodeError:\n            tried_encodings.append(encoding)\n\n    # Fall back:\n    try:\n        return str(r.content, encoding, errors=\"replace\")\n    except TypeError:\n        return r.content", "metadata": {"license": "Apache-2.0", "len_tokens": 205}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def requote_uri(uri):\n    \"\"\"Re-quote the given URI.\n\n    This function passes the given URI through an unquote/quote cycle to\n    ensure that it is fully and consistently quoted.\n\n    :rtype: str\n    \"\"\"\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        # Unquote only the unreserved characters\n        # Then quote only illegal characters (do not quote reserved,\n        # unreserved, or '%')\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except InvalidURL:\n        # We couldn't unquote the given URI, so let's try quoting it, but\n        # there may be unquoted '%'s in the URI. We need to make sure they're\n        # properly quoted so they do not cause issues elsewhere.\n        return quote(uri, safe=safe_without_percent)", "metadata": {"license": "Apache-2.0", "len_tokens": 201}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def should_bypass_proxies(url, no_proxy):\n    \"\"\"\n    Returns whether we should bypass proxies or not.\n\n    :rtype: bool\n    \"\"\"\n\n    # Prioritize lowercase environment variables over uppercase\n    # to keep a consistent behaviour with other http projects (curl, wget).\n    def get_proxy(key):\n        return os.environ.get(key) or os.environ.get(key.upper())\n\n    # First check whether no_proxy is defined. If it is, check that the URL\n    # we're getting isn't in the no_proxy list.\n    no_proxy_arg = no_proxy\n    if no_proxy is None:\n        no_proxy = get_proxy(\"no_proxy\")\n    parsed = urlparse(url)\n\n    if parsed.hostname is None:\n        # URLs don't always have hostnames, e.g. file:/// urls.\n        return True\n\n    if no_proxy:\n        # We need to check whether we match here. We need to see if we match\n        # the end of the hostname, both with and without the port.\n        no_proxy = (host for host in no_proxy.replace(\" \", \"\").split(\",\") if host)\n\n        if is_ipv4_address(parsed.hostname):\n            for proxy_ip in no_proxy:\n                if is_valid_cidr(proxy_ip):\n                    if address_in_network(parsed.hostname, proxy_ip):\n                        return True\n                elif parsed.hostname == proxy_ip:\n                    # If no_proxy ip was defined in plain IP notation instead of cidr notation &\n                    # matches the IP of the index\n                    return True\n        else:\n            host_with_port = parsed.hostname\n            if parsed.port:\n                host_with_port += f\":{parsed.port}\"\n\n            for host in no_proxy:\n                if parsed.hostname.endswith(host) or host_with_port.endswith(host):\n                    # The URL does match something in no_proxy, so we don't want\n                    # to apply the proxies on this URL.\n                    return True\n\n    with set_environ(\"no_proxy\", no_proxy_arg):\n        # parsed.hostname can be `None` in cases such as a file URI.\n        try:\n            bypass = proxy_bypass(parsed.hostname)\n        except (TypeError, socket.gaierror):\n            bypass = False\n\n    if bypass:\n        return True\n\n    return False", "metadata": {"license": "Apache-2.0", "len_tokens": 453}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def resolve_proxies(request, proxies, trust_env=True):\n    \"\"\"This method takes proxy information from a request and configuration\n    input to resolve a mapping of target proxies. This will consider settings\n    such as NO_PROXY to strip proxy configurations.\n\n    :param request: Request or PreparedRequest\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    :param trust_env: Boolean declaring whether to trust environment configs\n\n    :rtype: dict\n    \"\"\"\n    proxies = proxies if proxies is not None else {}\n    url = request.url\n    scheme = urlparse(url).scheme\n    no_proxy = proxies.get(\"no_proxy\")\n    new_proxies = proxies.copy()\n\n    if trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):\n        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n\n        proxy = environ_proxies.get(scheme, environ_proxies.get(\"all\"))\n\n        if proxy:\n            new_proxies.setdefault(scheme, proxy)\n    return new_proxies", "metadata": {"license": "Apache-2.0", "len_tokens": 214}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def parse_header_links(value):\n    \"\"\"Return a list of parsed link headers proxies.\n\n    i.e. Link: <http:/.../front.jpeg>; rel=front; type=\"image/jpeg\",<http://.../back.jpeg>; rel=back;type=\"image/jpeg\"\n\n    :rtype: list\n    \"\"\"\n\n    links = []\n\n    replace_chars = \" '\\\"\"\n\n    value = value.strip(replace_chars)\n    if not value:\n        return links\n\n    for val in re.split(\", *<\", value):\n        try:\n            url, params = val.split(\";\", 1)\n        except ValueError:\n            url, params = val, \"\"\n\n        link = {\"url\": url.strip(\"<> '\\\"\")}\n\n        for param in params.split(\";\"):\n            try:\n                key, value = param.split(\"=\")\n            except ValueError:\n                break\n\n            link[key.strip(replace_chars)] = value.strip(replace_chars)\n\n        links.append(link)\n\n    return links", "metadata": {"license": "Apache-2.0", "len_tokens": 200}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def guess_json_utf(data):\n    \"\"\"\n    :rtype: str\n    \"\"\"\n    # JSON always starts with two ASCII characters, so detection is as\n    # easy as counting the nulls and from their location and count\n    # determine the encoding. Also detect a BOM, if present.\n    sample = data[:4]\n    if sample in (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):\n        return \"utf-32\"  # BOM included\n    if sample[:3] == codecs.BOM_UTF8:\n        return \"utf-8-sig\"  # BOM included, MS style (discouraged)\n    if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):\n        return \"utf-16\"  # BOM included\n    nullcount = sample.count(_null)\n    if nullcount == 0:\n        return \"utf-8\"\n    if nullcount == 2:\n        if sample[::2] == _null2:  # 1st and 3rd are null\n            return \"utf-16-be\"\n        if sample[1::2] == _null2:  # 2nd and 4th are null\n            return \"utf-16-le\"\n        # Did not detect 2 valid UTF-16 ascii-range characters\n    if nullcount == 3:\n        if sample[:3] == _null3:\n            return \"utf-32-be\"\n        if sample[1:] == _null3:\n            return \"utf-32-le\"\n        # Did not detect a valid UTF-32 ascii-range character\n    return None", "metadata": {"license": "Apache-2.0", "len_tokens": 343}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def prepend_scheme_if_needed(url, new_scheme):\n    \"\"\"Given a URL that may or may not have a scheme, prepend the given scheme.\n    Does not replace a present scheme with the one provided as an argument.\n\n    :rtype: str\n    \"\"\"\n    parsed = parse_url(url)\n    scheme, auth, host, port, path, query, fragment = parsed\n\n    # A defect in urlparse determines that there isn't a netloc present in some\n    # urls. We previously assumed parsing was overly cautious, and swapped the\n    # netloc and path. Due to a lack of tests on the original defect, this is\n    # maintained with parse_url for backwards compatibility.\n    netloc = parsed.netloc\n    if not netloc:\n        netloc, path = path, netloc\n\n    if auth:\n        # parse_url doesn't provide the netloc with auth\n        # so we'll add it ourselves.\n        netloc = \"@\".join([auth, netloc])\n    if scheme is None:\n        scheme = new_scheme\n    if path is None:\n        path = \"\"\n\n    return urlunparse((scheme, netloc, path, \"\", query, fragment))", "metadata": {"license": "Apache-2.0", "len_tokens": 243}}
{"id": "requests:src/requests/utils.py", "language": "python", "code": "def proxy_bypass_registry(host):\n        try:\n            import winreg\n        except ImportError:\n            return False\n\n        try:\n            internetSettings = winreg.OpenKey(\n                winreg.HKEY_CURRENT_USER,\n                r\"Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\",\n            )\n            # ProxyEnable could be REG_SZ or REG_DWORD, normalizing it\n            proxyEnable = int(winreg.QueryValueEx(internetSettings, \"ProxyEnable\")[0])\n            # ProxyOverride is almost always a string\n            proxyOverride = winreg.QueryValueEx(internetSettings, \"ProxyOverride\")[0]\n        except (OSError, ValueError):\n            return False\n        if not proxyEnable or not proxyOverride:\n            return False\n\n        # make a check value list from the registry entry: replace the\n        # '<local>' string by the localhost entry and the corresponding\n        # canonical entry.\n        proxyOverride = proxyOverride.split(\";\")\n        # filter out empty strings to avoid re.match return true in the following code.\n        proxyOverride = filter(None, proxyOverride)\n        # now check if we match one of the registry values.\n        for test in proxyOverride:\n            if test == \"<local>\":\n                if \".\" not in host:\n                    return True\n            test = test.replace(\".\", r\"\\.\")  # mask dots\n            test = test.replace(\"*\", r\".*\")  # change glob sequence\n            test = test.replace(\"?\", r\".\")  # change glob char\n            if re.match(test, host, re.I):\n                return True\n        return False", "metadata": {"license": "Apache-2.0", "len_tokens": 328}}
{"id": "requests:src/requests/exceptions.py", "language": "python", "code": "class JSONDecodeError(InvalidJSONError, CompatJSONDecodeError):\n    \"\"\"Couldn't decode the text into json\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Construct the JSONDecodeError instance first with all\n        args. Then use it's args to construct the IOError so that\n        the json specific args aren't used as IOError specific args\n        and the error message from JSONDecodeError is preserved.\n        \"\"\"\n        CompatJSONDecodeError.__init__(self, *args)\n        InvalidJSONError.__init__(self, *self.args, **kwargs)\n\n    def __reduce__(self):\n        \"\"\"\n        The __reduce__ method called when pickling the object must\n        be the one from the JSONDecodeError (be it json/simplejson)\n        as it expects all the arguments for instantiation, not just\n        one like the IOError, and the MRO would by default call the\n        __reduce__ method from the IOError due to the inheritance order.\n        \"\"\"\n        return CompatJSONDecodeError.__reduce__(self)", "metadata": {"license": "Apache-2.0", "len_tokens": 214}}
{"id": "requests:src/requests/structures.py", "language": "python", "code": "\"\"\"\nrequests.structures\n~~~~~~~~~~~~~~~~~~~\n\nData structures that power Requests.\n\"\"\"\n\nfrom collections import OrderedDict\n\nfrom .compat import Mapping, MutableMapping\n\n\nclass CaseInsensitiveDict(MutableMapping):\n    \"\"\"A case-insensitive ``dict``-like object.\n\n    Implements all methods and operations of\n    ``MutableMapping`` as well as dict's ``copy``. Also\n    provides ``lower_items``.\n\n    All keys are expected to be strings. The structure remembers the\n    case of the last key to be set, and ``iter(instance)``,\n    ``keys()``, ``items()``, ``iterkeys()``, and ``iteritems()``\n    will contain case-sensitive keys. However, querying and contains\n    testing is case insensitive::\n\n        cid = CaseInsensitiveDict()\n        cid['Accept'] = 'application/json'\n        cid['aCCEPT'] == 'application/json'  # True\n        list(cid) == ['Accept']  # True\n\n    For example, ``headers['content-encoding']`` will return the\n    value of a ``'Content-Encoding'`` response header, regardless\n    of how the header name was originally stored.\n\n    If the constructor, ``.update``, or equality comparison\n    operations are given keys that have equal ``.lower()``s, the\n    behavior is undefined.\n    \"\"\"\n\n    def __init__(self, data=None, **kwargs):\n        self._store = OrderedDict()\n        if data is None:\n            data = {}\n        self.update(data, **kwargs)\n\n    def __setitem__(self, key, value):\n        # Use the lowercased key for lookups, but store the actual\n        # key alongside the value.\n        self._store[key.lower()] = (key, value)\n\n    def __getitem__(self, key):\n        return self._store[key.lower()][1]\n\n    def __delitem__(self, key):\n        del self._store[key.lower()]\n\n    def __iter__(self):\n        return (casedkey for casedkey, mappedvalue in self._store.values())\n\n    def __len__(self):\n        return len(self._store)\n\n    def lower_items(self):\n        \"\"\"Like iteritems(), but with all lowercase keys.\"\"\"\n        return ((lowerkey, keyval[1]) for (lowerkey, keyval) in self._store.items())\n\n    def __eq__(self, other):\n        if isinstance(other, Mapping):\n            other = CaseInsensitiveDict(other)\n        else:\n            return NotImplemented\n        # Compare insensitively\n        return dict(self.lower_items()) == dict(other.lower_items())\n\n    # Copy is required\n    def copy(self):\n        return CaseInsensitiveDict(self._store.values())\n\n    def __repr__(self):\n        return str(dict(self.items()))\n\n\nclass LookupDict(dict):\n    \"\"\"Dictionary lookup object.\"\"\"\n\n    def __init__(self, name=None):\n        self.name = name\n        super().__init__()\n\n    def __repr__(self):\n        return f\"<lookup '{self.name}'>\"\n\n    def __getitem__(self, key):\n        # We allow fall-through here, so values default to None\n\n        return self.__dict__.get(key, None)\n\n    def get(self, key, default=None):\n        return self.__dict__.get(key, default)\n", "metadata": {"license": "Apache-2.0", "len_tokens": 677}}
{"id": "requests:src/requests/structures.py", "language": "python", "code": "class CaseInsensitiveDict(MutableMapping):\n    \"\"\"A case-insensitive ``dict``-like object.\n\n    Implements all methods and operations of\n    ``MutableMapping`` as well as dict's ``copy``. Also\n    provides ``lower_items``.\n\n    All keys are expected to be strings. The structure remembers the\n    case of the last key to be set, and ``iter(instance)``,\n    ``keys()``, ``items()``, ``iterkeys()``, and ``iteritems()``\n    will contain case-sensitive keys. However, querying and contains\n    testing is case insensitive::\n\n        cid = CaseInsensitiveDict()\n        cid['Accept'] = 'application/json'\n        cid['aCCEPT'] == 'application/json'  # True\n        list(cid) == ['Accept']  # True\n\n    For example, ``headers['content-encoding']`` will return the\n    value of a ``'Content-Encoding'`` response header, regardless\n    of how the header name was originally stored.\n\n    If the constructor, ``.update``, or equality comparison\n    operations are given keys that have equal ``.lower()``s, the\n    behavior is undefined.\n    \"\"\"\n\n    def __init__(self, data=None, **kwargs):\n        self._store = OrderedDict()\n        if data is None:\n            data = {}\n        self.update(data, **kwargs)\n\n    def __setitem__(self, key, value):\n        # Use the lowercased key for lookups, but store the actual\n        # key alongside the value.\n        self._store[key.lower()] = (key, value)\n\n    def __getitem__(self, key):\n        return self._store[key.lower()][1]\n\n    def __delitem__(self, key):\n        del self._store[key.lower()]\n\n    def __iter__(self):\n        return (casedkey for casedkey, mappedvalue in self._store.values())\n\n    def __len__(self):\n        return len(self._store)\n\n    def lower_items(self):\n        \"\"\"Like iteritems(), but with all lowercase keys.\"\"\"\n        return ((lowerkey, keyval[1]) for (lowerkey, keyval) in self._store.items())\n\n    def __eq__(self, other):\n        if isinstance(other, Mapping):\n            other = CaseInsensitiveDict(other)\n        else:\n            return NotImplemented\n        # Compare insensitively\n        return dict(self.lower_items()) == dict(other.lower_items())\n\n    # Copy is required\n    def copy(self):\n        return CaseInsensitiveDict(self._store.values())\n\n    def __repr__(self):\n        return str(dict(self.items()))", "metadata": {"license": "Apache-2.0", "len_tokens": 541}}
{"id": "requests:src/requests/help.py", "language": "python", "code": "def _implementation():\n    \"\"\"Return a dict with the Python implementation and version.\n\n    Provide both the name and the version of the Python implementation\n    currently running. For example, on CPython 3.10.3 it will return\n    {'name': 'CPython', 'version': '3.10.3'}.\n\n    This function works best on CPython and PyPy: in particular, it probably\n    doesn't work for Jython or IronPython. Future investigation should be done\n    to work out the correct shape of the code for those platforms.\n    \"\"\"\n    implementation = platform.python_implementation()\n\n    if implementation == \"CPython\":\n        implementation_version = platform.python_version()\n    elif implementation == \"PyPy\":\n        implementation_version = \"{}.{}.{}\".format(\n            sys.pypy_version_info.major,\n            sys.pypy_version_info.minor,\n            sys.pypy_version_info.micro,\n        )\n        if sys.pypy_version_info.releaselevel != \"final\":\n            implementation_version = \"\".join(\n                [implementation_version, sys.pypy_version_info.releaselevel]\n            )\n    elif implementation == \"Jython\":\n        implementation_version = platform.python_version()  # Complete Guess\n    elif implementation == \"IronPython\":\n        implementation_version = platform.python_version()  # Complete Guess\n    else:\n        implementation_version = \"Unknown\"\n\n    return {\"name\": implementation, \"version\": implementation_version}", "metadata": {"license": "Apache-2.0", "len_tokens": 287}}
{"id": "requests:src/requests/help.py", "language": "python", "code": "def info():\n    \"\"\"Generate information for a bug report.\"\"\"\n    try:\n        platform_info = {\n            \"system\": platform.system(),\n            \"release\": platform.release(),\n        }\n    except OSError:\n        platform_info = {\n            \"system\": \"Unknown\",\n            \"release\": \"Unknown\",\n        }\n\n    implementation_info = _implementation()\n    urllib3_info = {\"version\": urllib3.__version__}\n    charset_normalizer_info = {\"version\": None}\n    chardet_info = {\"version\": None}\n    if charset_normalizer:\n        charset_normalizer_info = {\"version\": charset_normalizer.__version__}\n    if chardet:\n        chardet_info = {\"version\": chardet.__version__}\n\n    pyopenssl_info = {\n        \"version\": None,\n        \"openssl_version\": \"\",\n    }\n    if OpenSSL:\n        pyopenssl_info = {\n            \"version\": OpenSSL.__version__,\n            \"openssl_version\": f\"{OpenSSL.SSL.OPENSSL_VERSION_NUMBER:x}\",\n        }\n    cryptography_info = {\n        \"version\": getattr(cryptography, \"__version__\", \"\"),\n    }\n    idna_info = {\n        \"version\": getattr(idna, \"__version__\", \"\"),\n    }\n\n    system_ssl = ssl.OPENSSL_VERSION_NUMBER\n    system_ssl_info = {\"version\": f\"{system_ssl:x}\" if system_ssl is not None else \"\"}\n\n    return {\n        \"platform\": platform_info,\n        \"implementation\": implementation_info,\n        \"system_ssl\": system_ssl_info,\n        \"using_pyopenssl\": pyopenssl is not None,\n        \"using_charset_normalizer\": chardet is None,\n        \"pyOpenSSL\": pyopenssl_info,\n        \"urllib3\": urllib3_info,\n        \"chardet\": chardet_info,\n        \"charset_normalizer\": charset_normalizer_info,\n        \"cryptography\": cryptography_info,\n        \"idna\": idna_info,\n        \"requests\": {\n            \"version\": requests_version,\n        },\n    }", "metadata": {"license": "Apache-2.0", "len_tokens": 408}}
{"id": "requests:src/requests/adapters.py", "language": "python", "code": "def _urllib3_request_context(\n    request: \"PreparedRequest\",\n    verify: \"bool | str | None\",\n    client_cert: \"typing.Tuple[str, str] | str | None\",\n    poolmanager: \"PoolManager\",\n) -> \"(typing.Dict[str, typing.Any], typing.Dict[str, typing.Any])\":\n    host_params = {}\n    pool_kwargs = {}\n    parsed_request_url = urlparse(request.url)\n    scheme = parsed_request_url.scheme.lower()\n    port = parsed_request_url.port\n\n    cert_reqs = \"CERT_REQUIRED\"\n    if verify is False:\n        cert_reqs = \"CERT_NONE\"\n    elif isinstance(verify, str):\n        if not os.path.isdir(verify):\n            pool_kwargs[\"ca_certs\"] = verify\n        else:\n            pool_kwargs[\"ca_cert_dir\"] = verify\n    pool_kwargs[\"cert_reqs\"] = cert_reqs\n    if client_cert is not None:\n        if isinstance(client_cert, tuple) and len(client_cert) == 2:\n            pool_kwargs[\"cert_file\"] = client_cert[0]\n            pool_kwargs[\"key_file\"] = client_cert[1]\n        else:\n            # According to our docs, we allow users to specify just the client\n            # cert path\n            pool_kwargs[\"cert_file\"] = client_cert\n    host_params = {\n        \"scheme\": scheme,\n        \"host\": parsed_request_url.hostname,\n        \"port\": port,\n    }\n    return host_params, pool_kwargs", "metadata": {"license": "Apache-2.0", "len_tokens": 303}}
{"id": "requests:src/requests/adapters.py", "language": "python", "code": "class BaseAdapter:\n    \"\"\"The Base Transport Adapter\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def send(\n        self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None\n    ):\n        \"\"\"Sends PreparedRequest object. Returns Response object.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param stream: (optional) Whether to stream the request content.\n        :param timeout: (optional) How long to wait for the server to send\n            data before giving up, as a float, or a :ref:`(connect timeout,\n            read timeout) <timeouts>` tuple.\n        :type timeout: float or tuple\n        :param verify: (optional) Either a boolean, in which case it controls whether we verify\n            the server's TLS certificate, or a string, in which case it must be a path\n            to a CA bundle to use\n        :param cert: (optional) Any user-provided SSL certificate to be trusted.\n        :param proxies: (optional) The proxies dictionary to apply to the request.\n        \"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Cleans up adapter specific items.\"\"\"\n        raise NotImplementedError", "metadata": {"license": "Apache-2.0", "len_tokens": 260}}
{"id": "requests:src/requests/adapters.py", "language": "python", "code": "def send(\n        self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None\n    ):\n        \"\"\"Sends PreparedRequest object. Returns Response object.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param stream: (optional) Whether to stream the request content.\n        :param timeout: (optional) How long to wait for the server to send\n            data before giving up, as a float, or a :ref:`(connect timeout,\n            read timeout) <timeouts>` tuple.\n        :type timeout: float or tuple\n        :param verify: (optional) Either a boolean, in which case it controls whether we verify\n            the server's TLS certificate, or a string, in which case it must be a path\n            to a CA bundle to use\n        :param cert: (optional) Any user-provided SSL certificate to be trusted.\n        :param proxies: (optional) The proxies dictionary to apply to the request.\n        \"\"\"\n        raise NotImplementedError", "metadata": {"license": "Apache-2.0", "len_tokens": 216}}
{"id": "requests:src/requests/adapters.py", "language": "python", "code": "def init_poolmanager(\n        self, connections, maxsize, block=DEFAULT_POOLBLOCK, **pool_kwargs\n    ):\n        \"\"\"Initializes a urllib3 PoolManager.\n\n        This method should not be called from user code, and is only\n        exposed for use when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param connections: The number of urllib3 connection pools to cache.\n        :param maxsize: The maximum number of connections to save in the pool.\n        :param block: Block when no free connections are available.\n        :param pool_kwargs: Extra keyword arguments used to initialize the Pool Manager.\n        \"\"\"\n        # save these values for pickling\n        self._pool_connections = connections\n        self._pool_maxsize = maxsize\n        self._pool_block = block\n\n        self.poolmanager = PoolManager(\n            num_pools=connections,\n            maxsize=maxsize,\n            block=block,\n            **pool_kwargs,\n        )", "metadata": {"license": "Apache-2.0", "len_tokens": 203}}
{"id": "requests:src/requests/adapters.py", "language": "python", "code": "def proxy_manager_for(self, proxy, **proxy_kwargs):\n        \"\"\"Return urllib3 ProxyManager for the given proxy.\n\n        This method should not be called from user code, and is only\n        exposed for use when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param proxy: The proxy to return a urllib3 ProxyManager for.\n        :param proxy_kwargs: Extra keyword arguments used to configure the Proxy Manager.\n        :returns: ProxyManager\n        :rtype: urllib3.ProxyManager\n        \"\"\"\n        if proxy in self.proxy_manager:\n            manager = self.proxy_manager[proxy]\n        elif proxy.lower().startswith(\"socks\"):\n            username, password = get_auth_from_url(proxy)\n            manager = self.proxy_manager[proxy] = SOCKSProxyManager(\n                proxy,\n                username=username,\n                password=password,\n                num_pools=self._pool_connections,\n                maxsize=self._pool_maxsize,\n                block=self._pool_block,\n                **proxy_kwargs,\n            )\n        else:\n            proxy_headers = self.proxy_headers(proxy)\n            manager = self.proxy_manager[proxy] = proxy_from_url(\n                proxy,\n                proxy_headers=proxy_headers,\n                num_pools=self._pool_connections,\n                maxsize=self._pool_maxsize,\n                block=self._pool_block,\n                **proxy_kwargs,\n            )\n\n        return manager", "metadata": {"license": "Apache-2.0", "len_tokens": 278}}
{"id": "requests:src/requests/adapters.py", "language": "python", "code": "def cert_verify(self, conn, url, verify, cert):\n        \"\"\"Verify a SSL certificate. This method should not be called from user\n        code, and is only exposed for use when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param conn: The urllib3 connection object associated with the cert.\n        :param url: The requested URL.\n        :param verify: Either a boolean, in which case it controls whether we verify\n            the server's TLS certificate, or a string, in which case it must be a path\n            to a CA bundle to use\n        :param cert: The SSL certificate to verify.\n        \"\"\"\n        if url.lower().startswith(\"https\") and verify:\n            cert_loc = None\n\n            # Allow self-specified cert location.\n            if verify is not True:\n                cert_loc = verify\n\n            if not cert_loc:\n                cert_loc = extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)\n\n            if not cert_loc or not os.path.exists(cert_loc):\n                raise OSError(\n                    f\"Could not find a suitable TLS CA certificate bundle, \"\n                    f\"invalid path: {cert_loc}\"\n                )\n\n            conn.cert_reqs = \"CERT_REQUIRED\"\n\n            if not os.path.isdir(cert_loc):\n                conn.ca_certs = cert_loc\n            else:\n                conn.ca_cert_dir = cert_loc\n        else:\n            conn.cert_reqs = \"CERT_NONE\"\n            conn.ca_certs = None\n            conn.ca_cert_dir = None\n\n        if cert:\n            if not isinstance(cert, basestring):\n                conn.cert_file = cert[0]\n                conn.key_file = cert[1]\n            else:\n                conn.cert_file = cert\n                conn.key_file = None\n            if conn.cert_file and not os.path.exists(conn.cert_file):\n                raise OSError(\n                    f\"Could not find the TLS certificate file, \"\n                    f\"invalid path: {conn.cert_file}\"\n                )\n            if conn.key_file and not os.path.exists(conn.key_file):\n                raise OSError(\n                    f\"Could not find the TLS key file, invalid path: {conn.key_file}\"\n                )", "metadata": {"license": "Apache-2.0", "len_tokens": 444}}
{"id": "requests:src/requests/adapters.py", "language": "python", "code": "def build_response(self, req, resp):\n        \"\"\"Builds a :class:`Response <requests.Response>` object from a urllib3\n        response. This should not be called from user code, and is only exposed\n        for use when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`\n\n        :param req: The :class:`PreparedRequest <PreparedRequest>` used to generate the response.\n        :param resp: The urllib3 response object.\n        :rtype: requests.Response\n        \"\"\"\n        response = Response()\n\n        # Fallback to None if there's no status_code, for whatever reason.\n        response.status_code = getattr(resp, \"status\", None)\n\n        # Make headers case-insensitive.\n        response.headers = CaseInsensitiveDict(getattr(resp, \"headers\", {}))\n\n        # Set encoding.\n        response.encoding = get_encoding_from_headers(response.headers)\n        response.raw = resp\n        response.reason = response.raw.reason\n\n        if isinstance(req.url, bytes):\n            response.url = req.url.decode(\"utf-8\")\n        else:\n            response.url = req.url\n\n        # Add new cookies from the server.\n        extract_cookies_to_jar(response.cookies, req, resp)\n\n        # Give the Response some context.\n        response.request = req\n        response.connection = self\n\n        return response", "metadata": {"license": "Apache-2.0", "len_tokens": 271}}
{"id": "requests:src/requests/adapters.py", "language": "python", "code": "def build_connection_pool_key_attributes(self, request, verify, cert=None):\n        \"\"\"Build the PoolKey attributes used by urllib3 to return a connection.\n\n        This looks at the PreparedRequest, the user-specified verify value,\n        and the value of the cert parameter to determine what PoolKey values\n        to use to select a connection from a given urllib3 Connection Pool.\n\n        The SSL related pool key arguments are not consistently set. As of\n        this writing, use the following to determine what keys may be in that\n        dictionary:\n\n        * If ``verify`` is ``True``, ``\"ssl_context\"`` will be set and will be the\n          default Requests SSL Context\n        * If ``verify`` is ``False``, ``\"ssl_context\"`` will not be set but\n          ``\"cert_reqs\"`` will be set\n        * If ``verify`` is a string, (i.e., it is a user-specified trust bundle)\n          ``\"ca_certs\"`` will be set if the string is not a directory recognized\n          by :py:func:`os.path.isdir`, otherwise ``\"ca_cert_dir\"`` will be\n          set.\n        * If ``\"cert\"`` is specified, ``\"cert_file\"`` will always be set. If\n          ``\"cert\"`` is a tuple with a second item, ``\"key_file\"`` will also\n          be present\n\n        To override these settings, one may subclass this class, call this\n        method and use the above logic to change parameters as desired. For\n        example, if one wishes to use a custom :py:class:`ssl.SSLContext` one\n        must both set ``\"ssl_context\"`` and based on what else they require,\n        alter the other keys to ensure the desired behaviour.\n\n        :param request:\n            The PreparedReqest being sent over the connection.\n        :type request:\n            :class:`~requests.models.PreparedRequest`\n        :param verify:\n            Either a boolean, in which case it controls whether\n            we verify the server's TLS certificate, or a string, in which case it\n            must be a path to a CA bundle to use.\n        :param cert:\n            (optional) Any user-provided SSL certificate for client\n            authentication (a.k.a., mTLS). This may be a string (i.e., just\n            the path to a file which holds both certificate and key) or a\n            tuple of length 2 with the certificate file path and key file\n            path.\n        :returns:\n            A tuple of two dictionaries. The first is the \"host parameters\"\n            portion of the Pool Key including scheme, hostname, and port. The\n            second is a dictionary of SSLContext related parameters.\n        \"\"\"\n        return _urllib3_request_context(request, verify, cert, self.poolmanager)", "metadata": {"license": "Apache-2.0", "len_tokens": 592}}
{"id": "requests:src/requests/adapters.py", "language": "python", "code": "def get_connection_with_tls_context(self, request, verify, proxies=None, cert=None):\n        \"\"\"Returns a urllib3 connection for the given request and TLS settings.\n        This should not be called from user code, and is only exposed for use\n        when subclassing the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param request:\n            The :class:`PreparedRequest <PreparedRequest>` object to be sent\n            over the connection.\n        :param verify:\n            Either a boolean, in which case it controls whether we verify the\n            server's TLS certificate, or a string, in which case it must be a\n            path to a CA bundle to use.\n        :param proxies:\n            (optional) The proxies dictionary to apply to the request.\n        :param cert:\n            (optional) Any user-provided SSL certificate to be used for client\n            authentication (a.k.a., mTLS).\n        :rtype:\n            urllib3.ConnectionPool\n        \"\"\"\n        proxy = select_proxy(request.url, proxies)\n        try:\n            host_params, pool_kwargs = self.build_connection_pool_key_attributes(\n                request,\n                verify,\n                cert,\n            )\n        except ValueError as e:\n            raise InvalidURL(e, request=request)\n        if proxy:\n            proxy = prepend_scheme_if_needed(proxy, \"http\")\n            proxy_url = parse_url(proxy)\n            if not proxy_url.host:\n                raise InvalidProxyURL(\n                    \"Please check proxy URL. It is malformed \"\n                    \"and could be missing the host.\"\n                )\n            proxy_manager = self.proxy_manager_for(proxy)\n            conn = proxy_manager.connection_from_host(\n                **host_params, pool_kwargs=pool_kwargs\n            )\n        else:\n            # Only scheme should be lower case\n            conn = self.poolmanager.connection_from_host(\n                **host_params, pool_kwargs=pool_kwargs\n            )\n\n        return conn", "metadata": {"license": "Apache-2.0", "len_tokens": 384}}
{"id": "requests:src/requests/adapters.py", "language": "python", "code": "def get_connection(self, url, proxies=None):\n        \"\"\"DEPRECATED: Users should move to `get_connection_with_tls_context`\n        for all subclasses of HTTPAdapter using Requests>=2.32.2.\n\n        Returns a urllib3 connection for the given URL. This should not be\n        called from user code, and is only exposed for use when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param url: The URL to connect to.\n        :param proxies: (optional) A Requests-style dictionary of proxies used on this request.\n        :rtype: urllib3.ConnectionPool\n        \"\"\"\n        warnings.warn(\n            (\n                \"`get_connection` has been deprecated in favor of \"\n                \"`get_connection_with_tls_context`. Custom HTTPAdapter subclasses \"\n                \"will need to migrate for Requests>=2.32.2. Please see \"\n                \"https://github.com/psf/requests/pull/6710 for more details.\"\n            ),\n            DeprecationWarning,\n        )\n        proxy = select_proxy(url, proxies)\n\n        if proxy:\n            proxy = prepend_scheme_if_needed(proxy, \"http\")\n            proxy_url = parse_url(proxy)\n            if not proxy_url.host:\n                raise InvalidProxyURL(\n                    \"Please check proxy URL. It is malformed \"\n                    \"and could be missing the host.\"\n                )\n            proxy_manager = self.proxy_manager_for(proxy)\n            conn = proxy_manager.connection_from_url(url)\n        else:\n            # Only scheme should be lower case\n            parsed = urlparse(url)\n            url = parsed.geturl()\n            conn = self.poolmanager.connection_from_url(url)\n\n        return conn", "metadata": {"license": "Apache-2.0", "len_tokens": 336}}
{"id": "requests:src/requests/adapters.py", "language": "python", "code": "def request_url(self, request, proxies):\n        \"\"\"Obtain the url to use when making the final request.\n\n        If the message is being sent through a HTTP proxy, the full URL has to\n        be used. Otherwise, we should only use the path portion of the URL.\n\n        This should not be called from user code, and is only exposed for use\n        when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs.\n        :rtype: str\n        \"\"\"\n        proxy = select_proxy(request.url, proxies)\n        scheme = urlparse(request.url).scheme\n\n        is_proxied_http_request = proxy and scheme != \"https\"\n        using_socks_proxy = False\n        if proxy:\n            proxy_scheme = urlparse(proxy).scheme.lower()\n            using_socks_proxy = proxy_scheme.startswith(\"socks\")\n\n        url = request.path_url\n        if url.startswith(\"//\"):  # Don't confuse urllib3\n            url = f\"/{url.lstrip('/')}\"\n\n        if is_proxied_http_request and not using_socks_proxy:\n            url = urldefragauth(request.url)\n\n        return url", "metadata": {"license": "Apache-2.0", "len_tokens": 270}}
{"id": "pydantic:release/prepare.py", "language": "python", "code": "def update_version(new_version: str, dry_run: bool) -> None:\n    \"\"\"Update the version in the giving py version file.\"\"\"\n    version_file_path = ROOT_DIR / PACKAGE_VERSION_FILE\n    content = version_file_path.read_text(encoding='utf-8')\n\n    # Regex to match the VERSION assignment\n    pattern = r'(VERSION\\s*=\\s*[\\'\\\"])([^\\\"^\\']+)([\\'\\\"])'\n    version_stm = re.search(pattern, content)\n    if not version_stm:\n        print(\n            'Could not find the version assignment in the version file. '\n            \"Please make sure the version file has a line like `VERSION: Final = '1.2.3'`.\"\n        )\n        raise SystemExit(1)\n    old_version = version_stm.group(2)\n    if old_version == new_version:\n        warnings.warn('The new version is the same as the old version. The script might not have any effect.')\n    old_version_stm = ''.join(version_stm.groups())\n    new_version_stm = old_version_stm.replace(old_version, new_version)\n\n    if dry_run:\n        print(f'Updating version in version file at \"{PACKAGE_VERSION_FILE}\"')\n        print('--- Before ---')\n        print(old_version_stm)\n        print('--- After ---')\n        print(new_version_stm)\n        print('Running in dry mode, lock file is not updated.')\n        return\n\n    version_file_path.write_text(content.replace(old_version_stm, new_version_stm), encoding='utf-8')\n    run_command('uv', 'lock', '-P', 'pydantic')", "metadata": {"license": "MIT", "len_tokens": 338}}
{"id": "pydantic:release/prepare.py", "language": "python", "code": "def get_notes(new_version: str) -> str:\n    \"\"\"Fetch auto-generated release notes from github.\"\"\"\n    last_tag = run_command('git', 'describe', '--tags', '--abbrev=0')\n    auth_token = GITHUB_TOKEN\n\n    data = {'target_committish': 'main', 'previous_tag_name': last_tag, 'tag_name': f'v{new_version}'}\n    response = requests.post(\n        f'https://api.github.com/repos/{REPO}/releases/generate-notes',\n        headers={\n            'Accept': 'application/vnd.github+json',\n            'Authorization': f'Bearer {auth_token}',\n            'x-github-api-version': '2022-11-28',\n        },\n        data=json.dumps(data),\n        timeout=100,\n    )\n    response.raise_for_status()\n\n    body = response.json()['body']\n    body = body.replace('<!-- Release notes generated using configuration in .github/release.yml at main -->\\n\\n', '')\n\n    # Add one level to all headers so they match HISTORY.md, and add trailing newline\n    body = re.sub(pattern='^(#+ .+?)$', repl=r'#\\1\\n', string=body, flags=re.MULTILINE)\n\n    # Ensure a blank line before headers\n    body = re.sub(pattern='([^\\n])(\\n#+ .+?\\n)', repl=r'\\1\\n\\2', string=body)\n\n    # Render PR links nicely\n    body = re.sub(\n        pattern=f'https://github.com/{REPO}/pull/(\\\\d+)',\n        repl=f'[#\\\\1](https://github.com/{REPO}/pull/\\\\1)',\n        string=body,\n    )\n\n    # Remove \"full changelog\" link\n    body = re.sub(\n        pattern=r'\\*\\*Full Changelog\\*\\*: https://.*$',\n        repl='',\n        string=body,\n    )\n\n    return body.strip()", "metadata": {"license": "MIT", "len_tokens": 400}}
{"id": "pydantic:release/prepare.py", "language": "python", "code": "def update_history(new_version: str, dry_run: bool, force_update: bool) -> None:\n    \"\"\"Generate release notes and prepend them to HISTORY.md.\"\"\"\n    history_path = ROOT_DIR / HISTORY_FILE\n    history_content = history_path.read_text(encoding='utf8')\n\n    # use ( to avoid matching beta versions\n    if f'## v{new_version} (' in history_content and not force_update:\n        warnings.warn(\n            f'WARNING: v{new_version} already in history, {HISTORY_FILE} not updated. \\n'\n            'Use --force or -f to update the history file anyway.'\n        )\n        return\n\n    date_today_str = f'{date.today():%Y-%m-%d}'\n    title = f'v{new_version} ({date_today_str})'\n    notes = get_notes(new_version)\n    new_chunk = f'## {title}\\n\\n[GitHub release](https://github.com/{REPO}/releases/tag/v{new_version})\\n\\n{notes}\\n\\n'\n    if dry_run:\n        print(f'Would add the following to {history_path}:\\n{new_chunk}')\n    history = new_chunk + history_content\n\n    if not dry_run:\n        history_path.write_text(history)\n        print(f'\\nSUCCESS: added \"{title}\" section to {history_path.relative_to(ROOT_DIR)}')\n\n    citation_path = ROOT_DIR / 'CITATION.cff'\n    citation_text = citation_path.read_text()\n\n    citation_text = re.sub(r'(?<=\\nversion: ).*', f'v{new_version}', citation_text)\n    citation_text = re.sub(r'(?<=date-released: ).*', date_today_str, citation_text)\n    if dry_run:\n        print(\n            f'Would update version=v{new_version} and date-released={date_today_str} in '\n            f'{citation_path.relative_to(ROOT_DIR)}'\n        )\n        print(f'Updated content:\\n{citation_text}')\n    else:\n        citation_path.write_text(citation_text)\n        print(\n            f'SUCCESS: updated version=v{new_version} and date-released={date_today_str} '\n            f'in {citation_path.relative_to(ROOT_DIR)}'\n        )", "metadata": {"license": "MIT", "len_tokens": 466}}
{"id": "pydantic:release/push.py", "language": "python", "code": "def create_github_release_draft(rl_version: str, rl_release_notes: str):\n    \"\"\"Create a GitHub release draft.\"\"\"\n    url = f'https://api.github.com/repos/{REPO}/releases'\n    headers = {'Authorization': f'token {GITHUB_TOKEN}'}\n    data = {\n        'tag_name': f'v{rl_version}',\n        'name': f'v{rl_version}',\n        'body': rl_release_notes,\n        'draft': True,\n        'prerelease': False,\n    }\n    response = requests.post(url, json=data, headers=headers, timeout=10)\n    try:\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as e:\n        print(f'HTTP error occurred: {e}')\n        print(f'Response content: {response.content.decode()}')\n        raise e\n    release_url = response.json()['html_url']\n    # Publishing happens in the edit page\n    edit_url = release_url.replace('/releases/tag/', '/releases/edit/')\n    return edit_url", "metadata": {"license": "MIT", "len_tokens": 215}}
{"id": "pydantic:pydantic/functional_validators.py", "language": "python", "code": "class AfterValidator:\n    \"\"\"!!! abstract \"Usage Documentation\"\n        [field *after* validators](../concepts/validators.md#field-after-validator)\n\n    A metadata class that indicates that a validation should be applied **after** the inner validation logic.\n\n    Attributes:\n        func: The validator function.\n\n    Example:\n        ```python\n        from typing import Annotated\n\n        from pydantic import AfterValidator, BaseModel, ValidationError\n\n        MyInt = Annotated[int, AfterValidator(lambda v: v + 1)]\n\n        class Model(BaseModel):\n            a: MyInt\n\n        print(Model(a=1).a)\n        #> 2\n\n        try:\n            Model(a='a')\n        except ValidationError as e:\n            print(e.json(indent=2))\n            '''\n            [\n              {\n                \"type\": \"int_parsing\",\n                \"loc\": [\n                  \"a\"\n                ],\n                \"msg\": \"Input should be a valid integer, unable to parse string as an integer\",\n                \"input\": \"a\",\n                \"url\": \"https://errors.pydantic.dev/2/v/int_parsing\"\n              }\n            ]\n            '''\n        ```\n    \"\"\"\n\n    func: core_schema.NoInfoValidatorFunction | core_schema.WithInfoValidatorFunction\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source_type)\n        info_arg = _inspect_validator(self.func, mode='after', type='field')\n        if info_arg:\n            func = cast(core_schema.WithInfoValidatorFunction, self.func)\n            return core_schema.with_info_after_validator_function(func, schema=schema)\n        else:\n            func = cast(core_schema.NoInfoValidatorFunction, self.func)\n            return core_schema.no_info_after_validator_function(func, schema=schema)\n\n    @classmethod\n    def _from_decorator(cls, decorator: _decorators.Decorator[_decorators.FieldValidatorDecoratorInfo]) -> Self:\n        return cls(func=decorator.func)", "metadata": {"license": "MIT", "len_tokens": 419}}
{"id": "pydantic:pydantic/functional_validators.py", "language": "python", "code": "class BeforeValidator:\n    \"\"\"!!! abstract \"Usage Documentation\"\n        [field *before* validators](../concepts/validators.md#field-before-validator)\n\n    A metadata class that indicates that a validation should be applied **before** the inner validation logic.\n\n    Attributes:\n        func: The validator function.\n        json_schema_input_type: The input type used to generate the appropriate\n            JSON Schema (in validation mode). The actual input type is `Any`.\n\n    Example:\n        ```python\n        from typing import Annotated\n\n        from pydantic import BaseModel, BeforeValidator\n\n        MyInt = Annotated[int, BeforeValidator(lambda v: v + 1)]\n\n        class Model(BaseModel):\n            a: MyInt\n\n        print(Model(a=1).a)\n        #> 2\n\n        try:\n            Model(a='a')\n        except TypeError as e:\n            print(e)\n            #> can only concatenate str (not \"int\") to str\n        ```\n    \"\"\"\n\n    func: core_schema.NoInfoValidatorFunction | core_schema.WithInfoValidatorFunction\n    json_schema_input_type: Any = PydanticUndefined\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source_type)\n        input_schema = (\n            None\n            if self.json_schema_input_type is PydanticUndefined\n            else handler.generate_schema(self.json_schema_input_type)\n        )\n\n        info_arg = _inspect_validator(self.func, mode='before', type='field')\n        if info_arg:\n            func = cast(core_schema.WithInfoValidatorFunction, self.func)\n            return core_schema.with_info_before_validator_function(\n                func,\n                schema=schema,\n                json_schema_input_schema=input_schema,\n            )\n        else:\n            func = cast(core_schema.NoInfoValidatorFunction, self.func)\n            return core_schema.no_info_before_validator_function(\n                func, schema=schema, json_schema_input_schema=input_schema\n            )\n\n    @classmethod\n    def _from_decorator(cls, decorator: _decorators.Decorator[_decorators.FieldValidatorDecoratorInfo]) -> Self:\n        return cls(\n            func=decorator.func,\n            json_schema_input_type=decorator.info.json_schema_input_type,\n        )", "metadata": {"license": "MIT", "len_tokens": 469}}
{"id": "pydantic:pydantic/functional_validators.py", "language": "python", "code": "class WrapValidator:\n    \"\"\"!!! abstract \"Usage Documentation\"\n        [field *wrap* validators](../concepts/validators.md#field-wrap-validator)\n\n    A metadata class that indicates that a validation should be applied **around** the inner validation logic.\n\n    Attributes:\n        func: The validator function.\n        json_schema_input_type: The input type used to generate the appropriate\n            JSON Schema (in validation mode). The actual input type is `Any`.\n\n    ```python\n    from datetime import datetime\n    from typing import Annotated\n\n    from pydantic import BaseModel, ValidationError, WrapValidator\n\n    def validate_timestamp(v, handler):\n        if v == 'now':\n            # we don't want to bother with further validation, just return the new value\n            return datetime.now()\n        try:\n            return handler(v)\n        except ValidationError:\n            # validation failed, in this case we want to return a default value\n            return datetime(2000, 1, 1)\n\n    MyTimestamp = Annotated[datetime, WrapValidator(validate_timestamp)]\n\n    class Model(BaseModel):\n        a: MyTimestamp\n\n    print(Model(a='now').a)\n    #> 2032-01-02 03:04:05.000006\n    print(Model(a='invalid').a)\n    #> 2000-01-01 00:00:00\n    ```\n    \"\"\"\n\n    func: core_schema.NoInfoWrapValidatorFunction | core_schema.WithInfoWrapValidatorFunction\n    json_schema_input_type: Any = PydanticUndefined\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source_type)\n        input_schema = (\n            None\n            if self.json_schema_input_type is PydanticUndefined\n            else handler.generate_schema(self.json_schema_input_type)\n        )\n\n        info_arg = _inspect_validator(self.func, mode='wrap', type='field')\n        if info_arg:\n            func = cast(core_schema.WithInfoWrapValidatorFunction, self.func)\n            return core_schema.with_info_wrap_validator_function(\n                func,\n                schema=schema,\n                json_schema_input_schema=input_schema,\n            )\n        else:\n            func = cast(core_schema.NoInfoWrapValidatorFunction, self.func)\n            return core_schema.no_info_wrap_validator_function(\n                func,\n                schema=schema,\n                json_schema_input_schema=input_schema,\n            )\n\n    @classmethod\n    def _from_decorator(cls, decorator: _decorators.Decorator[_decorators.FieldValidatorDecoratorInfo]) -> Self:\n        return cls(\n            func=decorator.func,\n            json_schema_input_type=decorator.info.json_schema_input_type,\n        )", "metadata": {"license": "MIT", "len_tokens": 561}}
{"id": "pydantic:pydantic/functional_validators.py", "language": "python", "code": "def model_validator(\n    *,\n    mode: Literal['wrap', 'before', 'after'],\n) -> Any:\n    \"\"\"!!! abstract \"Usage Documentation\"\n        [Model Validators](../concepts/validators.md#model-validators)\n\n    Decorate model methods for validation purposes.\n\n    Example usage:\n    ```python\n    from typing_extensions import Self\n\n    from pydantic import BaseModel, ValidationError, model_validator\n\n    class Square(BaseModel):\n        width: float\n        height: float\n\n        @model_validator(mode='after')\n        def verify_square(self) -> Self:\n            if self.width != self.height:\n                raise ValueError('width and height do not match')\n            return self\n\n    s = Square(width=1, height=1)\n    print(repr(s))\n    #> Square(width=1.0, height=1.0)\n\n    try:\n        Square(width=1, height=2)\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Square\n          Value error, width and height do not match [type=value_error, input_value={'width': 1, 'height': 2}, input_type=dict]\n        '''\n    ```\n\n    For more in depth examples, see [Model Validators](../concepts/validators.md#model-validators).\n\n    Args:\n        mode: A required string literal that specifies the validation mode.\n            It can be one of the following: 'wrap', 'before', or 'after'.\n\n    Returns:\n        A decorator that can be used to decorate a function to be used as a model validator.\n    \"\"\"\n\n    def dec(f: Any) -> _decorators.PydanticDescriptorProxy[Any]:\n        # auto apply the @classmethod decorator. NOTE: in V3, do not apply the conversion for 'after' validators:\n        f = _decorators.ensure_classmethod_based_on_signature(f)\n        if mode == 'after' and isinstance(f, classmethod):\n            warnings.warn(\n                category=PydanticDeprecatedSince212,\n                message=(\n                    \"Using `@model_validator` with mode='after' on a classmethod is deprecated. Instead, use an instance method. \"\n                    f'See the documentation at https://docs.pydantic.dev/{version_short()}/concepts/validators/#model-after-validator.'\n                ),\n                stacklevel=2,\n            )\n\n        dec_info = _decorators.ModelValidatorDecoratorInfo(mode=mode)\n        return _decorators.PydanticDescriptorProxy(f, dec_info)\n\n    return dec", "metadata": {"license": "MIT", "len_tokens": 519}}
{"id": "pydantic:pydantic/functional_validators.py", "language": "python", "code": "class ValidateAs:\n    \"\"\"A helper class to validate a custom type from a type that is natively supported by Pydantic.\n\n    Args:\n        from_type: The type natively supported by Pydantic to use to perform validation.\n        instantiation_hook: A callable taking the validated type as an argument, and returning\n            the populated custom type.\n\n    Example:\n        ```python {lint=\"skip\"}\n        from typing import Annotated\n\n        from pydantic import BaseModel, TypeAdapter, ValidateAs\n\n        class MyCls:\n            def __init__(self, a: int) -> None:\n                self.a = a\n\n            def __repr__(self) -> str:\n                return f\"MyCls(a={self.a})\"\n\n        class Model(BaseModel):\n            a: int\n\n\n        ta = TypeAdapter(\n            Annotated[MyCls, ValidateAs(Model, lambda v: MyCls(a=v.a))]\n        )\n\n        print(ta.validate_python({'a': 1}))\n        #> MyCls(a=1)\n        ```\n    \"\"\"\n\n    # TODO: make use of PEP 747\n    def __init__(self, from_type: type[_FromTypeT], /, instantiation_hook: Callable[[_FromTypeT], Any]) -> None:\n        self.from_type = from_type\n        self.instantiation_hook = instantiation_hook\n\n    def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(self.from_type)\n        return core_schema.no_info_after_validator_function(\n            self.instantiation_hook,\n            schema=schema,\n        )", "metadata": {"license": "MIT", "len_tokens": 340}}
{"id": "pydantic:pydantic/functional_validators.py", "language": "python", "code": "def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        # Note that for some valid uses of PlainValidator, it is not possible to generate a core schema for the\n        # source_type, so calling `handler(source_type)` will error, which prevents us from generating a proper\n        # serialization schema. To work around this for use cases that will not involve serialization, we simply\n        # catch any PydanticSchemaGenerationError that may be raised while attempting to build the serialization schema\n        # and abort any attempts to handle special serialization.\n        from pydantic import PydanticSchemaGenerationError\n\n        try:\n            schema = handler(source_type)\n            # TODO if `schema['serialization']` is one of `'include-exclude-dict/sequence',\n            # schema validation will fail. That's why we use 'type ignore' comments below.\n            serialization = schema.get(\n                'serialization',\n                core_schema.wrap_serializer_function_ser_schema(\n                    function=lambda v, h: h(v),\n                    schema=schema,\n                    return_schema=handler.generate_schema(source_type),\n                ),\n            )\n        except PydanticSchemaGenerationError:\n            serialization = None\n\n        input_schema = handler.generate_schema(self.json_schema_input_type)\n\n        info_arg = _inspect_validator(self.func, mode='plain', type='field')\n        if info_arg:\n            func = cast(core_schema.WithInfoValidatorFunction, self.func)\n            return core_schema.with_info_plain_validator_function(\n                func,\n                serialization=serialization,  # pyright: ignore[reportArgumentType]\n                json_schema_input_schema=input_schema,\n            )\n        else:\n            func = cast(core_schema.NoInfoValidatorFunction, self.func)\n            return core_schema.no_info_plain_validator_function(\n                func,\n                serialization=serialization,  # pyright: ignore[reportArgumentType]\n                json_schema_input_schema=input_schema,\n            )", "metadata": {"license": "MIT", "len_tokens": 400}}
{"id": "pydantic:pydantic/functional_validators.py", "language": "python", "code": "class InstanceOf:\n        '''Generic type for annotating a type that is an instance of a given class.\n\n        Example:\n            ```python\n            from pydantic import BaseModel, InstanceOf\n\n            class Foo:\n                ...\n\n            class Bar(BaseModel):\n                foo: InstanceOf[Foo]\n\n            Bar(foo=Foo())\n            try:\n                Bar(foo=42)\n            except ValidationError as e:\n                print(e)\n                \"\"\"\n                [\n                   {\n                      'type': 'is_instance_of',\n                      'loc': ('foo',),\n                      'msg': 'Input should be an instance of Foo',\n                      'input': 42,\n                      'ctx': {'class': 'Foo'},\n                      'url': 'https://errors.pydantic.dev/0.38.0/v/is_instance_of'\n                   }\n                ]\n                \"\"\"\n            ```\n        '''\n\n        @classmethod\n        def __class_getitem__(cls, item: AnyType) -> AnyType:\n            return Annotated[item, cls()]\n\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n            from pydantic import PydanticSchemaGenerationError\n\n            # use the generic _origin_ as the second argument to isinstance when appropriate\n            instance_of_schema = core_schema.is_instance_schema(_generics.get_origin(source) or source)\n\n            try:\n                # Try to generate the \"standard\" schema, which will be used when loading from JSON\n                original_schema = handler(source)\n            except PydanticSchemaGenerationError:\n                # If that fails, just produce a schema that can validate from python\n                return instance_of_schema\n            else:\n                # Use the \"original\" approach to serialization\n                instance_of_schema['serialization'] = core_schema.wrap_serializer_function_ser_schema(\n                    function=lambda v, h: h(v), schema=original_schema\n                )\n                return core_schema.json_or_python_schema(python_schema=instance_of_schema, json_schema=original_schema)\n\n        __hash__ = object.__hash__", "metadata": {"license": "MIT", "len_tokens": 454}}
{"id": "pydantic:pydantic/functional_validators.py", "language": "python", "code": "class SkipValidation:\n        \"\"\"If this is applied as an annotation (e.g., via `x: Annotated[int, SkipValidation]`), validation will be\n            skipped. You can also use `SkipValidation[int]` as a shorthand for `Annotated[int, SkipValidation]`.\n\n        This can be useful if you want to use a type annotation for documentation/IDE/type-checking purposes,\n        and know that it is safe to skip validation for one or more of the fields.\n\n        Because this converts the validation schema to `any_schema`, subsequent annotation-applied transformations\n        may not have the expected effects. Therefore, when used, this annotation should generally be the final\n        annotation applied to a type.\n        \"\"\"\n\n        def __class_getitem__(cls, item: Any) -> Any:\n            return Annotated[item, SkipValidation()]\n\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore', ArbitraryTypeWarning)\n                original_schema = handler(source)\n            metadata = {'pydantic_js_annotation_functions': [lambda _c, h: h(original_schema)]}\n            return core_schema.any_schema(\n                metadata=metadata,\n                serialization=core_schema.wrap_serializer_function_ser_schema(\n                    function=lambda v, h: h(v), schema=original_schema\n                ),\n            )\n\n        __hash__ = object.__hash__", "metadata": {"license": "MIT", "len_tokens": 306}}
{"id": "pydantic:pydantic/functional_validators.py", "language": "python", "code": "def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n            from pydantic import PydanticSchemaGenerationError\n\n            # use the generic _origin_ as the second argument to isinstance when appropriate\n            instance_of_schema = core_schema.is_instance_schema(_generics.get_origin(source) or source)\n\n            try:\n                # Try to generate the \"standard\" schema, which will be used when loading from JSON\n                original_schema = handler(source)\n            except PydanticSchemaGenerationError:\n                # If that fails, just produce a schema that can validate from python\n                return instance_of_schema\n            else:\n                # Use the \"original\" approach to serialization\n                instance_of_schema['serialization'] = core_schema.wrap_serializer_function_ser_schema(\n                    function=lambda v, h: h(v), schema=original_schema\n                )\n                return core_schema.json_or_python_schema(python_schema=instance_of_schema, json_schema=original_schema)", "metadata": {"license": "MIT", "len_tokens": 208}}
{"id": "pydantic:pydantic/alias_generators.py", "language": "python", "code": "\"\"\"Alias generators for converting between different capitalization conventions.\"\"\"\n\nimport re\n\n__all__ = ('to_pascal', 'to_camel', 'to_snake')\n\n# TODO: in V3, change the argument names to be more descriptive\n# Generally, don't only convert from snake_case, or name the functions\n# more specifically like snake_to_camel.\n\n\ndef to_pascal(snake: str) -> str:\n    \"\"\"Convert a snake_case string to PascalCase.\n\n    Args:\n        snake: The string to convert.\n\n    Returns:\n        The PascalCase string.\n    \"\"\"\n    camel = snake.title()\n    return re.sub('([0-9A-Za-z])_(?=[0-9A-Z])', lambda m: m.group(1), camel)\n\n\ndef to_camel(snake: str) -> str:\n    \"\"\"Convert a snake_case string to camelCase.\n\n    Args:\n        snake: The string to convert.\n\n    Returns:\n        The converted camelCase string.\n    \"\"\"\n    # If the string is already in camelCase and does not contain a digit followed\n    # by a lowercase letter, return it as it is\n    if re.match('^[a-z]+[A-Za-z0-9]*$', snake) and not re.search(r'\\d[a-z]', snake):\n        return snake\n\n    camel = to_pascal(snake)\n    return re.sub('(^_*[A-Z])', lambda m: m.group(1).lower(), camel)\n\n\ndef to_snake(camel: str) -> str:\n    \"\"\"Convert a PascalCase, camelCase, or kebab-case string to snake_case.\n\n    Args:\n        camel: The string to convert.\n\n    Returns:\n        The converted string in snake_case.\n    \"\"\"\n    # Handle the sequence of uppercase letters followed by a lowercase letter\n    snake = re.sub(r'([A-Z]+)([A-Z][a-z])', lambda m: f'{m.group(1)}_{m.group(2)}', camel)\n    # Insert an underscore between a lowercase letter and an uppercase letter\n    snake = re.sub(r'([a-z])([A-Z])', lambda m: f'{m.group(1)}_{m.group(2)}', snake)\n    # Insert an underscore between a digit and an uppercase letter\n    snake = re.sub(r'([0-9])([A-Z])', lambda m: f'{m.group(1)}_{m.group(2)}', snake)\n    # Insert an underscore between a lowercase letter and a digit\n    snake = re.sub(r'([a-z])([0-9])', lambda m: f'{m.group(1)}_{m.group(2)}', snake)\n    # Replace hyphens with underscores to handle kebab-case\n    snake = snake.replace('-', '_')\n    return snake.lower()\n", "metadata": {"license": "MIT", "len_tokens": 584}}
{"id": "pydantic:pydantic/alias_generators.py", "language": "python", "code": "def to_snake(camel: str) -> str:\n    \"\"\"Convert a PascalCase, camelCase, or kebab-case string to snake_case.\n\n    Args:\n        camel: The string to convert.\n\n    Returns:\n        The converted string in snake_case.\n    \"\"\"\n    # Handle the sequence of uppercase letters followed by a lowercase letter\n    snake = re.sub(r'([A-Z]+)([A-Z][a-z])', lambda m: f'{m.group(1)}_{m.group(2)}', camel)\n    # Insert an underscore between a lowercase letter and an uppercase letter\n    snake = re.sub(r'([a-z])([A-Z])', lambda m: f'{m.group(1)}_{m.group(2)}', snake)\n    # Insert an underscore between a digit and an uppercase letter\n    snake = re.sub(r'([0-9])([A-Z])', lambda m: f'{m.group(1)}_{m.group(2)}', snake)\n    # Insert an underscore between a lowercase letter and a digit\n    snake = re.sub(r'([a-z])([0-9])', lambda m: f'{m.group(1)}_{m.group(2)}', snake)\n    # Replace hyphens with underscores to handle kebab-case\n    snake = snake.replace('-', '_')\n    return snake.lower()", "metadata": {"license": "MIT", "len_tokens": 282}}
{"id": "pydantic:pydantic/color.py", "language": "python", "code": "def parse_str(value: str) -> RGBA:\n    \"\"\"Parse a string representing a color to an RGBA tuple.\n\n    Possible formats for the input string include:\n\n    * named color, see `COLORS_BY_NAME`\n    * hex short eg. `<prefix>fff` (prefix can be `#`, `0x` or nothing)\n    * hex long eg. `<prefix>ffffff` (prefix can be `#`, `0x` or nothing)\n    * `rgb(<r>, <g>, <b>)`\n    * `rgba(<r>, <g>, <b>, <a>)`\n\n    Args:\n        value: A string representing a color.\n\n    Returns:\n        An `RGBA` tuple parsed from the input string.\n\n    Raises:\n        ValueError: If the input string cannot be parsed to an RGBA tuple.\n    \"\"\"\n    value_lower = value.lower()\n    try:\n        r, g, b = COLORS_BY_NAME[value_lower]\n    except KeyError:\n        pass\n    else:\n        return ints_to_rgba(r, g, b, None)\n\n    m = re.fullmatch(r_hex_short, value_lower)\n    if m:\n        *rgb, a = m.groups()\n        r, g, b = (int(v * 2, 16) for v in rgb)\n        if a:\n            alpha: Optional[float] = int(a * 2, 16) / 255\n        else:\n            alpha = None\n        return ints_to_rgba(r, g, b, alpha)\n\n    m = re.fullmatch(r_hex_long, value_lower)\n    if m:\n        *rgb, a = m.groups()\n        r, g, b = (int(v, 16) for v in rgb)\n        if a:\n            alpha = int(a, 16) / 255\n        else:\n            alpha = None\n        return ints_to_rgba(r, g, b, alpha)\n\n    m = re.fullmatch(r_rgb, value_lower) or re.fullmatch(r_rgb_v4_style, value_lower)\n    if m:\n        return ints_to_rgba(*m.groups())  # type: ignore\n\n    m = re.fullmatch(r_hsl, value_lower) or re.fullmatch(r_hsl_v4_style, value_lower)\n    if m:\n        return parse_hsl(*m.groups())  # type: ignore\n\n    raise PydanticCustomError('color_error', 'value is not a valid color: string not recognised as a valid color')", "metadata": {"license": "MIT", "len_tokens": 521}}
{"id": "pydantic:pydantic/color.py", "language": "python", "code": "def parse_color_value(value: Union[int, str], max_val: int = 255) -> float:\n    \"\"\"Parse the color value provided and return a number between 0 and 1.\n\n    Args:\n        value: An integer or string color value.\n        max_val: Maximum range value. Defaults to 255.\n\n    Raises:\n        PydanticCustomError: If the value is not a valid color.\n\n    Returns:\n        A number between 0 and 1.\n    \"\"\"\n    try:\n        color = float(value)\n    except ValueError:\n        raise PydanticCustomError('color_error', 'value is not a valid color: color values must be a valid number')\n    if 0 <= color <= max_val:\n        return color / max_val\n    else:\n        raise PydanticCustomError(\n            'color_error',\n            'value is not a valid color: color values must be in the range 0 to {max_val}',\n            {'max_val': max_val},\n        )", "metadata": {"license": "MIT", "len_tokens": 207}}
{"id": "pydantic:pydantic/color.py", "language": "python", "code": "def parse_float_alpha(value: Union[None, str, float, int]) -> Optional[float]:\n    \"\"\"Parse an alpha value checking it's a valid float in the range 0 to 1.\n\n    Args:\n        value: The input value to parse.\n\n    Returns:\n        The parsed value as a float, or `None` if the value was None or equal 1.\n\n    Raises:\n        PydanticCustomError: If the input value cannot be successfully parsed as a float in the expected range.\n    \"\"\"\n    if value is None:\n        return None\n    try:\n        if isinstance(value, str) and value.endswith('%'):\n            alpha = float(value[:-1]) / 100\n        else:\n            alpha = float(value)\n    except ValueError:\n        raise PydanticCustomError('color_error', 'value is not a valid color: alpha values must be a valid float')\n\n    if math.isclose(alpha, 1):\n        return None\n    elif 0 <= alpha <= 1:\n        return alpha\n    else:\n        raise PydanticCustomError('color_error', 'value is not a valid color: alpha values must be in the range 0 to 1')", "metadata": {"license": "MIT", "len_tokens": 247}}
{"id": "pydantic:pydantic/color.py", "language": "python", "code": "def parse_hsl(h: str, h_units: str, sat: str, light: str, alpha: Optional[float] = None) -> RGBA:\n    \"\"\"Parse raw hue, saturation, lightness, and alpha values and convert to RGBA.\n\n    Args:\n        h: The hue value.\n        h_units: The unit for hue value.\n        sat: The saturation value.\n        light: The lightness value.\n        alpha: Alpha value.\n\n    Returns:\n        An instance of `RGBA`.\n    \"\"\"\n    s_value, l_value = parse_color_value(sat, 100), parse_color_value(light, 100)\n\n    h_value = float(h)\n    if h_units in {None, 'deg'}:\n        h_value = h_value % 360 / 360\n    elif h_units == 'rad':\n        h_value = h_value % rads / rads\n    else:\n        # turns\n        h_value = h_value % 1\n\n    r, g, b = hls_to_rgb(h_value, l_value, s_value)\n    return RGBA(r, g, b, parse_float_alpha(alpha))", "metadata": {"license": "MIT", "len_tokens": 236}}
{"id": "pydantic:pydantic/color.py", "language": "python", "code": "def as_named(self, *, fallback: bool = False) -> str:\n        \"\"\"Returns the name of the color if it can be found in `COLORS_BY_VALUE` dictionary,\n        otherwise returns the hexadecimal representation of the color or raises `ValueError`.\n\n        Args:\n            fallback: If True, falls back to returning the hexadecimal representation of\n                the color instead of raising a ValueError when no named color is found.\n\n        Returns:\n            The name of the color, or the hexadecimal representation of the color.\n\n        Raises:\n            ValueError: When no named color is found and fallback is `False`.\n        \"\"\"\n        if self._rgba.alpha is None:\n            rgb = cast(tuple[int, int, int], self.as_rgb_tuple())\n            try:\n                return COLORS_BY_VALUE[rgb]\n            except KeyError as e:\n                if fallback:\n                    return self.as_hex()\n                else:\n                    raise ValueError('no named color found, use fallback=True, as_hex() or as_rgb()') from e\n        else:\n            return self.as_hex()", "metadata": {"license": "MIT", "len_tokens": 215}}
{"id": "pydantic:pydantic/color.py", "language": "python", "code": "def as_rgb_tuple(self, *, alpha: Optional[bool] = None) -> ColorTuple:\n        \"\"\"Returns the color as an RGB or RGBA tuple.\n\n        Args:\n            alpha: Whether to include the alpha channel. There are three options for this input:\n\n                - `None` (default): Include alpha only if it's set. (e.g. not `None`)\n                - `True`: Always include alpha.\n                - `False`: Always omit alpha.\n\n        Returns:\n            A tuple that contains the values of the red, green, and blue channels in the range 0 to 255.\n                If alpha is included, it is in the range 0 to 1.\n        \"\"\"\n        r, g, b = (float_to_255(c) for c in self._rgba[:3])\n        if alpha is None:\n            if self._rgba.alpha is None:\n                return r, g, b\n            else:\n                return r, g, b, self._alpha_float()\n        elif alpha:\n            return r, g, b, self._alpha_float()\n        else:\n            # alpha is False\n            return r, g, b", "metadata": {"license": "MIT", "len_tokens": 237}}
{"id": "pydantic:pydantic/color.py", "language": "python", "code": "def as_hsl_tuple(self, *, alpha: Optional[bool] = None) -> HslColorTuple:\n        \"\"\"Returns the color as an HSL or HSLA tuple.\n\n        Args:\n            alpha: Whether to include the alpha channel.\n\n                - `None` (default): Include the alpha channel only if it's set (e.g. not `None`).\n                - `True`: Always include alpha.\n                - `False`: Always omit alpha.\n\n        Returns:\n            The color as a tuple of hue, saturation, lightness, and alpha (if included).\n                All elements are in the range 0 to 1.\n\n        Note:\n            This is HSL as used in HTML and most other places, not HLS as used in Python's `colorsys`.\n        \"\"\"\n        h, l, s = rgb_to_hls(self._rgba.r, self._rgba.g, self._rgba.b)  # noqa: E741\n        if alpha is None:\n            if self._rgba.alpha is None:\n                return h, s, l\n            else:\n                return h, s, l, self._alpha_float()\n        if alpha:\n            return h, s, l, self._alpha_float()\n        else:\n            # alpha is False\n            return h, s, l", "metadata": {"license": "MIT", "len_tokens": 265}}
{"id": "pydantic:pydantic/config.py", "language": "python", "code": "def with_config(config: ConfigDict | None = None, /, **kwargs: Any) -> Callable[[_TypeT], _TypeT]:\n    \"\"\"!!! abstract \"Usage Documentation\"\n        [Configuration with other types](../concepts/config.md#configuration-on-other-supported-types)\n\n    A convenience decorator to set a [Pydantic configuration](config.md) on a `TypedDict` or a `dataclass` from the standard library.\n\n    Although the configuration can be set using the `__pydantic_config__` attribute, it does not play well with type checkers,\n    especially with `TypedDict`.\n\n    !!! example \"Usage\"\n\n        ```python\n        from typing_extensions import TypedDict\n\n        from pydantic import ConfigDict, TypeAdapter, with_config\n\n        @with_config(ConfigDict(str_to_lower=True))\n        class TD(TypedDict):\n            x: str\n\n        ta = TypeAdapter(TD)\n\n        print(ta.validate_python({'x': 'ABC'}))\n        #> {'x': 'abc'}\n        ```\n\n    /// deprecated-removed | v2.11 v3\n    Passing `config` as a keyword argument.\n    ///\n\n    /// version-changed | v2.11\n    Keyword arguments can be provided directly instead of a config dictionary.\n    ///\n    \"\"\"\n    if config is not None and kwargs:\n        raise ValueError('Cannot specify both `config` and keyword arguments')\n\n    if len(kwargs) == 1 and (kwargs_conf := kwargs.get('config')) is not None:\n        warnings.warn(\n            'Passing `config` as a keyword argument is deprecated. Pass `config` as a positional argument instead',\n            category=PydanticDeprecatedSince211,\n            stacklevel=2,\n        )\n        final_config = cast(ConfigDict, kwargs_conf)\n    else:\n        final_config = config if config is not None else cast(ConfigDict, kwargs)\n\n    def inner(class_: _TypeT, /) -> _TypeT:\n        # Ideally, we would check for `class_` to either be a `TypedDict` or a stdlib dataclass.\n        # However, the `@with_config` decorator can be applied *after* `@dataclass`. To avoid\n        # common mistakes, we at least check for `class_` to not be a Pydantic model.\n        from ._internal._utils import is_model_class\n\n        if is_model_class(class_):\n            raise PydanticUserError(\n                f'Cannot use `with_config` on {class_.__name__} as it is a Pydantic model',\n                code='with-config-on-model',\n            )\n        class_.__pydantic_config__ = final_config\n        return class_\n\n    return inner", "metadata": {"license": "MIT", "len_tokens": 565}}
{"id": "pydantic:pydantic/version.py", "language": "python", "code": "def version_info() -> str:\n    \"\"\"Return complete version information for Pydantic and its dependencies.\"\"\"\n    import importlib.metadata\n    import platform\n    from pathlib import Path\n\n    import pydantic_core._pydantic_core as pdc\n\n    from ._internal import _git as git\n\n    # get data about packages that are closely related to pydantic, use pydantic or often conflict with pydantic\n    package_names = {\n        'email-validator',\n        'fastapi',\n        'mypy',\n        'pydantic-extra-types',\n        'pydantic-settings',\n        'pyright',\n        'typing_extensions',\n    }\n    related_packages = []\n\n    for dist in importlib.metadata.distributions():\n        name = dist.metadata['Name']\n        if name in package_names:\n            related_packages.append(f'{name}-{dist.version}')\n\n    pydantic_dir = Path(__file__).parents[1].resolve()\n    most_recent_commit = (\n        git.git_revision(pydantic_dir) if git.is_git_repo(pydantic_dir) and git.have_git() else 'unknown'\n    )\n\n    info = {\n        'pydantic version': VERSION,\n        'pydantic-core version': __pydantic_core_version__,\n        'pydantic-core build': getattr(pdc, 'build_info', None) or pdc.build_profile,  # pyright: ignore[reportPrivateImportUsage]\n        'python version': sys.version,\n        'platform': platform.platform(),\n        'related packages': ' '.join(related_packages),\n        'commit': most_recent_commit,\n    }\n    return '\\n'.join('{:>30} {}'.format(k + ':', str(v).replace('\\n', ' ')) for k, v in info.items())", "metadata": {"license": "MIT", "len_tokens": 366}}
{"id": "pydantic:pydantic/version.py", "language": "python", "code": "def _ensure_pydantic_core_version() -> None:  # pragma: no cover\n    if not check_pydantic_core_version():\n        raise_error = True\n        # Do not raise the error if pydantic is installed in editable mode (i.e. in development):\n        if sys.version_info >= (3, 13):  # origin property added in 3.13\n            from importlib.metadata import distribution\n\n            dist = distribution('pydantic')\n            if getattr(getattr(dist.origin, 'dir_info', None), 'editable', False):\n                raise_error = False\n\n        if raise_error:\n            raise SystemError(\n                f'The installed pydantic-core version ({__pydantic_core_version__}) is incompatible '\n                f'with the current pydantic version, which requires {_COMPATIBLE_PYDANTIC_CORE_VERSION}. '\n                \"If you encounter this error, make sure that you haven't upgraded pydantic-core manually.\"\n            )", "metadata": {"license": "MIT", "len_tokens": 202}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "class _FromFieldInfoInputs(TypedDict, total=False):\n    \"\"\"This class exists solely to add type checking for the `**kwargs` in `FieldInfo.from_field`.\"\"\"\n\n    # TODO PEP 747: use TypeForm:\n    annotation: type[Any] | None\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any] | None\n    alias: str | None\n    alias_priority: int | None\n    validation_alias: str | AliasPath | AliasChoices | None\n    serialization_alias: str | None\n    title: str | None\n    field_title_generator: Callable[[str, FieldInfo], str] | None\n    description: str | None\n    examples: list[Any] | None\n    exclude: bool | None\n    exclude_if: Callable[[Any], bool] | None\n    gt: annotated_types.SupportsGt | None\n    ge: annotated_types.SupportsGe | None\n    lt: annotated_types.SupportsLt | None\n    le: annotated_types.SupportsLe | None\n    multiple_of: float | None\n    strict: bool | None\n    min_length: int | None\n    max_length: int | None\n    pattern: str | re.Pattern[str] | None\n    allow_inf_nan: bool | None\n    max_digits: int | None\n    decimal_places: int | None\n    union_mode: Literal['smart', 'left_to_right'] | None\n    discriminator: str | types.Discriminator | None\n    deprecated: Deprecated | str | bool | None\n    json_schema_extra: JsonDict | Callable[[JsonDict], None] | None\n    frozen: bool | None\n    validate_default: bool | None\n    repr: bool\n    init: bool | None\n    init_var: bool | None\n    kw_only: bool | None\n    coerce_numbers_to_str: bool | None\n    fail_fast: bool | None", "metadata": {"license": "MIT", "len_tokens": 406}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def Field(\n    default: ellipsis,  # noqa: F821  # TODO: use `_typing_extra.EllipsisType` when we drop Py3.9\n    *,\n    alias: str | None = _Unset,\n    alias_priority: int | None = _Unset,\n    validation_alias: str | AliasPath | AliasChoices | None = _Unset,\n    serialization_alias: str | None = _Unset,\n    title: str | None = _Unset,\n    field_title_generator: Callable[[str, FieldInfo], str] | None = _Unset,\n    description: str | None = _Unset,\n    examples: list[Any] | None = _Unset,\n    exclude: bool | None = _Unset,\n    exclude_if: Callable[[Any], bool] | None = _Unset,\n    discriminator: str | types.Discriminator | None = _Unset,\n    deprecated: Deprecated | str | bool | None = _Unset,\n    json_schema_extra: JsonDict | Callable[[JsonDict], None] | None = _Unset,\n    frozen: bool | None = _Unset,\n    validate_default: bool | None = _Unset,\n    repr: bool = _Unset,\n    init: bool | None = _Unset,\n    init_var: bool | None = _Unset,\n    kw_only: bool | None = _Unset,\n    pattern: str | re.Pattern[str] | None = _Unset,\n    strict: bool | None = _Unset,\n    coerce_numbers_to_str: bool | None = _Unset,\n    gt: annotated_types.SupportsGt | None = _Unset,\n    ge: annotated_types.SupportsGe | None = _Unset,\n    lt: annotated_types.SupportsLt | None = _Unset,\n    le: annotated_types.SupportsLe | None = _Unset,\n    multiple_of: float | None = _Unset,\n    allow_inf_nan: bool | None = _Unset,\n    max_digits: int | None = _Unset,\n    decimal_places: int | None = _Unset,\n    min_length: int | None = _Unset,\n    max_length: int | None = _Unset,\n    union_mode: Literal['smart', 'left_to_right'] = _Unset,\n    fail_fast: bool | None = _Unset,\n    **extra: Unpack[_EmptyKwargs],\n) -> Any: ...", "metadata": {"license": "MIT", "len_tokens": 481}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def Field(\n    default: Any,\n    *,\n    alias: str | None = _Unset,\n    alias_priority: int | None = _Unset,\n    validation_alias: str | AliasPath | AliasChoices | None = _Unset,\n    serialization_alias: str | None = _Unset,\n    title: str | None = _Unset,\n    field_title_generator: Callable[[str, FieldInfo], str] | None = _Unset,\n    description: str | None = _Unset,\n    examples: list[Any] | None = _Unset,\n    exclude: bool | None = _Unset,\n    exclude_if: Callable[[Any], bool] | None = _Unset,\n    discriminator: str | types.Discriminator | None = _Unset,\n    deprecated: Deprecated | str | bool | None = _Unset,\n    json_schema_extra: JsonDict | Callable[[JsonDict], None] | None = _Unset,\n    frozen: bool | None = _Unset,\n    validate_default: Literal[True],\n    repr: bool = _Unset,\n    init: bool | None = _Unset,\n    init_var: bool | None = _Unset,\n    kw_only: bool | None = _Unset,\n    pattern: str | re.Pattern[str] | None = _Unset,\n    strict: bool | None = _Unset,\n    coerce_numbers_to_str: bool | None = _Unset,\n    gt: annotated_types.SupportsGt | None = _Unset,\n    ge: annotated_types.SupportsGe | None = _Unset,\n    lt: annotated_types.SupportsLt | None = _Unset,\n    le: annotated_types.SupportsLe | None = _Unset,\n    multiple_of: float | None = _Unset,\n    allow_inf_nan: bool | None = _Unset,\n    max_digits: int | None = _Unset,\n    decimal_places: int | None = _Unset,\n    min_length: int | None = _Unset,\n    max_length: int | None = _Unset,\n    union_mode: Literal['smart', 'left_to_right'] = _Unset,\n    fail_fast: bool | None = _Unset,\n    **extra: Unpack[_EmptyKwargs],\n) -> Any: ...", "metadata": {"license": "MIT", "len_tokens": 450}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def Field(\n    default: _T,\n    *,\n    alias: str | None = _Unset,\n    alias_priority: int | None = _Unset,\n    validation_alias: str | AliasPath | AliasChoices | None = _Unset,\n    serialization_alias: str | None = _Unset,\n    title: str | None = _Unset,\n    field_title_generator: Callable[[str, FieldInfo], str] | None = _Unset,\n    description: str | None = _Unset,\n    examples: list[Any] | None = _Unset,\n    exclude: bool | None = _Unset,\n    # NOTE: to get proper type checking on `exclude_if`'s argument, we could use `_T` instead of `Any`. However,\n    # this requires (at least for pyright) adding an additional overload where `exclude_if` is required (otherwise\n    # `a: int = Field(default_factory=str)` results in a false negative).\n    exclude_if: Callable[[Any], bool] | None = _Unset,\n    discriminator: str | types.Discriminator | None = _Unset,\n    deprecated: Deprecated | str | bool | None = _Unset,\n    json_schema_extra: JsonDict | Callable[[JsonDict], None] | None = _Unset,\n    frozen: bool | None = _Unset,\n    validate_default: Literal[False] = ...,\n    repr: bool = _Unset,\n    init: bool | None = _Unset,\n    init_var: bool | None = _Unset,\n    kw_only: bool | None = _Unset,\n    pattern: str | re.Pattern[str] | None = _Unset,\n    strict: bool | None = _Unset,\n    coerce_numbers_to_str: bool | None = _Unset,\n    gt: annotated_types.SupportsGt | None = _Unset,\n    ge: annotated_types.SupportsGe | None = _Unset,\n    lt: annotated_types.SupportsLt | None = _Unset,\n    le: annotated_types.SupportsLe | None = _Unset,\n    multiple_of: float | None = _Unset,\n    allow_inf_nan: bool | None = _Unset,\n    max_digits: int | None = _Unset,\n    decimal_places: int | None = _Unset,\n    min_length: int | None = _Unset,\n    max_length: int | None = _Unset,\n    union_mode: Literal['smart', 'left_to_right'] = _Unset,\n    fail_fast: bool | None = _Unset,\n    **extra: Unpack[_EmptyKwargs],\n) -> _T: ...", "metadata": {"license": "MIT", "len_tokens": 529}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def Field(  # pyright: ignore[reportOverlappingOverload]\n    *,\n    default_factory: Callable[[], Any] | Callable[[dict[str, Any]], Any],\n    alias: str | None = _Unset,\n    alias_priority: int | None = _Unset,\n    validation_alias: str | AliasPath | AliasChoices | None = _Unset,\n    serialization_alias: str | None = _Unset,\n    title: str | None = _Unset,\n    field_title_generator: Callable[[str, FieldInfo], str] | None = _Unset,\n    description: str | None = _Unset,\n    examples: list[Any] | None = _Unset,\n    exclude: bool | None = _Unset,\n    exclude_if: Callable[[Any], bool] | None = _Unset,\n    discriminator: str | types.Discriminator | None = _Unset,\n    deprecated: Deprecated | str | bool | None = _Unset,\n    json_schema_extra: JsonDict | Callable[[JsonDict], None] | None = _Unset,\n    frozen: bool | None = _Unset,\n    validate_default: Literal[True],\n    repr: bool = _Unset,\n    init: bool | None = _Unset,\n    init_var: bool | None = _Unset,\n    kw_only: bool | None = _Unset,\n    pattern: str | re.Pattern[str] | None = _Unset,\n    strict: bool | None = _Unset,\n    coerce_numbers_to_str: bool | None = _Unset,\n    gt: annotated_types.SupportsGt | None = _Unset,\n    ge: annotated_types.SupportsGe | None = _Unset,\n    lt: annotated_types.SupportsLt | None = _Unset,\n    le: annotated_types.SupportsLe | None = _Unset,\n    multiple_of: float | None = _Unset,\n    allow_inf_nan: bool | None = _Unset,\n    max_digits: int | None = _Unset,\n    decimal_places: int | None = _Unset,\n    min_length: int | None = _Unset,\n    max_length: int | None = _Unset,\n    union_mode: Literal['smart', 'left_to_right'] = _Unset,\n    fail_fast: bool | None = _Unset,\n    **extra: Unpack[_EmptyKwargs],\n) -> Any: ...", "metadata": {"license": "MIT", "len_tokens": 477}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def Field(\n    *,\n    default_factory: Callable[[], _T] | Callable[[dict[str, Any]], _T],\n    alias: str | None = _Unset,\n    alias_priority: int | None = _Unset,\n    validation_alias: str | AliasPath | AliasChoices | None = _Unset,\n    serialization_alias: str | None = _Unset,\n    title: str | None = _Unset,\n    field_title_generator: Callable[[str, FieldInfo], str] | None = _Unset,\n    description: str | None = _Unset,\n    examples: list[Any] | None = _Unset,\n    exclude: bool | None = _Unset,\n    # NOTE: to get proper type checking on `exclude_if`'s argument, we could use `_T` instead of `Any`. However,\n    # this requires (at least for pyright) adding an additional overload where `exclude_if` is required (otherwise\n    # `a: int = Field(default_factory=str)` results in a false negative).\n    exclude_if: Callable[[Any], bool] | None = _Unset,\n    discriminator: str | types.Discriminator | None = _Unset,\n    deprecated: Deprecated | str | bool | None = _Unset,\n    json_schema_extra: JsonDict | Callable[[JsonDict], None] | None = _Unset,\n    frozen: bool | None = _Unset,\n    validate_default: Literal[False] | None = _Unset,\n    repr: bool = _Unset,\n    init: bool | None = _Unset,\n    init_var: bool | None = _Unset,\n    kw_only: bool | None = _Unset,\n    pattern: str | re.Pattern[str] | None = _Unset,\n    strict: bool | None = _Unset,\n    coerce_numbers_to_str: bool | None = _Unset,\n    gt: annotated_types.SupportsGt | None = _Unset,\n    ge: annotated_types.SupportsGe | None = _Unset,\n    lt: annotated_types.SupportsLt | None = _Unset,\n    le: annotated_types.SupportsLe | None = _Unset,\n    multiple_of: float | None = _Unset,\n    allow_inf_nan: bool | None = _Unset,\n    max_digits: int | None = _Unset,\n    decimal_places: int | None = _Unset,\n    min_length: int | None = _Unset,\n    max_length: int | None = _Unset,\n    union_mode: Literal['smart', 'left_to_right'] = _Unset,\n    fail_fast: bool | None = _Unset,\n    **extra: Unpack[_EmptyKwargs],\n) -> _T: ...", "metadata": {"license": "MIT", "len_tokens": 547}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def Field(  # No default set\n    *,\n    alias: str | None = _Unset,\n    alias_priority: int | None = _Unset,\n    validation_alias: str | AliasPath | AliasChoices | None = _Unset,\n    serialization_alias: str | None = _Unset,\n    title: str | None = _Unset,\n    field_title_generator: Callable[[str, FieldInfo], str] | None = _Unset,\n    description: str | None = _Unset,\n    examples: list[Any] | None = _Unset,\n    exclude: bool | None = _Unset,\n    exclude_if: Callable[[Any], bool] | None = _Unset,\n    discriminator: str | types.Discriminator | None = _Unset,\n    deprecated: Deprecated | str | bool | None = _Unset,\n    json_schema_extra: JsonDict | Callable[[JsonDict], None] | None = _Unset,\n    frozen: bool | None = _Unset,\n    validate_default: bool | None = _Unset,\n    repr: bool = _Unset,\n    init: bool | None = _Unset,\n    init_var: bool | None = _Unset,\n    kw_only: bool | None = _Unset,\n    pattern: str | re.Pattern[str] | None = _Unset,\n    strict: bool | None = _Unset,\n    coerce_numbers_to_str: bool | None = _Unset,\n    gt: annotated_types.SupportsGt | None = _Unset,\n    ge: annotated_types.SupportsGe | None = _Unset,\n    lt: annotated_types.SupportsLt | None = _Unset,\n    le: annotated_types.SupportsLe | None = _Unset,\n    multiple_of: float | None = _Unset,\n    allow_inf_nan: bool | None = _Unset,\n    max_digits: int | None = _Unset,\n    decimal_places: int | None = _Unset,\n    min_length: int | None = _Unset,\n    max_length: int | None = _Unset,\n    union_mode: Literal['smart', 'left_to_right'] = _Unset,\n    fail_fast: bool | None = _Unset,\n    **extra: Unpack[_EmptyKwargs],\n) -> Any: ...", "metadata": {"license": "MIT", "len_tokens": 454}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "class ModelPrivateAttr(_repr.Representation):\n    \"\"\"A descriptor for private attributes in class models.\n\n    !!! warning\n        You generally shouldn't be creating `ModelPrivateAttr` instances directly, instead use\n        `pydantic.fields.PrivateAttr`. (This is similar to `FieldInfo` vs. `Field`.)\n\n    Attributes:\n        default: The default value of the attribute if not provided.\n        default_factory: A callable function that generates the default value of the\n            attribute if not provided.\n    \"\"\"\n\n    __slots__ = ('default', 'default_factory')\n\n    def __init__(self, default: Any = PydanticUndefined, *, default_factory: Callable[[], Any] | None = None) -> None:\n        if default is Ellipsis:\n            self.default = PydanticUndefined\n        else:\n            self.default = default\n        self.default_factory = default_factory\n\n    if not TYPE_CHECKING:\n        # We put `__getattr__` in a non-TYPE_CHECKING block because otherwise, mypy allows arbitrary attribute access\n\n        def __getattr__(self, item: str) -> Any:\n            \"\"\"This function improves compatibility with custom descriptors by ensuring delegation happens\n            as expected when the default value of a private attribute is a descriptor.\n            \"\"\"\n            if item in {'__get__', '__set__', '__delete__'}:\n                if hasattr(self.default, item):\n                    return getattr(self.default, item)\n            raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\n\n    def __set_name__(self, cls: type[Any], name: str) -> None:\n        \"\"\"Preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487.\"\"\"\n        default = self.default\n        if default is PydanticUndefined:\n            return\n        set_name = getattr(default, '__set_name__', None)\n        if callable(set_name):\n            set_name(cls, name)\n\n    def get_default(self) -> Any:\n        \"\"\"Retrieve the default value of the object.\n\n        If `self.default_factory` is `None`, the method will return a deep copy of the `self.default` object.\n\n        If `self.default_factory` is not `None`, it will call `self.default_factory` and return the value returned.\n\n        Returns:\n            The default value of the object.\n        \"\"\"\n        return _utils.smart_deepcopy(self.default) if self.default_factory is None else self.default_factory()\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, self.__class__) and (self.default, self.default_factory) == (\n            other.default,\n            other.default_factory,\n        )", "metadata": {"license": "MIT", "len_tokens": 561}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def PrivateAttr(\n    default: Any = PydanticUndefined,\n    *,\n    default_factory: Callable[[], Any] | None = None,\n    init: Literal[False] = False,\n) -> Any:\n    \"\"\"!!! abstract \"Usage Documentation\"\n        [Private Model Attributes](../concepts/models.md#private-model-attributes)\n\n    Indicates that an attribute is intended for private use and not handled during normal validation/serialization.\n\n    Private attributes are not validated by Pydantic, so it's up to you to ensure they are used in a type-safe manner.\n\n    Private attributes are stored in `__private_attributes__` on the model.\n\n    Args:\n        default: The attribute's default value. Defaults to Undefined.\n        default_factory: Callable that will be\n            called when a default value is needed for this attribute.\n            If both `default` and `default_factory` are set, an error will be raised.\n        init: Whether the attribute should be included in the constructor of the dataclass. Always `False`.\n\n    Returns:\n        An instance of [`ModelPrivateAttr`][pydantic.fields.ModelPrivateAttr] class.\n\n    Raises:\n        ValueError: If both `default` and `default_factory` are set.\n    \"\"\"\n    if default is not PydanticUndefined and default_factory is not None:\n        raise TypeError('cannot specify both default and default_factory')\n\n    return ModelPrivateAttr(\n        default,\n        default_factory=default_factory,\n    )", "metadata": {"license": "MIT", "len_tokens": 301}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def from_annotation(annotation: type[Any], *, _source: AnnotationSource = AnnotationSource.ANY) -> FieldInfo:\n        \"\"\"Creates a `FieldInfo` instance from a bare annotation.\n\n        This function is used internally to create a `FieldInfo` from a bare annotation like this:\n\n        ```python\n        import pydantic\n\n        class MyModel(pydantic.BaseModel):\n            foo: int  # <-- like this\n        ```\n\n        We also account for the case where the annotation can be an instance of `Annotated` and where\n        one of the (not first) arguments in `Annotated` is an instance of `FieldInfo`, e.g.:\n\n        ```python\n        from typing import Annotated\n\n        import annotated_types\n\n        import pydantic\n\n        class MyModel(pydantic.BaseModel):\n            foo: Annotated[int, annotated_types.Gt(42)]\n            bar: Annotated[int, pydantic.Field(gt=42)]\n        ```\n\n        Args:\n            annotation: An annotation object.\n\n        Returns:\n            An instance of the field metadata.\n        \"\"\"\n        try:\n            inspected_ann = inspect_annotation(\n                annotation,\n                annotation_source=_source,\n                unpack_type_aliases='skip',\n            )\n        except ForbiddenQualifier as e:\n            raise PydanticForbiddenQualifier(e.qualifier, annotation)\n\n        # TODO check for classvar and error?\n\n        # No assigned value, this happens when using a bare `Final` qualifier (also for other\n        # qualifiers, but they shouldn't appear here). In this case we infer the type as `Any`\n        # because we don't have any assigned value.\n        type_expr: Any = Any if inspected_ann.type is UNKNOWN else inspected_ann.type\n        final = 'final' in inspected_ann.qualifiers\n        metadata = inspected_ann.metadata\n\n        attr_overrides = {'annotation': type_expr}\n        if final:\n            attr_overrides['frozen'] = True\n        field_info = FieldInfo._construct(metadata, **attr_overrides)\n        field_info._qualifiers = inspected_ann.qualifiers\n        field_info._final = True\n        return field_info", "metadata": {"license": "MIT", "len_tokens": 436}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def merge_field_infos(*field_infos: FieldInfo, **overrides: Any) -> FieldInfo:\n        \"\"\"Merge `FieldInfo` instances keeping only explicitly set attributes.\n\n        Later `FieldInfo` instances override earlier ones.\n\n        Returns:\n            FieldInfo: A merged FieldInfo instance.\n        \"\"\"\n        if len(field_infos) == 1:\n            # No merging necessary, but we still need to make a copy and apply the overrides\n            field_info = field_infos[0]._copy()\n            field_info._attributes_set.update(overrides)\n\n            default_override = overrides.pop('default', PydanticUndefined)\n            if default_override is Ellipsis:\n                default_override = PydanticUndefined\n            if default_override is not PydanticUndefined:\n                field_info.default = default_override\n\n            for k, v in overrides.items():\n                setattr(field_info, k, v)\n            return field_info  # type: ignore\n\n        merged_field_info_kwargs: dict[str, Any] = {}\n        metadata = {}\n        for field_info in field_infos:\n            attributes_set = field_info._attributes_set.copy()\n\n            try:\n                json_schema_extra = attributes_set.pop('json_schema_extra')\n                existing_json_schema_extra = merged_field_info_kwargs.get('json_schema_extra')\n\n                if existing_json_schema_extra is None:\n                    merged_field_info_kwargs['json_schema_extra'] = json_schema_extra\n                if isinstance(existing_json_schema_extra, dict):\n                    if isinstance(json_schema_extra, dict):\n                        merged_field_info_kwargs['json_schema_extra'] = {\n                            **existing_json_schema_extra,\n                            **json_schema_extra,\n                        }\n                    if callable(json_schema_extra):\n                        warn(\n                            'Composing `dict` and `callable` type `json_schema_extra` is not supported.'\n                            'The `callable` type is being ignored.'\n                            \"If you'd like support for this behavior, please open an issue on pydantic.\",\n                            PydanticJsonSchemaWarning,\n                        )\n                elif callable(json_schema_extra):\n                    # if ever there's a case of a callable, we'll just keep the last json schema extra spec\n                    merged_field_info_kwargs['json_schema_extra'] = json_schema_extra\n            except KeyError:\n                pass\n\n            # later FieldInfo instances override everything except json_schema_extra from earlier FieldInfo instances\n            merged_field_info_kwargs.update(attributes_set)\n\n            for x in field_info.metadata:\n                if not isinstance(x, FieldInfo):\n                    metadata[type(x)] = x\n\n        merged_field_info_kwargs.update(overrides)\n        field_info = FieldInfo(**merged_field_info_kwargs)\n        field_info.metadata = list(metadata.values())\n        return field_info", "metadata": {"license": "MIT", "len_tokens": 537}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def _from_dataclass_field(dc_field: DataclassField[Any]) -> FieldInfo:\n        \"\"\"Return a new `FieldInfo` instance from a `dataclasses.Field` instance.\n\n        Args:\n            dc_field: The `dataclasses.Field` instance to convert.\n\n        Returns:\n            The corresponding `FieldInfo` instance.\n\n        Raises:\n            TypeError: If any of the `FieldInfo` kwargs does not match the `dataclass.Field` kwargs.\n        \"\"\"\n        default = dc_field.default\n        if default is dataclasses.MISSING:\n            default = _Unset\n\n        if dc_field.default_factory is dataclasses.MISSING:\n            default_factory = _Unset\n        else:\n            default_factory = dc_field.default_factory\n\n        # use the `Field` function so in correct kwargs raise the correct `TypeError`\n        dc_field_metadata = {k: v for k, v in dc_field.metadata.items() if k in _FIELD_ARG_NAMES}\n        if sys.version_info >= (3, 14) and dc_field.doc is not None:\n            dc_field_metadata['description'] = dc_field.doc\n        return Field(default=default, default_factory=default_factory, repr=dc_field.repr, **dc_field_metadata)", "metadata": {"license": "MIT", "len_tokens": 251}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def get_default(self, *, call_default_factory: bool = False, validated_data: dict[str, Any] | None = None) -> Any:\n        \"\"\"Get the default value.\n\n        We expose an option for whether to call the default_factory (if present), as calling it may\n        result in side effects that we want to avoid. However, there are times when it really should\n        be called (namely, when instantiating a model via `model_construct`).\n\n        Args:\n            call_default_factory: Whether to call the default factory or not.\n            validated_data: The already validated data to be passed to the default factory.\n\n        Returns:\n            The default value, calling the default factory if requested or `None` if not set.\n        \"\"\"\n        if self.default_factory is None:\n            return _utils.smart_deepcopy(self.default)\n        elif call_default_factory:\n            if self.default_factory_takes_validated_data:\n                fac = cast('Callable[[dict[str, Any]], Any]', self.default_factory)\n                if validated_data is None:\n                    raise ValueError(\n                        \"The default factory requires the 'validated_data' argument, which was not provided when calling 'get_default'.\"\n                    )\n                return fac(validated_data)\n            else:\n                fac = cast('Callable[[], Any]', self.default_factory)\n                return fac()\n        else:\n            return None", "metadata": {"license": "MIT", "len_tokens": 281}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def apply_typevars_map(\n        self,\n        typevars_map: Mapping[TypeVar, Any] | None,\n        globalns: GlobalsNamespace | None = None,\n        localns: MappingNamespace | None = None,\n    ) -> None:\n        \"\"\"Apply a `typevars_map` to the annotation.\n\n        This method is used when analyzing parametrized generic types to replace typevars with their concrete types.\n\n        This method applies the `typevars_map` to the annotation in place.\n\n        Args:\n            typevars_map: A dictionary mapping type variables to their concrete types.\n            globalns: The globals namespace to use during type annotation evaluation.\n            localns: The locals namespace to use during type annotation evaluation.\n\n        See Also:\n            pydantic._internal._generics.replace_types is used for replacing the typevars with\n                their concrete types.\n        \"\"\"\n        annotation = _generics.replace_types(self.annotation, typevars_map)\n        annotation, evaluated = _typing_extra.try_eval_type(annotation, globalns, localns)\n        self.annotation = annotation\n        if not evaluated:\n            self._complete = False\n            self._original_annotation = self.annotation", "metadata": {"license": "MIT", "len_tokens": 237}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def __repr_args__(self) -> ReprArgs:\n        yield 'annotation', _repr.PlainRepr(_repr.display_as_type(self.annotation))\n        yield 'required', self.is_required()\n\n        for s in self.__slots__:\n            # TODO: properly make use of the protocol (https://rich.readthedocs.io/en/stable/pretty.html#rich-repr-protocol)\n            # By yielding a three-tuple:\n            if s in (\n                'annotation',\n                '_attributes_set',\n                '_qualifiers',\n                '_complete',\n                '_original_assignment',\n                '_original_annotation',\n                '_final',\n            ):\n                continue\n            elif s == 'metadata' and not self.metadata:\n                continue\n            elif s == 'repr' and self.repr is True:\n                continue\n            if s == 'frozen' and self.frozen is False:\n                continue\n            if s == 'validation_alias' and self.validation_alias == self.alias:\n                continue\n            if s == 'serialization_alias' and self.serialization_alias == self.alias:\n                continue\n            if s == 'default' and self.default is not PydanticUndefined:\n                yield 'default', self.default\n            elif s == 'default_factory' and self.default_factory is not None:\n                yield 'default_factory', _repr.PlainRepr(_repr.display_as_type(self.default_factory))\n            else:\n                value = getattr(self, s)\n                if value is not None and value is not PydanticUndefined:\n                    yield s, value", "metadata": {"license": "MIT", "len_tokens": 313}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def _apply_alias_generator(self, alias_generator: Callable[[str], str] | AliasGenerator, name: str) -> None:\n        \"\"\"Apply an alias generator to aliases if appropriate.\n\n        Args:\n            alias_generator: A callable that takes a string and returns a string, or an `AliasGenerator` instance.\n            name: The name of the computed field from which to generate the alias.\n        \"\"\"\n        # Apply an alias_generator if\n        # 1. An alias is not specified\n        # 2. An alias is specified, but the priority is <= 1\n\n        if self.alias_priority is None or self.alias_priority <= 1 or self.alias is None:\n            alias, _, serialization_alias = None, None, None\n\n            if isinstance(alias_generator, AliasGenerator):\n                alias, _, serialization_alias = alias_generator.generate_aliases(name)\n            elif callable(alias_generator):\n                alias = alias_generator(name)\n\n            # if priority is not set, we set to 1\n            # which supports the case where the alias_generator from a child class is used\n            # to generate an alias for a field in a parent class\n            if self.alias_priority is None or self.alias_priority <= 1:\n                self.alias_priority = 1\n\n            # if the priority is 1, then we set the aliases to the generated alias\n            # note that we use the serialization_alias with priority over alias, as computed_field\n            # aliases are used for serialization only (not validation)\n            if self.alias_priority == 1:\n                self.alias = _utils.get_first_not_none(serialization_alias, alias)", "metadata": {"license": "MIT", "len_tokens": 330}}
{"id": "pydantic:pydantic/fields.py", "language": "python", "code": "def dec(f: Any) -> Any:\n        nonlocal description, deprecated, return_type, alias_priority\n        unwrapped = _decorators.unwrap_wrapped_function(f)\n\n        if description is None and unwrapped.__doc__:\n            description = inspect.cleandoc(unwrapped.__doc__)\n\n        if deprecated is None and hasattr(unwrapped, '__deprecated__'):\n            deprecated = unwrapped.__deprecated__\n\n        # if the function isn't already decorated with `@property` (or another descriptor), then we wrap it now\n        f = _decorators.ensure_property(f)\n        alias_priority = (alias_priority or 2) if alias is not None else None\n\n        if repr is None:\n            repr_: bool = not _wrapped_property_is_private(property_=f)\n        else:\n            repr_ = repr\n\n        dec_info = ComputedFieldInfo(\n            f,\n            return_type,\n            alias,\n            alias_priority,\n            title,\n            field_title_generator,\n            description,\n            deprecated,\n            examples,\n            json_schema_extra,\n            repr_,\n        )\n        return _decorators.PydanticDescriptorProxy(f, dec_info)", "metadata": {"license": "MIT", "len_tokens": 230}}
{"id": "pydantic:pydantic/warnings.py", "language": "python", "code": "class PydanticDeprecationWarning(DeprecationWarning):\n    \"\"\"A Pydantic specific deprecation warning.\n\n    This warning is raised when using deprecated functionality in Pydantic. It provides information on when the\n    deprecation was introduced and the expected version in which the corresponding functionality will be removed.\n\n    Attributes:\n        message: Description of the warning.\n        since: Pydantic version in what the deprecation was introduced.\n        expected_removal: Pydantic version in what the corresponding functionality expected to be removed.\n    \"\"\"\n\n    message: str\n    since: tuple[int, int]\n    expected_removal: tuple[int, int]\n\n    def __init__(\n        self, message: str, *args: object, since: tuple[int, int], expected_removal: tuple[int, int] | None = None\n    ) -> None:\n        super().__init__(message, *args)\n        self.message = message.rstrip('.')\n        self.since = since\n        self.expected_removal = expected_removal if expected_removal is not None else (since[0] + 1, 0)\n\n    def __str__(self) -> str:\n        message = (\n            f'{self.message}. Deprecated in Pydantic V{self.since[0]}.{self.since[1]}'\n            f' to be removed in V{self.expected_removal[0]}.{self.expected_removal[1]}.'\n        )\n        if self.since == (2, 0):\n            message += f' See Pydantic V2 Migration Guide at https://errors.pydantic.dev/{version_short()}/migration/'\n        return message", "metadata": {"license": "MIT", "len_tokens": 343}}
{"id": "pydantic:pydantic/__init__.py", "language": "python", "code": "def __getattr__(attr_name: str) -> object:\n    if attr_name in _deprecated_dynamic_imports:\n        from pydantic.warnings import PydanticDeprecatedSince20\n\n        warn(\n            f'Importing {attr_name} from `pydantic` is deprecated. This feature is either no longer supported, or is not public.',\n            PydanticDeprecatedSince20,\n            stacklevel=2,\n        )\n\n    dynamic_attr = _dynamic_imports.get(attr_name)\n    if dynamic_attr is None:\n        return _getattr_migration(attr_name)\n\n    package, module_name = dynamic_attr\n\n    if module_name == '__module__':\n        result = import_module(f'.{attr_name}', package=package)\n        globals()[attr_name] = result\n        return result\n    else:\n        module = import_module(module_name, package=package)\n        result = getattr(module, attr_name)\n        g = globals()\n        for k, (_, v_module_name) in _dynamic_imports.items():\n            if v_module_name == module_name and k not in _deprecated_dynamic_imports:\n                g[k] = getattr(module, k)\n        return result", "metadata": {"license": "MIT", "len_tokens": 239}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "class PydanticPlugin(Plugin):\n    \"\"\"The Pydantic mypy plugin.\"\"\"\n\n    def __init__(self, options: Options) -> None:\n        self.plugin_config = PydanticPluginConfig(options)\n        self._plugin_data = self.plugin_config.to_data()\n        super().__init__(options)\n\n    def get_base_class_hook(self, fullname: str) -> Callable[[ClassDefContext], None] | None:\n        \"\"\"Update Pydantic model class.\"\"\"\n        sym = self.lookup_fully_qualified(fullname)\n        if sym and isinstance(sym.node, TypeInfo):  # pragma: no branch\n            # No branching may occur if the mypy cache has not been cleared\n            if sym.node.has_base(BASEMODEL_FULLNAME):\n                return self._pydantic_model_class_maker_callback\n        return None\n\n    def get_metaclass_hook(self, fullname: str) -> Callable[[ClassDefContext], None] | None:\n        \"\"\"Update Pydantic `ModelMetaclass` definition.\"\"\"\n        if fullname == MODEL_METACLASS_FULLNAME:\n            return self._pydantic_model_metaclass_marker_callback\n        return None\n\n    def get_method_hook(self, fullname: str) -> Callable[[MethodContext], Type] | None:\n        \"\"\"Adjust return type of `from_orm` method call.\"\"\"\n        if fullname.endswith('.from_orm'):\n            return from_attributes_callback\n        return None\n\n    def report_config_data(self, ctx: ReportConfigContext) -> dict[str, Any]:\n        \"\"\"Return all plugin config data.\n\n        Used by mypy to determine if cache needs to be discarded.\n        \"\"\"\n        return self._plugin_data\n\n    def _pydantic_model_class_maker_callback(self, ctx: ClassDefContext) -> None:\n        transformer = PydanticModelTransformer(ctx.cls, ctx.reason, ctx.api, self.plugin_config)\n        transformer.transform()\n\n    def _pydantic_model_metaclass_marker_callback(self, ctx: ClassDefContext) -> None:\n        \"\"\"Reset dataclass_transform_spec attribute of ModelMetaclass.\n\n        Let the plugin handle it. This behavior can be disabled\n        if 'debug_dataclass_transform' is set to True', for testing purposes.\n        \"\"\"\n        if self.plugin_config.debug_dataclass_transform:\n            return\n        info_metaclass = ctx.cls.info.declared_metaclass\n        assert info_metaclass, \"callback not passed from 'get_metaclass_hook'\"\n        if getattr(info_metaclass.type, 'dataclass_transform_spec', None):\n            info_metaclass.type.dataclass_transform_spec = None", "metadata": {"license": "MIT", "len_tokens": 524}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "class PydanticPluginConfig:\n    \"\"\"A Pydantic mypy plugin config holder.\n\n    Attributes:\n        init_forbid_extra: Whether to add a `**kwargs` at the end of the generated `__init__` signature.\n        init_typed: Whether to annotate fields in the generated `__init__`.\n        warn_required_dynamic_aliases: Whether to raise required dynamic aliases error.\n        debug_dataclass_transform: Whether to not reset `dataclass_transform_spec` attribute\n            of `ModelMetaclass` for testing purposes.\n    \"\"\"\n\n    __slots__ = (\n        'init_forbid_extra',\n        'init_typed',\n        'warn_required_dynamic_aliases',\n        'debug_dataclass_transform',\n    )\n    init_forbid_extra: bool\n    init_typed: bool\n    warn_required_dynamic_aliases: bool\n    debug_dataclass_transform: bool  # undocumented\n\n    def __init__(self, options: Options) -> None:\n        if options.config_file is None:  # pragma: no cover\n            return\n\n        toml_config = parse_toml(options.config_file)\n        if toml_config is not None:\n            config = toml_config.get('tool', {}).get('pydantic-mypy', {})\n            for key in self.__slots__:\n                setting = config.get(key, False)\n                if not isinstance(setting, bool):\n                    raise ValueError(f'Configuration value must be a boolean for key: {key}')\n                setattr(self, key, setting)\n        else:\n            plugin_config = ConfigParser()\n            plugin_config.read(options.config_file)\n            for key in self.__slots__:\n                setting = plugin_config.getboolean(CONFIGFILE_KEY, key, fallback=False)\n                setattr(self, key, setting)\n\n    def to_data(self) -> dict[str, Any]:\n        \"\"\"Returns a dict of config names to their values.\"\"\"\n        return {key: getattr(self, key) for key in self.__slots__}", "metadata": {"license": "MIT", "len_tokens": 402}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def from_attributes_callback(ctx: MethodContext) -> Type:\n    \"\"\"Raise an error if from_attributes is not enabled.\"\"\"\n    model_type: Instance\n    ctx_type = ctx.type\n    if isinstance(ctx_type, TypeType):\n        ctx_type = ctx_type.item\n    if isinstance(ctx_type, CallableType) and isinstance(ctx_type.ret_type, Instance):\n        model_type = ctx_type.ret_type  # called on the class\n    elif isinstance(ctx_type, Instance):\n        model_type = ctx_type  # called on an instance (unusual, but still valid)\n    else:  # pragma: no cover\n        detail = f'ctx.type: {ctx_type} (of type {ctx_type.__class__.__name__})'\n        error_unexpected_behavior(detail, ctx.api, ctx.context)\n        return ctx.default_return_type\n    pydantic_metadata = model_type.type.metadata.get(METADATA_KEY)\n    if pydantic_metadata is None:\n        return ctx.default_return_type\n    if not model_type.type.has_base(BASEMODEL_FULLNAME):\n        # not a Pydantic v2 model\n        return ctx.default_return_type\n    from_attributes = pydantic_metadata.get('config', {}).get('from_attributes')\n    if from_attributes is not True:\n        error_from_attributes(model_type.type.name, ctx.api, ctx.context)\n    return ctx.default_return_type", "metadata": {"license": "MIT", "len_tokens": 280}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "class ModelConfigData:\n    \"\"\"Pydantic mypy plugin model config class.\"\"\"\n\n    def __init__(\n        self,\n        forbid_extra: bool | None = None,\n        frozen: bool | None = None,\n        from_attributes: bool | None = None,\n        populate_by_name: bool | None = None,\n        validate_by_alias: bool | None = None,\n        validate_by_name: bool | None = None,\n        has_alias_generator: bool | None = None,\n        strict: bool | None = None,\n    ):\n        self.forbid_extra = forbid_extra\n        self.frozen = frozen\n        self.from_attributes = from_attributes\n        self.populate_by_name = populate_by_name\n        self.validate_by_alias = validate_by_alias\n        self.validate_by_name = validate_by_name\n        self.has_alias_generator = has_alias_generator\n        self.strict = strict\n\n    def get_values_dict(self) -> dict[str, Any]:\n        \"\"\"Returns a dict of Pydantic model config names to their values.\n\n        It includes the config if config value is not `None`.\n        \"\"\"\n        return {k: v for k, v in self.__dict__.items() if v is not None}\n\n    def update(self, config: ModelConfigData | None) -> None:\n        \"\"\"Update Pydantic model config values.\"\"\"\n        if config is None:\n            return\n        for k, v in config.get_values_dict().items():\n            setattr(self, k, v)\n\n    def setdefault(self, key: str, value: Any) -> None:\n        \"\"\"Set default value for Pydantic model config if config value is `None`.\"\"\"\n        if getattr(self, key) is None:\n            setattr(self, key, value)", "metadata": {"license": "MIT", "len_tokens": 356}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def to_argument(\n        self,\n        current_info: TypeInfo,\n        typed: bool,\n        model_strict: bool,\n        force_optional: bool,\n        use_alias: bool,\n        api: SemanticAnalyzerPluginInterface,\n        force_typevars_invariant: bool,\n        is_root_model_root: bool,\n    ) -> Argument:\n        \"\"\"Based on mypy.plugins.dataclasses.DataclassAttribute.to_argument.\"\"\"\n        variable = self.to_var(current_info, api, use_alias, force_typevars_invariant)\n\n        strict = model_strict if self.strict is None else self.strict\n        if typed or strict:\n            type_annotation = self.expand_type(current_info, api, include_root_type=True)\n        else:\n            type_annotation = AnyType(TypeOfAny.explicit)\n\n        return Argument(\n            variable=variable,\n            type_annotation=type_annotation,\n            initializer=None,\n            kind=ARG_OPT\n            if is_root_model_root\n            else (ARG_NAMED_OPT if force_optional or self.has_default else ARG_NAMED),\n        )", "metadata": {"license": "MIT", "len_tokens": 212}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def expand_type(\n        self,\n        current_info: TypeInfo,\n        api: SemanticAnalyzerPluginInterface,\n        force_typevars_invariant: bool = False,\n        include_root_type: bool = False,\n    ) -> Type | None:\n        \"\"\"Based on mypy.plugins.dataclasses.DataclassAttribute.expand_type.\"\"\"\n        if force_typevars_invariant:\n            # In some cases, mypy will emit an error \"Cannot use a covariant type variable as a parameter\"\n            # To prevent that, we add an option to replace typevars with invariant ones while building certain\n            # method signatures (in particular, `__init__`). There may be a better way to do this, if this causes\n            # us problems in the future, we should look into why the dataclasses plugin doesn't have this issue.\n            if isinstance(self.type, TypeVarType):\n                modified_type = self.type.copy_modified()\n                modified_type.variance = INVARIANT\n                self.type = modified_type\n\n        if self.type is not None and self.info.self_type is not None:\n            # In general, it is not safe to call `expand_type()` during semantic analysis,\n            # however this plugin is called very late, so all types should be fully ready.\n            # Also, it is tricky to avoid eager expansion of Self types here (e.g. because\n            # we serialize attributes).\n            with state.strict_optional_set(api.options.strict_optional):\n                filled_with_typevars = fill_typevars(current_info)\n                # Cannot be TupleType as current_info represents a Pydantic model:\n                assert isinstance(filled_with_typevars, Instance)\n                if force_typevars_invariant:\n                    for arg in filled_with_typevars.args:\n                        if isinstance(arg, TypeVarType):\n                            arg.variance = INVARIANT\n\n                expanded_type = expand_type(self.type, {self.info.self_type.id: filled_with_typevars})\n                if include_root_type and isinstance(expanded_type, Instance) and is_root_model(expanded_type.type):\n                    # When a root model is used as a field, Pydantic allows both an instance of the root model\n                    # as well as instances of the `root` field type:\n                    root_type = expanded_type.type['root'].type\n                    if root_type is None:\n                        # Happens if the hint for 'root' has unsolved forward references\n                        return expanded_type\n                    expanded_root_type = expand_type_by_instance(root_type, expanded_type)\n                    expanded_type = UnionType([expanded_type, expanded_root_type])\n                return expanded_type\n        return self.type", "metadata": {"license": "MIT", "len_tokens": 527}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def transform(self) -> bool:\n        \"\"\"Configures the BaseModel subclass according to the plugin settings.\n\n        In particular:\n\n        * determines the model config and fields,\n        * adds a fields-aware signature for the initializer and construct methods\n        * freezes the class if frozen = True\n        * stores the fields, config, and if the class is settings in the mypy metadata for access by subclasses\n        \"\"\"\n        info = self._cls.info\n        is_a_root_model = is_root_model(info)\n        config = self.collect_config()\n        fields, class_vars = self.collect_fields_and_class_vars(config, is_a_root_model)\n        if fields is None or class_vars is None:\n            # Some definitions are not ready. We need another pass.\n            return False\n        for field in fields:\n            if field.type is None:\n                return False\n\n        is_settings = info.has_base(BASESETTINGS_FULLNAME)\n        self.add_initializer(fields, config, is_settings, is_a_root_model)\n        self.add_model_construct_method(fields, config, is_settings, is_a_root_model)\n        self.set_frozen(fields, self._api, frozen=config.frozen is True)\n\n        self.adjust_decorator_signatures()\n\n        info.metadata[METADATA_KEY] = {\n            'fields': {field.name: field.serialize() for field in fields},\n            'class_vars': {class_var.name: class_var.serialize() for class_var in class_vars},\n            'config': config.get_values_dict(),\n        }\n\n        return True", "metadata": {"license": "MIT", "len_tokens": 308}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def adjust_decorator_signatures(self) -> None:\n        \"\"\"When we decorate a function `f` with `pydantic.validator(...)`, `pydantic.field_validator`\n        or `pydantic.serializer(...)`, mypy sees `f` as a regular method taking a `self` instance,\n        even though pydantic internally wraps `f` with `classmethod` if necessary.\n\n        Teach mypy this by marking any function whose outermost decorator is a `validator()`,\n        `field_validator()` or `serializer()` call as a `classmethod`.\n        \"\"\"\n        for sym in self._cls.info.names.values():\n            if isinstance(sym.node, Decorator):\n                first_dec = sym.node.original_decorators[0]\n                if (\n                    isinstance(first_dec, CallExpr)\n                    and isinstance(first_dec.callee, NameExpr)\n                    and first_dec.callee.fullname in IMPLICIT_CLASSMETHOD_DECORATOR_FULLNAMES\n                    # @model_validator(mode=\"after\") is an exception, it expects a regular method\n                    and not (\n                        first_dec.callee.fullname == MODEL_VALIDATOR_FULLNAME\n                        and any(\n                            first_dec.arg_names[i] == 'mode' and isinstance(arg, StrExpr) and arg.value == 'after'\n                            for i, arg in enumerate(first_dec.args)\n                        )\n                    )\n                ):\n                    # TODO: Only do this if the first argument of the decorated function is `cls`\n                    sym.node.func.is_class = True", "metadata": {"license": "MIT", "len_tokens": 301}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def collect_config(self) -> ModelConfigData:  # noqa: C901 (ignore complexity)\n        \"\"\"Collects the values of the config attributes that are used by the plugin, accounting for parent classes.\"\"\"\n        cls = self._cls\n        config = ModelConfigData()\n\n        has_config_kwargs = False\n        has_config_from_namespace = False\n\n        # Handle `class MyModel(BaseModel, <name>=<expr>, ...):`\n        for name, expr in cls.keywords.items():\n            config_data = self.get_config_update(name, expr)\n            if config_data:\n                has_config_kwargs = True\n                config.update(config_data)\n\n        # Handle `model_config`\n        stmt: Statement | None = None\n        for stmt in cls.defs.body:\n            if not isinstance(stmt, (AssignmentStmt, ClassDef)):\n                continue\n\n            if isinstance(stmt, AssignmentStmt):\n                lhs = stmt.lvalues[0]\n                if not isinstance(lhs, NameExpr) or lhs.name != 'model_config':\n                    continue\n\n                if isinstance(stmt.rvalue, CallExpr):  # calls to `dict` or `ConfigDict`\n                    for arg_name, arg in zip(stmt.rvalue.arg_names, stmt.rvalue.args):\n                        if arg_name is None:\n                            continue\n                        config.update(self.get_config_update(arg_name, arg, lax_extra=True))\n                elif isinstance(stmt.rvalue, DictExpr):  # dict literals\n                    for key_expr, value_expr in stmt.rvalue.items:\n                        if not isinstance(key_expr, StrExpr):\n                            continue\n                        config.update(self.get_config_update(key_expr.value, value_expr))\n\n            elif isinstance(stmt, ClassDef):\n                if stmt.name != 'Config':  # 'deprecated' Config-class\n                    continue\n                for substmt in stmt.defs.body:\n                    if not isinstance(substmt, AssignmentStmt):\n                        continue\n                    lhs = substmt.lvalues[0]\n                    if not isinstance(lhs, NameExpr):\n                        continue\n                    config.update(self.get_config_update(lhs.name, substmt.rvalue))\n\n            if has_config_kwargs:\n                self._api.fail(\n                    'Specifying config in two places is ambiguous, use either Config attribute or class kwargs',\n                    cls,\n                )\n                break\n\n            has_config_from_namespace = True\n\n        if has_config_kwargs or has_config_from_namespace:\n            if (\n                stmt\n                and config.has_alias_generator\n                and not (config.validate_by_name or config.populate_by_name)\n                and self.plugin_config.warn_required_dynamic_aliases\n            ):\n                error_required_dynamic_aliases(self._api, stmt)\n\n        for info in cls.info.mro[1:]:  # 0 is the current class\n            if METADATA_KEY not in info.metadata:\n                continue\n\n            # Each class depends on the set of fields in its ancestors\n            self._api.add_plugin_dependency(make_wildcard_trigger(info.fullname))\n            for name, value in info.metadata[METADATA_KEY]['config'].items():\n                config.setdefault(name, value)\n        return config", "metadata": {"license": "MIT", "len_tokens": 619}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def collect_fields_and_class_vars(\n        self, model_config: ModelConfigData, is_root_model: bool\n    ) -> tuple[list[PydanticModelField] | None, list[PydanticModelClassVar] | None]:\n        \"\"\"Collects the fields for the model, accounting for parent classes.\"\"\"\n        cls = self._cls\n\n        # First, collect fields and ClassVars belonging to any class in the MRO, ignoring duplicates.\n        #\n        # We iterate through the MRO in reverse because attrs defined in the parent must appear\n        # earlier in the attributes list than attrs defined in the child. See:\n        # https://docs.python.org/3/library/dataclasses.html#inheritance\n        #\n        # However, we also want fields defined in the subtype to override ones defined\n        # in the parent. We can implement this via a dict without disrupting the attr order\n        # because dicts preserve insertion order in Python 3.7+.\n        found_fields: dict[str, PydanticModelField] = {}\n        found_class_vars: dict[str, PydanticModelClassVar] = {}\n        for info in reversed(cls.info.mro[1:-1]):  # 0 is the current class, -2 is BaseModel, -1 is object\n            # if BASEMODEL_METADATA_TAG_KEY in info.metadata and BASEMODEL_METADATA_KEY not in info.metadata:\n            #     # We haven't processed the base class yet. Need another pass.\n            #     return None, None\n            if METADATA_KEY not in info.metadata:\n                continue\n\n            # Each class depends on the set of attributes in its dataclass ancestors.\n            self._api.add_plugin_dependency(make_wildcard_trigger(info.fullname))\n\n            for name, data in info.metadata[METADATA_KEY]['fields'].items():\n                field = PydanticModelField.deserialize(info, data, self._api)\n                # (The following comment comes directly from the dataclasses plugin)\n                # TODO: We shouldn't be performing type operations during the main\n                #       semantic analysis pass, since some TypeInfo attributes might\n                #       still be in flux. This should be performed in a later phase.\n                field.expand_typevar_from_subtype(cls.info, self._api)\n                found_fields[name] = field\n\n                sym_node = cls.info.names.get(name)\n                if sym_node and sym_node.node and not isinstance(sym_node.node, Var):\n                    self._api.fail(\n                        'BaseModel field may only be overridden by another field',\n                        sym_node.node,\n                    )\n            # Collect ClassVars\n            for name, data in info.metadata[METADATA_KEY]['class_vars'].items():\n                found_class_vars[name] = PydanticModelClassVar.deserialize(data)\n\n        # Second, collect fields and ClassVars belonging to the current class.\n        current_field_names: set[str] = set()\n        current_class_vars_names: set[str] = set()\n        for stmt in self._get_assignment_statements_from_block(cls.defs):\n            maybe_field = self.collect_field_or_class_var_from_stmt(stmt, model_config, found_class_vars)\n            if maybe_field is None:\n                continue\n\n            lhs = stmt.lvalues[0]\n            assert isinstance(lhs, NameExpr)  # collect_field_or_class_var_from_stmt guarantees this\n            if isinstance(maybe_field, PydanticModelField):\n                if is_root_model and lhs.name != 'root':\n                    error_extra_fields_on_root_model(self._api, stmt)\n                else:\n                    current_field_names.add(lhs.name)\n                    found_fields[lhs.name] = maybe_field\n            elif isinstance(maybe_field, PydanticModelClassVar):\n                current_class_vars_names.add(lhs.name)\n                found_class_vars[lhs.name] = maybe_field\n\n        return list(found_fields.values()), list(found_class_vars.values())", "metadata": {"license": "MIT", "len_tokens": 786}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def _infer_dataclass_attr_init_type(self, sym: SymbolTableNode, name: str, context: Context) -> Type | None:\n        \"\"\"Infer __init__ argument type for an attribute.\n\n        In particular, possibly use the signature of __set__.\n        \"\"\"\n        default = sym.type\n        if sym.implicit:\n            return default\n        t = get_proper_type(sym.type)\n\n        # Perform a simple-minded inference from the signature of __set__, if present.\n        # We can't use mypy.checkmember here, since this plugin runs before type checking.\n        # We only support some basic scanerios here, which is hopefully sufficient for\n        # the vast majority of use cases.\n        if not isinstance(t, Instance):\n            return default\n        setter = t.type.get('__set__')\n        if setter:\n            if isinstance(setter.node, FuncDef):\n                super_info = t.type.get_containing_type_info('__set__')\n                assert super_info\n                if setter.type:\n                    setter_type = get_proper_type(map_type_from_supertype(setter.type, t.type, super_info))\n                else:\n                    return AnyType(TypeOfAny.unannotated)\n                if isinstance(setter_type, CallableType) and setter_type.arg_kinds == [\n                    ARG_POS,\n                    ARG_POS,\n                    ARG_POS,\n                ]:\n                    return expand_type_by_instance(setter_type.arg_types[2], t)\n                else:\n                    self._api.fail(f'Unsupported signature for \"__set__\" in \"{t.type.name}\"', context)\n            else:\n                self._api.fail(f'Unsupported \"__set__\" in \"{t.type.name}\"', context)\n\n        return default", "metadata": {"license": "MIT", "len_tokens": 345}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def add_initializer(\n        self, fields: list[PydanticModelField], config: ModelConfigData, is_settings: bool, is_root_model: bool\n    ) -> None:\n        \"\"\"Adds a fields-aware `__init__` method to the class.\n\n        The added `__init__` will be annotated with types vs. all `Any` depending on the plugin settings.\n        \"\"\"\n        if '__init__' in self._cls.info.names and not self._cls.info.names['__init__'].plugin_generated:\n            return  # Don't generate an __init__ if one already exists\n\n        typed = self.plugin_config.init_typed\n        model_strict = bool(config.strict)\n        use_alias = not (config.validate_by_name or config.populate_by_name) and config.validate_by_alias is not False\n        requires_dynamic_aliases = bool(config.has_alias_generator and not config.validate_by_name)\n        args = self.get_field_arguments(\n            fields,\n            typed=typed,\n            model_strict=model_strict,\n            requires_dynamic_aliases=requires_dynamic_aliases,\n            use_alias=use_alias,\n            is_settings=is_settings,\n            is_root_model=is_root_model,\n            force_typevars_invariant=True,\n        )\n\n        if is_settings:\n            base_settings_node = self._api.lookup_fully_qualified(BASESETTINGS_FULLNAME).node\n            assert isinstance(base_settings_node, TypeInfo)\n            if '__init__' in base_settings_node.names:\n                base_settings_init_node = base_settings_node.names['__init__'].node\n                assert isinstance(base_settings_init_node, FuncDef)\n                if base_settings_init_node is not None and base_settings_init_node.type is not None:\n                    func_type = base_settings_init_node.type\n                    assert isinstance(func_type, CallableType)\n                    for arg_idx, arg_name in enumerate(func_type.arg_names):\n                        if arg_name is None or arg_name.startswith('__') or not arg_name.startswith('_'):\n                            continue\n                        analyzed_variable_type = self._api.anal_type(func_type.arg_types[arg_idx])\n                        if analyzed_variable_type is not None and arg_name == '_cli_settings_source':\n                            # _cli_settings_source is defined as CliSettingsSource[Any], and as such\n                            # the Any causes issues with --disallow-any-explicit. As a workaround, change\n                            # the Any type (as if CliSettingsSource was left unparameterized):\n                            analyzed_variable_type = analyzed_variable_type.accept(\n                                ChangeExplicitTypeOfAny(TypeOfAny.from_omitted_generics)\n                            )\n                        variable = Var(arg_name, analyzed_variable_type)\n                        args.append(Argument(variable, analyzed_variable_type, None, ARG_OPT))\n\n        if not self.should_init_forbid_extra(fields, config):\n            var = Var('kwargs')\n            args.append(Argument(var, AnyType(TypeOfAny.explicit), None, ARG_STAR2))\n\n        add_method(self._api, self._cls, '__init__', args=args, return_type=NoneType())", "metadata": {"license": "MIT", "len_tokens": 610}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def add_model_construct_method(\n        self,\n        fields: list[PydanticModelField],\n        config: ModelConfigData,\n        is_settings: bool,\n        is_root_model: bool,\n    ) -> None:\n        \"\"\"Adds a fully typed `model_construct` classmethod to the class.\n\n        Similar to the fields-aware __init__ method, but always uses the field names (not aliases),\n        and does not treat settings fields as optional.\n        \"\"\"\n        set_str = self._api.named_type(f'{BUILTINS_NAME}.set', [self._api.named_type(f'{BUILTINS_NAME}.str')])\n        optional_set_str = UnionType([set_str, NoneType()])\n        fields_set_argument = Argument(Var('_fields_set', optional_set_str), optional_set_str, None, ARG_OPT)\n        with state.strict_optional_set(self._api.options.strict_optional):\n            args = self.get_field_arguments(\n                fields,\n                typed=True,\n                model_strict=bool(config.strict),\n                requires_dynamic_aliases=False,\n                use_alias=False,\n                is_settings=is_settings,\n                is_root_model=is_root_model,\n            )\n        if not self.should_init_forbid_extra(fields, config):\n            var = Var('kwargs')\n            args.append(Argument(var, AnyType(TypeOfAny.explicit), None, ARG_STAR2))\n\n        args = args + [fields_set_argument] if is_root_model else [fields_set_argument] + args\n\n        add_method(\n            self._api,\n            self._cls,\n            'model_construct',\n            args=args,\n            return_type=fill_typevars(self._cls.info),\n            is_classmethod=True,\n        )", "metadata": {"license": "MIT", "len_tokens": 338}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def set_frozen(self, fields: list[PydanticModelField], api: SemanticAnalyzerPluginInterface, frozen: bool) -> None:\n        \"\"\"Marks all fields as properties so that attempts to set them trigger mypy errors.\n\n        This is the same approach used by the attrs and dataclasses plugins.\n        \"\"\"\n        info = self._cls.info\n        for field in fields:\n            sym_node = info.names.get(field.name)\n            if sym_node is not None:\n                var = sym_node.node\n                if isinstance(var, Var):\n                    var.is_property = frozen or field.is_frozen\n                elif isinstance(var, PlaceholderNode) and not self._api.final_iteration:\n                    # See https://github.com/pydantic/pydantic/issues/5191 to hit this branch for test coverage\n                    self._api.defer()\n                # `var` can also be a FuncDef or Decorator node (e.g. when overriding a field with a function or property).\n                # In that case, we don't want to do anything. Mypy will already raise an error that a field was not properly\n                # overridden.\n            else:\n                var = field.to_var(info, api, use_alias=False)\n                var.info = info\n                var.is_property = frozen\n                var._fullname = info.fullname + '.' + var.name\n                info.names[var.name] = SymbolTableNode(MDEF, var)", "metadata": {"license": "MIT", "len_tokens": 288}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def get_config_update(self, name: str, arg: Expression, lax_extra: bool = False) -> ModelConfigData | None:\n        \"\"\"Determines the config update due to a single kwarg in the ConfigDict definition.\n\n        Warns if a tracked config attribute is set to a value the plugin doesn't know how to interpret (e.g., an int)\n        \"\"\"\n        if name not in self.tracked_config_fields:\n            return None\n        if name == 'extra':\n            if isinstance(arg, StrExpr):\n                forbid_extra = arg.value == 'forbid'\n            elif isinstance(arg, MemberExpr):\n                forbid_extra = arg.name == 'forbid'\n            else:\n                if not lax_extra:\n                    # Only emit an error for other types of `arg` (e.g., `NameExpr`, `ConditionalExpr`, etc.) when\n                    # reading from a config class, etc. If a ConfigDict is used, then we don't want to emit an error\n                    # because you'll get type checking from the ConfigDict itself.\n                    #\n                    # It would be nice if we could introspect the types better otherwise, but I don't know what the API\n                    # is to evaluate an expr into its type and then check if that type is compatible with the expected\n                    # type. Note that you can still get proper type checking via: `model_config = ConfigDict(...)`, just\n                    # if you don't use an explicit string, the plugin won't be able to infer whether extra is forbidden.\n                    error_invalid_config_value(name, self._api, arg)\n                return None\n            return ModelConfigData(forbid_extra=forbid_extra)\n        if name == 'alias_generator':\n            has_alias_generator = True\n            if isinstance(arg, NameExpr) and arg.fullname == 'builtins.None':\n                has_alias_generator = False\n            return ModelConfigData(has_alias_generator=has_alias_generator)\n        if isinstance(arg, NameExpr) and arg.fullname in ('builtins.True', 'builtins.False'):\n            return ModelConfigData(**{name: arg.fullname == 'builtins.True'})\n        error_invalid_config_value(name, self._api, arg)\n        return None", "metadata": {"license": "MIT", "len_tokens": 448}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def get_has_default(stmt: AssignmentStmt) -> bool:\n        \"\"\"Returns a boolean indicating whether the field defined in `stmt` is a required field.\"\"\"\n        expr = stmt.rvalue\n        if isinstance(expr, TempNode):\n            # TempNode means annotation-only, so has no default\n            return False\n        if isinstance(expr, CallExpr) and isinstance(expr.callee, RefExpr) and expr.callee.fullname == FIELD_FULLNAME:\n            # The \"default value\" is a call to `Field`; at this point, the field has a default if and only if:\n            # * there is a positional argument that is not `...`\n            # * there is a keyword argument named \"default\" that is not `...`\n            # * there is a \"default_factory\" that is not `None`\n            for arg, name in zip(expr.args, expr.arg_names):\n                # If name is None, then this arg is the default because it is the only positional argument.\n                if name is None or name == 'default':\n                    return arg.__class__ is not EllipsisExpr\n                if name == 'default_factory':\n                    return not (isinstance(arg, NameExpr) and arg.fullname == 'builtins.None')\n            return False\n        # Has no default if the \"default value\" is Ellipsis (i.e., `field_name: Annotation = ...`)\n        return not isinstance(expr, EllipsisExpr)", "metadata": {"license": "MIT", "len_tokens": 294}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def get_alias_info(stmt: AssignmentStmt) -> tuple[str | None, bool]:\n        \"\"\"Returns a pair (alias, has_dynamic_alias), extracted from the declaration of the field defined in `stmt`.\n\n        `has_dynamic_alias` is True if and only if an alias is provided, but not as a string literal.\n        If `has_dynamic_alias` is True, `alias` will be None.\n        \"\"\"\n        expr = stmt.rvalue\n        if isinstance(expr, TempNode):\n            # TempNode means annotation-only\n            return None, False\n\n        if not (\n            isinstance(expr, CallExpr) and isinstance(expr.callee, RefExpr) and expr.callee.fullname == FIELD_FULLNAME\n        ):\n            # Assigned value is not a call to pydantic.fields.Field\n            return None, False\n\n        if 'validation_alias' in expr.arg_names:\n            arg = expr.args[expr.arg_names.index('validation_alias')]\n        elif 'alias' in expr.arg_names:\n            arg = expr.args[expr.arg_names.index('alias')]\n        else:\n            return None, False\n\n        if isinstance(arg, StrExpr):\n            return arg.value, False\n        else:\n            return None, True", "metadata": {"license": "MIT", "len_tokens": 247}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def is_field_frozen(stmt: AssignmentStmt) -> bool:\n        \"\"\"Returns whether the field is frozen, extracted from the declaration of the field defined in `stmt`.\n\n        Note that this is only whether the field was declared to be frozen in a `<field_name> = Field(frozen=True)`\n        sense; this does not determine whether the field is frozen because the entire model is frozen; that is\n        handled separately.\n        \"\"\"\n        expr = stmt.rvalue\n        if isinstance(expr, TempNode):\n            # TempNode means annotation-only\n            return False\n\n        if not (\n            isinstance(expr, CallExpr) and isinstance(expr.callee, RefExpr) and expr.callee.fullname == FIELD_FULLNAME\n        ):\n            # Assigned value is not a call to pydantic.fields.Field\n            return False\n\n        for i, arg_name in enumerate(expr.arg_names):\n            if arg_name == 'frozen':\n                arg = expr.args[i]\n                return isinstance(arg, NameExpr) and arg.fullname == 'builtins.True'\n        return False", "metadata": {"license": "MIT", "len_tokens": 216}}
{"id": "pydantic:pydantic/mypy.py", "language": "python", "code": "def get_field_arguments(\n        self,\n        fields: list[PydanticModelField],\n        typed: bool,\n        model_strict: bool,\n        use_alias: bool,\n        requires_dynamic_aliases: bool,\n        is_settings: bool,\n        is_root_model: bool,\n        force_typevars_invariant: bool = False,\n    ) -> list[Argument]:\n        \"\"\"Helper function used during the construction of the `__init__` and `model_construct` method signatures.\n\n        Returns a list of mypy Argument instances for use in the generated signatures.\n        \"\"\"\n        info = self._cls.info\n        arguments = [\n            field.to_argument(\n                info,\n                typed=typed,\n                model_strict=model_strict,\n                force_optional=requires_dynamic_aliases or is_settings,\n                use_alias=use_alias,\n                api=self._api,\n                force_typevars_invariant=force_typevars_invariant,\n                is_root_model_root=is_root_model and field.name == 'root',\n            )\n            for field in fields\n            if not (use_alias and field.has_dynamic_alias)\n        ]\n        return arguments", "metadata": {"license": "MIT", "len_tokens": 227}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "def conint(\n    *,\n    strict: bool | None = None,\n    gt: int | None = None,\n    ge: int | None = None,\n    lt: int | None = None,\n    le: int | None = None,\n    multiple_of: int | None = None,\n) -> type[int]:\n    \"\"\"\n    !!! warning \"Discouraged\"\n        This function is **discouraged** in favor of using\n        [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with\n        [`Field`][pydantic.fields.Field] instead.\n\n        This function will be **deprecated** in Pydantic 3.0.\n\n        The reason is that `conint` returns a type, which doesn't play well with static analysis tools.\n\n        === \":x: Don't do this\"\n            ```python\n            from pydantic import BaseModel, conint\n\n            class Foo(BaseModel):\n                bar: conint(strict=True, gt=0)\n            ```\n\n        === \":white_check_mark: Do this\"\n            ```python\n            from typing import Annotated\n\n            from pydantic import BaseModel, Field\n\n            class Foo(BaseModel):\n                bar: Annotated[int, Field(strict=True, gt=0)]\n            ```\n\n    A wrapper around `int` that allows for additional constraints.\n\n    Args:\n        strict: Whether to validate the integer in strict mode. Defaults to `None`.\n        gt: The value must be greater than this.\n        ge: The value must be greater than or equal to this.\n        lt: The value must be less than this.\n        le: The value must be less than or equal to this.\n        multiple_of: The value must be a multiple of this.\n\n    Returns:\n        The wrapped integer type.\n\n    ```python\n    from pydantic import BaseModel, ValidationError, conint\n\n    class ConstrainedExample(BaseModel):\n        constrained_int: conint(gt=1)\n\n    m = ConstrainedExample(constrained_int=2)\n    print(repr(m))\n    #> ConstrainedExample(constrained_int=2)\n\n    try:\n        ConstrainedExample(constrained_int=0)\n    except ValidationError as e:\n        print(e.errors())\n        '''\n        [\n            {\n                'type': 'greater_than',\n                'loc': ('constrained_int',),\n                'msg': 'Input should be greater than 1',\n                'input': 0,\n                'ctx': {'gt': 1},\n                'url': 'https://errors.pydantic.dev/2/v/greater_than',\n            }\n        ]\n        '''\n    ```\n\n    \"\"\"  # noqa: D212\n    return Annotated[  # pyright: ignore[reportReturnType]\n        int,\n        Strict(strict) if strict is not None else None,\n        annotated_types.Interval(gt=gt, ge=ge, lt=lt, le=le),\n        annotated_types.MultipleOf(multiple_of) if multiple_of is not None else None,\n    ]", "metadata": {"license": "MIT", "len_tokens": 627}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "def confloat(\n    *,\n    strict: bool | None = None,\n    gt: float | None = None,\n    ge: float | None = None,\n    lt: float | None = None,\n    le: float | None = None,\n    multiple_of: float | None = None,\n    allow_inf_nan: bool | None = None,\n) -> type[float]:\n    \"\"\"\n    !!! warning \"Discouraged\"\n        This function is **discouraged** in favor of using\n        [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with\n        [`Field`][pydantic.fields.Field] instead.\n\n        This function will be **deprecated** in Pydantic 3.0.\n\n        The reason is that `confloat` returns a type, which doesn't play well with static analysis tools.\n\n        === \":x: Don't do this\"\n            ```python\n            from pydantic import BaseModel, confloat\n\n            class Foo(BaseModel):\n                bar: confloat(strict=True, gt=0)\n            ```\n\n        === \":white_check_mark: Do this\"\n            ```python\n            from typing import Annotated\n\n            from pydantic import BaseModel, Field\n\n            class Foo(BaseModel):\n                bar: Annotated[float, Field(strict=True, gt=0)]\n            ```\n\n    A wrapper around `float` that allows for additional constraints.\n\n    Args:\n        strict: Whether to validate the float in strict mode.\n        gt: The value must be greater than this.\n        ge: The value must be greater than or equal to this.\n        lt: The value must be less than this.\n        le: The value must be less than or equal to this.\n        multiple_of: The value must be a multiple of this.\n        allow_inf_nan: Whether to allow `-inf`, `inf`, and `nan`.\n\n    Returns:\n        The wrapped float type.\n\n    ```python\n    from pydantic import BaseModel, ValidationError, confloat\n\n    class ConstrainedExample(BaseModel):\n        constrained_float: confloat(gt=1.0)\n\n    m = ConstrainedExample(constrained_float=1.1)\n    print(repr(m))\n    #> ConstrainedExample(constrained_float=1.1)\n\n    try:\n        ConstrainedExample(constrained_float=0.9)\n    except ValidationError as e:\n        print(e.errors())\n        '''\n        [\n            {\n                'type': 'greater_than',\n                'loc': ('constrained_float',),\n                'msg': 'Input should be greater than 1',\n                'input': 0.9,\n                'ctx': {'gt': 1.0},\n                'url': 'https://errors.pydantic.dev/2/v/greater_than',\n            }\n        ]\n        '''\n    ```\n    \"\"\"  # noqa: D212\n    return Annotated[  # pyright: ignore[reportReturnType]\n        float,\n        Strict(strict) if strict is not None else None,\n        annotated_types.Interval(gt=gt, ge=ge, lt=lt, le=le),\n        annotated_types.MultipleOf(multiple_of) if multiple_of is not None else None,\n        AllowInfNan(allow_inf_nan) if allow_inf_nan is not None else None,\n    ]", "metadata": {"license": "MIT", "len_tokens": 684}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class StringConstraints(annotated_types.GroupedMetadata):\n    \"\"\"!!! abstract \"Usage Documentation\"\n        [String types](./standard_library_types.md#strings)\n\n    A field metadata class to apply constraints to `str` types.\n    Use this class as an annotation via [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated), as seen below.\n\n    Attributes:\n        strip_whitespace: Whether to remove leading and trailing whitespace.\n        to_upper: Whether to convert the string to uppercase.\n        to_lower: Whether to convert the string to lowercase.\n        strict: Whether to validate the string in strict mode.\n        min_length: The minimum length of the string.\n        max_length: The maximum length of the string.\n        pattern: A regex pattern that the string must match.\n\n    Example:\n        ```python\n        from typing import Annotated\n\n        from pydantic.types import StringConstraints\n\n        ConstrainedStr = Annotated[str, StringConstraints(min_length=1, max_length=10)]\n        ```\n    \"\"\"\n\n    strip_whitespace: bool | None = None\n    to_upper: bool | None = None\n    to_lower: bool | None = None\n    strict: bool | None = None\n    min_length: int | None = None\n    max_length: int | None = None\n    pattern: str | Pattern[str] | None = None\n\n    def __iter__(self) -> Iterator[BaseMetadata]:\n        if self.min_length is not None:\n            yield MinLen(self.min_length)\n        if self.max_length is not None:\n            yield MaxLen(self.max_length)\n        if self.strict is not None:\n            yield Strict(self.strict)\n        if (\n            self.strip_whitespace is not None\n            or self.pattern is not None\n            or self.to_lower is not None\n            or self.to_upper is not None\n        ):\n            yield _fields.pydantic_general_metadata(\n                strip_whitespace=self.strip_whitespace,\n                to_upper=self.to_upper,\n                to_lower=self.to_lower,\n                pattern=self.pattern,\n            )", "metadata": {"license": "MIT", "len_tokens": 427}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "def constr(\n    *,\n    strip_whitespace: bool | None = None,\n    to_upper: bool | None = None,\n    to_lower: bool | None = None,\n    strict: bool | None = None,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    pattern: str | Pattern[str] | None = None,\n) -> type[str]:\n    \"\"\"\n    !!! warning \"Discouraged\"\n        This function is **discouraged** in favor of using\n        [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with\n        [`StringConstraints`][pydantic.types.StringConstraints] instead.\n\n        This function will be **deprecated** in Pydantic 3.0.\n\n        The reason is that `constr` returns a type, which doesn't play well with static analysis tools.\n\n        === \":x: Don't do this\"\n            ```python\n            from pydantic import BaseModel, constr\n\n            class Foo(BaseModel):\n                bar: constr(strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$')\n            ```\n\n        === \":white_check_mark: Do this\"\n            ```python\n            from typing import Annotated\n\n            from pydantic import BaseModel, StringConstraints\n\n            class Foo(BaseModel):\n                bar: Annotated[\n                    str,\n                    StringConstraints(\n                        strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$'\n                    ),\n                ]\n            ```\n\n    A wrapper around `str` that allows for additional constraints.\n\n    ```python\n    from pydantic import BaseModel, constr\n\n    class Foo(BaseModel):\n        bar: constr(strip_whitespace=True, to_upper=True)\n\n    foo = Foo(bar='  hello  ')\n    print(foo)\n    #> bar='HELLO'\n    ```\n\n    Args:\n        strip_whitespace: Whether to remove leading and trailing whitespace.\n        to_upper: Whether to turn all characters to uppercase.\n        to_lower: Whether to turn all characters to lowercase.\n        strict: Whether to validate the string in strict mode.\n        min_length: The minimum length of the string.\n        max_length: The maximum length of the string.\n        pattern: A regex pattern to validate the string against.\n\n    Returns:\n        The wrapped string type.\n    \"\"\"  # noqa: D212\n    return Annotated[  # pyright: ignore[reportReturnType]\n        str,\n        StringConstraints(\n            strip_whitespace=strip_whitespace,\n            to_upper=to_upper,\n            to_lower=to_lower,\n            strict=strict,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n        ),\n    ]", "metadata": {"license": "MIT", "len_tokens": 565}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "def conlist(\n    item_type: type[AnyItemType],\n    *,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    unique_items: bool | None = None,\n) -> type[list[AnyItemType]]:\n    \"\"\"A wrapper around [`list`][] that adds validation.\n\n    Args:\n        item_type: The type of the items in the list.\n        min_length: The minimum length of the list. Defaults to None.\n        max_length: The maximum length of the list. Defaults to None.\n        unique_items: Whether the items in the list must be unique. Defaults to None.\n            !!! warning Deprecated\n                The `unique_items` parameter is deprecated, use `Set` instead.\n                See [this issue](https://github.com/pydantic/pydantic-core/issues/296) for more details.\n\n    Returns:\n        The wrapped list type.\n    \"\"\"\n    if unique_items is not None:\n        raise PydanticUserError(\n            (\n                '`unique_items` is removed, use `Set` instead'\n                '(this feature is discussed in https://github.com/pydantic/pydantic-core/issues/296)'\n            ),\n            code='removed-kwargs',\n        )\n    return Annotated[list[item_type], annotated_types.Len(min_length or 0, max_length)]", "metadata": {"license": "MIT", "len_tokens": 272}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class UuidVersion:\n    \"\"\"A field metadata class to indicate a [UUID](https://docs.python.org/3/library/uuid.html) version.\n\n    Use this class as an annotation via [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated), as seen below.\n\n    Attributes:\n        uuid_version: The version of the UUID. Must be one of 1, 3, 4, 5, 6, 7 or 8.\n\n    Example:\n        ```python\n        from typing import Annotated\n        from uuid import UUID\n\n        from pydantic.types import UuidVersion\n\n        UUID1 = Annotated[UUID, UuidVersion(1)]\n        ```\n    \"\"\"\n\n    uuid_version: Literal[1, 3, 4, 5, 6, 7, 8]\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        field_schema = handler(core_schema)\n        field_schema.pop('anyOf', None)  # remove the bytes/str union\n        field_schema.update(type='string', format=f'uuid{self.uuid_version}')\n        return field_schema\n\n    def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source)\n        _check_annotated_type(schema['type'], 'uuid', self.__class__.__name__)\n        schema['version'] = self.uuid_version  # type: ignore\n        return schema\n\n    def __hash__(self) -> int:\n        return hash(type(self.uuid_version))", "metadata": {"license": "MIT", "len_tokens": 353}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class PathType:\n    path_type: Literal['file', 'dir', 'new', 'socket']\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        field_schema = handler(core_schema)\n        format_conversion = {'file': 'file-path', 'dir': 'directory-path'}\n        field_schema.update(format=format_conversion.get(self.path_type, 'path'), type='string')\n        return field_schema\n\n    def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        function_lookup = {\n            'file': cast(core_schema.WithInfoValidatorFunction, self.validate_file),\n            'dir': cast(core_schema.WithInfoValidatorFunction, self.validate_directory),\n            'new': cast(core_schema.WithInfoValidatorFunction, self.validate_new),\n            'socket': cast(core_schema.WithInfoValidatorFunction, self.validate_socket),\n        }\n\n        return core_schema.with_info_after_validator_function(\n            function_lookup[self.path_type],\n            handler(source),\n        )\n\n    @staticmethod\n    def validate_file(path: Path, _: core_schema.ValidationInfo) -> Path:\n        if path.is_file():\n            return path\n        else:\n            raise PydanticCustomError('path_not_file', 'Path does not point to a file')\n\n    @staticmethod\n    def validate_socket(path: Path, _: core_schema.ValidationInfo) -> Path:\n        if path.is_socket():\n            return path\n        else:\n            raise PydanticCustomError('path_not_socket', 'Path does not point to a socket')\n\n    @staticmethod\n    def validate_directory(path: Path, _: core_schema.ValidationInfo) -> Path:\n        if path.is_dir():\n            return path\n        else:\n            raise PydanticCustomError('path_not_directory', 'Path does not point to a directory')\n\n    @staticmethod\n    def validate_new(path: Path, _: core_schema.ValidationInfo) -> Path:\n        if path.exists():\n            raise PydanticCustomError('path_exists', 'Path already exists')\n        elif not path.parent.exists():\n            raise PydanticCustomError('parent_does_not_exist', 'Parent directory does not exist')\n        else:\n            return path\n\n    def __hash__(self) -> int:\n        return hash(type(self.path_type))", "metadata": {"license": "MIT", "len_tokens": 489}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class _SecretField(_SecretBase[SecretType]):\n    _inner_schema: ClassVar[CoreSchema]\n    _error_kind: ClassVar[str]\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source: type[Any], handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        def get_json_schema(_core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n            json_schema = handler(cls._inner_schema)\n            _utils.update_not_none(\n                json_schema,\n                type='string',\n                writeOnly=True,\n                format='password',\n            )\n            return json_schema\n\n        def get_secret_schema(strict: bool) -> CoreSchema:\n            inner_schema = {**cls._inner_schema, 'strict': strict}\n            json_schema = core_schema.no_info_after_validator_function(\n                source,  # construct the type\n                inner_schema,  # pyright: ignore[reportArgumentType]\n            )\n            return core_schema.json_or_python_schema(\n                python_schema=core_schema.union_schema(\n                    [\n                        core_schema.is_instance_schema(source),\n                        json_schema,\n                    ],\n                    custom_error_type=cls._error_kind,\n                ),\n                json_schema=json_schema,\n                serialization=core_schema.plain_serializer_function_ser_schema(\n                    _serialize_secret_field,\n                    info_arg=True,\n                    when_used='always',\n                ),\n            )\n\n        return core_schema.lax_or_strict_schema(\n            lax_schema=get_secret_schema(strict=False),\n            strict_schema=get_secret_schema(strict=True),\n            metadata={'pydantic_js_functions': [get_json_schema]},\n        )\n\n    __pydantic_serializer__ = SchemaSerializer(\n        core_schema.any_schema(\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                _serialize_secret_field,\n                info_arg=True,\n                when_used='always',\n            )\n        )\n    )", "metadata": {"license": "MIT", "len_tokens": 383}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class SecretStr(_SecretField[str]):\n    \"\"\"A string used for storing sensitive information that you do not want to be visible in logging or tracebacks.\n\n    When the secret value is nonempty, it is displayed as `'**********'` instead of the underlying value in\n    calls to `repr()` and `str()`. If the value _is_ empty, it is displayed as `''`.\n\n    ```python\n    from pydantic import BaseModel, SecretStr\n\n    class User(BaseModel):\n        username: str\n        password: SecretStr\n\n    user = User(username='scolvin', password='password1')\n\n    print(user)\n    #> username='scolvin' password=SecretStr('**********')\n    print(user.password.get_secret_value())\n    #> password1\n    print((SecretStr('password'), SecretStr('')))\n    #> (SecretStr('**********'), SecretStr(''))\n    ```\n\n    As seen above, by default, [`SecretStr`][pydantic.types.SecretStr] (and [`SecretBytes`][pydantic.types.SecretBytes])\n    will be serialized as `**********` when serializing to json.\n\n    You can use the [`field_serializer`][pydantic.functional_serializers.field_serializer] to dump the\n    secret as plain-text when serializing to json.\n\n    ```python\n    from pydantic import BaseModel, SecretBytes, SecretStr, field_serializer\n\n    class Model(BaseModel):\n        password: SecretStr\n        password_bytes: SecretBytes\n\n        @field_serializer('password', 'password_bytes', when_used='json')\n        def dump_secret(self, v):\n            return v.get_secret_value()\n\n    model = Model(password='IAmSensitive', password_bytes=b'IAmSensitiveBytes')\n    print(model)\n    #> password=SecretStr('**********') password_bytes=SecretBytes(b'**********')\n    print(model.password)\n    #> **********\n    print(model.model_dump())\n    '''\n    {\n        'password': SecretStr('**********'),\n        'password_bytes': SecretBytes(b'**********'),\n    }\n    '''\n    print(model.model_dump_json())\n    #> {\"password\":\"IAmSensitive\",\"password_bytes\":\"IAmSensitiveBytes\"}\n    ```\n    \"\"\"\n\n    _inner_schema: ClassVar[CoreSchema] = core_schema.str_schema()\n    _error_kind: ClassVar[str] = 'string_type'\n\n    def __len__(self) -> int:\n        return len(self._secret_value)\n\n    def _display(self) -> str:\n        return _secret_display(self._secret_value)", "metadata": {"license": "MIT", "len_tokens": 535}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class SecretBytes(_SecretField[bytes]):\n    \"\"\"A bytes used for storing sensitive information that you do not want to be visible in logging or tracebacks.\n\n    It displays `b'**********'` instead of the string value on `repr()` and `str()` calls.\n    When the secret value is nonempty, it is displayed as `b'**********'` instead of the underlying value in\n    calls to `repr()` and `str()`. If the value _is_ empty, it is displayed as `b''`.\n\n    ```python\n    from pydantic import BaseModel, SecretBytes\n\n    class User(BaseModel):\n        username: str\n        password: SecretBytes\n\n    user = User(username='scolvin', password=b'password1')\n    #> username='scolvin' password=SecretBytes(b'**********')\n    print(user.password.get_secret_value())\n    #> b'password1'\n    print((SecretBytes(b'password'), SecretBytes(b'')))\n    #> (SecretBytes(b'**********'), SecretBytes(b''))\n    ```\n    \"\"\"\n\n    _inner_schema: ClassVar[CoreSchema] = core_schema.bytes_schema()\n    _error_kind: ClassVar[str] = 'bytes_type'\n\n    def __len__(self) -> int:\n        return len(self._secret_value)\n\n    def _display(self) -> bytes:\n        return _secret_display(self._secret_value).encode()", "metadata": {"license": "MIT", "len_tokens": 298}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "def condate(\n    *,\n    strict: bool | None = None,\n    gt: date | None = None,\n    ge: date | None = None,\n    lt: date | None = None,\n    le: date | None = None,\n) -> type[date]:\n    \"\"\"A wrapper for date that adds constraints.\n\n    Args:\n        strict: Whether to validate the date value in strict mode. Defaults to `None`.\n        gt: The value must be greater than this. Defaults to `None`.\n        ge: The value must be greater than or equal to this. Defaults to `None`.\n        lt: The value must be less than this. Defaults to `None`.\n        le: The value must be less than or equal to this. Defaults to `None`.\n\n    Returns:\n        A date type with the specified constraints.\n    \"\"\"\n    return Annotated[  # pyright: ignore[reportReturnType]\n        date,\n        Strict(strict) if strict is not None else None,\n        annotated_types.Interval(gt=gt, ge=ge, lt=lt, le=le),\n    ]", "metadata": {"license": "MIT", "len_tokens": 225}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class Base64Encoder(EncoderProtocol):\n    \"\"\"Standard (non-URL-safe) Base64 encoder.\"\"\"\n\n    @classmethod\n    def decode(cls, data: bytes) -> bytes:\n        \"\"\"Decode the data from base64 encoded bytes to original bytes data.\n\n        Args:\n            data: The data to decode.\n\n        Returns:\n            The decoded data.\n        \"\"\"\n        try:\n            return base64.b64decode(data)\n        except ValueError as e:\n            raise PydanticCustomError('base64_decode', \"Base64 decoding error: '{error}'\", {'error': str(e)})\n\n    @classmethod\n    def encode(cls, value: bytes) -> bytes:\n        \"\"\"Encode the data from bytes to a base64 encoded bytes.\n\n        Args:\n            value: The data to encode.\n\n        Returns:\n            The encoded data.\n        \"\"\"\n        return base64.b64encode(value)\n\n    @classmethod\n    def get_json_format(cls) -> Literal['base64']:\n        \"\"\"Get the JSON format for the encoded data.\n\n        Returns:\n            The JSON format for the encoded data.\n        \"\"\"\n        return 'base64'", "metadata": {"license": "MIT", "len_tokens": 229}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class Base64UrlEncoder(EncoderProtocol):\n    \"\"\"URL-safe Base64 encoder.\"\"\"\n\n    @classmethod\n    def decode(cls, data: bytes) -> bytes:\n        \"\"\"Decode the data from base64 encoded bytes to original bytes data.\n\n        Args:\n            data: The data to decode.\n\n        Returns:\n            The decoded data.\n        \"\"\"\n        try:\n            return base64.urlsafe_b64decode(data)\n        except ValueError as e:\n            raise PydanticCustomError('base64_decode', \"Base64 decoding error: '{error}'\", {'error': str(e)})\n\n    @classmethod\n    def encode(cls, value: bytes) -> bytes:\n        \"\"\"Encode the data from bytes to a base64 encoded bytes.\n\n        Args:\n            value: The data to encode.\n\n        Returns:\n            The encoded data.\n        \"\"\"\n        return base64.urlsafe_b64encode(value)\n\n    @classmethod\n    def get_json_format(cls) -> Literal['base64url']:\n        \"\"\"Get the JSON format for the encoded data.\n\n        Returns:\n            The JSON format for the encoded data.\n        \"\"\"\n        return 'base64url'", "metadata": {"license": "MIT", "len_tokens": 231}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class EncodedBytes:\n    \"\"\"A bytes type that is encoded and decoded using the specified encoder.\n\n    `EncodedBytes` needs an encoder that implements `EncoderProtocol` to operate.\n\n    ```python\n    from typing import Annotated\n\n    from pydantic import BaseModel, EncodedBytes, EncoderProtocol, ValidationError\n\n    class MyEncoder(EncoderProtocol):\n        @classmethod\n        def decode(cls, data: bytes) -> bytes:\n            if data == b'**undecodable**':\n                raise ValueError('Cannot decode data')\n            return data[13:]\n\n        @classmethod\n        def encode(cls, value: bytes) -> bytes:\n            return b'**encoded**: ' + value\n\n        @classmethod\n        def get_json_format(cls) -> str:\n            return 'my-encoder'\n\n    MyEncodedBytes = Annotated[bytes, EncodedBytes(encoder=MyEncoder)]\n\n    class Model(BaseModel):\n        my_encoded_bytes: MyEncodedBytes\n\n    # Initialize the model with encoded data\n    m = Model(my_encoded_bytes=b'**encoded**: some bytes')\n\n    # Access decoded value\n    print(m.my_encoded_bytes)\n    #> b'some bytes'\n\n    # Serialize into the encoded form\n    print(m.model_dump())\n    #> {'my_encoded_bytes': b'**encoded**: some bytes'}\n\n    # Validate encoded data\n    try:\n        Model(my_encoded_bytes=b'**undecodable**')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Model\n        my_encoded_bytes\n          Value error, Cannot decode data [type=value_error, input_value=b'**undecodable**', input_type=bytes]\n        '''\n    ```\n    \"\"\"\n\n    encoder: type[EncoderProtocol]\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        field_schema = handler(core_schema)\n        field_schema.update(type='string', format=self.encoder.get_json_format())\n        return field_schema\n\n    def __get_pydantic_core_schema__(self, source: type[Any], handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source)\n        _check_annotated_type(schema['type'], 'bytes', self.__class__.__name__)\n        return core_schema.with_info_after_validator_function(\n            function=self.decode,\n            schema=schema,\n            serialization=core_schema.plain_serializer_function_ser_schema(function=self.encode),\n        )\n\n    def decode(self, data: bytes, _: core_schema.ValidationInfo) -> bytes:\n        \"\"\"Decode the data using the specified encoder.\n\n        Args:\n            data: The data to decode.\n\n        Returns:\n            The decoded data.\n        \"\"\"\n        return self.encoder.decode(data)\n\n    def encode(self, value: bytes) -> bytes:\n        \"\"\"Encode the data using the specified encoder.\n\n        Args:\n            value: The data to encode.\n\n        Returns:\n            The encoded data.\n        \"\"\"\n        return self.encoder.encode(value)\n\n    def __hash__(self) -> int:\n        return hash(self.encoder)", "metadata": {"license": "MIT", "len_tokens": 645}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class EncodedStr:\n    \"\"\"A str type that is encoded and decoded using the specified encoder.\n\n    `EncodedStr` needs an encoder that implements `EncoderProtocol` to operate.\n\n    ```python\n    from typing import Annotated\n\n    from pydantic import BaseModel, EncodedStr, EncoderProtocol, ValidationError\n\n    class MyEncoder(EncoderProtocol):\n        @classmethod\n        def decode(cls, data: bytes) -> bytes:\n            if data == b'**undecodable**':\n                raise ValueError('Cannot decode data')\n            return data[13:]\n\n        @classmethod\n        def encode(cls, value: bytes) -> bytes:\n            return b'**encoded**: ' + value\n\n        @classmethod\n        def get_json_format(cls) -> str:\n            return 'my-encoder'\n\n    MyEncodedStr = Annotated[str, EncodedStr(encoder=MyEncoder)]\n\n    class Model(BaseModel):\n        my_encoded_str: MyEncodedStr\n\n    # Initialize the model with encoded data\n    m = Model(my_encoded_str='**encoded**: some str')\n\n    # Access decoded value\n    print(m.my_encoded_str)\n    #> some str\n\n    # Serialize into the encoded form\n    print(m.model_dump())\n    #> {'my_encoded_str': '**encoded**: some str'}\n\n    # Validate encoded data\n    try:\n        Model(my_encoded_str='**undecodable**')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Model\n        my_encoded_str\n          Value error, Cannot decode data [type=value_error, input_value='**undecodable**', input_type=str]\n        '''\n    ```\n    \"\"\"\n\n    encoder: type[EncoderProtocol]\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        field_schema = handler(core_schema)\n        field_schema.update(type='string', format=self.encoder.get_json_format())\n        return field_schema\n\n    def __get_pydantic_core_schema__(self, source: type[Any], handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source)\n        _check_annotated_type(schema['type'], 'str', self.__class__.__name__)\n        return core_schema.with_info_after_validator_function(\n            function=self.decode_str,\n            schema=schema,\n            serialization=core_schema.plain_serializer_function_ser_schema(function=self.encode_str),\n        )\n\n    def decode_str(self, data: str, _: core_schema.ValidationInfo) -> str:\n        \"\"\"Decode the data using the specified encoder.\n\n        Args:\n            data: The data to decode.\n\n        Returns:\n            The decoded data.\n        \"\"\"\n        return self.encoder.decode(data.encode()).decode()\n\n    def encode_str(self, value: str) -> str:\n        \"\"\"Encode the data using the specified encoder.\n\n        Args:\n            value: The data to encode.\n\n        Returns:\n            The encoded data.\n        \"\"\"\n        return self.encoder.encode(value.encode()).decode()  # noqa: UP008\n\n    def __hash__(self) -> int:\n        return hash(self.encoder)", "metadata": {"license": "MIT", "len_tokens": 653}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class GetPydanticSchema:\n    \"\"\"!!! abstract \"Usage Documentation\"\n        [Using `GetPydanticSchema` to Reduce Boilerplate](../concepts/types.md#using-getpydanticschema-to-reduce-boilerplate)\n\n    A convenience class for creating an annotation that provides pydantic custom type hooks.\n\n    This class is intended to eliminate the need to create a custom \"marker\" which defines the\n     `__get_pydantic_core_schema__` and `__get_pydantic_json_schema__` custom hook methods.\n\n    For example, to have a field treated by type checkers as `int`, but by pydantic as `Any`, you can do:\n    ```python\n    from typing import Annotated, Any\n\n    from pydantic import BaseModel, GetPydanticSchema\n\n    HandleAsAny = GetPydanticSchema(lambda _s, h: h(Any))\n\n    class Model(BaseModel):\n        x: Annotated[int, HandleAsAny]  # pydantic sees `x: Any`\n\n    print(repr(Model(x='abc').x))\n    #> 'abc'\n    ```\n    \"\"\"\n\n    get_pydantic_core_schema: Callable[[Any, GetCoreSchemaHandler], CoreSchema] | None = None\n    get_pydantic_json_schema: Callable[[Any, GetJsonSchemaHandler], JsonSchemaValue] | None = None\n\n    # Note: we may want to consider adding a convenience staticmethod `def for_type(type_: Any) -> GetPydanticSchema:`\n    #   which returns `GetPydanticSchema(lambda _s, h: h(type_))`\n\n    if not TYPE_CHECKING:\n        # We put `__getattr__` in a non-TYPE_CHECKING block because otherwise, mypy allows arbitrary attribute access\n\n        def __getattr__(self, item: str) -> Any:\n            \"\"\"Use this rather than defining `__get_pydantic_core_schema__` etc. to reduce the number of nested calls.\"\"\"\n            if item == '__get_pydantic_core_schema__' and self.get_pydantic_core_schema:\n                return self.get_pydantic_core_schema\n            elif item == '__get_pydantic_json_schema__' and self.get_pydantic_json_schema:\n                return self.get_pydantic_json_schema\n            else:\n                return object.__getattribute__(self, item)\n\n    __hash__ = object.__hash__", "metadata": {"license": "MIT", "len_tokens": 506}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class Tag:\n    \"\"\"Provides a way to specify the expected tag to use for a case of a (callable) discriminated union.\n\n    Also provides a way to label a union case in error messages.\n\n    When using a callable `Discriminator`, attach a `Tag` to each case in the `Union` to specify the tag that\n    should be used to identify that case. For example, in the below example, the `Tag` is used to specify that\n    if `get_discriminator_value` returns `'apple'`, the input should be validated as an `ApplePie`, and if it\n    returns `'pumpkin'`, the input should be validated as a `PumpkinPie`.\n\n    The primary role of the `Tag` here is to map the return value from the callable `Discriminator` function to\n    the appropriate member of the `Union` in question.\n\n    ```python\n    from typing import Annotated, Any, Literal, Union\n\n    from pydantic import BaseModel, Discriminator, Tag\n\n    class Pie(BaseModel):\n        time_to_cook: int\n        num_ingredients: int\n\n    class ApplePie(Pie):\n        fruit: Literal['apple'] = 'apple'\n\n    class PumpkinPie(Pie):\n        filling: Literal['pumpkin'] = 'pumpkin'\n\n    def get_discriminator_value(v: Any) -> str:\n        if isinstance(v, dict):\n            return v.get('fruit', v.get('filling'))\n        return getattr(v, 'fruit', getattr(v, 'filling', None))\n\n    class ThanksgivingDinner(BaseModel):\n        dessert: Annotated[\n            Union[\n                Annotated[ApplePie, Tag('apple')],\n                Annotated[PumpkinPie, Tag('pumpkin')],\n            ],\n            Discriminator(get_discriminator_value),\n        ]\n\n    apple_variation = ThanksgivingDinner.model_validate(\n        {'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}\n    )\n    print(repr(apple_variation))\n    '''\n    ThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))\n    '''\n\n    pumpkin_variation = ThanksgivingDinner.model_validate(\n        {\n            'dessert': {\n                'filling': 'pumpkin',\n                'time_to_cook': 40,\n                'num_ingredients': 6,\n            }\n        }\n    )\n    print(repr(pumpkin_variation))\n    '''\n    ThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))\n    '''\n    ```\n\n    !!! note\n        You must specify a `Tag` for every case in a `Tag` that is associated with a\n        callable `Discriminator`. Failing to do so will result in a `PydanticUserError` with code\n        [`callable-discriminator-no-tag`](../errors/usage_errors.md#callable-discriminator-no-tag).\n\n    See the [Discriminated Unions] concepts docs for more details on how to use `Tag`s.\n\n    [Discriminated Unions]: ../concepts/unions.md#discriminated-unions\n    \"\"\"\n\n    tag: str\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> CoreSchema:\n        schema = handler(source_type)\n        metadata = cast('CoreMetadata', schema.setdefault('metadata', {}))\n        metadata['pydantic_internal_union_tag_key'] = self.tag\n        return schema", "metadata": {"license": "MIT", "len_tokens": 743}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class FailFast(_fields.PydanticMetadata, BaseMetadata):\n    \"\"\"A `FailFast` annotation can be used to specify that validation should stop at the first error.\n\n    This can be useful when you want to validate a large amount of data and you only need to know if it's valid or not.\n\n    You might want to enable this setting if you want to validate your data faster (basically, if you use this,\n    validation will be more performant with the caveat that you get less information).\n\n    ```python\n    from typing import Annotated\n\n    from pydantic import BaseModel, FailFast, ValidationError\n\n    class Model(BaseModel):\n        x: Annotated[list[int], FailFast()]\n\n    # This will raise a single error for the first invalid value and stop validation\n    try:\n        obj = Model(x=[1, 2, 'a', 4, 5, 'b', 7, 8, 9, 'c'])\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for Model\n        x.2\n          Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n        '''\n    ```\n    \"\"\"\n\n    fail_fast: bool = True", "metadata": {"license": "MIT", "len_tokens": 271}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "class Json:\n        \"\"\"A special type wrapper which loads JSON before parsing.\n\n        You can use the `Json` data type to make Pydantic first load a raw JSON string before\n        validating the loaded data into the parametrized type:\n\n        ```python\n        from typing import Any\n\n        from pydantic import BaseModel, Json, ValidationError\n\n        class AnyJsonModel(BaseModel):\n            json_obj: Json[Any]\n\n        class ConstrainedJsonModel(BaseModel):\n            json_obj: Json[list[int]]\n\n        print(AnyJsonModel(json_obj='{\"b\": 1}'))\n        #> json_obj={'b': 1}\n        print(ConstrainedJsonModel(json_obj='[1, 2, 3]'))\n        #> json_obj=[1, 2, 3]\n\n        try:\n            ConstrainedJsonModel(json_obj=12)\n        except ValidationError as e:\n            print(e)\n            '''\n            1 validation error for ConstrainedJsonModel\n            json_obj\n              JSON input should be string, bytes or bytearray [type=json_type, input_value=12, input_type=int]\n            '''\n\n        try:\n            ConstrainedJsonModel(json_obj='[a, b]')\n        except ValidationError as e:\n            print(e)\n            '''\n            1 validation error for ConstrainedJsonModel\n            json_obj\n              Invalid JSON: expected value at line 1 column 2 [type=json_invalid, input_value='[a, b]', input_type=str]\n            '''\n\n        try:\n            ConstrainedJsonModel(json_obj='[\"a\", \"b\"]')\n        except ValidationError as e:\n            print(e)\n            '''\n            2 validation errors for ConstrainedJsonModel\n            json_obj.0\n              Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]\n            json_obj.1\n              Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='b', input_type=str]\n            '''\n        ```\n\n        When you dump the model using `model_dump` or `model_dump_json`, the dumped value will be the result of validation,\n        not the original JSON string. However, you can use the argument `round_trip=True` to get the original JSON string back:\n\n        ```python\n        from pydantic import BaseModel, Json\n\n        class ConstrainedJsonModel(BaseModel):\n            json_obj: Json[list[int]]\n\n        print(ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json())\n        #> {\"json_obj\":[1,2,3]}\n        print(\n            ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json(round_trip=True)\n        )\n        #> {\"json_obj\":\"[1,2,3]\"}\n        ```\n        \"\"\"\n\n        @classmethod\n        def __class_getitem__(cls, item: AnyType) -> AnyType:\n            return Annotated[item, cls()]\n\n        @classmethod\n        def __get_pydantic_core_schema__(cls, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n            if cls is source:\n                return core_schema.json_schema(None)\n            else:\n                return core_schema.json_schema(handler(source))\n\n        def __repr__(self) -> str:\n            return 'Json'\n\n        def __hash__(self) -> int:\n            return hash(type(self))\n\n        def __eq__(self, other: Any) -> bool:\n            return type(other) is type(self)", "metadata": {"license": "MIT", "len_tokens": 743}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "def __get_pydantic_core_schema__(cls, source: type[Any], handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        inner_type = None\n        # if origin_type is Secret, then cls is a GenericAlias, and we can extract the inner type directly\n        origin_type = get_origin(source)\n        if origin_type is not None:\n            inner_type = get_args(source)[0]\n        # otherwise, we need to get the inner type from the base class\n        else:\n            bases = getattr(cls, '__orig_bases__', getattr(cls, '__bases__', []))\n            for base in bases:\n                if get_origin(base) is Secret:\n                    inner_type = get_args(base)[0]\n            if bases == [] or inner_type is None:\n                raise TypeError(\n                    f\"Can't get secret type from {cls.__name__}. \"\n                    'Please use Secret[<type>], or subclass from Secret[<type>] instead.'\n                )\n\n        inner_schema = handler.generate_schema(inner_type)  # type: ignore\n\n        def validate_secret_value(value, handler) -> Secret[SecretType]:\n            if isinstance(value, Secret):\n                value = value.get_secret_value()\n            validated_inner = handler(value)\n            return cls(validated_inner)\n\n        return core_schema.json_or_python_schema(\n            python_schema=core_schema.no_info_wrap_validator_function(\n                validate_secret_value,\n                inner_schema,\n            ),\n            json_schema=core_schema.no_info_after_validator_function(lambda x: cls(x), inner_schema),\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                _serialize_secret,\n                info_arg=True,\n                when_used='always',\n            ),\n        )", "metadata": {"license": "MIT", "len_tokens": 347}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "def __get_pydantic_core_schema__(cls, source: type[Any], handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        def get_json_schema(_core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n            json_schema = handler(cls._inner_schema)\n            _utils.update_not_none(\n                json_schema,\n                type='string',\n                writeOnly=True,\n                format='password',\n            )\n            return json_schema\n\n        def get_secret_schema(strict: bool) -> CoreSchema:\n            inner_schema = {**cls._inner_schema, 'strict': strict}\n            json_schema = core_schema.no_info_after_validator_function(\n                source,  # construct the type\n                inner_schema,  # pyright: ignore[reportArgumentType]\n            )\n            return core_schema.json_or_python_schema(\n                python_schema=core_schema.union_schema(\n                    [\n                        core_schema.is_instance_schema(source),\n                        json_schema,\n                    ],\n                    custom_error_type=cls._error_kind,\n                ),\n                json_schema=json_schema,\n                serialization=core_schema.plain_serializer_function_ser_schema(\n                    _serialize_secret_field,\n                    info_arg=True,\n                    when_used='always',\n                ),\n            )\n\n        return core_schema.lax_or_strict_schema(\n            lax_schema=get_secret_schema(strict=False),\n            strict_schema=get_secret_schema(strict=True),\n            metadata={'pydantic_js_functions': [get_json_schema]},\n        )", "metadata": {"license": "MIT", "len_tokens": 295}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "def validate_brand(card_number: str) -> PaymentCardBrand:\n        \"\"\"Validate length based on BIN for major brands:\n        https://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN).\n        \"\"\"\n        if card_number[0] == '4':\n            brand = PaymentCardBrand.visa\n        elif 51 <= int(card_number[:2]) <= 55:\n            brand = PaymentCardBrand.mastercard\n        elif card_number[:2] in {'34', '37'}:\n            brand = PaymentCardBrand.amex\n        else:\n            brand = PaymentCardBrand.other\n\n        required_length: None | int | str = None\n        if brand in PaymentCardBrand.mastercard:\n            required_length = 16\n            valid = len(card_number) == required_length\n        elif brand == PaymentCardBrand.visa:\n            required_length = '13, 16 or 19'\n            valid = len(card_number) in {13, 16, 19}\n        elif brand == PaymentCardBrand.amex:\n            required_length = 15\n            valid = len(card_number) == required_length\n        else:\n            valid = True\n\n        if not valid:\n            raise PydanticCustomError(\n                'payment_card_number_brand',\n                'Length for a {brand} card must be {required_length}',\n                {'brand': brand, 'required_length': required_length},\n            )\n        return brand", "metadata": {"license": "MIT", "len_tokens": 296}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "def human_readable(self, decimal: bool = False, separator: str = '') -> str:\n        \"\"\"Converts a byte size to a human readable string.\n\n        Args:\n            decimal: If True, use decimal units (e.g. 1000 bytes per KB). If False, use binary units\n                (e.g. 1024 bytes per KiB).\n            separator: A string used to split the value and unit. Defaults to an empty string ('').\n\n        Returns:\n            A human readable string representation of the byte size.\n        \"\"\"\n        if decimal:\n            divisor = 1000\n            units = 'B', 'KB', 'MB', 'GB', 'TB', 'PB'\n            final_unit = 'EB'\n        else:\n            divisor = 1024\n            units = 'B', 'KiB', 'MiB', 'GiB', 'TiB', 'PiB'\n            final_unit = 'EiB'\n\n        num = float(self)\n        for unit in units:\n            if abs(num) < divisor:\n                if unit == 'B':\n                    return f'{num:0.0f}{separator}{unit}'\n                else:\n                    return f'{num:0.1f}{separator}{unit}'\n            num /= divisor\n\n        return f'{num:0.1f}{separator}{final_unit}'", "metadata": {"license": "MIT", "len_tokens": 277}}
{"id": "pydantic:pydantic/types.py", "language": "python", "code": "def _convert_schema(\n        self, original_schema: core_schema.CoreSchema, handler: GetCoreSchemaHandler | None = None\n    ) -> core_schema.TaggedUnionSchema:\n        if original_schema['type'] != 'union':\n            # This likely indicates that the schema was a single-item union that was simplified.\n            # In this case, we do the same thing we do in\n            # `pydantic._internal._discriminated_union._ApplyInferredDiscriminator._apply_to_root`, namely,\n            # package the generated schema back into a single-item union.\n            original_schema = core_schema.union_schema([original_schema])\n\n        tagged_union_choices = {}\n        for choice in original_schema['choices']:\n            tag = None\n            if isinstance(choice, tuple):\n                choice, tag = choice\n            metadata = cast('CoreMetadata | None', choice.get('metadata'))\n            if metadata is not None:\n                tag = metadata.get('pydantic_internal_union_tag_key') or tag\n            if tag is None:\n                # `handler` is None when this method is called from `apply_discriminator()` (deferred discriminators)\n                if handler is not None and choice['type'] == 'definition-ref':\n                    # If choice was built from a PEP 695 type alias, try to resolve the def:\n                    try:\n                        choice = handler.resolve_ref_schema(choice)\n                    except LookupError:\n                        pass\n                    else:\n                        metadata = cast('CoreMetadata | None', choice.get('metadata'))\n                        if metadata is not None:\n                            tag = metadata.get('pydantic_internal_union_tag_key')\n\n                if tag is None:\n                    raise PydanticUserError(\n                        f'`Tag` not provided for choice {choice} used with `Discriminator`',\n                        code='callable-discriminator-no-tag',\n                    )\n            tagged_union_choices[tag] = choice\n\n        # Have to do these verbose checks to ensure falsy values ('' and {}) don't get ignored\n        custom_error_type = self.custom_error_type\n        if custom_error_type is None:\n            custom_error_type = original_schema.get('custom_error_type')\n\n        custom_error_message = self.custom_error_message\n        if custom_error_message is None:\n            custom_error_message = original_schema.get('custom_error_message')\n\n        custom_error_context = self.custom_error_context\n        if custom_error_context is None:\n            custom_error_context = original_schema.get('custom_error_context')\n\n        custom_error_type = original_schema.get('custom_error_type') if custom_error_type is None else custom_error_type\n        return core_schema.tagged_union_schema(\n            tagged_union_choices,\n            self.discriminator,\n            custom_error_type=custom_error_type,\n            custom_error_message=custom_error_message,\n            custom_error_context=custom_error_context,\n            strict=original_schema.get('strict'),\n            ref=original_schema.get('ref'),\n            metadata=original_schema.get('metadata'),\n            serialization=original_schema.get('serialization'),\n        )", "metadata": {"license": "MIT", "len_tokens": 607}}
{"id": "pydantic:pydantic/aliases.py", "language": "python", "code": "class AliasPath:\n    \"\"\"!!! abstract \"Usage Documentation\"\n        [`AliasPath` and `AliasChoices`](../concepts/alias.md#aliaspath-and-aliaschoices)\n\n    A data class used by `validation_alias` as a convenience to create aliases.\n\n    Attributes:\n        path: A list of string or integer aliases.\n    \"\"\"\n\n    path: list[int | str]\n\n    def __init__(self, first_arg: str, *args: str | int) -> None:\n        self.path = [first_arg] + list(args)\n\n    def convert_to_aliases(self) -> list[str | int]:\n        \"\"\"Converts arguments to a list of string or integer aliases.\n\n        Returns:\n            The list of aliases.\n        \"\"\"\n        return self.path\n\n    def search_dict_for_path(self, d: dict) -> Any:\n        \"\"\"Searches a dictionary for the path specified by the alias.\n\n        Returns:\n            The value at the specified path, or `PydanticUndefined` if the path is not found.\n        \"\"\"\n        v = d\n        for k in self.path:\n            if isinstance(v, str):\n                # disallow indexing into a str, like for AliasPath('x', 0) and x='abc'\n                return PydanticUndefined\n            try:\n                v = v[k]\n            except (KeyError, IndexError, TypeError):\n                return PydanticUndefined\n        return v", "metadata": {"license": "MIT", "len_tokens": 292}}
{"id": "pydantic:pydantic/aliases.py", "language": "python", "code": "class AliasChoices:\n    \"\"\"!!! abstract \"Usage Documentation\"\n        [`AliasPath` and `AliasChoices`](../concepts/alias.md#aliaspath-and-aliaschoices)\n\n    A data class used by `validation_alias` as a convenience to create aliases.\n\n    Attributes:\n        choices: A list containing a string or `AliasPath`.\n    \"\"\"\n\n    choices: list[str | AliasPath]\n\n    def __init__(self, first_choice: str | AliasPath, *choices: str | AliasPath) -> None:\n        self.choices = [first_choice] + list(choices)\n\n    def convert_to_aliases(self) -> list[list[str | int]]:\n        \"\"\"Converts arguments to a list of lists containing string or integer aliases.\n\n        Returns:\n            The list of aliases.\n        \"\"\"\n        aliases: list[list[str | int]] = []\n        for c in self.choices:\n            if isinstance(c, AliasPath):\n                aliases.append(c.convert_to_aliases())\n            else:\n                aliases.append([c])\n        return aliases", "metadata": {"license": "MIT", "len_tokens": 210}}
{"id": "pydantic:pydantic/aliases.py", "language": "python", "code": "class AliasGenerator:\n    \"\"\"!!! abstract \"Usage Documentation\"\n        [Using an `AliasGenerator`](../concepts/alias.md#using-an-aliasgenerator)\n\n    A data class used by `alias_generator` as a convenience to create various aliases.\n\n    Attributes:\n        alias: A callable that takes a field name and returns an alias for it.\n        validation_alias: A callable that takes a field name and returns a validation alias for it.\n        serialization_alias: A callable that takes a field name and returns a serialization alias for it.\n    \"\"\"\n\n    alias: Callable[[str], str] | None = None\n    validation_alias: Callable[[str], str | AliasPath | AliasChoices] | None = None\n    serialization_alias: Callable[[str], str] | None = None\n\n    def _generate_alias(\n        self,\n        alias_kind: Literal['alias', 'validation_alias', 'serialization_alias'],\n        allowed_types: tuple[type[str] | type[AliasPath] | type[AliasChoices], ...],\n        field_name: str,\n    ) -> str | AliasPath | AliasChoices | None:\n        \"\"\"Generate an alias of the specified kind. Returns None if the alias generator is None.\n\n        Raises:\n            TypeError: If the alias generator produces an invalid type.\n        \"\"\"\n        alias = None\n        if alias_generator := getattr(self, alias_kind):\n            alias = alias_generator(field_name)\n            if alias and not isinstance(alias, allowed_types):\n                raise TypeError(\n                    f'Invalid `{alias_kind}` type. `{alias_kind}` generator must produce one of `{allowed_types}`'\n                )\n        return alias\n\n    def generate_aliases(self, field_name: str) -> tuple[str | None, str | AliasPath | AliasChoices | None, str | None]:\n        \"\"\"Generate `alias`, `validation_alias`, and `serialization_alias` for a field.\n\n        Returns:\n            A tuple of three aliases - validation, alias, and serialization.\n        \"\"\"\n        alias = self._generate_alias('alias', (str,), field_name)\n        validation_alias = self._generate_alias('validation_alias', (str, AliasChoices, AliasPath), field_name)\n        serialization_alias = self._generate_alias('serialization_alias', (str,), field_name)\n\n        return alias, validation_alias, serialization_alias", "metadata": {"license": "MIT", "len_tokens": 471}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def model_json_schema(\n    cls: type[BaseModel] | type[PydanticDataclass],\n    by_alias: bool = True,\n    ref_template: str = DEFAULT_REF_TEMPLATE,\n    union_format: Literal['any_of', 'primitive_type_array'] = 'any_of',\n    schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n    mode: JsonSchemaMode = 'validation',\n) -> dict[str, Any]:\n    \"\"\"Utility function to generate a JSON Schema for a model.\n\n    Args:\n        cls: The model class to generate a JSON Schema for.\n        by_alias: If `True` (the default), fields will be serialized according to their alias.\n            If `False`, fields will be serialized according to their attribute name.\n        ref_template: The template to use for generating JSON Schema references.\n        union_format: The format to use when combining schemas from unions together. Can be one of:\n\n            - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)\n              keyword to combine schemas (the default).\n            - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)\n              keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive\n              type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to\n              `any_of`.\n        schema_generator: The class to use for generating the JSON Schema.\n        mode: The mode to use for generating the JSON Schema. It can be one of the following:\n\n            - 'validation': Generate a JSON Schema for validating data.\n            - 'serialization': Generate a JSON Schema for serializing data.\n\n    Returns:\n        The generated JSON Schema.\n    \"\"\"\n    from .main import BaseModel\n\n    schema_generator_instance = schema_generator(\n        by_alias=by_alias, ref_template=ref_template, union_format=union_format\n    )\n\n    if isinstance(cls.__pydantic_core_schema__, _mock_val_ser.MockCoreSchema):\n        cls.__pydantic_core_schema__.rebuild()\n\n    if cls is BaseModel:\n        raise AttributeError('model_json_schema() must be called on a subclass of BaseModel, not BaseModel itself.')\n\n    assert not isinstance(cls.__pydantic_core_schema__, _mock_val_ser.MockCoreSchema), 'this is a bug! please report it'\n    return schema_generator_instance.generate(cls.__pydantic_core_schema__, mode=mode)", "metadata": {"license": "MIT", "len_tokens": 533}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def models_json_schema(\n    models: Sequence[tuple[type[BaseModel] | type[PydanticDataclass], JsonSchemaMode]],\n    *,\n    by_alias: bool = True,\n    title: str | None = None,\n    description: str | None = None,\n    ref_template: str = DEFAULT_REF_TEMPLATE,\n    union_format: Literal['any_of', 'primitive_type_array'] = 'any_of',\n    schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n) -> tuple[dict[tuple[type[BaseModel] | type[PydanticDataclass], JsonSchemaMode], JsonSchemaValue], JsonSchemaValue]:\n    \"\"\"Utility function to generate a JSON Schema for multiple models.\n\n    Args:\n        models: A sequence of tuples of the form (model, mode).\n        by_alias: Whether field aliases should be used as keys in the generated JSON Schema.\n        title: The title of the generated JSON Schema.\n        description: The description of the generated JSON Schema.\n        ref_template: The reference template to use for generating JSON Schema references.\n        union_format: The format to use when combining schemas from unions together. Can be one of:\n\n            - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)\n              keyword to combine schemas (the default).\n            - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)\n              keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive\n              type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to\n              `any_of`.\n        schema_generator: The schema generator to use for generating the JSON Schema.\n\n    Returns:\n        A tuple where:\n            - The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\n                whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\n                JsonRef references to definitions that are defined in the second returned element.)\n            - The second element is a JSON schema containing all definitions referenced in the first returned\n                    element, along with the optional title and description keys.\n    \"\"\"\n    for cls, _ in models:\n        if isinstance(cls.__pydantic_core_schema__, _mock_val_ser.MockCoreSchema):\n            cls.__pydantic_core_schema__.rebuild()\n\n    instance = schema_generator(by_alias=by_alias, ref_template=ref_template, union_format=union_format)\n    inputs: list[tuple[type[BaseModel] | type[PydanticDataclass], JsonSchemaMode, CoreSchema]] = [\n        (m, mode, m.__pydantic_core_schema__) for m, mode in models\n    ]\n    json_schemas_map, definitions = instance.generate_definitions(inputs)\n\n    json_schema: dict[str, Any] = {}\n    if definitions:\n        json_schema['$defs'] = definitions\n    if title:\n        json_schema['title'] = title\n    if description:\n        json_schema['description'] = description\n\n    return json_schemas_map, json_schema", "metadata": {"license": "MIT", "len_tokens": 662}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "class WithJsonSchema:\n    \"\"\"!!! abstract \"Usage Documentation\"\n        [`WithJsonSchema` Annotation](../concepts/json_schema.md#withjsonschema-annotation)\n\n    Add this as an annotation on a field to override the (base) JSON schema that would be generated for that field.\n    This provides a way to set a JSON schema for types that would otherwise raise errors when producing a JSON schema,\n    such as Callable, or types that have an is-instance core schema, without needing to go so far as creating a\n    custom subclass of pydantic.json_schema.GenerateJsonSchema.\n    Note that any _modifications_ to the schema that would normally be made (such as setting the title for model fields)\n    will still be performed.\n\n    If `mode` is set this will only apply to that schema generation mode, allowing you\n    to set different json schemas for validation and serialization.\n    \"\"\"\n\n    json_schema: JsonSchemaValue | None\n    mode: Literal['validation', 'serialization'] | None = None\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        mode = self.mode or handler.mode\n        if mode != handler.mode:\n            return handler(core_schema)\n        if self.json_schema is None:\n            # This exception is handled in pydantic.json_schema.GenerateJsonSchema._named_required_fields_schema\n            raise PydanticOmit\n        else:\n            return self.json_schema.copy()\n\n    def __hash__(self) -> int:\n        return hash(type(self.mode))", "metadata": {"license": "MIT", "len_tokens": 331}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "class Examples:\n    \"\"\"Add examples to a JSON schema.\n\n    If the JSON Schema already contains examples, the provided examples\n    will be appended.\n\n    If `mode` is set this will only apply to that schema generation mode,\n    allowing you to add different examples for validation and serialization.\n    \"\"\"\n\n    @overload\n    @deprecated('Using a dict for `examples` is deprecated since v2.9 and will be removed in v3.0. Use a list instead.')\n    def __init__(\n        self, examples: dict[str, Any], mode: Literal['validation', 'serialization'] | None = None\n    ) -> None: ...\n\n    @overload\n    def __init__(self, examples: list[Any], mode: Literal['validation', 'serialization'] | None = None) -> None: ...\n\n    def __init__(\n        self, examples: dict[str, Any] | list[Any], mode: Literal['validation', 'serialization'] | None = None\n    ) -> None:\n        if isinstance(examples, dict):\n            warnings.warn(\n                'Using a dict for `examples` is deprecated, use a list instead.',\n                PydanticDeprecatedSince29,\n                stacklevel=2,\n            )\n        self.examples = examples\n        self.mode = mode\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        mode = self.mode or handler.mode\n        json_schema = handler(core_schema)\n        if mode != handler.mode:\n            return json_schema\n        examples = json_schema.get('examples')\n        if examples is None:\n            json_schema['examples'] = to_jsonable_python(self.examples)\n        if isinstance(examples, dict):\n            if isinstance(self.examples, list):\n                warnings.warn(\n                    'Updating existing JSON Schema examples of type dict with examples of type list. '\n                    'Only the existing examples values will be retained. Note that dict support for '\n                    'examples is deprecated and will be removed in v3.0.',\n                    UserWarning,\n                )\n                json_schema['examples'] = to_jsonable_python(\n                    [ex for value in examples.values() for ex in value] + self.examples\n                )\n            else:\n                json_schema['examples'] = to_jsonable_python({**examples, **self.examples})\n        if isinstance(examples, list):\n            if isinstance(self.examples, list):\n                json_schema['examples'] = to_jsonable_python(examples + self.examples)\n            elif isinstance(self.examples, dict):\n                warnings.warn(\n                    'Updating existing JSON Schema examples of type list with examples of type dict. '\n                    'Only the examples values will be retained. Note that dict support for '\n                    'examples is deprecated and will be removed in v3.0.',\n                    UserWarning,\n                )\n                json_schema['examples'] = to_jsonable_python(\n                    examples + [ex for value in self.examples.values() for ex in value]\n                )\n\n        return json_schema\n\n    def __hash__(self) -> int:\n        return hash(type(self.mode))", "metadata": {"license": "MIT", "len_tokens": 638}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def _get_all_json_refs(item: Any) -> set[JsonRef]:\n    \"\"\"Get all the definitions references from a JSON schema.\"\"\"\n    refs: set[JsonRef] = set()\n    stack = [item]\n\n    while stack:\n        current = stack.pop()\n        if isinstance(current, dict):\n            for key, value in current.items():\n                if key == 'examples' and isinstance(value, list):\n                    # Skip examples that may contain arbitrary values and references\n                    # (e.g. `{\"examples\": [{\"$ref\": \"...\"}]}`). Note: checking for value\n                    # of type list is necessary to avoid skipping valid portions of the schema,\n                    # for instance when \"examples\" is used as a property key. A more robust solution\n                    # could be found, but would require more advanced JSON Schema parsing logic.\n                    continue\n                if key == '$ref' and isinstance(value, str):\n                    refs.add(JsonRef(value))\n                elif isinstance(value, dict):\n                    stack.append(value)\n                elif isinstance(value, list):\n                    stack.extend(value)\n        elif isinstance(current, list):\n            stack.extend(current)\n\n    return refs", "metadata": {"license": "MIT", "len_tokens": 234}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def from_prioritized_choices(\n        prioritized_choices: dict[DefsRef, list[DefsRef]],\n        defs_to_json: dict[DefsRef, JsonRef],\n        definitions: dict[DefsRef, JsonSchemaValue],\n    ) -> _DefinitionsRemapping:\n        \"\"\"\n        This function should produce a remapping that replaces complex DefsRef with the simpler ones from the\n        prioritized_choices such that applying the name remapping would result in an equivalent JSON schema.\n        \"\"\"\n        # We need to iteratively simplify the definitions until we reach a fixed point.\n        # The reason for this is that outer definitions may reference inner definitions that get simplified\n        # into an equivalent reference, and the outer definitions won't be equivalent until we've simplified\n        # the inner definitions.\n        copied_definitions = deepcopy(definitions)\n        definitions_schema = {'$defs': copied_definitions}\n        for _iter in range(100):  # prevent an infinite loop in the case of a bug, 100 iterations should be enough\n            # For every possible remapped DefsRef, collect all schemas that that DefsRef might be used for:\n            schemas_for_alternatives: dict[DefsRef, list[JsonSchemaValue]] = defaultdict(list)\n            for defs_ref in copied_definitions:\n                alternatives = prioritized_choices[defs_ref]\n                for alternative in alternatives:\n                    schemas_for_alternatives[alternative].append(copied_definitions[defs_ref])\n\n            # Deduplicate the schemas for each alternative; the idea is that we only want to remap to a new DefsRef\n            # if it introduces no ambiguity, i.e., there is only one distinct schema for that DefsRef.\n            for defs_ref in schemas_for_alternatives:\n                schemas_for_alternatives[defs_ref] = _deduplicate_schemas(schemas_for_alternatives[defs_ref])\n\n            # Build the remapping\n            defs_remapping: dict[DefsRef, DefsRef] = {}\n            json_remapping: dict[JsonRef, JsonRef] = {}\n            for original_defs_ref in definitions:\n                alternatives = prioritized_choices[original_defs_ref]\n                # Pick the first alternative that has only one schema, since that means there is no collision\n                remapped_defs_ref = next(x for x in alternatives if len(schemas_for_alternatives[x]) == 1)\n                defs_remapping[original_defs_ref] = remapped_defs_ref\n\n                # Map all alternatives after the remapped one to the remapped one\n                # This ensures that intermediate simplifications are also remapped\n                remapped_index = alternatives.index(remapped_defs_ref)\n                for alt in alternatives[remapped_index:]:\n                    json_remapping[defs_to_json[alt]] = defs_to_json[remapped_defs_ref]\n            remapping = _DefinitionsRemapping(defs_remapping, json_remapping)\n            new_definitions_schema = remapping.remap_json_schema({'$defs': copied_definitions})\n            if definitions_schema == new_definitions_schema:\n                # We've reached the fixed point\n                return remapping\n            definitions_schema = new_definitions_schema\n\n        raise PydanticInvalidForJsonSchema('Failed to simplify the JSON schema definitions')", "metadata": {"license": "MIT", "len_tokens": 658}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def remap_json_schema(self, schema: Any) -> Any:\n        \"\"\"\n        Recursively update the JSON schema replacing all $refs\n        \"\"\"\n        if isinstance(schema, str):\n            # Note: this may not really be a JsonRef; we rely on having no collisions between JsonRefs and other strings\n            return self.remap_json_ref(JsonRef(schema))\n        elif isinstance(schema, list):\n            return [self.remap_json_schema(item) for item in schema]\n        elif isinstance(schema, dict):\n            for key, value in schema.items():\n                if key == '$ref' and isinstance(value, str):\n                    schema['$ref'] = self.remap_json_ref(JsonRef(value))\n                elif key == '$defs':\n                    schema['$defs'] = {\n                        self.remap_defs_ref(DefsRef(key)): self.remap_json_schema(value)\n                        for key, value in schema['$defs'].items()\n                    }\n                else:\n                    schema[key] = self.remap_json_schema(value)\n        return schema", "metadata": {"license": "MIT", "len_tokens": 207}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def __init__(\n        self,\n        by_alias: bool = True,\n        ref_template: str = DEFAULT_REF_TEMPLATE,\n        union_format: Literal['any_of', 'primitive_type_array'] = 'any_of',\n    ) -> None:\n        self.by_alias = by_alias\n        self.ref_template = ref_template\n        self.union_format: Literal['any_of', 'primitive_type_array'] = union_format\n\n        self.core_to_json_refs: dict[CoreModeRef, JsonRef] = {}\n        self.core_to_defs_refs: dict[CoreModeRef, DefsRef] = {}\n        self.defs_to_core_refs: dict[DefsRef, CoreModeRef] = {}\n        self.json_to_defs_refs: dict[JsonRef, DefsRef] = {}\n\n        self.definitions: dict[DefsRef, JsonSchemaValue] = {}\n        self._config_wrapper_stack = _config.ConfigWrapperStack(_config.ConfigWrapper({}))\n\n        self._mode: JsonSchemaMode = 'validation'\n\n        # The following includes a mapping of a fully-unique defs ref choice to a list of preferred\n        # alternatives, which are generally simpler, such as only including the class name.\n        # At the end of schema generation, we use these to produce a JSON schema with more human-readable\n        # definitions, which would also work better in a generated OpenAPI client, etc.\n        self._prioritized_defsref_choices: dict[DefsRef, list[DefsRef]] = {}\n        self._collision_counter: dict[str, int] = defaultdict(int)\n        self._collision_index: dict[str, int] = {}\n\n        self._schema_type_to_method = self.build_schema_type_to_method()\n\n        # When we encounter definitions we need to try to build them immediately\n        # so that they are available schemas that reference them\n        # But it's possible that CoreSchema was never going to be used\n        # (e.g. because the CoreSchema that references short circuits is JSON schema generation without needing\n        #  the reference) so instead of failing altogether if we can't build a definition we\n        # store the error raised and re-throw it if we end up needing that def\n        self._core_defs_invalid_for_json_schema: dict[DefsRef, PydanticInvalidForJsonSchema] = {}\n\n        # This changes to True after generating a schema, to prevent issues caused by accidental reuse\n        # of a single instance of a schema generator\n        self._used = False", "metadata": {"license": "MIT", "len_tokens": 509}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def build_schema_type_to_method(\n        self,\n    ) -> dict[CoreSchemaOrFieldType, Callable[[CoreSchemaOrField], JsonSchemaValue]]:\n        \"\"\"Builds a dictionary mapping fields to methods for generating JSON schemas.\n\n        Returns:\n            A dictionary containing the mapping of `CoreSchemaOrFieldType` to a handler method.\n\n        Raises:\n            TypeError: If no method has been defined for generating a JSON schema for a given pydantic core schema type.\n        \"\"\"\n        mapping: dict[CoreSchemaOrFieldType, Callable[[CoreSchemaOrField], JsonSchemaValue]] = {}\n        core_schema_types: list[CoreSchemaOrFieldType] = list(get_literal_values(CoreSchemaOrFieldType))\n        for key in core_schema_types:\n            method_name = f'{key.replace(\"-\", \"_\")}_schema'\n            try:\n                mapping[key] = getattr(self, method_name)\n            except AttributeError as e:  # pragma: no cover\n                if os.getenv('PYDANTIC_PRIVATE_ALLOW_UNHANDLED_SCHEMA_TYPES'):\n                    continue\n                raise TypeError(\n                    f'No method for generating JsonSchema for core_schema.type={key!r} '\n                    f'(expected: {type(self).__name__}.{method_name})'\n                ) from e\n        return mapping", "metadata": {"license": "MIT", "len_tokens": 265}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def generate_definitions(\n        self, inputs: Sequence[tuple[JsonSchemaKeyT, JsonSchemaMode, core_schema.CoreSchema]]\n    ) -> tuple[dict[tuple[JsonSchemaKeyT, JsonSchemaMode], JsonSchemaValue], dict[DefsRef, JsonSchemaValue]]:\n        \"\"\"Generates JSON schema definitions from a list of core schemas, pairing the generated definitions with a\n        mapping that links the input keys to the definition references.\n\n        Args:\n            inputs: A sequence of tuples, where:\n\n                - The first element is a JSON schema key type.\n                - The second element is the JSON mode: either 'validation' or 'serialization'.\n                - The third element is a core schema.\n\n        Returns:\n            A tuple where:\n\n                - The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\n                    whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\n                    JsonRef references to definitions that are defined in the second returned element.)\n                - The second element is a dictionary whose keys are definition references for the JSON schemas\n                    from the first returned element, and whose values are the actual JSON schema definitions.\n\n        Raises:\n            PydanticUserError: Raised if the JSON schema generator has already been used to generate a JSON schema.\n        \"\"\"\n        if self._used:\n            raise PydanticUserError(\n                'This JSON schema generator has already been used to generate a JSON schema. '\n                f'You must create a new instance of {type(self).__name__} to generate a new JSON schema.',\n                code='json-schema-already-used',\n            )\n\n        for _, mode, schema in inputs:\n            self._mode = mode\n            self.generate_inner(schema)\n\n        definitions_remapping = self._build_definitions_remapping()\n\n        json_schemas_map: dict[tuple[JsonSchemaKeyT, JsonSchemaMode], DefsRef] = {}\n        for key, mode, schema in inputs:\n            self._mode = mode\n            json_schema = self.generate_inner(schema)\n            json_schemas_map[(key, mode)] = definitions_remapping.remap_json_schema(json_schema)\n\n        json_schema = {'$defs': self.definitions}\n        json_schema = definitions_remapping.remap_json_schema(json_schema)\n        self._used = True\n        return json_schemas_map, self.sort(json_schema['$defs'])", "metadata": {"license": "MIT", "len_tokens": 495}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def generate(self, schema: CoreSchema, mode: JsonSchemaMode = 'validation') -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema for a specified schema in a specified mode.\n\n        Args:\n            schema: A Pydantic model.\n            mode: The mode in which to generate the schema. Defaults to 'validation'.\n\n        Returns:\n            A JSON schema representing the specified schema.\n\n        Raises:\n            PydanticUserError: If the JSON schema generator has already been used to generate a JSON schema.\n        \"\"\"\n        self._mode = mode\n        if self._used:\n            raise PydanticUserError(\n                'This JSON schema generator has already been used to generate a JSON schema. '\n                f'You must create a new instance of {type(self).__name__} to generate a new JSON schema.',\n                code='json-schema-already-used',\n            )\n\n        json_schema: JsonSchemaValue = self.generate_inner(schema)\n        json_ref_counts = self.get_json_ref_counts(json_schema)\n\n        ref = cast(JsonRef, json_schema.get('$ref'))\n        while ref is not None:  # may need to unpack multiple levels\n            ref_json_schema = self.get_schema_from_definitions(ref)\n            if json_ref_counts[ref] == 1 and ref_json_schema is not None and len(json_schema) == 1:\n                # \"Unpack\" the ref since this is the only reference and there are no sibling keys\n                json_schema = ref_json_schema.copy()  # copy to prevent recursive dict reference\n                json_ref_counts[ref] -= 1\n                ref = cast(JsonRef, json_schema.get('$ref'))\n            ref = None\n\n        self._garbage_collect_definitions(json_schema)\n        definitions_remapping = self._build_definitions_remapping()\n\n        if self.definitions:\n            json_schema['$defs'] = self.definitions\n\n        json_schema = definitions_remapping.remap_json_schema(json_schema)\n\n        # For now, we will not set the $schema key. However, if desired, this can be easily added by overriding\n        # this method and adding the following line after a call to super().generate(schema):\n        # json_schema['$schema'] = self.schema_dialect\n\n        self._used = True\n        return self.sort(json_schema)", "metadata": {"license": "MIT", "len_tokens": 472}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def decimal_schema(self, schema: core_schema.DecimalSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a decimal value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n\n        def get_decimal_pattern(schema: core_schema.DecimalSchema) -> str:\n            max_digits = schema.get('max_digits')\n            decimal_places = schema.get('decimal_places')\n\n            pattern = (\n                r'^(?!^[-+.]*$)[+-]?0*'  # check it is not empty string and not one or sequence of \".+-\" characters.\n            )\n\n            # Case 1: Both max_digits and decimal_places are set\n            if max_digits is not None and decimal_places is not None:\n                integer_places = max(0, max_digits - decimal_places)\n                pattern += (\n                    rf'(?:'\n                    rf'\\d{{0,{integer_places}}}'\n                    rf'|'\n                    rf'(?=[\\d.]{{1,{max_digits + 1}}}0*$)'\n                    rf'\\d{{0,{integer_places}}}\\.\\d{{0,{decimal_places}}}0*$'\n                    rf')'\n                )\n\n            # Case 2: Only max_digits is set\n            elif max_digits is not None and decimal_places is None:\n                pattern += (\n                    rf'(?:'\n                    rf'\\d{{0,{max_digits}}}'\n                    rf'|'\n                    rf'(?=[\\d.]{{1,{max_digits + 1}}}0*$)'\n                    rf'\\d*\\.\\d*0*$'\n                    rf')'\n                )\n\n            # Case 3: Only decimal_places is set\n            elif max_digits is None and decimal_places is not None:\n                pattern += rf'\\d*\\.?\\d{{0,{decimal_places}}}0*$'\n\n            # Case 4: Both are None (no restrictions)\n            else:\n                pattern += r'\\d*\\.?\\d*$'  # look for arbitrary integer or decimal\n\n            return pattern\n\n        json_schema = self.str_schema(core_schema.str_schema(pattern=get_decimal_pattern(schema)))\n        if self.mode == 'validation':\n            multiple_of = schema.get('multiple_of')\n            le = schema.get('le')\n            ge = schema.get('ge')\n            lt = schema.get('lt')\n            gt = schema.get('gt')\n            json_schema = {\n                'anyOf': [\n                    self.float_schema(\n                        core_schema.float_schema(\n                            allow_inf_nan=schema.get('allow_inf_nan'),\n                            multiple_of=None if multiple_of is None else float(multiple_of),\n                            le=None if le is None else float(le),\n                            ge=None if ge is None else float(ge),\n                            lt=None if lt is None else float(lt),\n                            gt=None if gt is None else float(gt),\n                        )\n                    ),\n                    json_schema,\n                ],\n            }\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 597}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def literal_schema(self, schema: core_schema.LiteralSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a literal value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        expected = [to_jsonable_python(v.value if isinstance(v, Enum) else v) for v in schema['expected']]\n\n        result: dict[str, Any] = {}\n        if len(expected) == 1:\n            result['const'] = expected[0]\n        else:\n            result['enum'] = expected\n\n        types = {type(e) for e in expected}\n        if types == {str}:\n            result['type'] = 'string'\n        elif types == {int}:\n            result['type'] = 'integer'\n        elif types == {float}:\n            result['type'] = 'number'\n        elif types == {bool}:\n            result['type'] = 'boolean'\n        elif types == {list}:\n            result['type'] = 'array'\n        elif types == {type(None)}:\n            result['type'] = 'null'\n        return result", "metadata": {"license": "MIT", "len_tokens": 237}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def enum_schema(self, schema: core_schema.EnumSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches an Enum value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        enum_type = schema['cls']\n        description = None if not enum_type.__doc__ else inspect.cleandoc(enum_type.__doc__)\n        if (\n            description == 'An enumeration.'\n        ):  # This is the default value provided by enum.EnumMeta.__new__; don't use it\n            description = None\n        result: dict[str, Any] = {'title': enum_type.__name__, 'description': description}\n        result = {k: v for k, v in result.items() if v is not None}\n\n        expected = [to_jsonable_python(v.value) for v in schema['members']]\n\n        result['enum'] = expected\n\n        types = {type(e) for e in expected}\n        if isinstance(enum_type, str) or types == {str}:\n            result['type'] = 'string'\n        elif isinstance(enum_type, int) or types == {int}:\n            result['type'] = 'integer'\n        elif isinstance(enum_type, float) or types == {float}:\n            result['type'] = 'number'\n        elif types == {bool}:\n            result['type'] = 'boolean'\n        elif types == {list}:\n            result['type'] = 'array'\n\n        return result", "metadata": {"license": "MIT", "len_tokens": 308}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def tuple_schema(self, schema: core_schema.TupleSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a tuple schema e.g. `tuple[int,\n        str, bool]` or `tuple[int, ...]`.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        json_schema: JsonSchemaValue = {'type': 'array'}\n        if 'variadic_item_index' in schema:\n            variadic_item_index = schema['variadic_item_index']\n            if variadic_item_index > 0:\n                json_schema['minItems'] = variadic_item_index\n                json_schema['prefixItems'] = [\n                    self.generate_inner(item) for item in schema['items_schema'][:variadic_item_index]\n                ]\n            if variadic_item_index + 1 == len(schema['items_schema']):\n                # if the variadic item is the last item, then represent it faithfully\n                json_schema['items'] = self.generate_inner(schema['items_schema'][variadic_item_index])\n            else:\n                # otherwise, 'items' represents the schema for the variadic\n                # item plus the suffix, so just allow anything for simplicity\n                # for now\n                json_schema['items'] = True\n        else:\n            prefixItems = [self.generate_inner(item) for item in schema['items_schema']]\n            if prefixItems:\n                json_schema['prefixItems'] = prefixItems\n            json_schema['minItems'] = len(prefixItems)\n            json_schema['maxItems'] = len(prefixItems)\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.array)\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 344}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def dict_schema(self, schema: core_schema.DictSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a dict schema.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        json_schema: JsonSchemaValue = {'type': 'object'}\n\n        keys_schema = self.generate_inner(schema['keys_schema']).copy() if 'keys_schema' in schema else {}\n        if '$ref' not in keys_schema:\n            keys_pattern = keys_schema.pop('pattern', None)\n            # Don't give a title to patternProperties/propertyNames:\n            keys_schema.pop('title', None)\n        else:\n            # Here, we assume that if the keys schema is a definition reference,\n            # it can't be a simple string core schema (and thus no pattern can exist).\n            # However, this is only in practice (in theory, a definition reference core\n            # schema could be generated for a simple string schema).\n            # Note that we avoid calling `self.resolve_ref_schema`, as it might not exist yet.\n            keys_pattern = None\n\n        values_schema = self.generate_inner(schema['values_schema']).copy() if 'values_schema' in schema else {}\n        # don't give a title to additionalProperties:\n        values_schema.pop('title', None)\n\n        if values_schema or keys_pattern is not None:\n            if keys_pattern is None:\n                json_schema['additionalProperties'] = values_schema\n            else:\n                json_schema['patternProperties'] = {keys_pattern: values_schema}\n        else:  # for `dict[str, Any]`, we allow any key and any value, since `str` is the default key type\n            json_schema['additionalProperties'] = True\n\n        if (\n            # The len check indicates that constraints are probably present:\n            (keys_schema.get('type') == 'string' and len(keys_schema) > 1)\n            # If this is a definition reference schema, it most likely has constraints:\n            or '$ref' in keys_schema\n        ):\n            keys_schema.pop('type', None)\n            json_schema['propertyNames'] = keys_schema\n\n        self.update_with_validations(json_schema, schema, self.ValidationsMapping.object)\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 458}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def default_schema(self, schema: core_schema.WithDefaultSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema with a default value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        json_schema = self.generate_inner(schema['schema'])\n\n        default = self.get_default_value(schema)\n        if default is NoDefault or default is MISSING:\n            return json_schema\n\n        # we reflect the application of custom plain, no-info serializers to defaults for\n        # JSON Schemas viewed in serialization mode:\n        # TODO: improvements along with https://github.com/pydantic/pydantic/issues/8208\n        if self.mode == 'serialization':\n            # `_get_ser_schema_for_default_value()` is used to unpack potentially nested validator schemas:\n            ser_schema = _get_ser_schema_for_default_value(schema['schema'])\n            if (\n                ser_schema is not None\n                and (ser_func := ser_schema.get('function'))\n                and not (default is None and ser_schema.get('when_used') in ('unless-none', 'json-unless-none'))\n            ):\n                try:\n                    default = ser_func(default)  # type: ignore\n                except Exception:\n                    # It might be that the provided default needs to be validated (read: parsed) first\n                    # (assuming `validate_default` is enabled). However, we can't perform\n                    # such validation during JSON Schema generation so we don't support\n                    # this pattern for now.\n                    # (One example is when using `foo: ByteSize = '1MB'`, which validates and\n                    # serializes as an int. In this case, `ser_func` is `int` and `int('1MB')` fails).\n                    self.emit_warning(\n                        'non-serializable-default',\n                        f'Unable to serialize value {default!r} with the plain serializer; excluding default from JSON schema',\n                    )\n                    return json_schema\n\n        try:\n            encoded_default = self.encode_default(default)\n        except pydantic_core.PydanticSerializationError:\n            self.emit_warning(\n                'non-serializable-default',\n                f'Default value {default} is not JSON serializable; excluding default from JSON schema',\n            )\n            # Return the inner schema, as though there was no default\n            return json_schema\n\n        json_schema['default'] = encoded_default\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 499}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def get_union_of_schemas(self, schemas: list[JsonSchemaValue]) -> JsonSchemaValue:\n        \"\"\"Returns the JSON Schema representation for the union of the provided JSON Schemas.\n\n        The result depends on the configured `'union_format'`.\n\n        Args:\n            schemas: The list of JSON Schemas to be included in the union.\n\n        Returns:\n            The JSON Schema representing the union of schemas.\n        \"\"\"\n        if self.union_format == 'primitive_type_array':\n            types: list[str] = []\n            for schema in schemas:\n                schema_types: list[str] | str | None = schema.get('type')\n                if schema_types is None:\n                    # No type, meaning it can be a ref or an empty schema.\n                    break\n                if not isinstance(schema_types, list):\n                    schema_types = [schema_types]\n                if not all(t in _PRIMITIVE_JSON_SCHEMA_TYPES for t in schema_types):\n                    break\n                if len(schema) != 1:\n                    # We only want to include types that don't have any constraints. For instance,\n                    # if `schemas = [{'type': 'string', 'maxLength': 3}, {'type': 'string', 'minLength': 5}]`,\n                    # we don't want to produce `{'type': 'string', 'maxLength': 3, 'minLength': 5}`.\n                    # Same if we have some metadata (e.g. `title`) on a specific union member, we want to preserve it.\n                    break\n\n                types.extend(schema_types)\n            else:\n                # If we got there, all the schemas where valid to be used with the `'primitive_type_array` format\n                return {'type': list(dict.fromkeys(types))}\n\n        return self.get_flattened_anyof(schemas)", "metadata": {"license": "MIT", "len_tokens": 366}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def tagged_union_schema(self, schema: core_schema.TaggedUnionSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that allows values matching any of the given schemas, where\n        the schemas are tagged with a discriminator field that indicates which schema should be used to validate\n        the value.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        generated: dict[str, JsonSchemaValue] = {}\n        for k, v in schema['choices'].items():\n            if isinstance(k, Enum):\n                k = k.value\n            try:\n                # Use str(k) since keys must be strings for json; while not technically correct,\n                # it's the closest that can be represented in valid JSON\n                generated[str(k)] = self.generate_inner(v).copy()\n            except PydanticOmit:\n                continue\n            except PydanticInvalidForJsonSchema as exc:\n                self.emit_warning('skipped-choice', exc.message)\n\n        one_of_choices = _deduplicate_schemas(generated.values())\n        json_schema: JsonSchemaValue = {'oneOf': one_of_choices}\n\n        # This reflects the v1 behavior; TODO: we should make it possible to exclude OpenAPI stuff from the JSON schema\n        openapi_discriminator = self._extract_discriminator(schema, one_of_choices)\n        if openapi_discriminator is not None:\n            json_schema['discriminator'] = {\n                'propertyName': openapi_discriminator,\n                'mapping': {k: v.get('$ref', v) for k, v in generated.items()},\n            }\n\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 337}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def _extract_discriminator(\n        self, schema: core_schema.TaggedUnionSchema, one_of_choices: list[JsonDict]\n    ) -> str | None:\n        \"\"\"Extract a compatible OpenAPI discriminator from the schema and one_of choices that end up in the final\n        schema.\"\"\"\n        openapi_discriminator: str | None = None\n\n        if isinstance(schema['discriminator'], str):\n            return schema['discriminator']\n\n        if isinstance(schema['discriminator'], list):\n            # If the discriminator is a single item list containing a string, that is equivalent to the string case\n            if len(schema['discriminator']) == 1 and isinstance(schema['discriminator'][0], str):\n                return schema['discriminator'][0]\n            # When an alias is used that is different from the field name, the discriminator will be a list of single\n            # str lists, one for the attribute and one for the actual alias. The logic here will work even if there is\n            # more than one possible attribute, and looks for whether a single alias choice is present as a documented\n            # property on all choices. If so, that property will be used as the OpenAPI discriminator.\n            for alias_path in schema['discriminator']:\n                if not isinstance(alias_path, list):\n                    break  # this means that the discriminator is not a list of alias paths\n                if len(alias_path) != 1:\n                    continue  # this means that the \"alias\" does not represent a single field\n                alias = alias_path[0]\n                if not isinstance(alias, str):\n                    continue  # this means that the \"alias\" does not represent a field\n                alias_is_present_on_all_choices = True\n                for choice in one_of_choices:\n                    try:\n                        choice = self.resolve_ref_schema(choice)\n                    except RuntimeError as exc:\n                        # TODO: fixme - this is a workaround for the fact that we can't always resolve refs\n                        # for tagged union choices at this point in the schema gen process, we might need to do\n                        # another pass at the end like we do for core schemas\n                        self.emit_warning('skipped-discriminator', str(exc))\n                        choice = {}\n                    properties = choice.get('properties', {})\n                    if not isinstance(properties, dict) or alias not in properties:\n                        alias_is_present_on_all_choices = False\n                        break\n                if alias_is_present_on_all_choices:\n                    openapi_discriminator = alias\n                    break\n        return openapi_discriminator", "metadata": {"license": "MIT", "len_tokens": 513}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def typed_dict_schema(self, schema: core_schema.TypedDictSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a typed dict.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        total = schema.get('total', True)\n        named_required_fields: list[tuple[str, bool, CoreSchemaField]] = [\n            (name, self.field_is_required(field, total), field)\n            for name, field in schema['fields'].items()\n            if self.field_is_present(field)\n        ]\n        if self.mode == 'serialization':\n            named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields', [])))\n        cls = schema.get('cls')\n        config = _get_typed_dict_config(cls)\n        with self._config_wrapper_stack.push(config):\n            json_schema = self._named_required_fields_schema(named_required_fields)\n\n        # There's some duplication between `extra_behavior` and\n        # the config's `extra`/core config's `extra_fields_behavior`.\n        # However, it is common to manually create TypedDictSchemas,\n        # where you don't necessarily have a class.\n        # At runtime, `extra_behavior` takes priority over the config\n        # for validation, so follow the same for the JSON Schema:\n        if schema.get('extra_behavior') == 'forbid':\n            json_schema['additionalProperties'] = False\n        elif schema.get('extra_behavior') == 'allow':\n            if 'extras_schema' in schema and schema['extras_schema'] != {'type': 'any'}:\n                json_schema['additionalProperties'] = self.generate_inner(schema['extras_schema'])\n            else:\n                json_schema['additionalProperties'] = True\n\n        if cls is not None:\n            # `_update_class_schema()` will not override\n            # `additionalProperties` if already present:\n            self._update_class_schema(json_schema, cls, config)\n        elif 'additionalProperties' not in json_schema:\n            extra = schema.get('config', {}).get('extra_fields_behavior')\n            if extra == 'forbid':\n                json_schema['additionalProperties'] = False\n            elif extra == 'allow':\n                json_schema['additionalProperties'] = True\n\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 466}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def _named_required_fields_schema(\n        self, named_required_fields: Sequence[tuple[str, bool, CoreSchemaField]]\n    ) -> JsonSchemaValue:\n        properties: dict[str, JsonSchemaValue] = {}\n        required_fields: list[str] = []\n        for name, required, field in named_required_fields:\n            if self.by_alias:\n                name = self._get_alias_name(field, name)\n            try:\n                field_json_schema = self.generate_inner(field).copy()\n            except PydanticOmit:\n                continue\n            if 'title' not in field_json_schema and self.field_title_should_be_set(field):\n                title = self.get_title_from_name(name)\n                field_json_schema['title'] = title\n            field_json_schema = self.handle_ref_overrides(field_json_schema)\n            properties[name] = field_json_schema\n            if required:\n                required_fields.append(name)\n\n        json_schema = {'type': 'object', 'properties': properties}\n        if required_fields:\n            json_schema['required'] = required_fields\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 215}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def _update_class_schema(self, json_schema: JsonSchemaValue, cls: type[Any], config: ConfigDict) -> None:\n        \"\"\"Update json_schema with the following, extracted from `config` and `cls`:\n\n        * title\n        * description\n        * additional properties\n        * json_schema_extra\n        * deprecated\n\n        Done in place, hence there's no return value as the original json_schema is mutated.\n        No ref resolving is involved here, as that's not appropriate for simple updates.\n        \"\"\"\n        from .main import BaseModel\n        from .root_model import RootModel\n\n        if (config_title := config.get('title')) is not None:\n            json_schema.setdefault('title', config_title)\n        elif model_title_generator := config.get('model_title_generator'):\n            title = model_title_generator(cls)\n            if not isinstance(title, str):\n                raise TypeError(f'model_title_generator {model_title_generator} must return str, not {title.__class__}')\n            json_schema.setdefault('title', title)\n        if 'title' not in json_schema:\n            json_schema['title'] = cls.__name__\n\n        # BaseModel and dataclasses; don't use cls.__doc__ as it will contain the verbose class signature by default\n        docstring = None if cls is BaseModel or dataclasses.is_dataclass(cls) else cls.__doc__\n\n        if docstring:\n            json_schema.setdefault('description', inspect.cleandoc(docstring))\n        elif issubclass(cls, RootModel) and (root_description := cls.__pydantic_fields__['root'].description):\n            json_schema.setdefault('description', root_description)\n\n        extra = config.get('extra')\n        if 'additionalProperties' not in json_schema:  # This check is particularly important for `typed_dict_schema()`\n            if extra == 'allow':\n                json_schema['additionalProperties'] = True\n            elif extra == 'forbid':\n                json_schema['additionalProperties'] = False\n\n        json_schema_extra = config.get('json_schema_extra')\n        if issubclass(cls, BaseModel) and cls.__pydantic_root_model__:\n            root_json_schema_extra = cls.model_fields['root'].json_schema_extra\n            if json_schema_extra and root_json_schema_extra:\n                raise ValueError(\n                    '\"model_config[\\'json_schema_extra\\']\" and \"Field.json_schema_extra\" on \"RootModel.root\"'\n                    ' field must not be set simultaneously'\n                )\n            if root_json_schema_extra:\n                json_schema_extra = root_json_schema_extra\n\n        if isinstance(json_schema_extra, (staticmethod, classmethod)):\n            # In older versions of python, this is necessary to ensure staticmethod/classmethods are callable\n            json_schema_extra = json_schema_extra.__get__(cls)\n\n        if isinstance(json_schema_extra, dict):\n            json_schema.update(json_schema_extra)\n        elif callable(json_schema_extra):\n            # FIXME: why are there type ignores here? We support two signatures for json_schema_extra callables...\n            if len(inspect.signature(json_schema_extra).parameters) > 1:\n                json_schema_extra(json_schema, cls)  # type: ignore\n            else:\n                json_schema_extra(json_schema)  # type: ignore\n        elif json_schema_extra is not None:\n            raise ValueError(\n                f\"model_config['json_schema_extra']={json_schema_extra} should be a dict, callable, or None\"\n            )\n\n        if hasattr(cls, '__deprecated__'):\n            json_schema['deprecated'] = True", "metadata": {"license": "MIT", "len_tokens": 719}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def model_fields_schema(self, schema: core_schema.ModelFieldsSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a model's fields.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        named_required_fields: list[tuple[str, bool, CoreSchemaField]] = [\n            (name, self.field_is_required(field, total=True), field)\n            for name, field in schema['fields'].items()\n            if self.field_is_present(field)\n        ]\n        if self.mode == 'serialization':\n            named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields', [])))\n        json_schema = self._named_required_fields_schema(named_required_fields)\n        extras_schema = schema.get('extras_schema', None)\n        if extras_schema is not None:\n            schema_to_update = self.resolve_ref_schema(json_schema)\n            schema_to_update['additionalProperties'] = self.generate_inner(extras_schema)\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 207}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def field_is_required(\n        self,\n        field: core_schema.ModelField | core_schema.DataclassField | core_schema.TypedDictField,\n        total: bool,\n    ) -> bool:\n        \"\"\"Whether the field should be marked as required in the generated JSON schema.\n        (Note that this is irrelevant if the field is not present in the JSON schema.).\n\n        Args:\n            field: The schema for the field itself.\n            total: Only applies to `TypedDictField`s.\n                Indicates if the `TypedDict` this field belongs to is total, in which case any fields that don't\n                explicitly specify `required=False` are required.\n\n        Returns:\n            `True` if the field should be marked as required in the generated JSON schema, `False` otherwise.\n        \"\"\"\n        if field['type'] == 'typed-dict-field':\n            required = field.get('required', total)\n        else:\n            required = field['schema']['type'] != 'default'\n\n        if self.mode == 'serialization':\n            has_exclude_if = field.get('serialization_exclude_if') is not None\n            if self._config.json_schema_serialization_defaults_required:\n                return not has_exclude_if\n            else:\n                return required and not has_exclude_if\n        else:\n            return required", "metadata": {"license": "MIT", "len_tokens": 262}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def dataclass_schema(self, schema: core_schema.DataclassSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a dataclass.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        from ._internal._dataclasses import is_stdlib_dataclass\n\n        cls = schema['cls']\n        config: ConfigDict = getattr(cls, '__pydantic_config__', cast('ConfigDict', {}))\n\n        with self._config_wrapper_stack.push(config):\n            json_schema = self.generate_inner(schema['schema']).copy()\n\n        self._update_class_schema(json_schema, cls, config)\n\n        # Dataclass-specific handling of description\n        if is_stdlib_dataclass(cls):\n            # vanilla dataclass; don't use cls.__doc__ as it will contain the class signature by default\n            description = None\n        else:\n            description = None if cls.__doc__ is None else inspect.cleandoc(cls.__doc__)\n        if description:\n            json_schema['description'] = description\n\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 224}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def arguments_schema(self, schema: core_schema.ArgumentsSchema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a function's arguments.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        prefer_positional = schema.get('metadata', {}).get('pydantic_js_prefer_positional_arguments')\n\n        arguments = schema['arguments_schema']\n        kw_only_arguments = [a for a in arguments if a.get('mode') == 'keyword_only']\n        kw_or_p_arguments = [a for a in arguments if a.get('mode') in {'positional_or_keyword', None}]\n        p_only_arguments = [a for a in arguments if a.get('mode') == 'positional_only']\n        var_args_schema = schema.get('var_args_schema')\n        var_kwargs_schema = schema.get('var_kwargs_schema')\n\n        if prefer_positional:\n            positional_possible = not kw_only_arguments and not var_kwargs_schema\n            if positional_possible:\n                return self.p_arguments_schema(p_only_arguments + kw_or_p_arguments, var_args_schema)\n\n        keyword_possible = not p_only_arguments and not var_args_schema\n        if keyword_possible:\n            return self.kw_arguments_schema(kw_or_p_arguments + kw_only_arguments, var_kwargs_schema)\n\n        if not prefer_positional:\n            positional_possible = not kw_only_arguments and not var_kwargs_schema\n            if positional_possible:\n                return self.p_arguments_schema(p_only_arguments + kw_or_p_arguments, var_args_schema)\n\n        raise PydanticInvalidForJsonSchema(\n            'Unable to generate JSON schema for arguments validator with positional-only and keyword-only arguments'\n        )", "metadata": {"license": "MIT", "len_tokens": 342}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def kw_arguments_schema(\n        self, arguments: list[core_schema.ArgumentsParameter], var_kwargs_schema: CoreSchema | None\n    ) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a function's keyword arguments.\n\n        Args:\n            arguments: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        properties: dict[str, JsonSchemaValue] = {}\n        required: list[str] = []\n        for argument in arguments:\n            name = self.get_argument_name(argument)\n            argument_schema = self.generate_inner(argument['schema']).copy()\n            if 'title' not in argument_schema and self.field_title_should_be_set(argument['schema']):\n                argument_schema['title'] = self.get_title_from_name(name)\n            properties[name] = argument_schema\n\n            if argument['schema']['type'] != 'default':\n                # This assumes that if the argument has a default value,\n                # the inner schema must be of type WithDefaultSchema.\n                # I believe this is true, but I am not 100% sure\n                required.append(name)\n\n        json_schema: JsonSchemaValue = {'type': 'object', 'properties': properties}\n        if required:\n            json_schema['required'] = required\n\n        if var_kwargs_schema:\n            additional_properties_schema = self.generate_inner(var_kwargs_schema)\n            if additional_properties_schema:\n                json_schema['additionalProperties'] = additional_properties_schema\n        else:\n            json_schema['additionalProperties'] = False\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 309}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def p_arguments_schema(\n        self, arguments: list[core_schema.ArgumentsParameter], var_args_schema: CoreSchema | None\n    ) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a function's positional arguments.\n\n        Args:\n            arguments: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        prefix_items: list[JsonSchemaValue] = []\n        min_items = 0\n\n        for argument in arguments:\n            name = self.get_argument_name(argument)\n\n            argument_schema = self.generate_inner(argument['schema']).copy()\n            if 'title' not in argument_schema and self.field_title_should_be_set(argument['schema']):\n                argument_schema['title'] = self.get_title_from_name(name)\n            prefix_items.append(argument_schema)\n\n            if argument['schema']['type'] != 'default':\n                # This assumes that if the argument has a default value,\n                # the inner schema must be of type WithDefaultSchema.\n                # I believe this is true, but I am not 100% sure\n                min_items += 1\n\n        json_schema: JsonSchemaValue = {'type': 'array'}\n        if prefix_items:\n            json_schema['prefixItems'] = prefix_items\n        if min_items:\n            json_schema['minItems'] = min_items\n\n        if var_args_schema:\n            items_schema = self.generate_inner(var_args_schema)\n            if items_schema:\n                json_schema['items'] = items_schema\n        else:\n            json_schema['maxItems'] = len(prefix_items)\n\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 321}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def arguments_v3_schema(self, schema: core_schema.ArgumentsV3Schema) -> JsonSchemaValue:\n        \"\"\"Generates a JSON schema that matches a schema that defines a function's arguments.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        arguments = schema['arguments_schema']\n        properties: dict[str, JsonSchemaValue] = {}\n        required: list[str] = []\n        for argument in arguments:\n            mode = argument.get('mode', 'positional_or_keyword')\n            name = self.get_argument_name(argument)\n            argument_schema = self.generate_inner(argument['schema']).copy()\n            if mode == 'var_args':\n                argument_schema = {'type': 'array', 'items': argument_schema}\n            elif mode == 'var_kwargs_uniform':\n                argument_schema = {'type': 'object', 'additionalProperties': argument_schema}\n\n            argument_schema.setdefault('title', self.get_title_from_name(name))\n            properties[name] = argument_schema\n\n            if (\n                (mode == 'var_kwargs_unpacked_typed_dict' and 'required' in argument_schema)\n                or mode not in {'var_args', 'var_kwargs_uniform', 'var_kwargs_unpacked_typed_dict'}\n                and argument['schema']['type'] != 'default'\n            ):\n                # This assumes that if the argument has a default value,\n                # the inner schema must be of type WithDefaultSchema.\n                # I believe this is true, but I am not 100% sure\n                required.append(name)\n\n        json_schema: JsonSchemaValue = {'type': 'object', 'properties': properties}\n        if required:\n            json_schema['required'] = required\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 348}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def ser_schema(\n        self, schema: core_schema.SerSchema | core_schema.IncExSeqSerSchema | core_schema.IncExDictSerSchema\n    ) -> JsonSchemaValue | None:\n        \"\"\"Generates a JSON schema that matches a schema that defines a serialized object.\n\n        Args:\n            schema: The core schema.\n\n        Returns:\n            The generated JSON schema.\n        \"\"\"\n        schema_type = schema['type']\n        if schema_type == 'function-plain' or schema_type == 'function-wrap':\n            # PlainSerializerFunctionSerSchema or WrapSerializerFunctionSerSchema\n            return_schema = schema.get('return_schema')\n            if return_schema is not None:\n                return self.generate_inner(return_schema)\n        elif schema_type == 'format' or schema_type == 'to-string':\n            # FormatSerSchema or ToStringSerSchema\n            return self.str_schema(core_schema.str_schema())\n        elif schema['type'] == 'model':\n            # ModelSerSchema\n            return self.generate_inner(schema['schema'])\n        return None", "metadata": {"license": "MIT", "len_tokens": 211}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def field_title_should_be_set(self, schema: CoreSchemaOrField) -> bool:\n        \"\"\"Returns true if a field with the given schema should have a title set based on the field name.\n\n        Intuitively, we want this to return true for schemas that wouldn't otherwise provide their own title\n        (e.g., int, float, str), and false for those that would (e.g., BaseModel subclasses).\n\n        Args:\n            schema: The schema to check.\n\n        Returns:\n            `True` if the field should have a title set, `False` otherwise.\n        \"\"\"\n        if _core_utils.is_core_schema_field(schema):\n            if schema['type'] == 'computed-field':\n                field_schema = schema['return_schema']\n            else:\n                field_schema = schema['schema']\n            return self.field_title_should_be_set(field_schema)\n\n        elif _core_utils.is_core_schema(schema):\n            if schema.get('ref'):  # things with refs, such as models and enums, should not have titles set\n                return False\n            if schema['type'] in {'default', 'nullable', 'definitions'}:\n                return self.field_title_should_be_set(schema['schema'])  # type: ignore[typeddict-item]\n            if _core_utils.is_function_with_inner_schema(schema):\n                return self.field_title_should_be_set(schema['schema'])\n            if schema['type'] == 'definition-ref':\n                # Referenced schemas should not have titles set for the same reason\n                # schemas with refs should not\n                return False\n            return True  # anything else should have title set\n\n        else:\n            raise PydanticInvalidForJsonSchema(f'Unexpected schema type: schema={schema}')", "metadata": {"license": "MIT", "len_tokens": 348}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def get_defs_ref(self, core_mode_ref: CoreModeRef) -> DefsRef:\n        \"\"\"Override this method to change the way that definitions keys are generated from a core reference.\n\n        Args:\n            core_mode_ref: The core reference.\n\n        Returns:\n            The definitions key.\n        \"\"\"\n        # Split the core ref into \"components\"; generic origins and arguments are each separate components\n        core_ref, mode = core_mode_ref\n        components = re.split(r'([\\][,])', core_ref)\n        # Remove IDs from each component\n        components = [x.rsplit(':', 1)[0] for x in components]\n        core_ref_no_id = ''.join(components)\n        # Remove everything before the last period from each \"component\"\n        components = [re.sub(r'(?:[^.[\\]]+\\.)+((?:[^.[\\]]+))', r'\\1', x) for x in components]\n        short_ref = ''.join(components)\n\n        mode_title = _MODE_TITLE_MAPPING[mode]\n\n        # It is important that the generated defs_ref values be such that at least one choice will not\n        # be generated for any other core_ref. Currently, this should be the case because we include\n        # the id of the source type in the core_ref\n        name = DefsRef(self.normalize_name(short_ref))\n        name_mode = DefsRef(self.normalize_name(short_ref) + f'-{mode_title}')\n        module_qualname = DefsRef(self.normalize_name(core_ref_no_id))\n        module_qualname_mode = DefsRef(f'{module_qualname}-{mode_title}')\n        module_qualname_id = DefsRef(self.normalize_name(core_ref))\n        occurrence_index = self._collision_index.get(module_qualname_id)\n        if occurrence_index is None:\n            self._collision_counter[module_qualname] += 1\n            occurrence_index = self._collision_index[module_qualname_id] = self._collision_counter[module_qualname]\n\n        module_qualname_occurrence = DefsRef(f'{module_qualname}__{occurrence_index}')\n        module_qualname_occurrence_mode = DefsRef(f'{module_qualname_mode}__{occurrence_index}')\n\n        self._prioritized_defsref_choices[module_qualname_occurrence_mode] = [\n            name,\n            name_mode,\n            module_qualname,\n            module_qualname_mode,\n            module_qualname_occurrence,\n            module_qualname_occurrence_mode,\n        ]\n\n        return module_qualname_occurrence_mode", "metadata": {"license": "MIT", "len_tokens": 527}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def get_cache_defs_ref_schema(self, core_ref: CoreRef) -> tuple[DefsRef, JsonSchemaValue]:\n        \"\"\"This method wraps the get_defs_ref method with some cache-lookup/population logic,\n        and returns both the produced defs_ref and the JSON schema that will refer to the right definition.\n\n        Args:\n            core_ref: The core reference to get the definitions reference for.\n\n        Returns:\n            A tuple of the definitions reference and the JSON schema that will refer to it.\n        \"\"\"\n        core_mode_ref = (core_ref, self.mode)\n        maybe_defs_ref = self.core_to_defs_refs.get(core_mode_ref)\n        if maybe_defs_ref is not None:\n            json_ref = self.core_to_json_refs[core_mode_ref]\n            return maybe_defs_ref, {'$ref': json_ref}\n\n        defs_ref = self.get_defs_ref(core_mode_ref)\n\n        # populate the ref translation mappings\n        self.core_to_defs_refs[core_mode_ref] = defs_ref\n        self.defs_to_core_refs[defs_ref] = core_mode_ref\n\n        json_ref = JsonRef(self.ref_template.format(model=defs_ref))\n        self.core_to_json_refs[core_mode_ref] = json_ref\n        self.json_to_defs_refs[json_ref] = defs_ref\n        ref_json_schema = {'$ref': json_ref}\n        return defs_ref, ref_json_schema", "metadata": {"license": "MIT", "len_tokens": 280}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def handle_ref_overrides(self, json_schema: JsonSchemaValue) -> JsonSchemaValue:\n        \"\"\"Remove any sibling keys that are redundant with the referenced schema.\n\n        Args:\n            json_schema: The schema to remove redundant sibling keys from.\n\n        Returns:\n            The schema with redundant sibling keys removed.\n        \"\"\"\n        if '$ref' in json_schema:\n            # prevent modifications to the input; this copy may be safe to drop if there is significant overhead\n            json_schema = json_schema.copy()\n\n            referenced_json_schema = self.get_schema_from_definitions(JsonRef(json_schema['$ref']))\n            if referenced_json_schema is None:\n                # This can happen when building schemas for models with not-yet-defined references.\n                # It may be a good idea to do a recursive pass at the end of the generation to remove\n                # any redundant override keys.\n                return json_schema\n            for k, v in list(json_schema.items()):\n                if k == '$ref':\n                    continue\n                if k in referenced_json_schema and referenced_json_schema[k] == v:\n                    del json_schema[k]  # redundant key\n\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 231}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def encode_default(self, dft: Any) -> Any:\n        \"\"\"Encode a default value to a JSON-serializable value.\n\n        This is used to encode default values for fields in the generated JSON schema.\n\n        Args:\n            dft: The default value to encode.\n\n        Returns:\n            The encoded default value.\n        \"\"\"\n        from .type_adapter import TypeAdapter, _type_has_config\n\n        config = self._config\n        try:\n            default = (\n                dft\n                if _type_has_config(type(dft))\n                else TypeAdapter(type(dft), config=config.config_dict).dump_python(\n                    dft, by_alias=self.by_alias, mode='json'\n                )\n            )\n        except PydanticSchemaGenerationError:\n            raise pydantic_core.PydanticSerializationError(f'Unable to encode default value {dft}')\n\n        return pydantic_core.to_jsonable_python(\n            default, timedelta_mode=config.ser_json_timedelta, bytes_mode=config.ser_json_bytes, by_alias=self.by_alias\n        )", "metadata": {"license": "MIT", "len_tokens": 211}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "class ValidationsMapping:\n        \"\"\"This class just contains mappings from core_schema attribute names to the corresponding\n        JSON schema attribute names. While I suspect it is unlikely to be necessary, you can in\n        principle override this class in a subclass of GenerateJsonSchema (by inheriting from\n        GenerateJsonSchema.ValidationsMapping) to change these mappings.\n        \"\"\"\n\n        numeric = {\n            'multiple_of': 'multipleOf',\n            'le': 'maximum',\n            'ge': 'minimum',\n            'lt': 'exclusiveMaximum',\n            'gt': 'exclusiveMinimum',\n        }\n        bytes = {\n            'min_length': 'minLength',\n            'max_length': 'maxLength',\n        }\n        string = {\n            'min_length': 'minLength',\n            'max_length': 'maxLength',\n            'pattern': 'pattern',\n        }\n        array = {\n            'min_length': 'minItems',\n            'max_length': 'maxItems',\n        }\n        object = {\n            'min_length': 'minProperties',\n            'max_length': 'maxProperties',\n        }", "metadata": {"license": "MIT", "len_tokens": 222}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def get_json_ref_counts(self, json_schema: JsonSchemaValue) -> dict[JsonRef, int]:\n        \"\"\"Get all values corresponding to the key '$ref' anywhere in the json_schema.\"\"\"\n        json_refs: dict[JsonRef, int] = Counter()\n\n        def _add_json_refs(schema: Any) -> None:\n            if isinstance(schema, dict):\n                if '$ref' in schema:\n                    json_ref = JsonRef(schema['$ref'])\n                    if not isinstance(json_ref, str):\n                        return  # in this case, '$ref' might have been the name of a property\n                    already_visited = json_ref in json_refs\n                    json_refs[json_ref] += 1\n                    if already_visited:\n                        return  # prevent recursion on a definition that was already visited\n                    try:\n                        defs_ref = self.json_to_defs_refs[json_ref]\n                        if defs_ref in self._core_defs_invalid_for_json_schema:\n                            raise self._core_defs_invalid_for_json_schema[defs_ref]\n                        _add_json_refs(self.definitions[defs_ref])\n                    except KeyError:\n                        if not json_ref.startswith(('http://', 'https://')):\n                            raise\n\n                for k, v in schema.items():\n                    if k == 'examples' and isinstance(v, list):\n                        # Skip examples that may contain arbitrary values and references\n                        # (see the comment in `_get_all_json_refs` for more details).\n                        continue\n                    _add_json_refs(v)\n            elif isinstance(schema, list):\n                for v in schema:\n                    _add_json_refs(v)\n\n        _add_json_refs(json_schema)\n        return json_refs", "metadata": {"license": "MIT", "len_tokens": 335}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def render_warning_message(self, kind: JsonSchemaWarningKind, detail: str) -> str | None:\n        \"\"\"This method is responsible for ignoring warnings as desired, and for formatting the warning messages.\n\n        You can override the value of `ignored_warning_kinds` in a subclass of GenerateJsonSchema\n        to modify what warnings are generated. If you want more control, you can override this method;\n        just return None in situations where you don't want warnings to be emitted.\n\n        Args:\n            kind: The kind of warning to render. It can be one of the following:\n\n                - 'skipped-choice': A choice field was skipped because it had no valid choices.\n                - 'non-serializable-default': A default value was skipped because it was not JSON-serializable.\n            detail: A string with additional details about the warning.\n\n        Returns:\n            The formatted warning message, or `None` if no warning should be emitted.\n        \"\"\"\n        if kind in self.ignored_warning_kinds:\n            return None\n        return f'{detail} [{kind}]'", "metadata": {"license": "MIT", "len_tokens": 219}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def __get_pydantic_json_schema__(\n        self, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        mode = self.mode or handler.mode\n        json_schema = handler(core_schema)\n        if mode != handler.mode:\n            return json_schema\n        examples = json_schema.get('examples')\n        if examples is None:\n            json_schema['examples'] = to_jsonable_python(self.examples)\n        if isinstance(examples, dict):\n            if isinstance(self.examples, list):\n                warnings.warn(\n                    'Updating existing JSON Schema examples of type dict with examples of type list. '\n                    'Only the existing examples values will be retained. Note that dict support for '\n                    'examples is deprecated and will be removed in v3.0.',\n                    UserWarning,\n                )\n                json_schema['examples'] = to_jsonable_python(\n                    [ex for value in examples.values() for ex in value] + self.examples\n                )\n            else:\n                json_schema['examples'] = to_jsonable_python({**examples, **self.examples})\n        if isinstance(examples, list):\n            if isinstance(self.examples, list):\n                json_schema['examples'] = to_jsonable_python(examples + self.examples)\n            elif isinstance(self.examples, dict):\n                warnings.warn(\n                    'Updating existing JSON Schema examples of type list with examples of type dict. '\n                    'Only the examples values will be retained. Note that dict support for '\n                    'examples is deprecated and will be removed in v3.0.',\n                    UserWarning,\n                )\n                json_schema['examples'] = to_jsonable_python(\n                    examples + [ex for value in self.examples.values() for ex in value]\n                )\n\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 355}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "class SkipJsonSchema:\n        \"\"\"!!! abstract \"Usage Documentation\"\n            [`SkipJsonSchema` Annotation](../concepts/json_schema.md#skipjsonschema-annotation)\n\n        Add this as an annotation on a field to skip generating a JSON schema for that field.\n\n        Example:\n            ```python\n            from pprint import pprint\n            from typing import Union\n\n            from pydantic import BaseModel\n            from pydantic.json_schema import SkipJsonSchema\n\n            class Model(BaseModel):\n                a: Union[int, None] = None  # (1)!\n                b: Union[int, SkipJsonSchema[None]] = None  # (2)!\n                c: SkipJsonSchema[Union[int, None]] = None  # (3)!\n\n            pprint(Model.model_json_schema())\n            '''\n            {\n                'properties': {\n                    'a': {\n                        'anyOf': [\n                            {'type': 'integer'},\n                            {'type': 'null'}\n                        ],\n                        'default': None,\n                        'title': 'A'\n                    },\n                    'b': {\n                        'default': None,\n                        'title': 'B',\n                        'type': 'integer'\n                    }\n                },\n                'title': 'Model',\n                'type': 'object'\n            }\n            '''\n            ```\n\n            1. The integer and null types are both included in the schema for `a`.\n            2. The integer type is the only type included in the schema for `b`.\n            3. The entirety of the `c` field is omitted from the schema.\n        \"\"\"\n\n        def __class_getitem__(cls, item: AnyType) -> AnyType:\n            return Annotated[item, cls()]\n\n        def __get_pydantic_json_schema__(\n            self, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            raise PydanticOmit\n\n        def __hash__(self) -> int:\n            return hash(type(self))", "metadata": {"license": "MIT", "len_tokens": 402}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def handler_func(schema_or_field: CoreSchemaOrField) -> JsonSchemaValue:\n            \"\"\"Generate a JSON schema based on the input schema.\n\n            Args:\n                schema_or_field: The core schema to generate a JSON schema from.\n\n            Returns:\n                The generated JSON schema.\n\n            Raises:\n                TypeError: If an unexpected schema type is encountered.\n            \"\"\"\n            # Generate the core-schema-type-specific bits of the schema generation:\n            json_schema: JsonSchemaValue | None = None\n            if self.mode == 'serialization' and 'serialization' in schema_or_field:\n                # In this case, we skip the JSON Schema generation of the schema\n                # and use the `'serialization'` schema instead (canonical example:\n                # `Annotated[int, PlainSerializer(str)]`).\n                ser_schema = schema_or_field['serialization']  # type: ignore\n                json_schema = self.ser_schema(ser_schema)\n\n                # It might be that the 'serialization'` is skipped depending on `when_used`.\n                # This is only relevant for `nullable` schemas though, so we special case here.\n                if (\n                    json_schema is not None\n                    and ser_schema.get('when_used') in ('unless-none', 'json-unless-none')\n                    and schema_or_field['type'] == 'nullable'\n                ):\n                    json_schema = self.get_union_of_schemas([{'type': 'null'}, json_schema])\n            if json_schema is None:\n                if _core_utils.is_core_schema(schema_or_field) or _core_utils.is_core_schema_field(schema_or_field):\n                    generate_for_schema_type = self._schema_type_to_method[schema_or_field['type']]\n                    json_schema = generate_for_schema_type(schema_or_field)\n                else:\n                    raise TypeError(f'Unexpected schema type: schema={schema_or_field}')\n            return json_schema", "metadata": {"license": "MIT", "len_tokens": 373}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def get_decimal_pattern(schema: core_schema.DecimalSchema) -> str:\n            max_digits = schema.get('max_digits')\n            decimal_places = schema.get('decimal_places')\n\n            pattern = (\n                r'^(?!^[-+.]*$)[+-]?0*'  # check it is not empty string and not one or sequence of \".+-\" characters.\n            )\n\n            # Case 1: Both max_digits and decimal_places are set\n            if max_digits is not None and decimal_places is not None:\n                integer_places = max(0, max_digits - decimal_places)\n                pattern += (\n                    rf'(?:'\n                    rf'\\d{{0,{integer_places}}}'\n                    rf'|'\n                    rf'(?=[\\d.]{{1,{max_digits + 1}}}0*$)'\n                    rf'\\d{{0,{integer_places}}}\\.\\d{{0,{decimal_places}}}0*$'\n                    rf')'\n                )\n\n            # Case 2: Only max_digits is set\n            elif max_digits is not None and decimal_places is None:\n                pattern += (\n                    rf'(?:'\n                    rf'\\d{{0,{max_digits}}}'\n                    rf'|'\n                    rf'(?=[\\d.]{{1,{max_digits + 1}}}0*$)'\n                    rf'\\d*\\.\\d*0*$'\n                    rf')'\n                )\n\n            # Case 3: Only decimal_places is set\n            elif max_digits is None and decimal_places is not None:\n                pattern += rf'\\d*\\.?\\d{{0,{decimal_places}}}0*$'\n\n            # Case 4: Both are None (no restrictions)\n            else:\n                pattern += r'\\d*\\.?\\d*$'  # look for arbitrary integer or decimal\n\n            return pattern", "metadata": {"license": "MIT", "len_tokens": 366}}
{"id": "pydantic:pydantic/json_schema.py", "language": "python", "code": "def _add_json_refs(schema: Any) -> None:\n            if isinstance(schema, dict):\n                if '$ref' in schema:\n                    json_ref = JsonRef(schema['$ref'])\n                    if not isinstance(json_ref, str):\n                        return  # in this case, '$ref' might have been the name of a property\n                    already_visited = json_ref in json_refs\n                    json_refs[json_ref] += 1\n                    if already_visited:\n                        return  # prevent recursion on a definition that was already visited\n                    try:\n                        defs_ref = self.json_to_defs_refs[json_ref]\n                        if defs_ref in self._core_defs_invalid_for_json_schema:\n                            raise self._core_defs_invalid_for_json_schema[defs_ref]\n                        _add_json_refs(self.definitions[defs_ref])\n                    except KeyError:\n                        if not json_ref.startswith(('http://', 'https://')):\n                            raise\n\n                for k, v in schema.items():\n                    if k == 'examples' and isinstance(v, list):\n                        # Skip examples that may contain arbitrary values and references\n                        # (see the comment in `_get_all_json_refs` for more details).\n                        continue\n                    _add_json_refs(v)\n            elif isinstance(schema, list):\n                for v in schema:\n                    _add_json_refs(v)", "metadata": {"license": "MIT", "len_tokens": 268}}
{"id": "pydantic:pydantic/_migration.py", "language": "python", "code": "def getattr_migration(module: str) -> Callable[[str], Any]:\n    \"\"\"Implement PEP 562 for objects that were either moved or removed on the migration\n    to V2.\n\n    Args:\n        module: The module name.\n\n    Returns:\n        A callable that will raise an error if the object is not found.\n    \"\"\"\n    # This avoids circular import with errors.py.\n    from .errors import PydanticImportError\n\n    def wrapper(name: str) -> object:\n        \"\"\"Raise an error if the object is not found, or warn if it was moved.\n\n        In case it was moved, it still returns the object.\n\n        Args:\n            name: The object name.\n\n        Returns:\n            The object.\n        \"\"\"\n        if name == '__path__':\n            raise AttributeError(f'module {module!r} has no attribute {name!r}')\n\n        import warnings\n\n        from ._internal._validators import import_string\n\n        import_path = f'{module}:{name}'\n        if import_path in MOVED_IN_V2.keys():\n            new_location = MOVED_IN_V2[import_path]\n            warnings.warn(\n                f'`{import_path}` has been moved to `{new_location}`.',\n                category=PydanticDeprecatedSince20,\n                stacklevel=2,\n            )\n            return import_string(MOVED_IN_V2[import_path])\n        if import_path in DEPRECATED_MOVED_IN_V2:\n            # skip the warning here because a deprecation warning will be raised elsewhere\n            return import_string(DEPRECATED_MOVED_IN_V2[import_path])\n        if import_path in REDIRECT_TO_V1:\n            new_location = REDIRECT_TO_V1[import_path]\n            warnings.warn(\n                f'`{import_path}` has been removed. We are importing from `{new_location}` instead.'\n                'See the migration guide for more details: https://docs.pydantic.dev/latest/migration/',\n                category=PydanticDeprecatedSince20,\n                stacklevel=2,\n            )\n            return import_string(REDIRECT_TO_V1[import_path])\n        if import_path == 'pydantic:BaseSettings':\n            raise PydanticImportError(\n                '`BaseSettings` has been moved to the `pydantic-settings` package. '\n                f'See https://docs.pydantic.dev/{version_short()}/migration/#basesettings-has-moved-to-pydantic-settings '\n                'for more details.'\n            )\n        if import_path in REMOVED_IN_V2:\n            raise PydanticImportError(f'`{import_path}` has been removed in V2.')\n        globals: dict[str, Any] = sys.modules[module].__dict__\n        if name in globals:\n            return globals[name]\n        raise AttributeError(f'module {module!r} has no attribute {name!r}')\n\n    return wrapper", "metadata": {"license": "MIT", "len_tokens": 585}}
{"id": "pydantic:pydantic/_migration.py", "language": "python", "code": "def wrapper(name: str) -> object:\n        \"\"\"Raise an error if the object is not found, or warn if it was moved.\n\n        In case it was moved, it still returns the object.\n\n        Args:\n            name: The object name.\n\n        Returns:\n            The object.\n        \"\"\"\n        if name == '__path__':\n            raise AttributeError(f'module {module!r} has no attribute {name!r}')\n\n        import warnings\n\n        from ._internal._validators import import_string\n\n        import_path = f'{module}:{name}'\n        if import_path in MOVED_IN_V2.keys():\n            new_location = MOVED_IN_V2[import_path]\n            warnings.warn(\n                f'`{import_path}` has been moved to `{new_location}`.',\n                category=PydanticDeprecatedSince20,\n                stacklevel=2,\n            )\n            return import_string(MOVED_IN_V2[import_path])\n        if import_path in DEPRECATED_MOVED_IN_V2:\n            # skip the warning here because a deprecation warning will be raised elsewhere\n            return import_string(DEPRECATED_MOVED_IN_V2[import_path])\n        if import_path in REDIRECT_TO_V1:\n            new_location = REDIRECT_TO_V1[import_path]\n            warnings.warn(\n                f'`{import_path}` has been removed. We are importing from `{new_location}` instead.'\n                'See the migration guide for more details: https://docs.pydantic.dev/latest/migration/',\n                category=PydanticDeprecatedSince20,\n                stacklevel=2,\n            )\n            return import_string(REDIRECT_TO_V1[import_path])\n        if import_path == 'pydantic:BaseSettings':\n            raise PydanticImportError(\n                '`BaseSettings` has been moved to the `pydantic-settings` package. '\n                f'See https://docs.pydantic.dev/{version_short()}/migration/#basesettings-has-moved-to-pydantic-settings '\n                'for more details.'\n            )\n        if import_path in REMOVED_IN_V2:\n            raise PydanticImportError(f'`{import_path}` has been removed in V2.')\n        globals: dict[str, Any] = sys.modules[module].__dict__\n        if name in globals:\n            return globals[name]\n        raise AttributeError(f'module {module!r} has no attribute {name!r}')", "metadata": {"license": "MIT", "len_tokens": 491}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "class UrlConstraints:\n    \"\"\"Url constraints.\n\n    Attributes:\n        max_length: The maximum length of the url. Defaults to `None`.\n        allowed_schemes: The allowed schemes. Defaults to `None`.\n        host_required: Whether the host is required. Defaults to `None`.\n        default_host: The default host. Defaults to `None`.\n        default_port: The default port. Defaults to `None`.\n        default_path: The default path. Defaults to `None`.\n        preserve_empty_path: Whether to preserve empty URL paths. Defaults to `None`.\n    \"\"\"\n\n    max_length: int | None = None\n    allowed_schemes: list[str] | None = None\n    host_required: bool | None = None\n    default_host: str | None = None\n    default_port: int | None = None\n    default_path: str | None = None\n    preserve_empty_path: bool | None = None\n\n    def __hash__(self) -> int:\n        return hash(\n            (\n                self.max_length,\n                tuple(self.allowed_schemes) if self.allowed_schemes is not None else None,\n                self.host_required,\n                self.default_host,\n                self.default_port,\n                self.default_path,\n                self.preserve_empty_path,\n            )\n        )\n\n    @property\n    def defined_constraints(self) -> dict[str, Any]:\n        \"\"\"Fetch a key / value mapping of constraints to values that are not None. Used for core schema updates.\"\"\"\n        return {field.name: value for field in fields(self) if (value := getattr(self, field.name)) is not None}\n\n    def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source)\n\n        # for function-wrap schemas, url constraints is applied to the inner schema\n        # because when we generate schemas for urls, we wrap a core_schema.url_schema() with a function-wrap schema\n        # that helps with validation on initialization, see _BaseUrl and _BaseMultiHostUrl below.\n        schema_to_mutate = schema['schema'] if schema['type'] == 'function-wrap' else schema\n        if annotated_type := schema_to_mutate['type'] not in ('url', 'multi-host-url'):\n            raise PydanticUserError(\n                f\"'UrlConstraints' cannot annotate '{annotated_type}'.\", code='invalid-annotated-type'\n            )\n        for constraint_key, constraint_value in self.defined_constraints.items():\n            schema_to_mutate[constraint_key] = constraint_value\n        return schema", "metadata": {"license": "MIT", "len_tokens": 532}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "class AnyUrl(_BaseUrl):\n    \"\"\"Base type for all URLs.\n\n    * Any scheme allowed\n    * Top-level domain (TLD) not required\n    * Host not required\n\n    Assuming an input URL of `http://samuel:pass@example.com:8000/the/path/?query=here#fragment=is;this=bit`,\n    the types export the following properties:\n\n    - `scheme`: the URL scheme (`http`), always set.\n    - `host`: the URL host (`example.com`).\n    - `username`: optional username if included (`samuel`).\n    - `password`: optional password if included (`pass`).\n    - `port`: optional port (`8000`).\n    - `path`: optional path (`/the/path/`).\n    - `query`: optional URL query (for example, `GET` arguments or \"search string\", such as `query=here`).\n    - `fragment`: optional fragment (`fragment=is;this=bit`).\n    \"\"\"", "metadata": {"license": "MIT", "len_tokens": 207}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "class HttpUrl(AnyUrl):\n    \"\"\"A type that will accept any http or https URL.\n\n    * TLD not required\n    * Host not required\n    * Max length 2083\n\n    ```python\n    from pydantic import BaseModel, HttpUrl, ValidationError\n\n    class MyModel(BaseModel):\n        url: HttpUrl\n\n    m = MyModel(url='http://www.example.com')  # (1)!\n    print(m.url)\n    #> http://www.example.com/\n\n    try:\n        MyModel(url='ftp://invalid.url')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for MyModel\n        url\n          URL scheme should be 'http' or 'https' [type=url_scheme, input_value='ftp://invalid.url', input_type=str]\n        '''\n\n    try:\n        MyModel(url='not a url')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for MyModel\n        url\n          Input should be a valid URL, relative URL without a base [type=url_parsing, input_value='not a url', input_type=str]\n        '''\n    ```\n\n    1. Note: mypy would prefer `m = MyModel(url=HttpUrl('http://www.example.com'))`, but Pydantic will convert the string to an HttpUrl instance anyway.\n\n    \"International domains\" (e.g. a URL where the host or TLD includes non-ascii characters) will be encoded via\n    [punycode](https://en.wikipedia.org/wiki/Punycode) (see\n    [this article](https://www.xudongz.com/blog/2017/idn-phishing/) for a good description of why this is important):\n\n    ```python\n    from pydantic import BaseModel, HttpUrl\n\n    class MyModel(BaseModel):\n        url: HttpUrl\n\n    m1 = MyModel(url='http://punycode.com')\n    print(m1.url)\n    #> http://xn--punycode-eja.com/\n    m2 = MyModel(url='https://www..com/')\n    print(m2.url)\n    #> https://www.xn--80ak6aa92e.com/\n    m3 = MyModel(url='https://www.example./')\n    print(m3.url)\n    #> https://www.example.xn--pbt977c/\n    ```\n\n\n    !!! warning \"Underscores in Hostnames\"\n        In Pydantic, underscores are allowed in all parts of a domain except the TLD.\n        Technically this might be wrong - in theory the hostname cannot have underscores, but subdomains can.\n\n        To explain this; consider the following two cases:\n\n        - `exam_ple.co.uk`: the hostname is `exam_ple`, which should not be allowed since it contains an underscore.\n        - `foo_bar.example.com` the hostname is `example`, which should be allowed since the underscore is in the subdomain.\n\n        Without having an exhaustive list of TLDs, it would be impossible to differentiate between these two. Therefore\n        underscores are allowed, but you can always do further validation in a validator if desired.\n\n        Also, Chrome, Firefox, and Safari all currently accept `http://exam_ple.com` as a URL, so we're in good\n        (or at least big) company.\n    \"\"\"\n\n    _constraints = UrlConstraints(max_length=2083, allowed_schemes=['http', 'https'])", "metadata": {"license": "MIT", "len_tokens": 735}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "class PostgresDsn(_BaseMultiHostUrl):\n    \"\"\"A type that will accept any Postgres DSN.\n\n    * User info required\n    * TLD not required\n    * Host required\n    * Supports multiple hosts\n\n    If further validation is required, these properties can be used by validators to enforce specific behaviour:\n\n    ```python\n    from pydantic import (\n        BaseModel,\n        HttpUrl,\n        PostgresDsn,\n        ValidationError,\n        field_validator,\n    )\n\n    class MyModel(BaseModel):\n        url: HttpUrl\n\n    m = MyModel(url='http://www.example.com')\n\n    # the repr() method for a url will display all properties of the url\n    print(repr(m.url))\n    #> HttpUrl('http://www.example.com/')\n    print(m.url.scheme)\n    #> http\n    print(m.url.host)\n    #> www.example.com\n    print(m.url.port)\n    #> 80\n\n    class MyDatabaseModel(BaseModel):\n        db: PostgresDsn\n\n        @field_validator('db')\n        def check_db_name(cls, v):\n            assert v.path and len(v.path) > 1, 'database must be provided'\n            return v\n\n    m = MyDatabaseModel(db='postgres://user:pass@localhost:5432/foobar')\n    print(m.db)\n    #> postgres://user:pass@localhost:5432/foobar\n\n    try:\n        MyDatabaseModel(db='postgres://user:pass@localhost:5432')\n    except ValidationError as e:\n        print(e)\n        '''\n        1 validation error for MyDatabaseModel\n        db\n          Assertion failed, database must be provided\n        assert (None)\n         +  where None = PostgresDsn('postgres://user:pass@localhost:5432').path [type=assertion_error, input_value='postgres://user:pass@localhost:5432', input_type=str]\n        '''\n    ```\n    \"\"\"\n\n    _constraints = UrlConstraints(\n        host_required=True,\n        allowed_schemes=[\n            'postgres',\n            'postgresql',\n            'postgresql+asyncpg',\n            'postgresql+pg8000',\n            'postgresql+psycopg',\n            'postgresql+psycopg2',\n            'postgresql+psycopg2cffi',\n            'postgresql+py-postgresql',\n            'postgresql+pygresql',\n        ],\n    )\n\n    @property\n    def host(self) -> str:\n        \"\"\"The required URL host.\"\"\"\n        return self._url.host", "metadata": {"license": "MIT", "len_tokens": 535}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "class MongoDsn(_BaseMultiHostUrl):\n    \"\"\"A type that will accept any MongoDB DSN.\n\n    * User info not required\n    * Database name not required\n    * Port not required\n    * User info may be passed without user part (e.g., `mongodb://mongodb0.example.com:27017`).\n\n    !!! warning\n        If a port isn't specified, the default MongoDB port `27017` will be used. If this behavior is\n        undesirable, you can use the following:\n\n        ```python\n        from typing import Annotated\n\n        from pydantic_core import MultiHostUrl\n\n        from pydantic import UrlConstraints\n\n        MongoDsnNoDefaultPort = Annotated[\n            MultiHostUrl,\n            UrlConstraints(allowed_schemes=['mongodb', 'mongodb+srv']),\n        ]\n        ```\n    \"\"\"\n\n    _constraints = UrlConstraints(allowed_schemes=['mongodb', 'mongodb+srv'], default_port=27017)", "metadata": {"license": "MIT", "len_tokens": 201}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "class NameEmail(_repr.Representation):\n    \"\"\"\n    Info:\n        To use this type, you need to install the optional\n        [`email-validator`](https://github.com/JoshData/python-email-validator) package:\n\n        ```bash\n        pip install email-validator\n        ```\n\n    Validate a name and email address combination, as specified by\n    [RFC 5322](https://datatracker.ietf.org/doc/html/rfc5322#section-3.4).\n\n    The `NameEmail` has two properties: `name` and `email`.\n    In case the `name` is not provided, it's inferred from the email address.\n\n    ```python\n    from pydantic import BaseModel, NameEmail\n\n    class User(BaseModel):\n        email: NameEmail\n\n    user = User(email='Fred Bloggs <fred.bloggs@example.com>')\n    print(user.email)\n    #> Fred Bloggs <fred.bloggs@example.com>\n    print(user.email.name)\n    #> Fred Bloggs\n\n    user = User(email='fred.bloggs@example.com')\n    print(user.email)\n    #> fred.bloggs <fred.bloggs@example.com>\n    print(user.email.name)\n    #> fred.bloggs\n    ```\n    \"\"\"  # noqa: D212\n\n    __slots__ = 'name', 'email'\n\n    def __init__(self, name: str, email: str):\n        self.name = name\n        self.email = email\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, NameEmail) and (self.name, self.email) == (other.name, other.email)\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: core_schema.CoreSchema, handler: _schema_generation_shared.GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        field_schema = handler(core_schema)\n        field_schema.update(type='string', format='name-email')\n        return field_schema\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls,\n        _source: type[Any],\n        _handler: GetCoreSchemaHandler,\n    ) -> core_schema.CoreSchema:\n        import_email_validator()\n\n        return core_schema.no_info_after_validator_function(\n            cls._validate,\n            core_schema.json_or_python_schema(\n                json_schema=core_schema.str_schema(),\n                python_schema=core_schema.union_schema(\n                    [core_schema.is_instance_schema(cls), core_schema.str_schema()],\n                    custom_error_type='name_email_type',\n                    custom_error_message='Input is not a valid NameEmail',\n                ),\n                serialization=core_schema.to_string_ser_schema(),\n            ),\n        )\n\n    @classmethod\n    def _validate(cls, input_value: Self | str, /) -> Self:\n        if isinstance(input_value, str):\n            name, email = validate_email(input_value)\n            return cls(name, email)\n        else:\n            return input_value\n\n    def __str__(self) -> str:\n        if '@' in self.name:\n            return f'\"{self.name}\" <{self.email}>'\n\n        return f'{self.name} <{self.email}>'", "metadata": {"license": "MIT", "len_tokens": 646}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "def validate_email(value: str) -> tuple[str, str]:\n    \"\"\"Email address validation using [email-validator](https://pypi.org/project/email-validator/).\n\n    Returns:\n        A tuple containing the local part of the email (or the name for \"pretty\" email addresses)\n            and the normalized email.\n\n    Raises:\n        PydanticCustomError: If the email is invalid.\n\n    Note:\n        Note that:\n\n        * Raw IP address (literal) domain parts are not allowed.\n        * `\"John Doe <local_part@domain.com>\"` style \"pretty\" email addresses are processed.\n        * Spaces are striped from the beginning and end of addresses, but no error is raised.\n    \"\"\"\n    if email_validator is None:\n        import_email_validator()\n\n    if len(value) > MAX_EMAIL_LENGTH:\n        raise PydanticCustomError(\n            'value_error',\n            'value is not a valid email address: {reason}',\n            {'reason': f'Length must not exceed {MAX_EMAIL_LENGTH} characters'},\n        )\n\n    m = pretty_email_regex.fullmatch(value)\n    name: str | None = None\n    if m:\n        unquoted_name, quoted_name, value = m.groups()\n        name = unquoted_name or quoted_name\n\n    email = value.strip()\n\n    try:\n        parts = email_validator.validate_email(email, check_deliverability=False)\n    except email_validator.EmailNotValidError as e:\n        raise PydanticCustomError(\n            'value_error', 'value is not a valid email address: {reason}', {'reason': str(e.args[0])}\n        ) from e\n\n    email = parts.normalized\n    assert email is not None\n    name = name or parts.local_part\n    return name, email", "metadata": {"license": "MIT", "len_tokens": 360}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "def __get_pydantic_core_schema__(self, source: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        schema = handler(source)\n\n        # for function-wrap schemas, url constraints is applied to the inner schema\n        # because when we generate schemas for urls, we wrap a core_schema.url_schema() with a function-wrap schema\n        # that helps with validation on initialization, see _BaseUrl and _BaseMultiHostUrl below.\n        schema_to_mutate = schema['schema'] if schema['type'] == 'function-wrap' else schema\n        if annotated_type := schema_to_mutate['type'] not in ('url', 'multi-host-url'):\n            raise PydanticUserError(\n                f\"'UrlConstraints' cannot annotate '{annotated_type}'.\", code='invalid-annotated-type'\n            )\n        for constraint_key, constraint_value in self.defined_constraints.items():\n            schema_to_mutate[constraint_key] = constraint_value\n        return schema", "metadata": {"license": "MIT", "len_tokens": 202}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "def build(\n        cls,\n        *,\n        scheme: str,\n        username: str | None = None,\n        password: str | None = None,\n        host: str,\n        port: int | None = None,\n        path: str | None = None,\n        query: str | None = None,\n        fragment: str | None = None,\n    ) -> Self:\n        \"\"\"Build a new `Url` instance from its component parts.\n\n        Args:\n            scheme: The scheme part of the URL.\n            username: The username part of the URL, or omit for no username.\n            password: The password part of the URL, or omit for no password.\n            host: The host part of the URL.\n            port: The port part of the URL, or omit for no port.\n            path: The path part of the URL, or omit for no path.\n            query: The query part of the URL, or omit for no query.\n            fragment: The fragment part of the URL, or omit for no fragment.\n\n        Returns:\n            An instance of URL\n        \"\"\"\n        return cls(\n            _CoreUrl.build(\n                scheme=scheme,\n                username=username,\n                password=password,\n                host=host,\n                port=port,\n                path=path,\n                query=query,\n                fragment=fragment,\n            )\n        )", "metadata": {"license": "MIT", "len_tokens": 272}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "def build(\n        cls,\n        *,\n        scheme: str,\n        hosts: list[MultiHostHost] | None = None,\n        username: str | None = None,\n        password: str | None = None,\n        host: str | None = None,\n        port: int | None = None,\n        path: str | None = None,\n        query: str | None = None,\n        fragment: str | None = None,\n    ) -> Self:\n        \"\"\"Build a new `MultiHostUrl` instance from its component parts.\n\n        This method takes either `hosts` - a list of `MultiHostHost` typed dicts, or the individual components\n        `username`, `password`, `host` and `port`.\n\n        Args:\n            scheme: The scheme part of the URL.\n            hosts: Multiple hosts to build the URL from.\n            username: The username part of the URL.\n            password: The password part of the URL.\n            host: The host part of the URL.\n            port: The port part of the URL.\n            path: The path part of the URL.\n            query: The query part of the URL, or omit for no query.\n            fragment: The fragment part of the URL, or omit for no fragment.\n\n        Returns:\n            An instance of `MultiHostUrl`\n        \"\"\"\n        return cls(\n            _CoreMultiHostUrl.build(\n                scheme=scheme,\n                hosts=hosts,\n                username=username,\n                password=password,\n                host=host,\n                port=port,\n                path=path,\n                query=query,\n                fragment=fragment,\n            )\n        )", "metadata": {"license": "MIT", "len_tokens": 329}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "class EmailStr:\n        \"\"\"\n        Info:\n            To use this type, you need to install the optional\n            [`email-validator`](https://github.com/JoshData/python-email-validator) package:\n\n            ```bash\n            pip install email-validator\n            ```\n\n        Validate email addresses.\n\n        ```python\n        from pydantic import BaseModel, EmailStr\n\n        class Model(BaseModel):\n            email: EmailStr\n\n        print(Model(email='contact@mail.com'))\n        #> email='contact@mail.com'\n        ```\n        \"\"\"  # noqa: D212\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls,\n            _source: type[Any],\n            _handler: GetCoreSchemaHandler,\n        ) -> core_schema.CoreSchema:\n            import_email_validator()\n            return core_schema.no_info_after_validator_function(cls._validate, core_schema.str_schema())\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: _schema_generation_shared.GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            field_schema = handler(core_schema)\n            field_schema.update(type='string', format='email')\n            return field_schema\n\n        @classmethod\n        def _validate(cls, input_value: str, /) -> str:\n            return validate_email(input_value)[1]", "metadata": {"license": "MIT", "len_tokens": 276}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "class IPvAnyAddress:\n        \"\"\"Validate an IPv4 or IPv6 address.\n\n        ```python\n        from pydantic import BaseModel\n        from pydantic.networks import IPvAnyAddress\n\n        class IpModel(BaseModel):\n            ip: IPvAnyAddress\n\n        print(IpModel(ip='127.0.0.1'))\n        #> ip=IPv4Address('127.0.0.1')\n\n        try:\n            IpModel(ip='http://www.example.com')\n        except ValueError as e:\n            print(e.errors())\n            '''\n            [\n                {\n                    'type': 'ip_any_address',\n                    'loc': ('ip',),\n                    'msg': 'value is not a valid IPv4 or IPv6 address',\n                    'input': 'http://www.example.com',\n                }\n            ]\n            '''\n        ```\n        \"\"\"\n\n        __slots__ = ()\n\n        def __new__(cls, value: Any) -> IPvAnyAddressType:\n            \"\"\"Validate an IPv4 or IPv6 address.\"\"\"\n            try:\n                return IPv4Address(value)\n            except ValueError:\n                pass\n\n            try:\n                return IPv6Address(value)\n            except ValueError:\n                raise PydanticCustomError('ip_any_address', 'value is not a valid IPv4 or IPv6 address')\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: _schema_generation_shared.GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            field_schema = {}\n            field_schema.update(type='string', format='ipvanyaddress')\n            return field_schema\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls,\n            _source: type[Any],\n            _handler: GetCoreSchemaHandler,\n        ) -> core_schema.CoreSchema:\n            return core_schema.no_info_plain_validator_function(\n                cls._validate, serialization=core_schema.to_string_ser_schema()\n            )\n\n        @classmethod\n        def _validate(cls, input_value: Any, /) -> IPvAnyAddressType:\n            return cls(input_value)", "metadata": {"license": "MIT", "len_tokens": 429}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "class IPvAnyInterface:\n        \"\"\"Validate an IPv4 or IPv6 interface.\"\"\"\n\n        __slots__ = ()\n\n        def __new__(cls, value: NetworkType) -> IPvAnyInterfaceType:\n            \"\"\"Validate an IPv4 or IPv6 interface.\"\"\"\n            try:\n                return IPv4Interface(value)\n            except ValueError:\n                pass\n\n            try:\n                return IPv6Interface(value)\n            except ValueError:\n                raise PydanticCustomError('ip_any_interface', 'value is not a valid IPv4 or IPv6 interface')\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: _schema_generation_shared.GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            field_schema = {}\n            field_schema.update(type='string', format='ipvanyinterface')\n            return field_schema\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls,\n            _source: type[Any],\n            _handler: GetCoreSchemaHandler,\n        ) -> core_schema.CoreSchema:\n            return core_schema.no_info_plain_validator_function(\n                cls._validate, serialization=core_schema.to_string_ser_schema()\n            )\n\n        @classmethod\n        def _validate(cls, input_value: NetworkType, /) -> IPvAnyInterfaceType:\n            return cls(input_value)", "metadata": {"license": "MIT", "len_tokens": 273}}
{"id": "pydantic:pydantic/networks.py", "language": "python", "code": "class IPvAnyNetwork:\n        \"\"\"Validate an IPv4 or IPv6 network.\"\"\"\n\n        __slots__ = ()\n\n        def __new__(cls, value: NetworkType) -> IPvAnyNetworkType:\n            \"\"\"Validate an IPv4 or IPv6 network.\"\"\"\n            # Assume IP Network is defined with a default value for `strict` argument.\n            # Define your own class if you want to specify network address check strictness.\n            try:\n                return IPv4Network(value)\n            except ValueError:\n                pass\n\n            try:\n                return IPv6Network(value)\n            except ValueError:\n                raise PydanticCustomError('ip_any_network', 'value is not a valid IPv4 or IPv6 network')\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: core_schema.CoreSchema, handler: _schema_generation_shared.GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            field_schema = {}\n            field_schema.update(type='string', format='ipvanynetwork')\n            return field_schema\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls,\n            _source: type[Any],\n            _handler: GetCoreSchemaHandler,\n        ) -> core_schema.CoreSchema:\n            return core_schema.no_info_plain_validator_function(\n                cls._validate, serialization=core_schema.to_string_ser_schema()\n            )\n\n        @classmethod\n        def _validate(cls, input_value: NetworkType, /) -> IPvAnyNetworkType:\n            return cls(input_value)", "metadata": {"license": "MIT", "len_tokens": 307}}
{"id": "pydantic:pydantic/root_model.py", "language": "python", "code": "def model_dump(  # type: ignore\n            self,\n            *,\n            mode: Literal['json', 'python'] | str = 'python',\n            include: Any = None,\n            exclude: Any = None,\n            context: dict[str, Any] | None = None,\n            by_alias: bool | None = None,\n            exclude_unset: bool = False,\n            exclude_defaults: bool = False,\n            exclude_none: bool = False,\n            exclude_computed_fields: bool = False,\n            round_trip: bool = False,\n            warnings: bool | Literal['none', 'warn', 'error'] = True,\n            serialize_as_any: bool = False,\n        ) -> Any:\n            \"\"\"This method is included just to get a more accurate return type for type checkers.\n            It is included in this `if TYPE_CHECKING:` block since no override is actually necessary.\n\n            See the documentation of `BaseModel.model_dump` for more details about the arguments.\n\n            Generally, this method will have a return type of `RootModelRootType`, assuming that `RootModelRootType` is\n            not a `BaseModel` subclass. If `RootModelRootType` is a `BaseModel` subclass, then the return\n            type will likely be `dict[str, Any]`, as `model_dump` calls are recursive. The return type could\n            even be something different, in the case of a custom serializer.\n            Thus, `Any` is used here to catch all of these cases.\n            \"\"\"\n            ...", "metadata": {"license": "MIT", "len_tokens": 315}}
{"id": "pydantic:pydantic/type_adapter.py", "language": "python", "code": "def __init__(\n        self,\n        type: Any,\n        *,\n        config: ConfigDict | None = None,\n        _parent_depth: int = 2,\n        module: str | None = None,\n    ) -> None:\n        if _type_has_config(type) and config is not None:\n            raise PydanticUserError(\n                'Cannot use `config` when the type is a BaseModel, dataclass or TypedDict.'\n                ' These types can have their own config and setting the config via the `config`'\n                ' parameter to TypeAdapter will not override it, thus the `config` you passed to'\n                ' TypeAdapter becomes meaningless, which is probably not what you want.',\n                code='type-adapter-config-unused',\n            )\n\n        self._type = type\n        self._config = config\n        self._parent_depth = _parent_depth\n        self.pydantic_complete = False\n\n        parent_frame = self._fetch_parent_frame()\n        if isinstance(type, types.FunctionType):\n            # Special case functions, which are *not* pushed to the `NsResolver` stack and without this special case\n            # would only have access to the parent namespace where the `TypeAdapter` was instantiated (if the function is defined\n            # in another module, we need to look at that module's globals).\n            if parent_frame is not None:\n                # `f_locals` is the namespace where the type adapter was instantiated (~ to `f_globals` if at the module level):\n                parent_ns = parent_frame.f_locals\n            else:  # pragma: no cover\n                parent_ns = None\n            globalns, localns = _namespace_utils.ns_for_function(\n                type,\n                parent_namespace=parent_ns,\n            )\n            parent_namespace = None\n        else:\n            if parent_frame is not None:\n                globalns = parent_frame.f_globals\n                # Do not provide a local ns if the type adapter happens to be instantiated at the module level:\n                localns = parent_frame.f_locals if parent_frame.f_locals is not globalns else {}\n            else:  # pragma: no cover\n                globalns = {}\n                localns = {}\n            parent_namespace = localns\n\n        self._module_name = module or cast(str, globalns.get('__name__', ''))\n        self._init_core_attrs(\n            ns_resolver=_namespace_utils.NsResolver(\n                namespaces_tuple=_namespace_utils.NamespacesTuple(locals=localns, globals=globalns),\n                parent_namespace=parent_namespace,\n            ),\n            force=False,\n        )", "metadata": {"license": "MIT", "len_tokens": 531}}
{"id": "pydantic:pydantic/type_adapter.py", "language": "python", "code": "def _init_core_attrs(\n        self, ns_resolver: _namespace_utils.NsResolver, force: bool, raise_errors: bool = False\n    ) -> bool:\n        \"\"\"Initialize the core schema, validator, and serializer for the type.\n\n        Args:\n            ns_resolver: The namespace resolver to use when building the core schema for the adapted type.\n            force: Whether to force the construction of the core schema, validator, and serializer.\n                If `force` is set to `False` and `_defer_build` is `True`, the core schema, validator, and serializer will be set to mocks.\n            raise_errors: Whether to raise errors if initializing any of the core attrs fails.\n\n        Returns:\n            `True` if the core schema, validator, and serializer were successfully initialized, otherwise `False`.\n\n        Raises:\n            PydanticUndefinedAnnotation: If `PydanticUndefinedAnnotation` occurs in`__get_pydantic_core_schema__`\n                and `raise_errors=True`.\n        \"\"\"\n        if not force and self._defer_build:\n            _mock_val_ser.set_type_adapter_mocks(self)\n            self.pydantic_complete = False\n            return False\n\n        try:\n            self.core_schema = _getattr_no_parents(self._type, '__pydantic_core_schema__')\n            self.validator = _getattr_no_parents(self._type, '__pydantic_validator__')\n            self.serializer = _getattr_no_parents(self._type, '__pydantic_serializer__')\n\n            # TODO: we don't go through the rebuild logic here directly because we don't want\n            # to repeat all of the namespace fetching logic that we've already done\n            # so we simply skip to the block below that does the actual schema generation\n            if (\n                isinstance(self.core_schema, _mock_val_ser.MockCoreSchema)\n                or isinstance(self.validator, _mock_val_ser.MockValSer)\n                or isinstance(self.serializer, _mock_val_ser.MockValSer)\n            ):\n                raise AttributeError()\n        except AttributeError:\n            config_wrapper = _config.ConfigWrapper(self._config)\n\n            schema_generator = _generate_schema.GenerateSchema(config_wrapper, ns_resolver=ns_resolver)\n\n            try:\n                core_schema = schema_generator.generate_schema(self._type)\n            except PydanticUndefinedAnnotation:\n                if raise_errors:\n                    raise\n                _mock_val_ser.set_type_adapter_mocks(self)\n                return False\n\n            try:\n                self.core_schema = schema_generator.clean_schema(core_schema)\n            except _generate_schema.InvalidSchemaError:\n                _mock_val_ser.set_type_adapter_mocks(self)\n                return False\n\n            core_config = config_wrapper.core_config(None)\n\n            self.validator = create_schema_validator(\n                schema=self.core_schema,\n                schema_type=self._type,\n                schema_type_module=self._module_name,\n                schema_type_name=str(self._type),\n                schema_kind='TypeAdapter',\n                config=core_config,\n                plugin_settings=config_wrapper.plugin_settings,\n            )\n            self.serializer = SchemaSerializer(self.core_schema, core_config)\n\n        self.pydantic_complete = True\n        return True", "metadata": {"license": "MIT", "len_tokens": 634}}
{"id": "pydantic:pydantic/type_adapter.py", "language": "python", "code": "def rebuild(\n        self,\n        *,\n        force: bool = False,\n        raise_errors: bool = True,\n        _parent_namespace_depth: int = 2,\n        _types_namespace: _namespace_utils.MappingNamespace | None = None,\n    ) -> bool | None:\n        \"\"\"Try to rebuild the pydantic-core schema for the adapter's type.\n\n        This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n        the initial attempt to build the schema, and automatic rebuilding fails.\n\n        Args:\n            force: Whether to force the rebuilding of the type adapter's schema, defaults to `False`.\n            raise_errors: Whether to raise errors, defaults to `True`.\n            _parent_namespace_depth: Depth at which to search for the [parent frame][frame-objects]. This\n                frame is used when resolving forward annotations during schema rebuilding, by looking for\n                the locals of this frame. Defaults to 2, which will result in the frame where the method\n                was called.\n            _types_namespace: An explicit types namespace to use, instead of using the local namespace\n                from the parent frame. Defaults to `None`.\n\n        Returns:\n            Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n            If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n        \"\"\"\n        if not force and self.pydantic_complete:\n            return None\n\n        if _types_namespace is not None:\n            rebuild_ns = _types_namespace\n        elif _parent_namespace_depth > 0:\n            rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n        else:\n            rebuild_ns = {}\n\n        # we have to manually fetch globals here because there's no type on the stack of the NsResolver\n        # and so we skip the globalns = get_module_ns_of(typ) call that would normally happen\n        globalns = sys._getframe(max(_parent_namespace_depth - 1, 1)).f_globals\n        ns_resolver = _namespace_utils.NsResolver(\n            namespaces_tuple=_namespace_utils.NamespacesTuple(locals=rebuild_ns, globals=globalns),\n            parent_namespace=rebuild_ns,\n        )\n        return self._init_core_attrs(ns_resolver=ns_resolver, force=True, raise_errors=raise_errors)", "metadata": {"license": "MIT", "len_tokens": 490}}
{"id": "pydantic:pydantic/type_adapter.py", "language": "python", "code": "def validate_python(\n        self,\n        object: Any,\n        /,\n        *,\n        strict: bool | None = None,\n        extra: ExtraValues | None = None,\n        from_attributes: bool | None = None,\n        context: Any | None = None,\n        experimental_allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False,\n        by_alias: bool | None = None,\n        by_name: bool | None = None,\n    ) -> T:\n        \"\"\"Validate a Python object against the model.\n\n        Args:\n            object: The Python object to validate against the model.\n            strict: Whether to strictly check types.\n            extra: Whether to ignore, allow, or forbid extra data during model validation.\n                See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n            from_attributes: Whether to extract data from object attributes.\n            context: Additional context to pass to the validator.\n            experimental_allow_partial: **Experimental** whether to enable\n                [partial validation](../concepts/experimental.md#partial-validation), e.g. to process streams.\n                * False / 'off': Default behavior, no partial validation.\n                * True / 'on': Enable partial validation.\n                * 'trailing-strings': Enable partial validation and allow trailing strings in the input.\n            by_alias: Whether to use the field's alias when validating against the provided input data.\n            by_name: Whether to use the field's name when validating against the provided input data.\n\n        !!! note\n            When using `TypeAdapter` with a Pydantic `dataclass`, the use of the `from_attributes`\n            argument is not supported.\n\n        Returns:\n            The validated object.\n        \"\"\"\n        if by_alias is False and by_name is not True:\n            raise PydanticUserError(\n                'At least one of `by_alias` or `by_name` must be set to True.',\n                code='validate-by-alias-and-name-false',\n            )\n\n        return self.validator.validate_python(\n            object,\n            strict=strict,\n            extra=extra,\n            from_attributes=from_attributes,\n            context=context,\n            allow_partial=experimental_allow_partial,\n            by_alias=by_alias,\n            by_name=by_name,\n        )", "metadata": {"license": "MIT", "len_tokens": 467}}
{"id": "pydantic:pydantic/type_adapter.py", "language": "python", "code": "def validate_json(\n        self,\n        data: str | bytes | bytearray,\n        /,\n        *,\n        strict: bool | None = None,\n        extra: ExtraValues | None = None,\n        context: Any | None = None,\n        experimental_allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False,\n        by_alias: bool | None = None,\n        by_name: bool | None = None,\n    ) -> T:\n        \"\"\"!!! abstract \"Usage Documentation\"\n            [JSON Parsing](../concepts/json.md#json-parsing)\n\n        Validate a JSON string or bytes against the model.\n\n        Args:\n            data: The JSON data to validate against the model.\n            strict: Whether to strictly check types.\n            extra: Whether to ignore, allow, or forbid extra data during model validation.\n                See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n            context: Additional context to use during validation.\n            experimental_allow_partial: **Experimental** whether to enable\n                [partial validation](../concepts/experimental.md#partial-validation), e.g. to process streams.\n                * False / 'off': Default behavior, no partial validation.\n                * True / 'on': Enable partial validation.\n                * 'trailing-strings': Enable partial validation and allow trailing strings in the input.\n            by_alias: Whether to use the field's alias when validating against the provided input data.\n            by_name: Whether to use the field's name when validating against the provided input data.\n\n        Returns:\n            The validated object.\n        \"\"\"\n        if by_alias is False and by_name is not True:\n            raise PydanticUserError(\n                'At least one of `by_alias` or `by_name` must be set to True.',\n                code='validate-by-alias-and-name-false',\n            )\n\n        return self.validator.validate_json(\n            data,\n            strict=strict,\n            extra=extra,\n            context=context,\n            allow_partial=experimental_allow_partial,\n            by_alias=by_alias,\n            by_name=by_name,\n        )", "metadata": {"license": "MIT", "len_tokens": 432}}
{"id": "pydantic:pydantic/type_adapter.py", "language": "python", "code": "def validate_strings(\n        self,\n        obj: Any,\n        /,\n        *,\n        strict: bool | None = None,\n        extra: ExtraValues | None = None,\n        context: Any | None = None,\n        experimental_allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False,\n        by_alias: bool | None = None,\n        by_name: bool | None = None,\n    ) -> T:\n        \"\"\"Validate object contains string data against the model.\n\n        Args:\n            obj: The object contains string data to validate.\n            strict: Whether to strictly check types.\n            extra: Whether to ignore, allow, or forbid extra data during model validation.\n                See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n            context: Additional context to use during validation.\n            experimental_allow_partial: **Experimental** whether to enable\n                [partial validation](../concepts/experimental.md#partial-validation), e.g. to process streams.\n                * False / 'off': Default behavior, no partial validation.\n                * True / 'on': Enable partial validation.\n                * 'trailing-strings': Enable partial validation and allow trailing strings in the input.\n            by_alias: Whether to use the field's alias when validating against the provided input data.\n            by_name: Whether to use the field's name when validating against the provided input data.\n\n        Returns:\n            The validated object.\n        \"\"\"\n        if by_alias is False and by_name is not True:\n            raise PydanticUserError(\n                'At least one of `by_alias` or `by_name` must be set to True.',\n                code='validate-by-alias-and-name-false',\n            )\n\n        return self.validator.validate_strings(\n            obj,\n            strict=strict,\n            extra=extra,\n            context=context,\n            allow_partial=experimental_allow_partial,\n            by_alias=by_alias,\n            by_name=by_name,\n        )", "metadata": {"license": "MIT", "len_tokens": 403}}
{"id": "pydantic:pydantic/type_adapter.py", "language": "python", "code": "def dump_python(\n        self,\n        instance: T,\n        /,\n        *,\n        mode: Literal['json', 'python'] = 'python',\n        include: IncEx | None = None,\n        exclude: IncEx | None = None,\n        by_alias: bool | None = None,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n        exclude_computed_fields: bool = False,\n        round_trip: bool = False,\n        warnings: bool | Literal['none', 'warn', 'error'] = True,\n        fallback: Callable[[Any], Any] | None = None,\n        serialize_as_any: bool = False,\n        context: Any | None = None,\n    ) -> Any:\n        \"\"\"Dump an instance of the adapted type to a Python object.\n\n        Args:\n            instance: The Python object to serialize.\n            mode: The output format.\n            include: Fields to include in the output.\n            exclude: Fields to exclude from the output.\n            by_alias: Whether to use alias names for field names.\n            exclude_unset: Whether to exclude unset fields.\n            exclude_defaults: Whether to exclude fields with default values.\n            exclude_none: Whether to exclude fields with None values.\n            exclude_computed_fields: Whether to exclude computed fields.\n                While this can be useful for round-tripping, it is usually recommended to use the dedicated\n                `round_trip` parameter instead.\n            round_trip: Whether to output the serialized data in a way that is compatible with deserialization.\n            warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n                \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n            fallback: A function to call when an unknown value is encountered. If not provided,\n                a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n            serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n            context: Additional context to pass to the serializer.\n\n        Returns:\n            The serialized object.\n        \"\"\"\n        return self.serializer.to_python(\n            instance,\n            mode=mode,\n            by_alias=by_alias,\n            include=include,\n            exclude=exclude,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n            exclude_computed_fields=exclude_computed_fields,\n            round_trip=round_trip,\n            warnings=warnings,\n            fallback=fallback,\n            serialize_as_any=serialize_as_any,\n            context=context,\n        )", "metadata": {"license": "MIT", "len_tokens": 544}}
{"id": "pydantic:pydantic/type_adapter.py", "language": "python", "code": "def dump_json(\n        self,\n        instance: T,\n        /,\n        *,\n        indent: int | None = None,\n        ensure_ascii: bool = False,\n        include: IncEx | None = None,\n        exclude: IncEx | None = None,\n        by_alias: bool | None = None,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n        exclude_computed_fields: bool = False,\n        round_trip: bool = False,\n        warnings: bool | Literal['none', 'warn', 'error'] = True,\n        fallback: Callable[[Any], Any] | None = None,\n        serialize_as_any: bool = False,\n        context: Any | None = None,\n    ) -> bytes:\n        \"\"\"!!! abstract \"Usage Documentation\"\n            [JSON Serialization](../concepts/json.md#json-serialization)\n\n        Serialize an instance of the adapted type to JSON.\n\n        Args:\n            instance: The instance to be serialized.\n            indent: Number of spaces for JSON indentation.\n            ensure_ascii: If `True`, the output is guaranteed to have all incoming non-ASCII characters escaped.\n                If `False` (the default), these characters will be output as-is.\n            include: Fields to include.\n            exclude: Fields to exclude.\n            by_alias: Whether to use alias names for field names.\n            exclude_unset: Whether to exclude unset fields.\n            exclude_defaults: Whether to exclude fields with default values.\n            exclude_none: Whether to exclude fields with a value of `None`.\n            exclude_computed_fields: Whether to exclude computed fields.\n                While this can be useful for round-tripping, it is usually recommended to use the dedicated\n                `round_trip` parameter instead.\n            round_trip: Whether to serialize and deserialize the instance to ensure round-tripping.\n            warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n                \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n            fallback: A function to call when an unknown value is encountered. If not provided,\n                a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n            serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n            context: Additional context to pass to the serializer.\n\n        Returns:\n            The JSON representation of the given instance as bytes.\n        \"\"\"\n        return self.serializer.to_json(\n            instance,\n            indent=indent,\n            ensure_ascii=ensure_ascii,\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n            exclude_computed_fields=exclude_computed_fields,\n            round_trip=round_trip,\n            warnings=warnings,\n            fallback=fallback,\n            serialize_as_any=serialize_as_any,\n            context=context,\n        )", "metadata": {"license": "MIT", "len_tokens": 616}}
{"id": "pydantic:pydantic/type_adapter.py", "language": "python", "code": "def json_schema(\n        self,\n        *,\n        by_alias: bool = True,\n        ref_template: str = DEFAULT_REF_TEMPLATE,\n        union_format: Literal['any_of', 'primitive_type_array'] = 'any_of',\n        schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n        mode: JsonSchemaMode = 'validation',\n    ) -> dict[str, Any]:\n        \"\"\"Generate a JSON schema for the adapted type.\n\n        Args:\n            by_alias: Whether to use alias names for field names.\n            ref_template: The format string used for generating $ref strings.\n            union_format: The format to use when combining schemas from unions together. Can be one of:\n\n                - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)\n                keyword to combine schemas (the default).\n                - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)\n                keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive\n                type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to\n                `any_of`.\n            schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n                `GenerateJsonSchema` with your desired modifications\n            mode: The mode in which to generate the schema.\n            schema_generator: The generator class used for creating the schema.\n            mode: The mode to use for schema generation.\n\n        Returns:\n            The JSON schema for the model as a dictionary.\n        \"\"\"\n        schema_generator_instance = schema_generator(\n            by_alias=by_alias, ref_template=ref_template, union_format=union_format\n        )\n        if isinstance(self.core_schema, _mock_val_ser.MockCoreSchema):\n            self.core_schema.rebuild()\n            assert not isinstance(self.core_schema, _mock_val_ser.MockCoreSchema), 'this is a bug! please report it'\n        return schema_generator_instance.generate(self.core_schema, mode=mode)", "metadata": {"license": "MIT", "len_tokens": 438}}
{"id": "pydantic:pydantic/type_adapter.py", "language": "python", "code": "def json_schemas(\n        inputs: Iterable[tuple[JsonSchemaKeyT, JsonSchemaMode, TypeAdapter[Any]]],\n        /,\n        *,\n        by_alias: bool = True,\n        title: str | None = None,\n        description: str | None = None,\n        ref_template: str = DEFAULT_REF_TEMPLATE,\n        union_format: Literal['any_of', 'primitive_type_array'] = 'any_of',\n        schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n    ) -> tuple[dict[tuple[JsonSchemaKeyT, JsonSchemaMode], JsonSchemaValue], JsonSchemaValue]:\n        \"\"\"Generate a JSON schema including definitions from multiple type adapters.\n\n        Args:\n            inputs: Inputs to schema generation. The first two items will form the keys of the (first)\n                output mapping; the type adapters will provide the core schemas that get converted into\n                definitions in the output JSON schema.\n            by_alias: Whether to use alias names.\n            title: The title for the schema.\n            description: The description for the schema.\n            ref_template: The format string used for generating $ref strings.\n            union_format: The format to use when combining schemas from unions together. Can be one of:\n\n                - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)\n                keyword to combine schemas (the default).\n                - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)\n                keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive\n                type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to\n                `any_of`.\n            schema_generator: The generator class used for creating the schema.\n\n        Returns:\n            A tuple where:\n\n                - The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\n                    whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\n                    JsonRef references to definitions that are defined in the second returned element.)\n                - The second element is a JSON schema containing all definitions referenced in the first returned\n                    element, along with the optional title and description keys.\n\n        \"\"\"\n        schema_generator_instance = schema_generator(\n            by_alias=by_alias, ref_template=ref_template, union_format=union_format\n        )\n\n        inputs_ = []\n        for key, mode, adapter in inputs:\n            # This is the same pattern we follow for model json schemas - we attempt a core schema rebuild if we detect a mock\n            if isinstance(adapter.core_schema, _mock_val_ser.MockCoreSchema):\n                adapter.core_schema.rebuild()\n                assert not isinstance(adapter.core_schema, _mock_val_ser.MockCoreSchema), (\n                    'this is a bug! please report it'\n                )\n            inputs_.append((key, mode, adapter.core_schema))\n\n        json_schemas_map, definitions = schema_generator_instance.generate_definitions(inputs_)\n\n        json_schema: dict[str, Any] = {}\n        if definitions:\n            json_schema['$defs'] = definitions\n        if title:\n            json_schema['title'] = title\n        if description:\n            json_schema['description'] = description\n\n        return json_schemas_map, json_schema", "metadata": {"license": "MIT", "len_tokens": 698}}
{"id": "pydantic:pydantic/functional_serializers.py", "language": "python", "code": "class PlainSerializer:\n    \"\"\"Plain serializers use a function to modify the output of serialization.\n\n    This is particularly helpful when you want to customize the serialization for annotated types.\n    Consider an input of `list`, which will be serialized into a space-delimited string.\n\n    ```python\n    from typing import Annotated\n\n    from pydantic import BaseModel, PlainSerializer\n\n    CustomStr = Annotated[\n        list, PlainSerializer(lambda x: ' '.join(x), return_type=str)\n    ]\n\n    class StudentModel(BaseModel):\n        courses: CustomStr\n\n    student = StudentModel(courses=['Math', 'Chemistry', 'English'])\n    print(student.model_dump())\n    #> {'courses': 'Math Chemistry English'}\n    ```\n\n    Attributes:\n        func: The serializer function.\n        return_type: The return type for the function. If omitted it will be inferred from the type annotation.\n        when_used: Determines when this serializer should be used. Accepts a string with values `'always'`,\n            `'unless-none'`, `'json'`, and `'json-unless-none'`. Defaults to 'always'.\n    \"\"\"\n\n    func: core_schema.SerializerFunction\n    return_type: Any = PydanticUndefined\n    when_used: WhenUsed = 'always'\n\n    def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        \"\"\"Gets the Pydantic core schema.\n\n        Args:\n            source_type: The source type.\n            handler: The `GetCoreSchemaHandler` instance.\n\n        Returns:\n            The Pydantic core schema.\n        \"\"\"\n        schema = handler(source_type)\n        if self.return_type is not PydanticUndefined:\n            return_type = self.return_type\n        else:\n            try:\n                # Do not pass in globals as the function could be defined in a different module.\n                # Instead, let `get_callable_return_type` infer the globals to use, but still pass\n                # in locals that may contain a parent/rebuild namespace:\n                return_type = _decorators.get_callable_return_type(\n                    self.func,\n                    localns=handler._get_types_namespace().locals,\n                )\n            except NameError as e:\n                raise PydanticUndefinedAnnotation.from_name_error(e) from e\n\n        return_schema = None if return_type is PydanticUndefined else handler.generate_schema(return_type)\n        schema['serialization'] = core_schema.plain_serializer_function_ser_schema(\n            function=self.func,\n            info_arg=_decorators.inspect_annotated_serializer(self.func, 'plain'),\n            return_schema=return_schema,\n            when_used=self.when_used,\n        )\n        return schema", "metadata": {"license": "MIT", "len_tokens": 549}}
{"id": "pydantic:pydantic/functional_serializers.py", "language": "python", "code": "def field_serializer(  # noqa: D417\n    field: str,\n    /,\n    *fields: str,\n    mode: Literal['plain', 'wrap'] = 'plain',\n    # TODO PEP 747 (grep for 'return_type' on the whole code base):\n    return_type: Any = PydanticUndefined,\n    when_used: WhenUsed = 'always',\n    check_fields: bool | None = None,\n) -> (\n    Callable[[_FieldWrapSerializerT], _FieldWrapSerializerT]\n    | Callable[[_FieldPlainSerializerT], _FieldPlainSerializerT]\n):\n    \"\"\"Decorator that enables custom field serialization.\n\n    In the below example, a field of type `set` is used to mitigate duplication. A `field_serializer` is used to serialize the data as a sorted list.\n\n    ```python\n    from pydantic import BaseModel, field_serializer\n\n    class StudentModel(BaseModel):\n        name: str = 'Jane'\n        courses: set[str]\n\n        @field_serializer('courses', when_used='json')\n        def serialize_courses_in_order(self, courses: set[str]):\n            return sorted(courses)\n\n    student = StudentModel(courses={'Math', 'Chemistry', 'English'})\n    print(student.model_dump_json())\n    #> {\"name\":\"Jane\",\"courses\":[\"Chemistry\",\"English\",\"Math\"]}\n    ```\n\n    See [the usage documentation](../concepts/serialization.md#serializers) for more information.\n\n    Four signatures are supported for the decorated serializer:\n\n    - `(self, value: Any, info: FieldSerializationInfo)`\n    - `(self, value: Any, nxt: SerializerFunctionWrapHandler, info: FieldSerializationInfo)`\n    - `(value: Any, info: SerializationInfo)`\n    - `(value: Any, nxt: SerializerFunctionWrapHandler, info: SerializationInfo)`\n\n    Args:\n        *fields: The field names the serializer should apply to.\n        mode: The serialization mode.\n\n            - `plain` means the function will be called instead of the default serialization logic,\n            - `wrap` means the function will be called with an argument to optionally call the\n               default serialization logic.\n        return_type: Optional return type for the function, if omitted it will be inferred from the type annotation.\n        when_used: Determines the serializer will be used for serialization.\n        check_fields: Whether to check that the fields actually exist on the model.\n\n    Raises:\n        PydanticUserError:\n            - If the decorator is used without any arguments (at least one field name must be provided).\n            - If the provided field names are not strings.\n    \"\"\"\n    if callable(field) or isinstance(field, classmethod):\n        raise PydanticUserError(\n            'The `@field_serializer` decorator cannot be used without arguments, at least one field must be provided. '\n            \"For example: `@field_serializer('<field_name>', ...)`.\",\n            code='decorator-missing-arguments',\n        )\n\n    fields = field, *fields\n    if not all(isinstance(field, str) for field in fields):\n        raise PydanticUserError(\n            'The provided field names to the `@field_serializer` decorator should be strings. '\n            \"For example: `@field_serializer('<field_name_1>', '<field_name_2>', ...).`\",\n            code='decorator-invalid-fields',\n        )\n\n    def dec(f: FieldSerializer) -> _decorators.PydanticDescriptorProxy[Any]:\n        dec_info = _decorators.FieldSerializerDecoratorInfo(\n            fields=fields,\n            mode=mode,\n            return_type=return_type,\n            when_used=when_used,\n            check_fields=check_fields,\n        )\n        return _decorators.PydanticDescriptorProxy(f, dec_info)  # pyright: ignore[reportArgumentType]\n\n    return dec", "metadata": {"license": "MIT", "len_tokens": 790}}
{"id": "pydantic:pydantic/functional_serializers.py", "language": "python", "code": "def model_serializer(\n    f: _ModelPlainSerializerT | _ModelWrapSerializerT | None = None,\n    /,\n    *,\n    mode: Literal['plain', 'wrap'] = 'plain',\n    when_used: WhenUsed = 'always',\n    return_type: Any = PydanticUndefined,\n) -> (\n    _ModelPlainSerializerT\n    | Callable[[_ModelWrapSerializerT], _ModelWrapSerializerT]\n    | Callable[[_ModelPlainSerializerT], _ModelPlainSerializerT]\n):\n    \"\"\"Decorator that enables custom model serialization.\n\n    This is useful when a model need to be serialized in a customized manner, allowing for flexibility beyond just specific fields.\n\n    An example would be to serialize temperature to the same temperature scale, such as degrees Celsius.\n\n    ```python\n    from typing import Literal\n\n    from pydantic import BaseModel, model_serializer\n\n    class TemperatureModel(BaseModel):\n        unit: Literal['C', 'F']\n        value: int\n\n        @model_serializer()\n        def serialize_model(self):\n            if self.unit == 'F':\n                return {'unit': 'C', 'value': int((self.value - 32) / 1.8)}\n            return {'unit': self.unit, 'value': self.value}\n\n    temperature = TemperatureModel(unit='F', value=212)\n    print(temperature.model_dump())\n    #> {'unit': 'C', 'value': 100}\n    ```\n\n    Two signatures are supported for `mode='plain'`, which is the default:\n\n    - `(self)`\n    - `(self, info: SerializationInfo)`\n\n    And two other signatures for `mode='wrap'`:\n\n    - `(self, nxt: SerializerFunctionWrapHandler)`\n    - `(self, nxt: SerializerFunctionWrapHandler, info: SerializationInfo)`\n\n        See [the usage documentation](../concepts/serialization.md#serializers) for more information.\n\n    Args:\n        f: The function to be decorated.\n        mode: The serialization mode.\n\n            - `'plain'` means the function will be called instead of the default serialization logic\n            - `'wrap'` means the function will be called with an argument to optionally call the default\n                serialization logic.\n        when_used: Determines when this serializer should be used.\n        return_type: The return type for the function. If omitted it will be inferred from the type annotation.\n\n    Returns:\n        The decorator function.\n    \"\"\"\n\n    def dec(f: ModelSerializer) -> _decorators.PydanticDescriptorProxy[Any]:\n        dec_info = _decorators.ModelSerializerDecoratorInfo(mode=mode, return_type=return_type, when_used=when_used)\n        return _decorators.PydanticDescriptorProxy(f, dec_info)\n\n    if f is None:\n        return dec  # pyright: ignore[reportReturnType]\n    else:\n        return dec(f)", "metadata": {"license": "MIT", "len_tokens": 591}}
{"id": "pydantic:pydantic/functional_serializers.py", "language": "python", "code": "def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        \"\"\"Gets the Pydantic core schema.\n\n        Args:\n            source_type: The source type.\n            handler: The `GetCoreSchemaHandler` instance.\n\n        Returns:\n            The Pydantic core schema.\n        \"\"\"\n        schema = handler(source_type)\n        if self.return_type is not PydanticUndefined:\n            return_type = self.return_type\n        else:\n            try:\n                # Do not pass in globals as the function could be defined in a different module.\n                # Instead, let `get_callable_return_type` infer the globals to use, but still pass\n                # in locals that may contain a parent/rebuild namespace:\n                return_type = _decorators.get_callable_return_type(\n                    self.func,\n                    localns=handler._get_types_namespace().locals,\n                )\n            except NameError as e:\n                raise PydanticUndefinedAnnotation.from_name_error(e) from e\n\n        return_schema = None if return_type is PydanticUndefined else handler.generate_schema(return_type)\n        schema['serialization'] = core_schema.plain_serializer_function_ser_schema(\n            function=self.func,\n            info_arg=_decorators.inspect_annotated_serializer(self.func, 'plain'),\n            return_schema=return_schema,\n            when_used=self.when_used,\n        )\n        return schema", "metadata": {"license": "MIT", "len_tokens": 288}}
{"id": "pydantic:pydantic/functional_serializers.py", "language": "python", "code": "def __get_pydantic_core_schema__(self, source_type: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:\n        \"\"\"This method is used to get the Pydantic core schema of the class.\n\n        Args:\n            source_type: Source type.\n            handler: Core schema handler.\n\n        Returns:\n            The generated core schema of the class.\n        \"\"\"\n        schema = handler(source_type)\n        if self.return_type is not PydanticUndefined:\n            return_type = self.return_type\n        else:\n            try:\n                # Do not pass in globals as the function could be defined in a different module.\n                # Instead, let `get_callable_return_type` infer the globals to use, but still pass\n                # in locals that may contain a parent/rebuild namespace:\n                return_type = _decorators.get_callable_return_type(\n                    self.func,\n                    localns=handler._get_types_namespace().locals,\n                )\n            except NameError as e:\n                raise PydanticUndefinedAnnotation.from_name_error(e) from e\n\n        return_schema = None if return_type is PydanticUndefined else handler.generate_schema(return_type)\n        schema['serialization'] = core_schema.wrap_serializer_function_ser_schema(\n            function=self.func,\n            info_arg=_decorators.inspect_annotated_serializer(self.func, 'wrap'),\n            return_schema=return_schema,\n            when_used=self.when_used,\n        )\n        return schema", "metadata": {"license": "MIT", "len_tokens": 290}}
{"id": "pydantic:pydantic/validate_call_decorator.py", "language": "python", "code": "def _check_function_type(function: object) -> None:\n    \"\"\"Check if the input function is a supported type for `validate_call`.\"\"\"\n    if isinstance(function, _generate_schema.VALIDATE_CALL_SUPPORTED_TYPES):\n        try:\n            inspect.signature(cast(_generate_schema.ValidateCallSupportedTypes, function))\n        except ValueError:\n            raise PydanticUserError(\n                f\"Input function `{function}` doesn't have a valid signature\", code=_INVALID_TYPE_ERROR_CODE\n            )\n\n        if isinstance(function, partial):\n            try:\n                assert not isinstance(partial.func, partial), 'Partial of partial'\n                _check_function_type(function.func)\n            except PydanticUserError as e:\n                raise PydanticUserError(\n                    f'Partial of `{function.func}` is invalid because the type of `{function.func}` is not supported by `validate_call`',\n                    code=_INVALID_TYPE_ERROR_CODE,\n                ) from e\n\n        return\n\n    if isinstance(function, BuiltinFunctionType):\n        raise PydanticUserError(f'Input built-in function `{function}` is not supported', code=_INVALID_TYPE_ERROR_CODE)\n    if isinstance(function, (classmethod, staticmethod, property)):\n        name = type(function).__name__\n        raise PydanticUserError(\n            f'The `@{name}` decorator should be applied after `@validate_call` (put `@{name}` on top)',\n            code=_INVALID_TYPE_ERROR_CODE,\n        )\n\n    if inspect.isclass(function):\n        raise PydanticUserError(\n            f'Unable to validate {function}: `validate_call` should be applied to functions, not classes (put `@validate_call` on top of `__init__` or `__new__` instead)',\n            code=_INVALID_TYPE_ERROR_CODE,\n        )\n    if callable(function):\n        raise PydanticUserError(\n            f'Unable to validate {function}: `validate_call` should be applied to functions, not instances or other callables. Use `validate_call` explicitly on `__call__` instead.',\n            code=_INVALID_TYPE_ERROR_CODE,\n        )\n\n    raise PydanticUserError(\n        f'Unable to validate {function}: `validate_call` should be applied to one of the following: function, method, partial, or lambda',\n        code=_INVALID_TYPE_ERROR_CODE,\n    )", "metadata": {"license": "MIT", "len_tokens": 480}}
{"id": "pydantic:pydantic/validate_call_decorator.py", "language": "python", "code": "def validate_call(\n    func: AnyCallableT | None = None,\n    /,\n    *,\n    config: ConfigDict | None = None,\n    validate_return: bool = False,\n) -> AnyCallableT | Callable[[AnyCallableT], AnyCallableT]:\n    \"\"\"!!! abstract \"Usage Documentation\"\n        [Validation Decorator](../concepts/validation_decorator.md)\n\n    Returns a decorated wrapper around the function that validates the arguments and, optionally, the return value.\n\n    Usage may be either as a plain decorator `@validate_call` or with arguments `@validate_call(...)`.\n\n    Args:\n        func: The function to be decorated.\n        config: The configuration dictionary.\n        validate_return: Whether to validate the return value.\n\n    Returns:\n        The decorated function.\n    \"\"\"\n    parent_namespace = _typing_extra.parent_frame_namespace()\n\n    def validate(function: AnyCallableT) -> AnyCallableT:\n        _check_function_type(function)\n        validate_call_wrapper = _validate_call.ValidateCallWrapper(\n            cast(_generate_schema.ValidateCallSupportedTypes, function), config, validate_return, parent_namespace\n        )\n        return _validate_call.update_wrapper_attributes(function, validate_call_wrapper.__call__)  # type: ignore\n\n    if func is not None:\n        return validate(func)\n    else:\n        return validate", "metadata": {"license": "MIT", "len_tokens": 266}}
{"id": "pydantic:pydantic/errors.py", "language": "python", "code": "class PydanticUndefinedAnnotation(PydanticErrorMixin, NameError):\n    \"\"\"A subclass of `NameError` raised when handling undefined annotations during `CoreSchema` generation.\n\n    Attributes:\n        name: Name of the error.\n        message: Description of the error.\n    \"\"\"\n\n    def __init__(self, name: str, message: str) -> None:\n        self.name = name\n        super().__init__(message=message, code='undefined-annotation')\n\n    @classmethod\n    def from_name_error(cls, name_error: NameError) -> Self:\n        \"\"\"Convert a `NameError` to a `PydanticUndefinedAnnotation` error.\n\n        Args:\n            name_error: `NameError` to be converted.\n\n        Returns:\n            Converted `PydanticUndefinedAnnotation` error.\n        \"\"\"\n        try:\n            name = name_error.name  # type: ignore  # python > 3.10\n        except AttributeError:\n            name = re.search(r\".*'(.+?)'\", str(name_error)).group(1)  # type: ignore[union-attr]\n        return cls(name=name, message=str(name_error))", "metadata": {"license": "MIT", "len_tokens": 236}}
{"id": "pydantic:pydantic/dataclasses.py", "language": "python", "code": "def rebuild_dataclass(\n    cls: type[PydanticDataclass],\n    *,\n    force: bool = False,\n    raise_errors: bool = True,\n    _parent_namespace_depth: int = 2,\n    _types_namespace: MappingNamespace | None = None,\n) -> bool | None:\n    \"\"\"Try to rebuild the pydantic-core schema for the dataclass.\n\n    This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n    the initial attempt to build the schema, and automatic rebuilding fails.\n\n    This is analogous to `BaseModel.model_rebuild`.\n\n    Args:\n        cls: The class to rebuild the pydantic-core schema for.\n        force: Whether to force the rebuilding of the schema, defaults to `False`.\n        raise_errors: Whether to raise errors, defaults to `True`.\n        _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n        _types_namespace: The types namespace, defaults to `None`.\n\n    Returns:\n        Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n        If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n    \"\"\"\n    if not force and cls.__pydantic_complete__:\n        return None\n\n    for attr in ('__pydantic_core_schema__', '__pydantic_validator__', '__pydantic_serializer__'):\n        if attr in cls.__dict__ and not isinstance(getattr(cls, attr), _mock_val_ser.MockValSer):\n            # Deleting the validator/serializer is necessary as otherwise they can get reused in\n            # pydantic-core. Same applies for the core schema that can be reused in schema generation.\n            delattr(cls, attr)\n\n    cls.__pydantic_complete__ = False\n\n    if _types_namespace is not None:\n        rebuild_ns = _types_namespace\n    elif _parent_namespace_depth > 0:\n        rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n    else:\n        rebuild_ns = {}\n\n    ns_resolver = _namespace_utils.NsResolver(\n        parent_namespace=rebuild_ns,\n    )\n\n    return _pydantic_dataclasses.complete_dataclass(\n        cls,\n        _config.ConfigWrapper(cls.__pydantic_config__, check=False),\n        raise_errors=raise_errors,\n        ns_resolver=ns_resolver,\n        # We could provide a different config instead (with `'defer_build'` set to `True`)\n        # of this explicit `_force_build` argument, but because config can come from the\n        # decorator parameter or the `__pydantic_config__` attribute, `complete_dataclass`\n        # will overwrite `__pydantic_config__` with the provided config above:\n        _force_build=True,\n    )", "metadata": {"license": "MIT", "len_tokens": 589}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def __init__(self, /, **data: Any) -> None:\n        \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n        Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n        validated to form a valid model.\n\n        `self` is explicitly positional-only to allow `self` as a field name.\n        \"\"\"\n        # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n        __tracebackhide__ = True\n        validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n        if self is not validated_self:\n            warnings.warn(\n                'A custom validator is returning a value other than `self`.\\n'\n                \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n                'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n                stacklevel=2,\n            )", "metadata": {"license": "MIT", "len_tokens": 228}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def model_copy(self, *, update: Mapping[str, Any] | None = None, deep: bool = False) -> Self:\n        \"\"\"!!! abstract \"Usage Documentation\"\n            [`model_copy`](../concepts/models.md#model-copy)\n\n        Returns a copy of the model.\n\n        !!! note\n            The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n            might have unexpected side effects if you store anything in it, on top of the model\n            fields (e.g. the value of [cached properties][functools.cached_property]).\n\n        Args:\n            update: Values to change/add in the new model. Note: the data is not validated\n                before creating the new model. You should trust this data.\n            deep: Set to `True` to make a deep copy of the model.\n\n        Returns:\n            New model instance.\n        \"\"\"\n        copied = self.__deepcopy__() if deep else self.__copy__()\n        if update:\n            if self.model_config.get('extra') == 'allow':\n                for k, v in update.items():\n                    if k in self.__pydantic_fields__:\n                        copied.__dict__[k] = v\n                    else:\n                        if copied.__pydantic_extra__ is None:\n                            copied.__pydantic_extra__ = {}\n                        copied.__pydantic_extra__[k] = v\n            else:\n                copied.__dict__.update(update)\n            copied.__pydantic_fields_set__.update(update.keys())\n        return copied", "metadata": {"license": "MIT", "len_tokens": 314}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def model_dump(\n        self,\n        *,\n        mode: Literal['json', 'python'] | str = 'python',\n        include: IncEx | None = None,\n        exclude: IncEx | None = None,\n        context: Any | None = None,\n        by_alias: bool | None = None,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n        exclude_computed_fields: bool = False,\n        round_trip: bool = False,\n        warnings: bool | Literal['none', 'warn', 'error'] = True,\n        fallback: Callable[[Any], Any] | None = None,\n        serialize_as_any: bool = False,\n    ) -> dict[str, Any]:\n        \"\"\"!!! abstract \"Usage Documentation\"\n            [`model_dump`](../concepts/serialization.md#python-mode)\n\n        Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n\n        Args:\n            mode: The mode in which `to_python` should run.\n                If mode is 'json', the output will only contain JSON serializable types.\n                If mode is 'python', the output may contain non-JSON-serializable Python objects.\n            include: A set of fields to include in the output.\n            exclude: A set of fields to exclude from the output.\n            context: Additional context to pass to the serializer.\n            by_alias: Whether to use the field's alias in the dictionary key if defined.\n            exclude_unset: Whether to exclude fields that have not been explicitly set.\n            exclude_defaults: Whether to exclude fields that are set to their default value.\n            exclude_none: Whether to exclude fields that have a value of `None`.\n            exclude_computed_fields: Whether to exclude computed fields.\n                While this can be useful for round-tripping, it is usually recommended to use the dedicated\n                `round_trip` parameter instead.\n            round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n            warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n                \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n            fallback: A function to call when an unknown value is encountered. If not provided,\n                a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n            serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n\n        Returns:\n            A dictionary representation of the model.\n        \"\"\"\n        return self.__pydantic_serializer__.to_python(\n            self,\n            mode=mode,\n            by_alias=by_alias,\n            include=include,\n            exclude=exclude,\n            context=context,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n            exclude_computed_fields=exclude_computed_fields,\n            round_trip=round_trip,\n            warnings=warnings,\n            fallback=fallback,\n            serialize_as_any=serialize_as_any,\n        )", "metadata": {"license": "MIT", "len_tokens": 640}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def model_dump_json(\n        self,\n        *,\n        indent: int | None = None,\n        ensure_ascii: bool = False,\n        include: IncEx | None = None,\n        exclude: IncEx | None = None,\n        context: Any | None = None,\n        by_alias: bool | None = None,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n        exclude_computed_fields: bool = False,\n        round_trip: bool = False,\n        warnings: bool | Literal['none', 'warn', 'error'] = True,\n        fallback: Callable[[Any], Any] | None = None,\n        serialize_as_any: bool = False,\n    ) -> str:\n        \"\"\"!!! abstract \"Usage Documentation\"\n            [`model_dump_json`](../concepts/serialization.md#json-mode)\n\n        Generates a JSON representation of the model using Pydantic's `to_json` method.\n\n        Args:\n            indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n            ensure_ascii: If `True`, the output is guaranteed to have all incoming non-ASCII characters escaped.\n                If `False` (the default), these characters will be output as-is.\n            include: Field(s) to include in the JSON output.\n            exclude: Field(s) to exclude from the JSON output.\n            context: Additional context to pass to the serializer.\n            by_alias: Whether to serialize using field aliases.\n            exclude_unset: Whether to exclude fields that have not been explicitly set.\n            exclude_defaults: Whether to exclude fields that are set to their default value.\n            exclude_none: Whether to exclude fields that have a value of `None`.\n            exclude_computed_fields: Whether to exclude computed fields.\n                While this can be useful for round-tripping, it is usually recommended to use the dedicated\n                `round_trip` parameter instead.\n            round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n            warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n                \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n            fallback: A function to call when an unknown value is encountered. If not provided,\n                a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n            serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n\n        Returns:\n            A JSON string representation of the model.\n        \"\"\"\n        return self.__pydantic_serializer__.to_json(\n            self,\n            indent=indent,\n            ensure_ascii=ensure_ascii,\n            include=include,\n            exclude=exclude,\n            context=context,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n            exclude_computed_fields=exclude_computed_fields,\n            round_trip=round_trip,\n            warnings=warnings,\n            fallback=fallback,\n            serialize_as_any=serialize_as_any,\n        ).decode()", "metadata": {"license": "MIT", "len_tokens": 655}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def model_json_schema(\n        cls,\n        by_alias: bool = True,\n        ref_template: str = DEFAULT_REF_TEMPLATE,\n        schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n        mode: JsonSchemaMode = 'validation',\n        *,\n        union_format: Literal['any_of', 'primitive_type_array'] = 'any_of',\n    ) -> dict[str, Any]:\n        \"\"\"Generates a JSON schema for a model class.\n\n        Args:\n            by_alias: Whether to use attribute aliases or not.\n            ref_template: The reference template.\n            union_format: The format to use when combining schemas from unions together. Can be one of:\n\n                - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)\n                keyword to combine schemas (the default).\n                - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)\n                keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive\n                type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to\n                `any_of`.\n            schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n                `GenerateJsonSchema` with your desired modifications\n            mode: The mode in which to generate the schema.\n\n        Returns:\n            The JSON schema for the given model class.\n        \"\"\"\n        return model_json_schema(\n            cls,\n            by_alias=by_alias,\n            ref_template=ref_template,\n            union_format=union_format,\n            schema_generator=schema_generator,\n            mode=mode,\n        )", "metadata": {"license": "MIT", "len_tokens": 361}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def model_parametrized_name(cls, params: tuple[type[Any], ...]) -> str:\n        \"\"\"Compute the class name for parametrizations of generic classes.\n\n        This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n\n        Args:\n            params: Tuple of types of the class. Given a generic class\n                `Model` with 2 type variables and a concrete model `Model[str, int]`,\n                the value `(str, int)` would be passed to `params`.\n\n        Returns:\n            String representing the new class where `params` are passed to `cls` as type variables.\n\n        Raises:\n            TypeError: Raised when trying to generate concrete names for non-generic models.\n        \"\"\"\n        if not issubclass(cls, Generic):\n            raise TypeError('Concrete names should only be generated for generic models.')\n\n        # Any strings received should represent forward references, so we handle them specially below.\n        # If we eventually move toward wrapping them in a ForwardRef in __class_getitem__ in the future,\n        # we may be able to remove this special case.\n        param_names = [param if isinstance(param, str) else _repr.display_as_type(param) for param in params]\n        params_component = ', '.join(param_names)\n        return f'{cls.__name__}[{params_component}]'", "metadata": {"license": "MIT", "len_tokens": 273}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def model_rebuild(\n        cls,\n        *,\n        force: bool = False,\n        raise_errors: bool = True,\n        _parent_namespace_depth: int = 2,\n        _types_namespace: MappingNamespace | None = None,\n    ) -> bool | None:\n        \"\"\"Try to rebuild the pydantic-core schema for the model.\n\n        This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n        the initial attempt to build the schema, and automatic rebuilding fails.\n\n        Args:\n            force: Whether to force the rebuilding of the model schema, defaults to `False`.\n            raise_errors: Whether to raise errors, defaults to `True`.\n            _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n            _types_namespace: The types namespace, defaults to `None`.\n\n        Returns:\n            Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n            If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n        \"\"\"\n        already_complete = cls.__pydantic_complete__\n        if already_complete and not force:\n            return None\n\n        cls.__pydantic_complete__ = False\n\n        for attr in ('__pydantic_core_schema__', '__pydantic_validator__', '__pydantic_serializer__'):\n            if attr in cls.__dict__ and not isinstance(getattr(cls, attr), _mock_val_ser.MockValSer):\n                # Deleting the validator/serializer is necessary as otherwise they can get reused in\n                # pydantic-core. We do so only if they aren't mock instances, otherwise  as `model_rebuild()`\n                # isn't thread-safe  concurrent model instantiations can lead to the parent validator being used.\n                # Same applies for the core schema that can be reused in schema generation.\n                delattr(cls, attr)\n\n        if _types_namespace is not None:\n            rebuild_ns = _types_namespace\n        elif _parent_namespace_depth > 0:\n            rebuild_ns = _typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth, force=True) or {}\n        else:\n            rebuild_ns = {}\n\n        parent_ns = _model_construction.unpack_lenient_weakvaluedict(cls.__pydantic_parent_namespace__) or {}\n\n        ns_resolver = _namespace_utils.NsResolver(\n            parent_namespace={**rebuild_ns, **parent_ns},\n        )\n\n        return _model_construction.complete_model_class(\n            cls,\n            _config.ConfigWrapper(cls.model_config, check=False),\n            ns_resolver,\n            raise_errors=raise_errors,\n            # If the model was already complete, we don't need to call the hook again.\n            call_on_complete_hook=not already_complete,\n        )", "metadata": {"license": "MIT", "len_tokens": 567}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def model_validate(\n        cls,\n        obj: Any,\n        *,\n        strict: bool | None = None,\n        extra: ExtraValues | None = None,\n        from_attributes: bool | None = None,\n        context: Any | None = None,\n        by_alias: bool | None = None,\n        by_name: bool | None = None,\n    ) -> Self:\n        \"\"\"Validate a pydantic model instance.\n\n        Args:\n            obj: The object to validate.\n            strict: Whether to enforce types strictly.\n            extra: Whether to ignore, allow, or forbid extra data during model validation.\n                See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n            from_attributes: Whether to extract data from object attributes.\n            context: Additional context to pass to the validator.\n            by_alias: Whether to use the field's alias when validating against the provided input data.\n            by_name: Whether to use the field's name when validating against the provided input data.\n\n        Raises:\n            ValidationError: If the object could not be validated.\n\n        Returns:\n            The validated model instance.\n        \"\"\"\n        # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n        __tracebackhide__ = True\n\n        if by_alias is False and by_name is not True:\n            raise PydanticUserError(\n                'At least one of `by_alias` or `by_name` must be set to True.',\n                code='validate-by-alias-and-name-false',\n            )\n\n        return cls.__pydantic_validator__.validate_python(\n            obj,\n            strict=strict,\n            extra=extra,\n            from_attributes=from_attributes,\n            context=context,\n            by_alias=by_alias,\n            by_name=by_name,\n        )", "metadata": {"license": "MIT", "len_tokens": 370}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def model_validate_json(\n        cls,\n        json_data: str | bytes | bytearray,\n        *,\n        strict: bool | None = None,\n        extra: ExtraValues | None = None,\n        context: Any | None = None,\n        by_alias: bool | None = None,\n        by_name: bool | None = None,\n    ) -> Self:\n        \"\"\"!!! abstract \"Usage Documentation\"\n            [JSON Parsing](../concepts/json.md#json-parsing)\n\n        Validate the given JSON data against the Pydantic model.\n\n        Args:\n            json_data: The JSON data to validate.\n            strict: Whether to enforce types strictly.\n            extra: Whether to ignore, allow, or forbid extra data during model validation.\n                See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n            context: Extra variables to pass to the validator.\n            by_alias: Whether to use the field's alias when validating against the provided input data.\n            by_name: Whether to use the field's name when validating against the provided input data.\n\n        Returns:\n            The validated Pydantic model.\n\n        Raises:\n            ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n        \"\"\"\n        # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n        __tracebackhide__ = True\n\n        if by_alias is False and by_name is not True:\n            raise PydanticUserError(\n                'At least one of `by_alias` or `by_name` must be set to True.',\n                code='validate-by-alias-and-name-false',\n            )\n\n        return cls.__pydantic_validator__.validate_json(\n            json_data, strict=strict, extra=extra, context=context, by_alias=by_alias, by_name=by_name\n        )", "metadata": {"license": "MIT", "len_tokens": 384}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def model_validate_strings(\n        cls,\n        obj: Any,\n        *,\n        strict: bool | None = None,\n        extra: ExtraValues | None = None,\n        context: Any | None = None,\n        by_alias: bool | None = None,\n        by_name: bool | None = None,\n    ) -> Self:\n        \"\"\"Validate the given object with string data against the Pydantic model.\n\n        Args:\n            obj: The object containing string data to validate.\n            strict: Whether to enforce types strictly.\n            extra: Whether to ignore, allow, or forbid extra data during model validation.\n                See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n            context: Extra variables to pass to the validator.\n            by_alias: Whether to use the field's alias when validating against the provided input data.\n            by_name: Whether to use the field's name when validating against the provided input data.\n\n        Returns:\n            The validated Pydantic model.\n        \"\"\"\n        # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n        __tracebackhide__ = True\n\n        if by_alias is False and by_name is not True:\n            raise PydanticUserError(\n                'At least one of `by_alias` or `by_name` must be set to True.',\n                code='validate-by-alias-and-name-false',\n            )\n\n        return cls.__pydantic_validator__.validate_strings(\n            obj, strict=strict, extra=extra, context=context, by_alias=by_alias, by_name=by_name\n        )", "metadata": {"license": "MIT", "len_tokens": 333}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def __get_pydantic_core_schema__(cls, source: type[BaseModel], handler: GetCoreSchemaHandler, /) -> CoreSchema:\n        # This warning is only emitted when calling `super().__get_pydantic_core_schema__` from a model subclass.\n        # In the generate schema logic, this method (`BaseModel.__get_pydantic_core_schema__`) is special cased to\n        # *not* be called if not overridden.\n        warnings.warn(\n            'The `__get_pydantic_core_schema__` method of the `BaseModel` class is deprecated. If you are calling '\n            '`super().__get_pydantic_core_schema__` when overriding the method on a Pydantic model, consider using '\n            '`handler(source)` instead. However, note that overriding this method on models can lead to unexpected '\n            'side effects.',\n            PydanticDeprecatedSince211,\n            stacklevel=2,\n        )\n        # Logic copied over from `GenerateSchema._model_schema`:\n        schema = cls.__dict__.get('__pydantic_core_schema__')\n        if schema is not None and not isinstance(schema, _mock_val_ser.MockCoreSchema):\n            return cls.__pydantic_core_schema__\n\n        return handler(source)", "metadata": {"license": "MIT", "len_tokens": 260}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def __get_pydantic_json_schema__(\n        cls,\n        core_schema: CoreSchema,\n        handler: GetJsonSchemaHandler,\n        /,\n    ) -> JsonSchemaValue:\n        \"\"\"Hook into generating the model's JSON schema.\n\n        Args:\n            core_schema: A `pydantic-core` CoreSchema.\n                You can ignore this argument and call the handler with a new CoreSchema,\n                wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n                or just call the handler with the original schema.\n            handler: Call into Pydantic's internal JSON schema generation.\n                This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n                generation fails.\n                Since this gets called by `BaseModel.model_json_schema` you can override the\n                `schema_generator` argument to that function to change JSON schema generation globally\n                for a type.\n\n        Returns:\n            A JSON schema, as a Python object.\n        \"\"\"\n        return handler(core_schema)", "metadata": {"license": "MIT", "len_tokens": 211}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def __pydantic_init_subclass__(cls, **kwargs: Any) -> None:\n        \"\"\"This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n        only after basic class initialization is complete. In particular, attributes like `model_fields` will\n        be present when this is called, but forward annotations are not guaranteed to be resolved yet,\n        meaning that creating an instance of the class may fail.\n\n        This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n        and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n        `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n\n        This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n        any kwargs passed to the class definition that aren't used internally by Pydantic.\n\n        Args:\n            **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n                by Pydantic.\n\n        Note:\n            You may want to override [`__pydantic_on_complete__()`][pydantic.main.BaseModel.__pydantic_on_complete__]\n            instead, which is called once the class and its fields are fully initialized and ready for validation.\n        \"\"\"", "metadata": {"license": "MIT", "len_tokens": 285}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def __class_getitem__(\n        cls, typevar_values: type[Any] | tuple[type[Any], ...]\n    ) -> type[BaseModel] | _forward_ref.PydanticRecursiveRef:\n        cached = _generics.get_cached_generic_type_early(cls, typevar_values)\n        if cached is not None:\n            return cached\n\n        if cls is BaseModel:\n            raise TypeError('Type parameters should be placed on typing.Generic, not BaseModel')\n        if not hasattr(cls, '__parameters__'):\n            raise TypeError(f'{cls} cannot be parametrized because it does not inherit from typing.Generic')\n        if not cls.__pydantic_generic_metadata__['parameters'] and Generic not in cls.__bases__:\n            raise TypeError(f'{cls} is not a generic class')\n\n        if not isinstance(typevar_values, tuple):\n            typevar_values = (typevar_values,)\n\n        # For a model `class Model[T, U, V = int](BaseModel): ...` parametrized with `(str, bool)`,\n        # this gives us `{T: str, U: bool, V: int}`:\n        typevars_map = _generics.map_generic_model_arguments(cls, typevar_values)\n        # We also update the provided args to use defaults values (`(str, bool)` becomes `(str, bool, int)`):\n        typevar_values = tuple(v for v in typevars_map.values())\n\n        if _utils.all_identical(typevars_map.keys(), typevars_map.values()) and typevars_map:\n            submodel = cls  # if arguments are equal to parameters it's the same object\n            _generics.set_cached_generic_type(cls, typevar_values, submodel)\n        else:\n            parent_args = cls.__pydantic_generic_metadata__['args']\n            if not parent_args:\n                args = typevar_values\n            else:\n                args = tuple(_generics.replace_types(arg, typevars_map) for arg in parent_args)\n\n            origin = cls.__pydantic_generic_metadata__['origin'] or cls\n            model_name = origin.model_parametrized_name(args)\n            params = tuple(\n                dict.fromkeys(_generics.iter_contained_typevars(typevars_map.values()))\n            )  # use dict as ordered set\n\n            with _generics.generic_recursion_self_type(origin, args) as maybe_self_type:\n                cached = _generics.get_cached_generic_type_late(cls, typevar_values, origin, args)\n                if cached is not None:\n                    return cached\n\n                if maybe_self_type is not None:\n                    return maybe_self_type\n\n                # Attempt to rebuild the origin in case new types have been defined\n                try:\n                    # depth 2 gets you above this __class_getitem__ call.\n                    # Note that we explicitly provide the parent ns, otherwise\n                    # `model_rebuild` will use the parent ns no matter if it is the ns of a module.\n                    # We don't want this here, as this has unexpected effects when a model\n                    # is being parametrized during a forward annotation evaluation.\n                    parent_ns = _typing_extra.parent_frame_namespace(parent_depth=2) or {}\n                    origin.model_rebuild(_types_namespace=parent_ns)\n                except PydanticUndefinedAnnotation:\n                    # It's okay if it fails, it just means there are still undefined types\n                    # that could be evaluated later.\n                    pass\n\n                submodel = _generics.create_generic_submodel(model_name, origin, args, params)\n\n                _generics.set_cached_generic_type(cls, typevar_values, submodel, origin, args)\n\n        return submodel", "metadata": {"license": "MIT", "len_tokens": 742}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def __deepcopy__(self, memo: dict[int, Any] | None = None) -> Self:\n        \"\"\"Returns a deep copy of the model.\"\"\"\n        cls = type(self)\n        m = cls.__new__(cls)\n        _object_setattr(m, '__dict__', deepcopy(self.__dict__, memo=memo))\n        _object_setattr(m, '__pydantic_extra__', deepcopy(self.__pydantic_extra__, memo=memo))\n        # This next line doesn't need a deepcopy because __pydantic_fields_set__ is a set[str],\n        # and attempting a deepcopy would be marginally slower.\n        _object_setattr(m, '__pydantic_fields_set__', copy(self.__pydantic_fields_set__))\n\n        if not hasattr(self, '__pydantic_private__') or self.__pydantic_private__ is None:\n            _object_setattr(m, '__pydantic_private__', None)\n        else:\n            _object_setattr(\n                m,\n                '__pydantic_private__',\n                deepcopy({k: v for k, v in self.__pydantic_private__.items() if v is not PydanticUndefined}, memo=memo),\n            )\n\n        return m", "metadata": {"license": "MIT", "len_tokens": 250}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def __repr_args__(self) -> _repr.ReprArgs:\n        # Eagerly create the repr of computed fields, as this may trigger access of cached properties and as such\n        # modify the instance's `__dict__`. If we don't do it now, it could happen when iterating over the `__dict__`\n        # below if the instance happens to be referenced in a field, and would modify the `__dict__` size *during* iteration.\n        computed_fields_repr_args = [\n            (k, getattr(self, k)) for k, v in self.__pydantic_computed_fields__.items() if v.repr\n        ]\n\n        for k, v in self.__dict__.items():\n            field = self.__pydantic_fields__.get(k)\n            if field and field.repr:\n                if v is not self:\n                    yield k, v\n                else:\n                    yield k, self.__repr_recursion__(v)\n        # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.\n        # This can happen if a `ValidationError` is raised during initialization and the instance's\n        # repr is generated as part of the exception handling. Therefore, we use `getattr` here\n        # with a fallback, even though the type hints indicate the attribute will always be present.\n        try:\n            pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')\n        except AttributeError:\n            pydantic_extra = None\n\n        if pydantic_extra is not None:\n            yield from ((k, v) for k, v in pydantic_extra.items())\n        yield from computed_fields_repr_args", "metadata": {"license": "MIT", "len_tokens": 355}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def json(  # noqa: D102\n        self,\n        *,\n        include: IncEx | None = None,\n        exclude: IncEx | None = None,\n        by_alias: bool = False,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n        encoder: Callable[[Any], Any] | None = PydanticUndefined,  # type: ignore[assignment]\n        models_as_dict: bool = PydanticUndefined,  # type: ignore[assignment]\n        **dumps_kwargs: Any,\n    ) -> str:\n        warnings.warn(\n            'The `json` method is deprecated; use `model_dump_json` instead.',\n            category=PydanticDeprecatedSince20,\n            stacklevel=2,\n        )\n        if encoder is not PydanticUndefined:\n            raise TypeError('The `encoder` argument is no longer supported; use field serializers instead.')\n        if models_as_dict is not PydanticUndefined:\n            raise TypeError('The `models_as_dict` argument is no longer supported; use a model serializer instead.')\n        if dumps_kwargs:\n            raise TypeError('`dumps_kwargs` keyword arguments are no longer supported.')\n        return self.model_dump_json(\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n        )", "metadata": {"license": "MIT", "len_tokens": 301}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def parse_raw(  # noqa: D102\n        cls,\n        b: str | bytes,\n        *,\n        content_type: str | None = None,\n        encoding: str = 'utf8',\n        proto: DeprecatedParseProtocol | None = None,\n        allow_pickle: bool = False,\n    ) -> Self:  # pragma: no cover\n        warnings.warn(\n            'The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, '\n            'otherwise load the data then use `model_validate` instead.',\n            category=PydanticDeprecatedSince20,\n            stacklevel=2,\n        )\n        from .deprecated import parse\n\n        try:\n            obj = parse.load_str_bytes(\n                b,\n                proto=proto,\n                content_type=content_type,\n                encoding=encoding,\n                allow_pickle=allow_pickle,\n            )\n        except (ValueError, TypeError) as exc:\n            import json\n\n            # try to match V1\n            if isinstance(exc, UnicodeDecodeError):\n                type_str = 'value_error.unicodedecode'\n            elif isinstance(exc, json.JSONDecodeError):\n                type_str = 'value_error.jsondecode'\n            elif isinstance(exc, ValueError):\n                type_str = 'value_error'\n            else:\n                type_str = 'type_error'\n\n            # ctx is missing here, but since we've added `input` to the error, we're not pretending it's the same\n            error: pydantic_core.InitErrorDetails = {\n                # The type: ignore on the next line is to ignore the requirement of LiteralString\n                'type': pydantic_core.PydanticCustomError(type_str, str(exc)),  # type: ignore\n                'loc': ('__root__',),\n                'input': b,\n            }\n            raise pydantic_core.ValidationError.from_exception_data(cls.__name__, [error])\n        return cls.model_validate(obj)", "metadata": {"license": "MIT", "len_tokens": 394}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def copy(\n        self,\n        *,\n        include: AbstractSetIntStr | MappingIntStrAny | None = None,\n        exclude: AbstractSetIntStr | MappingIntStrAny | None = None,\n        update: Dict[str, Any] | None = None,  # noqa UP006\n        deep: bool = False,\n    ) -> Self:  # pragma: no cover\n        \"\"\"Returns a copy of the model.\n\n        !!! warning \"Deprecated\"\n            This method is now deprecated; use `model_copy` instead.\n\n        If you need `include` or `exclude`, use:\n\n        ```python {test=\"skip\" lint=\"skip\"}\n        data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n        data = {**data, **(update or {})}\n        copied = self.model_validate(data)\n        ```\n\n        Args:\n            include: Optional set or mapping specifying which fields to include in the copied model.\n            exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n            update: Optional dictionary of field-value pairs to override field values in the copied model.\n            deep: If True, the values of fields that are Pydantic models will be deep-copied.\n\n        Returns:\n            A copy of the model with included, excluded and updated fields as specified.\n        \"\"\"\n        warnings.warn(\n            'The `copy` method is deprecated; use `model_copy` instead. '\n            'See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`.',\n            category=PydanticDeprecatedSince20,\n            stacklevel=2,\n        )\n        from .deprecated import copy_internals\n\n        values = dict(\n            copy_internals._iter(\n                self, to_dict=False, by_alias=False, include=include, exclude=exclude, exclude_unset=False\n            ),\n            **(update or {}),\n        )\n        if self.__pydantic_private__ is None:\n            private = None\n        else:\n            private = {k: v for k, v in self.__pydantic_private__.items() if v is not PydanticUndefined}\n\n        if self.__pydantic_extra__ is None:\n            extra: dict[str, Any] | None = None\n        else:\n            extra = self.__pydantic_extra__.copy()\n            for k in list(self.__pydantic_extra__):\n                if k not in values:  # k was in the exclude\n                    extra.pop(k)\n            for k in list(values):\n                if k in self.__pydantic_extra__:  # k must have come from extra\n                    extra[k] = values.pop(k)\n\n        # new `__pydantic_fields_set__` can have unset optional fields with a set value in `update` kwarg\n        if update:\n            fields_set = self.__pydantic_fields_set__ | update.keys()\n        else:\n            fields_set = set(self.__pydantic_fields_set__)\n\n        # removing excluded fields from `__pydantic_fields_set__`\n        if exclude:\n            fields_set -= set(exclude)\n\n        return copy_internals._copy_and_set_values(self, values, fields_set, extra, private, deep=deep)", "metadata": {"license": "MIT", "len_tokens": 670}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def __getattr__(self, item: str) -> Any:\n            private_attributes = object.__getattribute__(self, '__private_attributes__')\n            if item in private_attributes:\n                attribute = private_attributes[item]\n                if hasattr(attribute, '__get__'):\n                    return attribute.__get__(self, type(self))  # type: ignore\n\n                try:\n                    # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items\n                    return self.__pydantic_private__[item]  # type: ignore\n                except KeyError as exc:\n                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\n            else:\n                # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.\n                # See `BaseModel.__repr_args__` for more details\n                try:\n                    pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')\n                except AttributeError:\n                    pydantic_extra = None\n\n                if pydantic_extra and item in pydantic_extra:\n                    return pydantic_extra[item]\n                else:\n                    if hasattr(self.__class__, item):\n                        return super().__getattribute__(item)  # Raises AttributeError if appropriate\n                    else:\n                        # this is the current error\n                        raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')", "metadata": {"license": "MIT", "len_tokens": 309}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def _setattr_handler(self, name: str, value: Any) -> Callable[[BaseModel, str, Any], None] | None:\n            \"\"\"Get a handler for setting an attribute on the model instance.\n\n            Returns:\n                A handler for setting an attribute on the model instance. Used for memoization of the handler.\n                Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`\n                Returns `None` when memoization is not safe, then the attribute is set directly.\n            \"\"\"\n            cls = self.__class__\n            if name in cls.__class_vars__:\n                raise AttributeError(\n                    f'{name!r} is a ClassVar of `{cls.__name__}` and cannot be set on an instance. '\n                    f'If you want to set a value on the class, use `{cls.__name__}.{name} = value`.'\n                )\n            elif not _fields.is_valid_field_name(name):\n                if (attribute := cls.__private_attributes__.get(name)) is not None:\n                    if hasattr(attribute, '__set__'):\n                        return lambda model, _name, val: attribute.__set__(model, val)\n                    else:\n                        return _SIMPLE_SETATTR_HANDLERS['private']\n                else:\n                    _object_setattr(self, name, value)\n                    return None  # Can not return memoized handler with possibly freeform attr names\n\n            attr = getattr(cls, name, None)\n            # NOTE: We currently special case properties and `cached_property`, but we might need\n            # to generalize this to all data/non-data descriptors at some point. For non-data descriptors\n            # (such as `cached_property`), it isn't obvious though. `cached_property` caches the value\n            # to the instance's `__dict__`, but other non-data descriptors might do things differently.\n            if isinstance(attr, cached_property):\n                return _SIMPLE_SETATTR_HANDLERS['cached_property']\n\n            _check_frozen(cls, name, value)\n\n            # We allow properties to be set only on non frozen models for now (to match dataclasses).\n            # This can be changed if it ever gets requested.\n            if isinstance(attr, property):\n                return lambda model, _name, val: attr.__set__(model, val)\n            elif cls.model_config.get('validate_assignment'):\n                return _SIMPLE_SETATTR_HANDLERS['validate_assignment']\n            elif name not in cls.__pydantic_fields__:\n                if cls.model_config.get('extra') != 'allow':\n                    # TODO - matching error\n                    raise ValueError(f'\"{cls.__name__}\" object has no field \"{name}\"')\n                elif attr is None:\n                    # attribute does not exist, so put it in extra\n                    self.__pydantic_extra__[name] = value\n                    return None  # Can not return memoized handler with possibly freeform attr names\n                else:\n                    # attribute _does_ exist, and was not in extra, so update it\n                    return _SIMPLE_SETATTR_HANDLERS['extra_known']\n            else:\n                return _SIMPLE_SETATTR_HANDLERS['model_field']", "metadata": {"license": "MIT", "len_tokens": 651}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def __delattr__(self, item: str) -> Any:\n            cls = self.__class__\n\n            if item in self.__private_attributes__:\n                attribute = self.__private_attributes__[item]\n                if hasattr(attribute, '__delete__'):\n                    attribute.__delete__(self)  # type: ignore\n                    return\n\n                try:\n                    # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items\n                    del self.__pydantic_private__[item]  # type: ignore\n                    return\n                except KeyError as exc:\n                    raise AttributeError(f'{cls.__name__!r} object has no attribute {item!r}') from exc\n\n            # Allow cached properties to be deleted (even if the class is frozen):\n            attr = getattr(cls, item, None)\n            if isinstance(attr, cached_property):\n                return object.__delattr__(self, item)\n\n            _check_frozen(cls, name=item, value=None)\n\n            if item in self.__pydantic_fields__:\n                object.__delattr__(self, item)\n            elif self.__pydantic_extra__ is not None and item in self.__pydantic_extra__:\n                del self.__pydantic_extra__[item]\n            else:\n                try:\n                    object.__delattr__(self, item)\n                except AttributeError:\n                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')", "metadata": {"license": "MIT", "len_tokens": 302}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def __eq__(self, other: Any) -> bool:\n            if isinstance(other, BaseModel):\n                # When comparing instances of generic types for equality, as long as all field values are equal,\n                # only require their generic origin types to be equal, rather than exact type equality.\n                # This prevents headaches like MyGeneric(x=1) != MyGeneric[Any](x=1).\n                self_type = self.__pydantic_generic_metadata__['origin'] or self.__class__\n                other_type = other.__pydantic_generic_metadata__['origin'] or other.__class__\n\n                # Perform common checks first\n                if not (\n                    self_type == other_type\n                    and getattr(self, '__pydantic_private__', None) == getattr(other, '__pydantic_private__', None)\n                    and self.__pydantic_extra__ == other.__pydantic_extra__\n                ):\n                    return False\n\n                # We only want to compare pydantic fields but ignoring fields is costly.\n                # We'll perform a fast check first, and fallback only when needed\n                # See GH-7444 and GH-7825 for rationale and a performance benchmark\n\n                # First, do the fast (and sometimes faulty) __dict__ comparison\n                if self.__dict__ == other.__dict__:\n                    # If the check above passes, then pydantic fields are equal, we can return early\n                    return True\n\n                # We don't want to trigger unnecessary costly filtering of __dict__ on all unequal objects, so we return\n                # early if there are no keys to ignore (we would just return False later on anyway)\n                model_fields = type(self).__pydantic_fields__.keys()\n                if self.__dict__.keys() <= model_fields and other.__dict__.keys() <= model_fields:\n                    return False\n\n                # If we reach here, there are non-pydantic-fields keys, mapped to unequal values, that we need to ignore\n                # Resort to costly filtering of the __dict__ objects\n                # We use operator.itemgetter because it is much faster than dict comprehensions\n                # NOTE: Contrary to standard python class and instances, when the Model class has a default value for an\n                # attribute and the model instance doesn't have a corresponding attribute, accessing the missing attribute\n                # raises an error in BaseModel.__getattr__ instead of returning the class attribute\n                # So we can use operator.itemgetter() instead of operator.attrgetter()\n                getter = operator.itemgetter(*model_fields) if model_fields else lambda _: _utils._SENTINEL\n                try:\n                    return getter(self.__dict__) == getter(other.__dict__)\n                except KeyError:\n                    # In rare cases (such as when using the deprecated BaseModel.copy() method),\n                    # the __dict__ may not contain all model fields, which is how we can get here.\n                    # getter(self.__dict__) is much faster than any 'safe' method that accounts\n                    # for missing keys, and wrapping it in a `try` doesn't slow things down much\n                    # in the common case.\n                    self_fields_proxy = _utils.SafeGetItemProxy(self.__dict__)\n                    other_fields_proxy = _utils.SafeGetItemProxy(other.__dict__)\n                    return getter(self_fields_proxy) == getter(other_fields_proxy)\n\n            # other instance is not a BaseModel\n            else:\n                return NotImplemented", "metadata": {"license": "MIT", "len_tokens": 699}}
{"id": "pydantic:pydantic/main.py", "language": "python", "code": "def __init_subclass__(cls, **kwargs: Unpack[ConfigDict]):\n            \"\"\"This signature is included purely to help type-checkers check arguments to class declaration, which\n            provides a way to conveniently set model_config key/value pairs.\n\n            ```python\n            from pydantic import BaseModel\n\n            class MyModel(BaseModel, extra='allow'): ...\n            ```\n\n            However, this may be deceiving, since the _actual_ calls to `__init_subclass__` will not receive any\n            of the config arguments, and will only receive any keyword arguments passed during class initialization\n            that are _not_ expected keys in ConfigDict. (This is due to the way `ModelMetaclass.__new__` works.)\n\n            Args:\n                **kwargs: Keyword arguments passed to the class definition, which set model_config\n\n            Note:\n                You may want to override `__pydantic_init_subclass__` instead, which behaves similarly but is called\n                *after* the class is fully initialized.\n            \"\"\"", "metadata": {"license": "MIT", "len_tokens": 213}}
{"id": "pydantic:pydantic/annotated_handlers.py", "language": "python", "code": "class GetJsonSchemaHandler:\n    \"\"\"Handler to call into the next JSON schema generation function.\n\n    Attributes:\n        mode: Json schema mode, can be `validation` or `serialization`.\n    \"\"\"\n\n    mode: JsonSchemaMode\n\n    def __call__(self, core_schema: CoreSchemaOrField, /) -> JsonSchemaValue:\n        \"\"\"Call the inner handler and get the JsonSchemaValue it returns.\n        This will call the next JSON schema modifying function up until it calls\n        into `pydantic.json_schema.GenerateJsonSchema`, which will raise a\n        `pydantic.errors.PydanticInvalidForJsonSchema` error if it cannot generate\n        a JSON schema.\n\n        Args:\n            core_schema: A `pydantic_core.core_schema.CoreSchema`.\n\n        Returns:\n            JsonSchemaValue: The JSON schema generated by the inner JSON schema modify\n            functions.\n        \"\"\"\n        raise NotImplementedError\n\n    def resolve_ref_schema(self, maybe_ref_json_schema: JsonSchemaValue, /) -> JsonSchemaValue:\n        \"\"\"Get the real schema for a `{\"$ref\": ...}` schema.\n        If the schema given is not a `$ref` schema, it will be returned as is.\n        This means you don't have to check before calling this function.\n\n        Args:\n            maybe_ref_json_schema: A JsonSchemaValue which may be a `$ref` schema.\n\n        Raises:\n            LookupError: If the ref is not found.\n\n        Returns:\n            JsonSchemaValue: A JsonSchemaValue that has no `$ref`.\n        \"\"\"\n        raise NotImplementedError", "metadata": {"license": "MIT", "len_tokens": 323}}
{"id": "pydantic:pydantic/annotated_handlers.py", "language": "python", "code": "class GetCoreSchemaHandler:\n    \"\"\"Handler to call into the next CoreSchema schema generation function.\"\"\"\n\n    def __call__(self, source_type: Any, /) -> core_schema.CoreSchema:\n        \"\"\"Call the inner handler and get the CoreSchema it returns.\n        This will call the next CoreSchema modifying function up until it calls\n        into Pydantic's internal schema generation machinery, which will raise a\n        `pydantic.errors.PydanticSchemaGenerationError` error if it cannot generate\n        a CoreSchema for the given source type.\n\n        Args:\n            source_type: The input type.\n\n        Returns:\n            CoreSchema: The `pydantic-core` CoreSchema generated.\n        \"\"\"\n        raise NotImplementedError\n\n    def generate_schema(self, source_type: Any, /) -> core_schema.CoreSchema:\n        \"\"\"Generate a schema unrelated to the current context.\n        Use this function if e.g. you are handling schema generation for a sequence\n        and want to generate a schema for its items.\n        Otherwise, you may end up doing something like applying a `min_length` constraint\n        that was intended for the sequence itself to its items!\n\n        Args:\n            source_type: The input type.\n\n        Returns:\n            CoreSchema: The `pydantic-core` CoreSchema generated.\n        \"\"\"\n        raise NotImplementedError\n\n    def resolve_ref_schema(self, maybe_ref_schema: core_schema.CoreSchema, /) -> core_schema.CoreSchema:\n        \"\"\"Get the real schema for a `definition-ref` schema.\n        If the schema given is not a `definition-ref` schema, it will be returned as is.\n        This means you don't have to check before calling this function.\n\n        Args:\n            maybe_ref_schema: A `CoreSchema`, `ref`-based or not.\n\n        Raises:\n            LookupError: If the `ref` is not found.\n\n        Returns:\n            A concrete `CoreSchema`.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def field_name(self) -> str | None:\n        \"\"\"Get the name of the closest field to this validator.\"\"\"\n        raise NotImplementedError\n\n    def _get_types_namespace(self) -> NamespacesTuple:\n        \"\"\"Internal method used during type resolution for serializer annotations.\"\"\"\n        raise NotImplementedError", "metadata": {"license": "MIT", "len_tokens": 467}}
{"id": "pydantic:pydantic/v1/_hypothesis_plugin.py", "language": "python", "code": "def resolve_json(cls):  # type: ignore[no-untyped-def]\n    try:\n        inner = st.none() if cls.inner_type is None else st.from_type(cls.inner_type)\n    except Exception:  # pragma: no cover\n        finite = st.floats(allow_infinity=False, allow_nan=False)\n        inner = st.recursive(\n            base=st.one_of(st.none(), st.booleans(), st.integers(), finite, st.text()),\n            extend=lambda x: st.lists(x) | st.dictionaries(st.text(), x),  # type: ignore\n        )\n    inner_type = getattr(cls, 'inner_type', None)\n    return st.builds(\n        cls.inner_type.json if lenient_issubclass(inner_type, pydantic.BaseModel) else json.dumps,\n        inner,\n        ensure_ascii=st.booleans(),\n        indent=st.none() | st.integers(0, 16),\n        sort_keys=st.booleans(),\n    )", "metadata": {"license": "MIT", "len_tokens": 209}}
{"id": "pydantic:pydantic/v1/_hypothesis_plugin.py", "language": "python", "code": "def resolve_conbytes(cls):  # type: ignore[no-untyped-def]  # pragma: no cover\n    min_size = cls.min_length or 0\n    max_size = cls.max_length\n    if not cls.strip_whitespace:\n        return st.binary(min_size=min_size, max_size=max_size)\n    # Fun with regex to ensure we neither start nor end with whitespace\n    repeats = '{{{},{}}}'.format(\n        min_size - 2 if min_size > 2 else 0,\n        max_size - 2 if (max_size or 0) > 2 else '',\n    )\n    if min_size >= 2:\n        pattern = rf'\\W.{repeats}\\W'\n    elif min_size == 1:\n        pattern = rf'\\W(.{repeats}\\W)?'\n    else:\n        assert min_size == 0\n        pattern = rf'(\\W(.{repeats}\\W)?)?'\n    return st.from_regex(pattern.encode(), fullmatch=True)", "metadata": {"license": "MIT", "len_tokens": 212}}
{"id": "pydantic:pydantic/v1/_hypothesis_plugin.py", "language": "python", "code": "def resolve_confloat(cls):  # type: ignore[no-untyped-def]\n    min_value = cls.ge\n    max_value = cls.le\n    exclude_min = False\n    exclude_max = False\n\n    if cls.gt is not None:\n        assert min_value is None, 'Set `gt` or `ge`, but not both'\n        min_value = cls.gt\n        exclude_min = True\n    if cls.lt is not None:\n        assert max_value is None, 'Set `lt` or `le`, but not both'\n        max_value = cls.lt\n        exclude_max = True\n\n    if cls.multiple_of is None:\n        return st.floats(min_value, max_value, exclude_min=exclude_min, exclude_max=exclude_max, allow_nan=False)\n\n    if min_value is not None:\n        min_value = math.ceil(min_value / cls.multiple_of)\n        if exclude_min:\n            min_value = min_value + 1\n    if max_value is not None:\n        assert max_value >= cls.multiple_of, 'Cannot build model with max value smaller than multiple of'\n        max_value = math.floor(max_value / cls.multiple_of)\n        if exclude_max:\n            max_value = max_value - 1\n\n    return st.integers(min_value, max_value).map(lambda x: x * cls.multiple_of)", "metadata": {"license": "MIT", "len_tokens": 281}}
{"id": "pydantic:pydantic/v1/_hypothesis_plugin.py", "language": "python", "code": "def resolve_conint(cls):  # type: ignore[no-untyped-def]\n    min_value = cls.ge\n    max_value = cls.le\n    if cls.gt is not None:\n        assert min_value is None, 'Set `gt` or `ge`, but not both'\n        min_value = cls.gt + 1\n    if cls.lt is not None:\n        assert max_value is None, 'Set `lt` or `le`, but not both'\n        max_value = cls.lt - 1\n\n    if cls.multiple_of is None or cls.multiple_of == 1:\n        return st.integers(min_value, max_value)\n\n    # These adjustments and the .map handle integer-valued multiples, while the\n    # .filter handles trickier cases as for confloat.\n    if min_value is not None:\n        min_value = math.ceil(Fraction(min_value) / Fraction(cls.multiple_of))\n    if max_value is not None:\n        max_value = math.floor(Fraction(max_value) / Fraction(cls.multiple_of))\n    return st.integers(min_value, max_value).map(lambda x: x * cls.multiple_of)", "metadata": {"license": "MIT", "len_tokens": 241}}
{"id": "pydantic:pydantic/v1/_hypothesis_plugin.py", "language": "python", "code": "def resolve_constr(cls):  # type: ignore[no-untyped-def]  # pragma: no cover\n    min_size = cls.min_length or 0\n    max_size = cls.max_length\n\n    if cls.regex is None and not cls.strip_whitespace:\n        return st.text(min_size=min_size, max_size=max_size)\n\n    if cls.regex is not None:\n        strategy = st.from_regex(cls.regex)\n        if cls.strip_whitespace:\n            strategy = strategy.filter(lambda s: s == s.strip())\n    elif cls.strip_whitespace:\n        repeats = '{{{},{}}}'.format(\n            min_size - 2 if min_size > 2 else 0,\n            max_size - 2 if (max_size or 0) > 2 else '',\n        )\n        if min_size >= 2:\n            strategy = st.from_regex(rf'\\W.{repeats}\\W')\n        elif min_size == 1:\n            strategy = st.from_regex(rf'\\W(.{repeats}\\W)?')\n        else:\n            assert min_size == 0\n            strategy = st.from_regex(rf'(\\W(.{repeats}\\W)?)?')\n\n    if min_size == 0 and max_size is None:\n        return strategy\n    elif max_size is None:\n        return strategy.filter(lambda s: min_size <= len(s))\n    return strategy.filter(lambda s: min_size <= len(s) <= max_size)", "metadata": {"license": "MIT", "len_tokens": 300}}
{"id": "pydantic:pydantic/v1/color.py", "language": "python", "code": "def parse_str(value: str) -> RGBA:\n    \"\"\"\n    Parse a string to an RGBA tuple, trying the following formats (in this order):\n    * named color, see COLORS_BY_NAME below\n    * hex short eg. `<prefix>fff` (prefix can be `#`, `0x` or nothing)\n    * hex long eg. `<prefix>ffffff` (prefix can be `#`, `0x` or nothing)\n    * `rgb(<r>, <g>, <b>) `\n    * `rgba(<r>, <g>, <b>, <a>)`\n    \"\"\"\n    value_lower = value.lower()\n    try:\n        r, g, b = COLORS_BY_NAME[value_lower]\n    except KeyError:\n        pass\n    else:\n        return ints_to_rgba(r, g, b, None)\n\n    m = re.fullmatch(r_hex_short, value_lower)\n    if m:\n        *rgb, a = m.groups()\n        r, g, b = (int(v * 2, 16) for v in rgb)\n        if a:\n            alpha: Optional[float] = int(a * 2, 16) / 255\n        else:\n            alpha = None\n        return ints_to_rgba(r, g, b, alpha)\n\n    m = re.fullmatch(r_hex_long, value_lower)\n    if m:\n        *rgb, a = m.groups()\n        r, g, b = (int(v, 16) for v in rgb)\n        if a:\n            alpha = int(a, 16) / 255\n        else:\n            alpha = None\n        return ints_to_rgba(r, g, b, alpha)\n\n    m = re.fullmatch(r_rgb, value_lower)\n    if m:\n        return ints_to_rgba(*m.groups(), None)  # type: ignore\n\n    m = re.fullmatch(r_rgba, value_lower)\n    if m:\n        return ints_to_rgba(*m.groups())  # type: ignore\n\n    m = re.fullmatch(r_hsl, value_lower)\n    if m:\n        h, h_units, s, l_ = m.groups()\n        return parse_hsl(h, h_units, s, l_)\n\n    m = re.fullmatch(r_hsla, value_lower)\n    if m:\n        h, h_units, s, l_, a = m.groups()\n        return parse_hsl(h, h_units, s, l_, parse_float_alpha(a))\n\n    raise ColorError(reason='string not recognised as a valid color')", "metadata": {"license": "MIT", "len_tokens": 531}}
{"id": "pydantic:pydantic/v1/color.py", "language": "python", "code": "def as_rgb_tuple(self, *, alpha: Optional[bool] = None) -> ColorTuple:\n        \"\"\"\n        Color as an RGB or RGBA tuple; red, green and blue are in the range 0 to 255, alpha if included is\n        in the range 0 to 1.\n\n        :param alpha: whether to include the alpha channel, options are\n          None - (default) include alpha only if it's set (e.g. not None)\n          True - always include alpha,\n          False - always omit alpha,\n        \"\"\"\n        r, g, b = (float_to_255(c) for c in self._rgba[:3])\n        if alpha is None:\n            if self._rgba.alpha is None:\n                return r, g, b\n            else:\n                return r, g, b, self._alpha_float()\n        elif alpha:\n            return r, g, b, self._alpha_float()\n        else:\n            # alpha is False\n            return r, g, b", "metadata": {"license": "MIT", "len_tokens": 207}}
{"id": "pydantic:pydantic/v1/color.py", "language": "python", "code": "def as_hsl_tuple(self, *, alpha: Optional[bool] = None) -> HslColorTuple:\n        \"\"\"\n        Color as an HSL or HSLA tuple, e.g. hue, saturation, lightness and optionally alpha; all elements are in\n        the range 0 to 1.\n\n        NOTE: this is HSL as used in HTML and most other places, not HLS as used in python's colorsys.\n\n        :param alpha: whether to include the alpha channel, options are\n          None - (default) include alpha only if it's set (e.g. not None)\n          True - always include alpha,\n          False - always omit alpha,\n        \"\"\"\n        h, l, s = rgb_to_hls(self._rgba.r, self._rgba.g, self._rgba.b)\n        if alpha is None:\n            if self._rgba.alpha is None:\n                return h, s, l\n            else:\n                return h, s, l, self._alpha_float()\n        if alpha:\n            return h, s, l, self._alpha_float()\n        else:\n            # alpha is False\n            return h, s, l", "metadata": {"license": "MIT", "len_tokens": 238}}
{"id": "pydantic:pydantic/v1/config.py", "language": "python", "code": "class BaseConfig:\n    title: Optional[str] = None\n    anystr_lower: bool = False\n    anystr_upper: bool = False\n    anystr_strip_whitespace: bool = False\n    min_anystr_length: int = 0\n    max_anystr_length: Optional[int] = None\n    validate_all: bool = False\n    extra: Extra = Extra.ignore\n    allow_mutation: bool = True\n    frozen: bool = False\n    allow_population_by_field_name: bool = False\n    use_enum_values: bool = False\n    fields: Dict[str, Union[str, Dict[str, str]]] = {}\n    validate_assignment: bool = False\n    error_msg_templates: Dict[str, str] = {}\n    arbitrary_types_allowed: bool = False\n    orm_mode: bool = False\n    getter_dict: Type[GetterDict] = GetterDict\n    alias_generator: Optional[Callable[[str], str]] = None\n    keep_untouched: Tuple[type, ...] = ()\n    schema_extra: Union[Dict[str, Any], 'SchemaExtraCallable'] = {}\n    json_loads: Callable[[str], Any] = json.loads\n    json_dumps: Callable[..., str] = json.dumps\n    json_encoders: Dict[Union[Type[Any], str, ForwardRef], AnyCallable] = {}\n    underscore_attrs_are_private: bool = False\n    allow_inf_nan: bool = True\n\n    # whether inherited models as fields should be reconstructed as base model,\n    # and whether such a copy should be shallow or deep\n    copy_on_model_validation: Literal['none', 'deep', 'shallow'] = 'shallow'\n\n    # whether `Union` should check all allowed types before even trying to coerce\n    smart_union: bool = False\n    # whether dataclass `__post_init__` should be run before or after validation\n    post_init_call: Literal['before_validation', 'after_validation'] = 'before_validation'\n\n    @classmethod\n    def get_field_info(cls, name: str) -> Dict[str, Any]:\n        \"\"\"\n        Get properties of FieldInfo from the `fields` property of the config class.\n        \"\"\"\n\n        fields_value = cls.fields.get(name)\n\n        if isinstance(fields_value, str):\n            field_info: Dict[str, Any] = {'alias': fields_value}\n        elif isinstance(fields_value, dict):\n            field_info = fields_value\n        else:\n            field_info = {}\n\n        if 'alias' in field_info:\n            field_info.setdefault('alias_priority', 2)\n\n        if field_info.get('alias_priority', 0) <= 1 and cls.alias_generator:\n            alias = cls.alias_generator(name)\n            if not isinstance(alias, str):\n                raise TypeError(f'Config.alias_generator must return str, not {alias.__class__}')\n            field_info.update(alias=alias, alias_priority=1)\n        return field_info\n\n    @classmethod\n    def prepare_field(cls, field: 'ModelField') -> None:\n        \"\"\"\n        Optional hook to check or modify fields during model creation.\n        \"\"\"\n        pass", "metadata": {"license": "MIT", "len_tokens": 640}}
{"id": "pydantic:pydantic/v1/config.py", "language": "python", "code": "class ConfigDict(TypedDict, total=False):\n        title: Optional[str]\n        anystr_lower: bool\n        anystr_strip_whitespace: bool\n        min_anystr_length: int\n        max_anystr_length: Optional[int]\n        validate_all: bool\n        extra: Extra\n        allow_mutation: bool\n        frozen: bool\n        allow_population_by_field_name: bool\n        use_enum_values: bool\n        fields: Dict[str, Union[str, Dict[str, str]]]\n        validate_assignment: bool\n        error_msg_templates: Dict[str, str]\n        arbitrary_types_allowed: bool\n        orm_mode: bool\n        getter_dict: Type[GetterDict]\n        alias_generator: Optional[Callable[[str], str]]\n        keep_untouched: Tuple[type, ...]\n        schema_extra: Union[Dict[str, object], 'SchemaExtraCallable']\n        json_loads: Callable[[str], object]\n        json_dumps: AnyArgTCallable[str]\n        json_encoders: Dict[Type[object], AnyCallable]\n        underscore_attrs_are_private: bool\n        allow_inf_nan: bool\n        copy_on_model_validation: Literal['none', 'deep', 'shallow']\n        # whether dataclass `__post_init__` should be run after validation\n        post_init_call: Literal['before_validation', 'after_validation']", "metadata": {"license": "MIT", "len_tokens": 279}}
{"id": "pydantic:pydantic/v1/version.py", "language": "python", "code": "__all__ = 'compiled', 'VERSION', 'version_info'\n\nVERSION = '1.10.21'\n\ntry:\n    import cython  # type: ignore\nexcept ImportError:\n    compiled: bool = False\nelse:  # pragma: no cover\n    try:\n        compiled = cython.compiled\n    except AttributeError:\n        compiled = False\n\n\ndef version_info() -> str:\n    import platform\n    import sys\n    from importlib import import_module\n    from pathlib import Path\n\n    optional_deps = []\n    for p in ('devtools', 'dotenv', 'email-validator', 'typing-extensions'):\n        try:\n            import_module(p.replace('-', '_'))\n        except ImportError:\n            continue\n        optional_deps.append(p)\n\n    info = {\n        'pydantic version': VERSION,\n        'pydantic compiled': compiled,\n        'install path': Path(__file__).resolve().parent,\n        'python version': sys.version,\n        'platform': platform.platform(),\n        'optional deps. installed': optional_deps,\n    }\n    return '\\n'.join('{:>30} {}'.format(k + ':', str(v).replace('\\n', ' ')) for k, v in info.items())\n", "metadata": {"license": "MIT", "len_tokens": 255}}
{"id": "pydantic:pydantic/v1/fields.py", "language": "python", "code": "def __init__(self, default: Any = Undefined, **kwargs: Any) -> None:\n        self.default = default\n        self.default_factory = kwargs.pop('default_factory', None)\n        self.alias = kwargs.pop('alias', None)\n        self.alias_priority = kwargs.pop('alias_priority', 2 if self.alias is not None else None)\n        self.title = kwargs.pop('title', None)\n        self.description = kwargs.pop('description', None)\n        self.exclude = kwargs.pop('exclude', None)\n        self.include = kwargs.pop('include', None)\n        self.const = kwargs.pop('const', None)\n        self.gt = kwargs.pop('gt', None)\n        self.ge = kwargs.pop('ge', None)\n        self.lt = kwargs.pop('lt', None)\n        self.le = kwargs.pop('le', None)\n        self.multiple_of = kwargs.pop('multiple_of', None)\n        self.allow_inf_nan = kwargs.pop('allow_inf_nan', None)\n        self.max_digits = kwargs.pop('max_digits', None)\n        self.decimal_places = kwargs.pop('decimal_places', None)\n        self.min_items = kwargs.pop('min_items', None)\n        self.max_items = kwargs.pop('max_items', None)\n        self.unique_items = kwargs.pop('unique_items', None)\n        self.min_length = kwargs.pop('min_length', None)\n        self.max_length = kwargs.pop('max_length', None)\n        self.allow_mutation = kwargs.pop('allow_mutation', True)\n        self.regex = kwargs.pop('regex', None)\n        self.discriminator = kwargs.pop('discriminator', None)\n        self.repr = kwargs.pop('repr', True)\n        self.extra = kwargs", "metadata": {"license": "MIT", "len_tokens": 348}}
{"id": "pydantic:pydantic/v1/fields.py", "language": "python", "code": "def __init__(\n        self,\n        *,\n        name: str,\n        type_: Type[Any],\n        class_validators: Optional[Dict[str, Validator]],\n        model_config: Type['BaseConfig'],\n        default: Any = None,\n        default_factory: Optional[NoArgAnyCallable] = None,\n        required: 'BoolUndefined' = Undefined,\n        final: bool = False,\n        alias: Optional[str] = None,\n        field_info: Optional[FieldInfo] = None,\n    ) -> None:\n        self.name: str = name\n        self.has_alias: bool = alias is not None\n        self.alias: str = alias if alias is not None else name\n        self.annotation = type_\n        self.type_: Any = convert_generics(type_)\n        self.outer_type_: Any = type_\n        self.class_validators = class_validators or {}\n        self.default: Any = default\n        self.default_factory: Optional[NoArgAnyCallable] = default_factory\n        self.required: 'BoolUndefined' = required\n        self.final: bool = final\n        self.model_config = model_config\n        self.field_info: FieldInfo = field_info or FieldInfo(default)\n        self.discriminator_key: Optional[str] = self.field_info.discriminator\n        self.discriminator_alias: Optional[str] = self.discriminator_key\n\n        self.allow_none: bool = False\n        self.validate_always: bool = False\n        self.sub_fields: Optional[List[ModelField]] = None\n        self.sub_fields_mapping: Optional[Dict[str, 'ModelField']] = None  # used for discriminated union\n        self.key_field: Optional[ModelField] = None\n        self.validators: 'ValidatorsList' = []\n        self.pre_validators: Optional['ValidatorsList'] = None\n        self.post_validators: Optional['ValidatorsList'] = None\n        self.parse_json: bool = False\n        self.shape: int = SHAPE_SINGLETON\n        self.model_config.prepare_field(self)\n        self.prepare()", "metadata": {"license": "MIT", "len_tokens": 415}}
{"id": "pydantic:pydantic/v1/fields.py", "language": "python", "code": "def _get_field_info(\n        field_name: str, annotation: Any, value: Any, config: Type['BaseConfig']\n    ) -> Tuple[FieldInfo, Any]:\n        \"\"\"\n        Get a FieldInfo from a root typing.Annotated annotation, value, or config default.\n\n        The FieldInfo may be set in typing.Annotated or the value, but not both. If neither contain\n        a FieldInfo, a new one will be created using the config.\n\n        :param field_name: name of the field for use in error messages\n        :param annotation: a type hint such as `str` or `Annotated[str, Field(..., min_length=5)]`\n        :param value: the field's assigned value\n        :param config: the model's config object\n        :return: the FieldInfo contained in the `annotation`, the value, or a new one from the config.\n        \"\"\"\n        field_info_from_config = config.get_field_info(field_name)\n\n        field_info = None\n        if get_origin(annotation) is Annotated:\n            field_infos = [arg for arg in get_args(annotation)[1:] if isinstance(arg, FieldInfo)]\n            if len(field_infos) > 1:\n                raise ValueError(f'cannot specify multiple `Annotated` `Field`s for {field_name!r}')\n            field_info = next(iter(field_infos), None)\n            if field_info is not None:\n                field_info = copy.copy(field_info)\n                field_info.update_from_config(field_info_from_config)\n                if field_info.default not in (Undefined, Required):\n                    raise ValueError(f'`Field` default cannot be set in `Annotated` for {field_name!r}')\n                if value is not Undefined and value is not Required:\n                    # check also `Required` because of `validate_arguments` that sets `...` as default value\n                    field_info.default = value\n\n        if isinstance(value, FieldInfo):\n            if field_info is not None:\n                raise ValueError(f'cannot specify `Annotated` and value `Field`s together for {field_name!r}')\n            field_info = value\n            field_info.update_from_config(field_info_from_config)\n        elif field_info is None:\n            field_info = FieldInfo(value, **field_info_from_config)\n        value = None if field_info.default_factory is not None else field_info.default\n        field_info._validate()\n        return field_info, value", "metadata": {"license": "MIT", "len_tokens": 495}}
{"id": "pydantic:pydantic/v1/fields.py", "language": "python", "code": "def infer(\n        cls,\n        *,\n        name: str,\n        value: Any,\n        annotation: Any,\n        class_validators: Optional[Dict[str, Validator]],\n        config: Type['BaseConfig'],\n    ) -> 'ModelField':\n        from pydantic.v1.schema import get_annotation_from_field_info\n\n        field_info, value = cls._get_field_info(name, annotation, value, config)\n        required: 'BoolUndefined' = Undefined\n        if value is Required:\n            required = True\n            value = None\n        elif value is not Undefined:\n            required = False\n        annotation = get_annotation_from_field_info(annotation, field_info, name, config.validate_assignment)\n\n        return cls(\n            name=name,\n            type_=annotation,\n            alias=field_info.alias,\n            class_validators=class_validators,\n            default=value,\n            default_factory=field_info.default_factory,\n            required=required,\n            model_config=config,\n            field_info=field_info,\n        )", "metadata": {"license": "MIT", "len_tokens": 202}}
{"id": "pydantic:pydantic/v1/fields.py", "language": "python", "code": "def prepare_discriminated_union_sub_fields(self) -> None:\n        \"\"\"\n        Prepare the mapping <discriminator key> -> <ModelField> and update `sub_fields`\n        Note that this process can be aborted if a `ForwardRef` is encountered\n        \"\"\"\n        assert self.discriminator_key is not None\n\n        if self.type_.__class__ is DeferredType:\n            return\n\n        assert self.sub_fields is not None\n        sub_fields_mapping: Dict[str, 'ModelField'] = {}\n        all_aliases: Set[str] = set()\n\n        for sub_field in self.sub_fields:\n            t = sub_field.type_\n            if t.__class__ is ForwardRef:\n                # Stopping everything...will need to call `update_forward_refs`\n                return\n\n            alias, discriminator_values = get_discriminator_alias_and_values(t, self.discriminator_key)\n            all_aliases.add(alias)\n            for discriminator_value in discriminator_values:\n                sub_fields_mapping[discriminator_value] = sub_field\n\n        self.sub_fields_mapping = sub_fields_mapping\n        self.discriminator_alias = get_unique_discriminator_alias(all_aliases, self.discriminator_key)", "metadata": {"license": "MIT", "len_tokens": 234}}
{"id": "pydantic:pydantic/v1/fields.py", "language": "python", "code": "def populate_validators(self) -> None:\n        \"\"\"\n        Prepare self.pre_validators, self.validators, and self.post_validators based on self.type_'s  __get_validators__\n        and class validators. This method should be idempotent, e.g. it should be safe to call multiple times\n        without mis-configuring the field.\n        \"\"\"\n        self.validate_always = getattr(self.type_, 'validate_always', False) or any(\n            v.always for v in self.class_validators.values()\n        )\n\n        class_validators_ = self.class_validators.values()\n        if not self.sub_fields or self.shape == SHAPE_GENERIC:\n            get_validators = getattr(self.type_, '__get_validators__', None)\n            v_funcs = (\n                *[v.func for v in class_validators_ if v.each_item and v.pre],\n                *(get_validators() if get_validators else list(find_validators(self.type_, self.model_config))),\n                *[v.func for v in class_validators_ if v.each_item and not v.pre],\n            )\n            self.validators = prep_validators(v_funcs)\n\n        self.pre_validators = []\n        self.post_validators = []\n\n        if self.field_info and self.field_info.const:\n            self.post_validators.append(make_generic_validator(constant_validator))\n\n        if class_validators_:\n            self.pre_validators += prep_validators(v.func for v in class_validators_ if not v.each_item and v.pre)\n            self.post_validators += prep_validators(v.func for v in class_validators_ if not v.each_item and not v.pre)\n\n        if self.parse_json:\n            self.pre_validators.append(make_generic_validator(validate_json))\n\n        self.pre_validators = self.pre_validators or None\n        self.post_validators = self.post_validators or None", "metadata": {"license": "MIT", "len_tokens": 374}}
{"id": "pydantic:pydantic/v1/fields.py", "language": "python", "code": "def validate(\n        self, v: Any, values: Dict[str, Any], *, loc: 'LocStr', cls: Optional['ModelOrDc'] = None\n    ) -> 'ValidateReturn':\n        assert self.type_.__class__ is not DeferredType\n\n        if self.type_.__class__ is ForwardRef:\n            assert cls is not None\n            raise ConfigError(\n                f'field \"{self.name}\" not yet prepared so type is still a ForwardRef, '\n                f'you might need to call {cls.__name__}.update_forward_refs().'\n            )\n\n        errors: Optional['ErrorList']\n        if self.pre_validators:\n            v, errors = self._apply_validators(v, values, loc, cls, self.pre_validators)\n            if errors:\n                return v, errors\n\n        if v is None:\n            if is_none_type(self.type_):\n                # keep validating\n                pass\n            elif self.allow_none:\n                if self.post_validators:\n                    return self._apply_validators(v, values, loc, cls, self.post_validators)\n                else:\n                    return None, None\n            else:\n                return v, ErrorWrapper(NoneIsNotAllowedError(), loc)\n\n        if self.shape == SHAPE_SINGLETON:\n            v, errors = self._validate_singleton(v, values, loc, cls)\n        elif self.shape in MAPPING_LIKE_SHAPES:\n            v, errors = self._validate_mapping_like(v, values, loc, cls)\n        elif self.shape == SHAPE_TUPLE:\n            v, errors = self._validate_tuple(v, values, loc, cls)\n        elif self.shape == SHAPE_ITERABLE:\n            v, errors = self._validate_iterable(v, values, loc, cls)\n        elif self.shape == SHAPE_GENERIC:\n            v, errors = self._apply_validators(v, values, loc, cls, self.validators)\n        else:\n            #  sequence, list, set, generator, tuple with ellipsis, frozen set\n            v, errors = self._validate_sequence_like(v, values, loc, cls)\n\n        if not errors and self.post_validators:\n            v, errors = self._apply_validators(v, values, loc, cls, self.post_validators)\n        return v, errors", "metadata": {"license": "MIT", "len_tokens": 473}}
{"id": "pydantic:pydantic/v1/fields.py", "language": "python", "code": "def _validate_sequence_like(  # noqa: C901 (ignore complexity)\n        self, v: Any, values: Dict[str, Any], loc: 'LocStr', cls: Optional['ModelOrDc']\n    ) -> 'ValidateReturn':\n        \"\"\"\n        Validate sequence-like containers: lists, tuples, sets and generators\n        Note that large if-else blocks are necessary to enable Cython\n        optimization, which is why we disable the complexity check above.\n        \"\"\"\n        if not sequence_like(v):\n            e: errors_.PydanticTypeError\n            if self.shape == SHAPE_LIST:\n                e = errors_.ListError()\n            elif self.shape in (SHAPE_TUPLE, SHAPE_TUPLE_ELLIPSIS):\n                e = errors_.TupleError()\n            elif self.shape == SHAPE_SET:\n                e = errors_.SetError()\n            elif self.shape == SHAPE_FROZENSET:\n                e = errors_.FrozenSetError()\n            else:\n                e = errors_.SequenceError()\n            return v, ErrorWrapper(e, loc)\n\n        loc = loc if isinstance(loc, tuple) else (loc,)\n        result = []\n        errors: List[ErrorList] = []\n        for i, v_ in enumerate(v):\n            v_loc = *loc, i\n            r, ee = self._validate_singleton(v_, values, v_loc, cls)\n            if ee:\n                errors.append(ee)\n            else:\n                result.append(r)\n\n        if errors:\n            return v, errors\n\n        converted: Union[List[Any], Set[Any], FrozenSet[Any], Tuple[Any, ...], Iterator[Any], Deque[Any]] = result\n\n        if self.shape == SHAPE_SET:\n            converted = set(result)\n        elif self.shape == SHAPE_FROZENSET:\n            converted = frozenset(result)\n        elif self.shape == SHAPE_TUPLE_ELLIPSIS:\n            converted = tuple(result)\n        elif self.shape == SHAPE_DEQUE:\n            converted = deque(result, maxlen=getattr(v, 'maxlen', None))\n        elif self.shape == SHAPE_SEQUENCE:\n            if isinstance(v, tuple):\n                converted = tuple(result)\n            elif isinstance(v, set):\n                converted = set(result)\n            elif isinstance(v, Generator):\n                converted = iter(result)\n            elif isinstance(v, deque):\n                converted = deque(result, maxlen=getattr(v, 'maxlen', None))\n        return converted, None", "metadata": {"license": "MIT", "len_tokens": 506}}
{"id": "pydantic:pydantic/v1/fields.py", "language": "python", "code": "def _validate_tuple(\n        self, v: Any, values: Dict[str, Any], loc: 'LocStr', cls: Optional['ModelOrDc']\n    ) -> 'ValidateReturn':\n        e: Optional[Exception] = None\n        if not sequence_like(v):\n            e = errors_.TupleError()\n        else:\n            actual_length, expected_length = len(v), len(self.sub_fields)  # type: ignore\n            if actual_length != expected_length:\n                e = errors_.TupleLengthError(actual_length=actual_length, expected_length=expected_length)\n\n        if e:\n            return v, ErrorWrapper(e, loc)\n\n        loc = loc if isinstance(loc, tuple) else (loc,)\n        result = []\n        errors: List[ErrorList] = []\n        for i, (v_, field) in enumerate(zip(v, self.sub_fields)):  # type: ignore\n            v_loc = *loc, i\n            r, ee = field.validate(v_, values, loc=v_loc, cls=cls)\n            if ee:\n                errors.append(ee)\n            else:\n                result.append(r)\n\n        if errors:\n            return v, errors\n        else:\n            return tuple(result), None", "metadata": {"license": "MIT", "len_tokens": 248}}
{"id": "pydantic:pydantic/v1/fields.py", "language": "python", "code": "def _validate_mapping_like(\n        self, v: Any, values: Dict[str, Any], loc: 'LocStr', cls: Optional['ModelOrDc']\n    ) -> 'ValidateReturn':\n        try:\n            v_iter = dict_validator(v)\n        except TypeError as exc:\n            return v, ErrorWrapper(exc, loc)\n\n        loc = loc if isinstance(loc, tuple) else (loc,)\n        result, errors = {}, []\n        for k, v_ in v_iter.items():\n            v_loc = *loc, '__key__'\n            key_result, key_errors = self.key_field.validate(k, values, loc=v_loc, cls=cls)  # type: ignore\n            if key_errors:\n                errors.append(key_errors)\n                continue\n\n            v_loc = *loc, k\n            value_result, value_errors = self._validate_singleton(v_, values, v_loc, cls)\n            if value_errors:\n                errors.append(value_errors)\n                continue\n\n            result[key_result] = value_result\n        if errors:\n            return v, errors\n        elif self.shape == SHAPE_DICT:\n            return result, None\n        elif self.shape == SHAPE_DEFAULTDICT:\n            return defaultdict(self.type_, result), None\n        elif self.shape == SHAPE_COUNTER:\n            return CollectionCounter(result), None\n        else:\n            return self._get_mapping_value(v, result), None", "metadata": {"license": "MIT", "len_tokens": 285}}
{"id": "pydantic:pydantic/v1/fields.py", "language": "python", "code": "def _validate_singleton(\n        self, v: Any, values: Dict[str, Any], loc: 'LocStr', cls: Optional['ModelOrDc']\n    ) -> 'ValidateReturn':\n        if self.sub_fields:\n            if self.discriminator_key is not None:\n                return self._validate_discriminated_union(v, values, loc, cls)\n\n            errors = []\n\n            if self.model_config.smart_union and is_union(get_origin(self.type_)):\n                # 1st pass: check if the value is an exact instance of one of the Union types\n                # (e.g. to avoid coercing a bool into an int)\n                for field in self.sub_fields:\n                    if v.__class__ is field.outer_type_:\n                        return v, None\n\n                # 2nd pass: check if the value is an instance of any subclass of the Union types\n                for field in self.sub_fields:\n                    # This whole logic will be improved later on to support more complex `isinstance` checks\n                    # It will probably be done once a strict mode is added and be something like:\n                    # ```\n                    #     value, error = field.validate(v, values, strict=True)\n                    #     if error is None:\n                    #         return value, None\n                    # ```\n                    try:\n                        if isinstance(v, field.outer_type_):\n                            return v, None\n                    except TypeError:\n                        # compound type\n                        if lenient_isinstance(v, get_origin(field.outer_type_)):\n                            value, error = field.validate(v, values, loc=loc, cls=cls)\n                            if not error:\n                                return value, None\n\n            # 1st pass by default or 3rd pass with `smart_union` enabled:\n            # check if the value can be coerced into one of the Union types\n            for field in self.sub_fields:\n                value, error = field.validate(v, values, loc=loc, cls=cls)\n                if error:\n                    errors.append(error)\n                else:\n                    return value, None\n            return v, errors\n        else:\n            return self._apply_validators(v, values, loc, cls, self.validators)", "metadata": {"license": "MIT", "len_tokens": 447}}
{"id": "pydantic:pydantic/v1/fields.py", "language": "python", "code": "def _validate_discriminated_union(\n        self, v: Any, values: Dict[str, Any], loc: 'LocStr', cls: Optional['ModelOrDc']\n    ) -> 'ValidateReturn':\n        assert self.discriminator_key is not None\n        assert self.discriminator_alias is not None\n\n        try:\n            try:\n                discriminator_value = v[self.discriminator_alias]\n            except KeyError:\n                if self.model_config.allow_population_by_field_name:\n                    discriminator_value = v[self.discriminator_key]\n                else:\n                    raise\n        except KeyError:\n            return v, ErrorWrapper(MissingDiscriminator(discriminator_key=self.discriminator_key), loc)\n        except TypeError:\n            try:\n                # BaseModel or dataclass\n                discriminator_value = getattr(v, self.discriminator_key)\n            except (AttributeError, TypeError):\n                return v, ErrorWrapper(MissingDiscriminator(discriminator_key=self.discriminator_key), loc)\n\n        if self.sub_fields_mapping is None:\n            assert cls is not None\n            raise ConfigError(\n                f'field \"{self.name}\" not yet prepared so type is still a ForwardRef, '\n                f'you might need to call {cls.__name__}.update_forward_refs().'\n            )\n\n        try:\n            sub_field = self.sub_fields_mapping[discriminator_value]\n        except (KeyError, TypeError):\n            # KeyError: `discriminator_value` is not in the dictionary.\n            # TypeError: `discriminator_value` is unhashable.\n            assert self.sub_fields_mapping is not None\n            return v, ErrorWrapper(\n                InvalidDiscriminator(\n                    discriminator_key=self.discriminator_key,\n                    discriminator_value=discriminator_value,\n                    allowed_values=list(self.sub_fields_mapping),\n                ),\n                loc,\n            )\n        else:\n            if not isinstance(loc, tuple):\n                loc = (loc,)\n            return sub_field.validate(v, values, loc=(*loc, display_as_type(sub_field.type_)), cls=cls)", "metadata": {"license": "MIT", "len_tokens": 408}}
{"id": "pydantic:pydantic/v1/fields.py", "language": "python", "code": "def _type_display(self) -> PyObjectStr:\n        t = display_as_type(self.type_)\n\n        if self.shape in MAPPING_LIKE_SHAPES:\n            t = f'Mapping[{display_as_type(self.key_field.type_)}, {t}]'  # type: ignore\n        elif self.shape == SHAPE_TUPLE:\n            t = 'Tuple[{}]'.format(', '.join(display_as_type(f.type_) for f in self.sub_fields))  # type: ignore\n        elif self.shape == SHAPE_GENERIC:\n            assert self.sub_fields\n            t = '{}[{}]'.format(\n                display_as_type(self.type_), ', '.join(display_as_type(f.type_) for f in self.sub_fields)\n            )\n        elif self.shape != SHAPE_SINGLETON:\n            t = SHAPE_NAME_LOOKUP[self.shape].format(t)\n\n        if self.allow_none and (self.shape != SHAPE_SINGLETON or not self.sub_fields):\n            t = f'Optional[{t}]'\n        return PyObjectStr(t)", "metadata": {"license": "MIT", "len_tokens": 214}}
{"id": "pydantic:pydantic/v1/error_wrappers.py", "language": "python", "code": "class ValidationError(Representation, ValueError):\n    __slots__ = 'raw_errors', 'model', '_error_cache'\n\n    def __init__(self, errors: Sequence[ErrorList], model: 'ModelOrDc') -> None:\n        self.raw_errors = errors\n        self.model = model\n        self._error_cache: Optional[List['ErrorDict']] = None\n\n    def errors(self) -> List['ErrorDict']:\n        if self._error_cache is None:\n            try:\n                config = self.model.__config__  # type: ignore\n            except AttributeError:\n                config = self.model.__pydantic_model__.__config__  # type: ignore\n            self._error_cache = list(flatten_errors(self.raw_errors, config))\n        return self._error_cache\n\n    def json(self, *, indent: Union[None, int, str] = 2) -> str:\n        return json.dumps(self.errors(), indent=indent, default=pydantic_encoder)\n\n    def __str__(self) -> str:\n        errors = self.errors()\n        no_errors = len(errors)\n        return (\n            f'{no_errors} validation error{\"\" if no_errors == 1 else \"s\"} for {self.model.__name__}\\n'\n            f'{display_errors(errors)}'\n        )\n\n    def __repr_args__(self) -> 'ReprArgs':\n        return [('model', self.model.__name__), ('errors', self.errors())]", "metadata": {"license": "MIT", "len_tokens": 298}}
{"id": "pydantic:pydantic/v1/decorator.py", "language": "python", "code": "def __init__(self, function: 'AnyCallableT', config: 'ConfigType'):  # noqa C901\n        from inspect import Parameter, signature\n\n        parameters: Mapping[str, Parameter] = signature(function).parameters\n\n        if parameters.keys() & {ALT_V_ARGS, ALT_V_KWARGS, V_POSITIONAL_ONLY_NAME, V_DUPLICATE_KWARGS}:\n            raise ConfigError(\n                f'\"{ALT_V_ARGS}\", \"{ALT_V_KWARGS}\", \"{V_POSITIONAL_ONLY_NAME}\" and \"{V_DUPLICATE_KWARGS}\" '\n                f'are not permitted as argument names when using the \"{validate_arguments.__name__}\" decorator'\n            )\n\n        self.raw_function = function\n        self.arg_mapping: Dict[int, str] = {}\n        self.positional_only_args = set()\n        self.v_args_name = 'args'\n        self.v_kwargs_name = 'kwargs'\n\n        type_hints = get_all_type_hints(function)\n        takes_args = False\n        takes_kwargs = False\n        fields: Dict[str, Tuple[Any, Any]] = {}\n        for i, (name, p) in enumerate(parameters.items()):\n            if p.annotation is p.empty:\n                annotation = Any\n            else:\n                annotation = type_hints[name]\n\n            default = ... if p.default is p.empty else p.default\n            if p.kind == Parameter.POSITIONAL_ONLY:\n                self.arg_mapping[i] = name\n                fields[name] = annotation, default\n                fields[V_POSITIONAL_ONLY_NAME] = List[str], None\n                self.positional_only_args.add(name)\n            elif p.kind == Parameter.POSITIONAL_OR_KEYWORD:\n                self.arg_mapping[i] = name\n                fields[name] = annotation, default\n                fields[V_DUPLICATE_KWARGS] = List[str], None\n            elif p.kind == Parameter.KEYWORD_ONLY:\n                fields[name] = annotation, default\n            elif p.kind == Parameter.VAR_POSITIONAL:\n                self.v_args_name = name\n                fields[name] = Tuple[annotation, ...], None\n                takes_args = True\n            else:\n                assert p.kind == Parameter.VAR_KEYWORD, p.kind\n                self.v_kwargs_name = name\n                fields[name] = Dict[str, annotation], None  # type: ignore\n                takes_kwargs = True\n\n        # these checks avoid a clash between \"args\" and a field with that name\n        if not takes_args and self.v_args_name in fields:\n            self.v_args_name = ALT_V_ARGS\n\n        # same with \"kwargs\"\n        if not takes_kwargs and self.v_kwargs_name in fields:\n            self.v_kwargs_name = ALT_V_KWARGS\n\n        if not takes_args:\n            # we add the field so validation below can raise the correct exception\n            fields[self.v_args_name] = List[Any], None\n\n        if not takes_kwargs:\n            # same with kwargs\n            fields[self.v_kwargs_name] = Dict[Any, Any], None\n\n        self.create_model(fields, takes_args, takes_kwargs, config)", "metadata": {"license": "MIT", "len_tokens": 627}}
{"id": "pydantic:pydantic/v1/decorator.py", "language": "python", "code": "def build_values(self, args: Tuple[Any, ...], kwargs: Dict[str, Any]) -> Dict[str, Any]:\n        values: Dict[str, Any] = {}\n        if args:\n            arg_iter = enumerate(args)\n            while True:\n                try:\n                    i, a = next(arg_iter)\n                except StopIteration:\n                    break\n                arg_name = self.arg_mapping.get(i)\n                if arg_name is not None:\n                    values[arg_name] = a\n                else:\n                    values[self.v_args_name] = [a] + [a for _, a in arg_iter]\n                    break\n\n        var_kwargs: Dict[str, Any] = {}\n        wrong_positional_args = []\n        duplicate_kwargs = []\n        fields_alias = [\n            field.alias\n            for name, field in self.model.__fields__.items()\n            if name not in (self.v_args_name, self.v_kwargs_name)\n        ]\n        non_var_fields = set(self.model.__fields__) - {self.v_args_name, self.v_kwargs_name}\n        for k, v in kwargs.items():\n            if k in non_var_fields or k in fields_alias:\n                if k in self.positional_only_args:\n                    wrong_positional_args.append(k)\n                if k in values:\n                    duplicate_kwargs.append(k)\n                values[k] = v\n            else:\n                var_kwargs[k] = v\n\n        if var_kwargs:\n            values[self.v_kwargs_name] = var_kwargs\n        if wrong_positional_args:\n            values[V_POSITIONAL_ONLY_NAME] = wrong_positional_args\n        if duplicate_kwargs:\n            values[V_DUPLICATE_KWARGS] = duplicate_kwargs\n        return values", "metadata": {"license": "MIT", "len_tokens": 338}}
{"id": "pydantic:pydantic/v1/decorator.py", "language": "python", "code": "def execute(self, m: BaseModel) -> Any:\n        d = {k: v for k, v in m._iter() if k in m.__fields_set__ or m.__fields__[k].default_factory}\n        var_kwargs = d.pop(self.v_kwargs_name, {})\n\n        if self.v_args_name in d:\n            args_: List[Any] = []\n            in_kwargs = False\n            kwargs = {}\n            for name, value in d.items():\n                if in_kwargs:\n                    kwargs[name] = value\n                elif name == self.v_args_name:\n                    args_ += value\n                    in_kwargs = True\n                else:\n                    args_.append(value)\n            return self.raw_function(*args_, **kwargs, **var_kwargs)\n        elif self.positional_only_args:\n            args_ = []\n            kwargs = {}\n            for name, value in d.items():\n                if name in self.positional_only_args:\n                    args_.append(value)\n                else:\n                    kwargs[name] = value\n            return self.raw_function(*args_, **kwargs, **var_kwargs)\n        else:\n            return self.raw_function(**d, **var_kwargs)", "metadata": {"license": "MIT", "len_tokens": 234}}
{"id": "pydantic:pydantic/v1/decorator.py", "language": "python", "code": "def create_model(self, fields: Dict[str, Any], takes_args: bool, takes_kwargs: bool, config: 'ConfigType') -> None:\n        pos_args = len(self.arg_mapping)\n\n        class CustomConfig:\n            pass\n\n        if not TYPE_CHECKING:  # pragma: no branch\n            if isinstance(config, dict):\n                CustomConfig = type('Config', (), config)  # noqa: F811\n            elif config is not None:\n                CustomConfig = config  # noqa: F811\n\n        if hasattr(CustomConfig, 'fields') or hasattr(CustomConfig, 'alias_generator'):\n            raise ConfigError(\n                'Setting the \"fields\" and \"alias_generator\" property on custom Config for '\n                '@validate_arguments is not yet supported, please remove.'\n            )\n\n        class DecoratorBaseModel(BaseModel):\n            @validator(self.v_args_name, check_fields=False, allow_reuse=True)\n            def check_args(cls, v: Optional[List[Any]]) -> Optional[List[Any]]:\n                if takes_args or v is None:\n                    return v\n\n                raise TypeError(f'{pos_args} positional arguments expected but {pos_args + len(v)} given')\n\n            @validator(self.v_kwargs_name, check_fields=False, allow_reuse=True)\n            def check_kwargs(cls, v: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\n                if takes_kwargs or v is None:\n                    return v\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v.keys()))\n                raise TypeError(f'unexpected keyword argument{plural}: {keys}')\n\n            @validator(V_POSITIONAL_ONLY_NAME, check_fields=False, allow_reuse=True)\n            def check_positional_only(cls, v: Optional[List[str]]) -> None:\n                if v is None:\n                    return\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v))\n                raise TypeError(f'positional-only argument{plural} passed as keyword argument{plural}: {keys}')\n\n            @validator(V_DUPLICATE_KWARGS, check_fields=False, allow_reuse=True)\n            def check_duplicate_kwargs(cls, v: Optional[List[str]]) -> None:\n                if v is None:\n                    return\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v))\n                raise TypeError(f'multiple values for argument{plural}: {keys}')\n\n            class Config(CustomConfig):\n                extra = getattr(CustomConfig, 'extra', Extra.forbid)\n\n        self.model = create_model(to_camel(self.raw_function.__name__), __base__=DecoratorBaseModel, **fields)", "metadata": {"license": "MIT", "len_tokens": 563}}
{"id": "pydantic:pydantic/v1/decorator.py", "language": "python", "code": "class DecoratorBaseModel(BaseModel):\n            @validator(self.v_args_name, check_fields=False, allow_reuse=True)\n            def check_args(cls, v: Optional[List[Any]]) -> Optional[List[Any]]:\n                if takes_args or v is None:\n                    return v\n\n                raise TypeError(f'{pos_args} positional arguments expected but {pos_args + len(v)} given')\n\n            @validator(self.v_kwargs_name, check_fields=False, allow_reuse=True)\n            def check_kwargs(cls, v: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\n                if takes_kwargs or v is None:\n                    return v\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v.keys()))\n                raise TypeError(f'unexpected keyword argument{plural}: {keys}')\n\n            @validator(V_POSITIONAL_ONLY_NAME, check_fields=False, allow_reuse=True)\n            def check_positional_only(cls, v: Optional[List[str]]) -> None:\n                if v is None:\n                    return\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v))\n                raise TypeError(f'positional-only argument{plural} passed as keyword argument{plural}: {keys}')\n\n            @validator(V_DUPLICATE_KWARGS, check_fields=False, allow_reuse=True)\n            def check_duplicate_kwargs(cls, v: Optional[List[str]]) -> None:\n                if v is None:\n                    return\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v))\n                raise TypeError(f'multiple values for argument{plural}: {keys}')\n\n            class Config(CustomConfig):\n                extra = getattr(CustomConfig, 'extra', Extra.forbid)", "metadata": {"license": "MIT", "len_tokens": 375}}
{"id": "pydantic:pydantic/v1/validators.py", "language": "python", "code": "def int_validator(v: Any) -> int:\n    if isinstance(v, int) and not (v is True or v is False):\n        return v\n\n    # see https://github.com/pydantic/pydantic/issues/1477 and in turn, https://github.com/python/cpython/issues/95778\n    # this check should be unnecessary once patch releases are out for 3.7, 3.8, 3.9 and 3.10\n    # but better to check here until then.\n    # NOTICE: this does not fully protect user from the DOS risk since the standard library JSON implementation\n    # (and other std lib modules like xml) use `int()` and are likely called before this, the best workaround is to\n    # 1. update to the latest patch release of python once released, 2. use a different JSON library like ujson\n    if isinstance(v, (str, bytes, bytearray)) and len(v) > max_str_int:\n        raise errors.IntegerError()\n\n    try:\n        return int(v)\n    except (TypeError, ValueError, OverflowError):\n        raise errors.IntegerError()", "metadata": {"license": "MIT", "len_tokens": 239}}
{"id": "pydantic:pydantic/v1/validators.py", "language": "python", "code": "def make_namedtuple_validator(\n    namedtuple_cls: Type[NamedTupleT], config: Type['BaseConfig']\n) -> Callable[[Tuple[Any, ...]], NamedTupleT]:\n    from pydantic.v1.annotated_types import create_model_from_namedtuple\n\n    NamedTupleModel = create_model_from_namedtuple(\n        namedtuple_cls,\n        __config__=config,\n        __module__=namedtuple_cls.__module__,\n    )\n    namedtuple_cls.__pydantic_model__ = NamedTupleModel  # type: ignore[attr-defined]\n\n    def namedtuple_validator(values: Tuple[Any, ...]) -> NamedTupleT:\n        annotations = NamedTupleModel.__annotations__\n\n        if len(values) > len(annotations):\n            raise errors.ListMaxLengthError(limit_value=len(annotations))\n\n        dict_values: Dict[str, Any] = dict(zip(annotations, values))\n        validated_dict_values: Dict[str, Any] = dict(NamedTupleModel(**dict_values))\n        return namedtuple_cls(**validated_dict_values)\n\n    return namedtuple_validator", "metadata": {"license": "MIT", "len_tokens": 213}}
{"id": "pydantic:pydantic/v1/validators.py", "language": "python", "code": "def find_validators(  # noqa: C901 (ignore complexity)\n    type_: Type[Any], config: Type['BaseConfig']\n) -> Generator[AnyCallable, None, None]:\n    from pydantic.v1.dataclasses import is_builtin_dataclass, make_dataclass_validator\n\n    if type_ is Any or type_ is object:\n        return\n    type_type = type_.__class__\n    if type_type == ForwardRef or type_type == TypeVar:\n        return\n\n    if is_none_type(type_):\n        yield none_validator\n        return\n    if type_ is Pattern or type_ is re.Pattern:\n        yield pattern_validator\n        return\n    if type_ is Hashable or type_ is CollectionsHashable:\n        yield hashable_validator\n        return\n    if is_callable_type(type_):\n        yield callable_validator\n        return\n    if is_literal_type(type_):\n        yield make_literal_validator(type_)\n        return\n    if is_builtin_dataclass(type_):\n        yield from make_dataclass_validator(type_, config)\n        return\n    if type_ is Enum:\n        yield enum_validator\n        return\n    if type_ is IntEnum:\n        yield int_enum_validator\n        return\n    if is_namedtuple(type_):\n        yield tuple_validator\n        yield make_namedtuple_validator(type_, config)\n        return\n    if is_typeddict(type_):\n        yield make_typeddict_validator(type_, config)\n        return\n\n    class_ = get_class(type_)\n    if class_ is not None:\n        if class_ is not Any and isinstance(class_, type):\n            yield make_class_validator(class_)\n        else:\n            yield any_class_validator\n        return\n\n    for val_type, validators in _VALIDATORS:\n        try:\n            if issubclass(type_, val_type):\n                for v in validators:\n                    if isinstance(v, IfConfig):\n                        if v.check(config):\n                            yield v.validator\n                    else:\n                        yield v\n                return\n        except TypeError:\n            raise RuntimeError(f'error checking inheritance of {type_!r} (type: {display_as_type(type_)})')\n\n    if config.arbitrary_types_allowed:\n        yield make_arbitrary_type_validator(type_)\n    else:\n        if hasattr(type_, '__pydantic_core_schema__'):\n            warn(f'Mixing V1 and V2 models is not supported. `{type_.__name__}` is a V2 model.', UserWarning)\n        raise RuntimeError(f'no validator found for {type_}, see `arbitrary_types_allowed` in Config')", "metadata": {"license": "MIT", "len_tokens": 525}}
{"id": "pydantic:pydantic/v1/tools.py", "language": "python", "code": "import json\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Callable, Optional, Type, TypeVar, Union\n\nfrom pydantic.v1.parse import Protocol, load_file, load_str_bytes\nfrom pydantic.v1.types import StrBytes\nfrom pydantic.v1.typing import display_as_type\n\n__all__ = ('parse_file_as', 'parse_obj_as', 'parse_raw_as', 'schema_of', 'schema_json_of')\n\nNameFactory = Union[str, Callable[[Type[Any]], str]]\n\nif TYPE_CHECKING:\n    from pydantic.v1.typing import DictStrAny\n\n\ndef _generate_parsing_type_name(type_: Any) -> str:\n    return f'ParsingModel[{display_as_type(type_)}]'\n\n\n@lru_cache(maxsize=2048)\ndef _get_parsing_type(type_: Any, *, type_name: Optional[NameFactory] = None) -> Any:\n    from pydantic.v1.main import create_model\n\n    if type_name is None:\n        type_name = _generate_parsing_type_name\n    if not isinstance(type_name, str):\n        type_name = type_name(type_)\n    return create_model(type_name, __root__=(type_, ...))\n\n\nT = TypeVar('T')\n\n\ndef parse_obj_as(type_: Type[T], obj: Any, *, type_name: Optional[NameFactory] = None) -> T:\n    model_type = _get_parsing_type(type_, type_name=type_name)  # type: ignore[arg-type]\n    return model_type(__root__=obj).__root__\n\n\ndef parse_file_as(\n    type_: Type[T],\n    path: Union[str, Path],\n    *,\n    content_type: str = None,\n    encoding: str = 'utf8',\n    proto: Protocol = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n    type_name: Optional[NameFactory] = None,\n) -> T:\n    obj = load_file(\n        path,\n        proto=proto,\n        content_type=content_type,\n        encoding=encoding,\n        allow_pickle=allow_pickle,\n        json_loads=json_loads,\n    )\n    return parse_obj_as(type_, obj, type_name=type_name)\n\n\ndef parse_raw_as(\n    type_: Type[T],\n    b: StrBytes,\n    *,\n    content_type: str = None,\n    encoding: str = 'utf8',\n    proto: Protocol = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n    type_name: Optional[NameFactory] = None,\n) -> T:\n    obj = load_str_bytes(\n        b,\n        proto=proto,\n        content_type=content_type,\n        encoding=encoding,\n        allow_pickle=allow_pickle,\n        json_loads=json_loads,\n    )\n    return parse_obj_as(type_, obj, type_name=type_name)\n\n\ndef schema_of(type_: Any, *, title: Optional[NameFactory] = None, **schema_kwargs: Any) -> 'DictStrAny':\n    \"\"\"Generate a JSON schema (as dict) for the passed model or dynamically generated one\"\"\"\n    return _get_parsing_type(type_, type_name=title).schema(**schema_kwargs)\n\n\ndef schema_json_of(type_: Any, *, title: Optional[NameFactory] = None, **schema_json_kwargs: Any) -> str:\n    \"\"\"Generate a JSON schema (as JSON) for the passed model or dynamically generated one\"\"\"\n    return _get_parsing_type(type_, type_name=title).schema_json(**schema_json_kwargs)\n", "metadata": {"license": "MIT", "len_tokens": 755}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "class PydanticPluginConfig:\n    __slots__ = (\n        'init_forbid_extra',\n        'init_typed',\n        'warn_required_dynamic_aliases',\n        'warn_untyped_fields',\n        'debug_dataclass_transform',\n    )\n    init_forbid_extra: bool\n    init_typed: bool\n    warn_required_dynamic_aliases: bool\n    warn_untyped_fields: bool\n    debug_dataclass_transform: bool  # undocumented\n\n    def __init__(self, options: Options) -> None:\n        if options.config_file is None:  # pragma: no cover\n            return\n\n        toml_config = parse_toml(options.config_file)\n        if toml_config is not None:\n            config = toml_config.get('tool', {}).get('pydantic-mypy', {})\n            for key in self.__slots__:\n                setting = config.get(key, False)\n                if not isinstance(setting, bool):\n                    raise ValueError(f'Configuration value must be a boolean for key: {key}')\n                setattr(self, key, setting)\n        else:\n            plugin_config = ConfigParser()\n            plugin_config.read(options.config_file)\n            for key in self.__slots__:\n                setting = plugin_config.getboolean(CONFIGFILE_KEY, key, fallback=False)\n                setattr(self, key, setting)\n\n    def to_data(self) -> Dict[str, Any]:\n        return {key: getattr(self, key) for key in self.__slots__}", "metadata": {"license": "MIT", "len_tokens": 300}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "def from_orm_callback(ctx: MethodContext) -> Type:\n    \"\"\"\n    Raise an error if orm_mode is not enabled\n    \"\"\"\n    model_type: Instance\n    ctx_type = ctx.type\n    if isinstance(ctx_type, TypeType):\n        ctx_type = ctx_type.item\n    if isinstance(ctx_type, CallableType) and isinstance(ctx_type.ret_type, Instance):\n        model_type = ctx_type.ret_type  # called on the class\n    elif isinstance(ctx_type, Instance):\n        model_type = ctx_type  # called on an instance (unusual, but still valid)\n    else:  # pragma: no cover\n        detail = f'ctx.type: {ctx_type} (of type {ctx_type.__class__.__name__})'\n        error_unexpected_behavior(detail, ctx.api, ctx.context)\n        return ctx.default_return_type\n    pydantic_metadata = model_type.type.metadata.get(METADATA_KEY)\n    if pydantic_metadata is None:\n        return ctx.default_return_type\n    orm_mode = pydantic_metadata.get('config', {}).get('orm_mode')\n    if orm_mode is not True:\n        error_from_orm(get_name(model_type.type), ctx.api, ctx.context)\n    return ctx.default_return_type", "metadata": {"license": "MIT", "len_tokens": 255}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "class PydanticModelField:\n    def __init__(\n        self, name: str, is_required: bool, alias: Optional[str], has_dynamic_alias: bool, line: int, column: int\n    ):\n        self.name = name\n        self.is_required = is_required\n        self.alias = alias\n        self.has_dynamic_alias = has_dynamic_alias\n        self.line = line\n        self.column = column\n\n    def to_var(self, info: TypeInfo, use_alias: bool) -> Var:\n        name = self.name\n        if use_alias and self.alias is not None:\n            name = self.alias\n        return Var(name, info[self.name].type)\n\n    def to_argument(self, info: TypeInfo, typed: bool, force_optional: bool, use_alias: bool) -> Argument:\n        if typed and info[self.name].type is not None:\n            type_annotation = info[self.name].type\n        else:\n            type_annotation = AnyType(TypeOfAny.explicit)\n        return Argument(\n            variable=self.to_var(info, use_alias),\n            type_annotation=type_annotation,\n            initializer=None,\n            kind=ARG_NAMED_OPT if force_optional or not self.is_required else ARG_NAMED,\n        )\n\n    def serialize(self) -> JsonDict:\n        return self.__dict__\n\n    @classmethod\n    def deserialize(cls, info: TypeInfo, data: JsonDict) -> 'PydanticModelField':\n        return cls(**data)", "metadata": {"license": "MIT", "len_tokens": 303}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "class ModelConfigData:\n    def __init__(\n        self,\n        forbid_extra: Optional[bool] = None,\n        allow_mutation: Optional[bool] = None,\n        frozen: Optional[bool] = None,\n        orm_mode: Optional[bool] = None,\n        allow_population_by_field_name: Optional[bool] = None,\n        has_alias_generator: Optional[bool] = None,\n    ):\n        self.forbid_extra = forbid_extra\n        self.allow_mutation = allow_mutation\n        self.frozen = frozen\n        self.orm_mode = orm_mode\n        self.allow_population_by_field_name = allow_population_by_field_name\n        self.has_alias_generator = has_alias_generator\n\n    def set_values_dict(self) -> Dict[str, Any]:\n        return {k: v for k, v in self.__dict__.items() if v is not None}\n\n    def update(self, config: Optional['ModelConfigData']) -> None:\n        if config is None:\n            return\n        for k, v in config.set_values_dict().items():\n            setattr(self, k, v)\n\n    def setdefault(self, key: str, value: Any) -> None:\n        if getattr(self, key) is None:\n            setattr(self, key, value)", "metadata": {"license": "MIT", "len_tokens": 260}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "def add_method(\n    ctx: ClassDefContext,\n    name: str,\n    args: List[Argument],\n    return_type: Type,\n    self_type: Optional[Type] = None,\n    tvar_def: Optional[TypeVarDef] = None,\n    is_classmethod: bool = False,\n    is_new: bool = False,\n    # is_staticmethod: bool = False,\n) -> None:\n    \"\"\"\n    Adds a new method to a class.\n\n    This can be dropped if/when https://github.com/python/mypy/issues/7301 is merged\n    \"\"\"\n    info = ctx.cls.info\n\n    # First remove any previously generated methods with the same name\n    # to avoid clashes and problems in the semantic analyzer.\n    if name in info.names:\n        sym = info.names[name]\n        if sym.plugin_generated and isinstance(sym.node, FuncDef):\n            ctx.cls.defs.body.remove(sym.node)  # pragma: no cover\n\n    self_type = self_type or fill_typevars(info)\n    if is_classmethod or is_new:\n        first = [Argument(Var('_cls'), TypeType.make_normalized(self_type), None, ARG_POS)]\n    # elif is_staticmethod:\n    #     first = []\n    else:\n        self_type = self_type or fill_typevars(info)\n        first = [Argument(Var('__pydantic_self__'), self_type, None, ARG_POS)]\n    args = first + args\n    arg_types, arg_names, arg_kinds = [], [], []\n    for arg in args:\n        assert arg.type_annotation, 'All arguments must be fully typed.'\n        arg_types.append(arg.type_annotation)\n        arg_names.append(get_name(arg.variable))\n        arg_kinds.append(arg.kind)\n\n    function_type = ctx.api.named_type(f'{BUILTINS_NAME}.function')\n    signature = CallableType(\n        arg_types, arg_kinds, arg_names, return_type, function_type, variables=[tvar_def] if tvar_def else None\n    )\n\n    func = FuncDef(name, args, Block([PassStmt()]))\n    func.info = info\n    func.type = set_callable_name(signature, func)\n    func.is_class = is_classmethod\n    # func.is_static = is_staticmethod\n    func._fullname = get_fullname(info) + '.' + name\n    func.line = info.line\n\n    # NOTE: we would like the plugin generated node to dominate, but we still\n    # need to keep any existing definitions so they get semantically analyzed.\n    if name in info.names:\n        # Get a nice unique name instead.\n        r_name = get_unique_redefinition_name(name, info.names)\n        info.names[r_name] = info.names[name]\n\n    if is_classmethod:  # or is_staticmethod:\n        func.is_decorated = True\n        v = Var(name, func.type)\n        v.info = info\n        v._fullname = func._fullname\n        # if is_classmethod:\n        v.is_classmethod = True\n        dec = Decorator(func, [NameExpr('classmethod')], v)\n        # else:\n        #     v.is_staticmethod = True\n        #     dec = Decorator(func, [NameExpr('staticmethod')], v)\n\n        dec.line = info.line\n        sym = SymbolTableNode(MDEF, dec)\n    else:\n        sym = SymbolTableNode(MDEF, func)\n    sym.plugin_generated = True\n\n    info.names[name] = sym\n    info.defn.defs.body.append(func)", "metadata": {"license": "MIT", "len_tokens": 724}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "def _pydantic_field_callback(self, ctx: FunctionContext) -> 'Type':\n        \"\"\"\n        Extract the type of the `default` argument from the Field function, and use it as the return type.\n\n        In particular:\n        * Check whether the default and default_factory argument is specified.\n        * Output an error if both are specified.\n        * Retrieve the type of the argument which is specified, and use it as return type for the function.\n        \"\"\"\n        default_any_type = ctx.default_return_type\n\n        assert ctx.callee_arg_names[0] == 'default', '\"default\" is no longer first argument in Field()'\n        assert ctx.callee_arg_names[1] == 'default_factory', '\"default_factory\" is no longer second argument in Field()'\n        default_args = ctx.args[0]\n        default_factory_args = ctx.args[1]\n\n        if default_args and default_factory_args:\n            error_default_and_default_factory_specified(ctx.api, ctx.context)\n            return default_any_type\n\n        if default_args:\n            default_type = ctx.arg_types[0][0]\n            default_arg = default_args[0]\n\n            # Fallback to default Any type if the field is required\n            if not isinstance(default_arg, EllipsisExpr):\n                return default_type\n\n        elif default_factory_args:\n            default_factory_type = ctx.arg_types[1][0]\n\n            # Functions which use `ParamSpec` can be overloaded, exposing the callable's types as a parameter\n            # Pydantic calls the default factory without any argument, so we retrieve the first item\n            if isinstance(default_factory_type, Overloaded):\n                if MYPY_VERSION_TUPLE > (0, 910):\n                    default_factory_type = default_factory_type.items[0]\n                else:\n                    # Mypy0.910 exposes the items of overloaded types in a function\n                    default_factory_type = default_factory_type.items()[0]  # type: ignore[operator]\n\n            if isinstance(default_factory_type, CallableType):\n                ret_type = get_proper_type(default_factory_type.ret_type)\n                if (\n                    isinstance(ret_type, Instance)\n                    and ret_type.args\n                    and all(isinstance(arg, TypeVarType) for arg in ret_type.args)\n                ):\n                    # Looks like the default factory is a type like `list` or `dict`, replace all args with `Any`\n                    ret_type = ret_type.copy_modified(args=[default_any_type] * len(ret_type.args))\n                return ret_type\n\n        return default_any_type", "metadata": {"license": "MIT", "len_tokens": 516}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "def transform(self) -> None:\n        \"\"\"\n        Configures the BaseModel subclass according to the plugin settings.\n\n        In particular:\n        * determines the model config and fields,\n        * adds a fields-aware signature for the initializer and construct methods\n        * freezes the class if allow_mutation = False or frozen = True\n        * stores the fields, config, and if the class is settings in the mypy metadata for access by subclasses\n        \"\"\"\n        ctx = self._ctx\n        info = ctx.cls.info\n\n        self.adjust_validator_signatures()\n        config = self.collect_config()\n        fields = self.collect_fields(config)\n        is_settings = any(get_fullname(base) == BASESETTINGS_FULLNAME for base in info.mro[:-1])\n        self.add_initializer(fields, config, is_settings)\n        self.add_construct_method(fields)\n        self.set_frozen(fields, frozen=config.allow_mutation is False or config.frozen is True)\n        info.metadata[METADATA_KEY] = {\n            'fields': {field.name: field.serialize() for field in fields},\n            'config': config.set_values_dict(),\n        }", "metadata": {"license": "MIT", "len_tokens": 228}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "def collect_config(self) -> 'ModelConfigData':\n        \"\"\"\n        Collects the values of the config attributes that are used by the plugin, accounting for parent classes.\n        \"\"\"\n        ctx = self._ctx\n        cls = ctx.cls\n        config = ModelConfigData()\n        for stmt in cls.defs.body:\n            if not isinstance(stmt, ClassDef):\n                continue\n            if stmt.name == 'Config':\n                for substmt in stmt.defs.body:\n                    if not isinstance(substmt, AssignmentStmt):\n                        continue\n                    config.update(self.get_config_update(substmt))\n                if (\n                    config.has_alias_generator\n                    and not config.allow_population_by_field_name\n                    and self.plugin_config.warn_required_dynamic_aliases\n                ):\n                    error_required_dynamic_aliases(ctx.api, stmt)\n        for info in cls.info.mro[1:]:  # 0 is the current class\n            if METADATA_KEY not in info.metadata:\n                continue\n\n            # Each class depends on the set of fields in its ancestors\n            ctx.api.add_plugin_dependency(make_wildcard_trigger(get_fullname(info)))\n            for name, value in info.metadata[METADATA_KEY]['config'].items():\n                config.setdefault(name, value)\n        return config", "metadata": {"license": "MIT", "len_tokens": 252}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "def collect_fields(self, model_config: 'ModelConfigData') -> List['PydanticModelField']:\n        \"\"\"\n        Collects the fields for the model, accounting for parent classes\n        \"\"\"\n        # First, collect fields belonging to the current class.\n        ctx = self._ctx\n        cls = self._ctx.cls\n        fields = []  # type: List[PydanticModelField]\n        known_fields = set()  # type: Set[str]\n        for stmt in cls.defs.body:\n            if not isinstance(stmt, AssignmentStmt):  # `and stmt.new_syntax` to require annotation\n                continue\n\n            lhs = stmt.lvalues[0]\n            if not isinstance(lhs, NameExpr) or not is_valid_field(lhs.name):\n                continue\n\n            if not stmt.new_syntax and self.plugin_config.warn_untyped_fields:\n                error_untyped_fields(ctx.api, stmt)\n\n            # if lhs.name == '__config__':  # BaseConfig not well handled; I'm not sure why yet\n            #     continue\n\n            sym = cls.info.names.get(lhs.name)\n            if sym is None:  # pragma: no cover\n                # This is likely due to a star import (see the dataclasses plugin for a more detailed explanation)\n                # This is the same logic used in the dataclasses plugin\n                continue\n\n            node = sym.node\n            if isinstance(node, PlaceholderNode):  # pragma: no cover\n                # See the PlaceholderNode docstring for more detail about how this can occur\n                # Basically, it is an edge case when dealing with complex import logic\n                # This is the same logic used in the dataclasses plugin\n                continue\n            if not isinstance(node, Var):  # pragma: no cover\n                # Don't know if this edge case still happens with the `is_valid_field` check above\n                # but better safe than sorry\n                continue\n\n            # x: ClassVar[int] is ignored by dataclasses.\n            if node.is_classvar:\n                continue\n\n            is_required = self.get_is_required(cls, stmt, lhs)\n            alias, has_dynamic_alias = self.get_alias_info(stmt)\n            if (\n                has_dynamic_alias\n                and not model_config.allow_population_by_field_name\n                and self.plugin_config.warn_required_dynamic_aliases\n            ):\n                error_required_dynamic_aliases(ctx.api, stmt)\n            fields.append(\n                PydanticModelField(\n                    name=lhs.name,\n                    is_required=is_required,\n                    alias=alias,\n                    has_dynamic_alias=has_dynamic_alias,\n                    line=stmt.line,\n                    column=stmt.column,\n                )\n            )\n            known_fields.add(lhs.name)\n        all_fields = fields.copy()\n        for info in cls.info.mro[1:]:  # 0 is the current class, -2 is BaseModel, -1 is object\n            if METADATA_KEY not in info.metadata:\n                continue\n\n            superclass_fields = []\n            # Each class depends on the set of fields in its ancestors\n            ctx.api.add_plugin_dependency(make_wildcard_trigger(get_fullname(info)))\n\n            for name, data in info.metadata[METADATA_KEY]['fields'].items():\n                if name not in known_fields:\n                    field = PydanticModelField.deserialize(info, data)\n                    known_fields.add(name)\n                    superclass_fields.append(field)\n                else:\n                    (field,) = (a for a in all_fields if a.name == name)\n                    all_fields.remove(field)\n                    superclass_fields.append(field)\n            all_fields = superclass_fields + all_fields\n        return all_fields", "metadata": {"license": "MIT", "len_tokens": 730}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "def add_initializer(self, fields: List['PydanticModelField'], config: 'ModelConfigData', is_settings: bool) -> None:\n        \"\"\"\n        Adds a fields-aware `__init__` method to the class.\n\n        The added `__init__` will be annotated with types vs. all `Any` depending on the plugin settings.\n        \"\"\"\n        ctx = self._ctx\n        typed = self.plugin_config.init_typed\n        use_alias = config.allow_population_by_field_name is not True\n        force_all_optional = is_settings or bool(\n            config.has_alias_generator and not config.allow_population_by_field_name\n        )\n        init_arguments = self.get_field_arguments(\n            fields, typed=typed, force_all_optional=force_all_optional, use_alias=use_alias\n        )\n        if not self.should_init_forbid_extra(fields, config):\n            var = Var('kwargs')\n            init_arguments.append(Argument(var, AnyType(TypeOfAny.explicit), None, ARG_STAR2))\n\n        if '__init__' not in ctx.cls.info.names:\n            add_method(ctx, '__init__', init_arguments, NoneType())", "metadata": {"license": "MIT", "len_tokens": 233}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "def add_construct_method(self, fields: List['PydanticModelField']) -> None:\n        \"\"\"\n        Adds a fully typed `construct` classmethod to the class.\n\n        Similar to the fields-aware __init__ method, but always uses the field names (not aliases),\n        and does not treat settings fields as optional.\n        \"\"\"\n        ctx = self._ctx\n        set_str = ctx.api.named_type(f'{BUILTINS_NAME}.set', [ctx.api.named_type(f'{BUILTINS_NAME}.str')])\n        optional_set_str = UnionType([set_str, NoneType()])\n        fields_set_argument = Argument(Var('_fields_set', optional_set_str), optional_set_str, None, ARG_OPT)\n        construct_arguments = self.get_field_arguments(fields, typed=True, force_all_optional=False, use_alias=False)\n        construct_arguments = [fields_set_argument] + construct_arguments\n\n        obj_type = ctx.api.named_type(f'{BUILTINS_NAME}.object')\n        self_tvar_name = '_PydanticBaseModel'  # Make sure it does not conflict with other names in the class\n        tvar_fullname = ctx.cls.fullname + '.' + self_tvar_name\n        if MYPY_VERSION_TUPLE >= (1, 4):\n            tvd = TypeVarType(\n                self_tvar_name,\n                tvar_fullname,\n                (\n                    TypeVarId(-1, namespace=ctx.cls.fullname + '.construct')\n                    if MYPY_VERSION_TUPLE >= (1, 11)\n                    else TypeVarId(-1)\n                ),\n                [],\n                obj_type,\n                AnyType(TypeOfAny.from_omitted_generics),  # type: ignore[arg-type]\n            )\n            self_tvar_expr = TypeVarExpr(\n                self_tvar_name,\n                tvar_fullname,\n                [],\n                obj_type,\n                AnyType(TypeOfAny.from_omitted_generics),  # type: ignore[arg-type]\n            )\n        else:\n            tvd = TypeVarDef(self_tvar_name, tvar_fullname, -1, [], obj_type)\n            self_tvar_expr = TypeVarExpr(self_tvar_name, tvar_fullname, [], obj_type)\n        ctx.cls.info.names[self_tvar_name] = SymbolTableNode(MDEF, self_tvar_expr)\n\n        # Backward-compatible with TypeVarDef from Mypy 0.910.\n        if isinstance(tvd, TypeVarType):\n            self_type = tvd\n        else:\n            self_type = TypeVarType(tvd)\n\n        add_method(\n            ctx,\n            'construct',\n            construct_arguments,\n            return_type=self_type,\n            self_type=self_type,\n            tvar_def=tvd,\n            is_classmethod=True,\n        )", "metadata": {"license": "MIT", "len_tokens": 566}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "def set_frozen(self, fields: List['PydanticModelField'], frozen: bool) -> None:\n        \"\"\"\n        Marks all fields as properties so that attempts to set them trigger mypy errors.\n\n        This is the same approach used by the attrs and dataclasses plugins.\n        \"\"\"\n        ctx = self._ctx\n        info = ctx.cls.info\n        for field in fields:\n            sym_node = info.names.get(field.name)\n            if sym_node is not None:\n                var = sym_node.node\n                if isinstance(var, Var):\n                    var.is_property = frozen\n                elif isinstance(var, PlaceholderNode) and not ctx.api.final_iteration:\n                    # See https://github.com/pydantic/pydantic/issues/5191 to hit this branch for test coverage\n                    ctx.api.defer()\n                else:  # pragma: no cover\n                    # I don't know whether it's possible to hit this branch, but I've added it for safety\n                    try:\n                        var_str = str(var)\n                    except TypeError:\n                        # This happens for PlaceholderNode; perhaps it will happen for other types in the future..\n                        var_str = repr(var)\n                    detail = f'sym_node.node: {var_str} (of type {var.__class__})'\n                    error_unexpected_behavior(detail, ctx.api, ctx.cls)\n            else:\n                var = field.to_var(info, use_alias=False)\n                var.info = info\n                var.is_property = frozen\n                var._fullname = get_fullname(info) + '.' + get_name(var)\n                info.names[get_name(var)] = SymbolTableNode(MDEF, var)", "metadata": {"license": "MIT", "len_tokens": 331}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "def get_config_update(self, substmt: AssignmentStmt) -> Optional['ModelConfigData']:\n        \"\"\"\n        Determines the config update due to a single statement in the Config class definition.\n\n        Warns if a tracked config attribute is set to a value the plugin doesn't know how to interpret (e.g., an int)\n        \"\"\"\n        lhs = substmt.lvalues[0]\n        if not (isinstance(lhs, NameExpr) and lhs.name in self.tracked_config_fields):\n            return None\n        if lhs.name == 'extra':\n            if isinstance(substmt.rvalue, StrExpr):\n                forbid_extra = substmt.rvalue.value == 'forbid'\n            elif isinstance(substmt.rvalue, MemberExpr):\n                forbid_extra = substmt.rvalue.name == 'forbid'\n            else:\n                error_invalid_config_value(lhs.name, self._ctx.api, substmt)\n                return None\n            return ModelConfigData(forbid_extra=forbid_extra)\n        if lhs.name == 'alias_generator':\n            has_alias_generator = True\n            if isinstance(substmt.rvalue, NameExpr) and substmt.rvalue.fullname == 'builtins.None':\n                has_alias_generator = False\n            return ModelConfigData(has_alias_generator=has_alias_generator)\n        if isinstance(substmt.rvalue, NameExpr) and substmt.rvalue.fullname in ('builtins.True', 'builtins.False'):\n            return ModelConfigData(**{lhs.name: substmt.rvalue.fullname == 'builtins.True'})\n        error_invalid_config_value(lhs.name, self._ctx.api, substmt)\n        return None", "metadata": {"license": "MIT", "len_tokens": 325}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "def get_is_required(cls: ClassDef, stmt: AssignmentStmt, lhs: NameExpr) -> bool:\n        \"\"\"\n        Returns a boolean indicating whether the field defined in `stmt` is a required field.\n        \"\"\"\n        expr = stmt.rvalue\n        if isinstance(expr, TempNode):\n            # TempNode means annotation-only, so only non-required if Optional\n            value_type = get_proper_type(cls.info[lhs.name].type)\n            return not PydanticModelTransformer.type_has_implicit_default(value_type)\n        if isinstance(expr, CallExpr) and isinstance(expr.callee, RefExpr) and expr.callee.fullname == FIELD_FULLNAME:\n            # The \"default value\" is a call to `Field`; at this point, the field is\n            # only required if default is Ellipsis (i.e., `field_name: Annotation = Field(...)`) or if default_factory\n            # is specified.\n            for arg, name in zip(expr.args, expr.arg_names):\n                # If name is None, then this arg is the default because it is the only positional argument.\n                if name is None or name == 'default':\n                    return arg.__class__ is EllipsisExpr\n                if name == 'default_factory':\n                    return False\n            # In this case, default and default_factory are not specified, so we need to look at the annotation\n            value_type = get_proper_type(cls.info[lhs.name].type)\n            return not PydanticModelTransformer.type_has_implicit_default(value_type)\n        # Only required if the \"default value\" is Ellipsis (i.e., `field_name: Annotation = ...`)\n        return isinstance(expr, EllipsisExpr)", "metadata": {"license": "MIT", "len_tokens": 346}}
{"id": "pydantic:pydantic/v1/mypy.py", "language": "python", "code": "def get_alias_info(stmt: AssignmentStmt) -> Tuple[Optional[str], bool]:\n        \"\"\"\n        Returns a pair (alias, has_dynamic_alias), extracted from the declaration of the field defined in `stmt`.\n\n        `has_dynamic_alias` is True if and only if an alias is provided, but not as a string literal.\n        If `has_dynamic_alias` is True, `alias` will be None.\n        \"\"\"\n        expr = stmt.rvalue\n        if isinstance(expr, TempNode):\n            # TempNode means annotation-only\n            return None, False\n\n        if not (\n            isinstance(expr, CallExpr) and isinstance(expr.callee, RefExpr) and expr.callee.fullname == FIELD_FULLNAME\n        ):\n            # Assigned value is not a call to pydantic.fields.Field\n            return None, False\n\n        for i, arg_name in enumerate(expr.arg_names):\n            if arg_name != 'alias':\n                continue\n            arg = expr.args[i]\n            if isinstance(arg, StrExpr):\n                return arg.value, False\n            else:\n                return None, True\n        return None, False", "metadata": {"license": "MIT", "len_tokens": 227}}
{"id": "pydantic:pydantic/v1/types.py", "language": "python", "code": "class ConstrainedFloat(float, metaclass=ConstrainedNumberMeta):\n    strict: bool = False\n    gt: OptionalIntFloat = None\n    ge: OptionalIntFloat = None\n    lt: OptionalIntFloat = None\n    le: OptionalIntFloat = None\n    multiple_of: OptionalIntFloat = None\n    allow_inf_nan: Optional[bool] = None\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(\n            field_schema,\n            exclusiveMinimum=cls.gt,\n            exclusiveMaximum=cls.lt,\n            minimum=cls.ge,\n            maximum=cls.le,\n            multipleOf=cls.multiple_of,\n        )\n        # Modify constraints to account for differences between IEEE floats and JSON\n        if field_schema.get('exclusiveMinimum') == -math.inf:\n            del field_schema['exclusiveMinimum']\n        if field_schema.get('minimum') == -math.inf:\n            del field_schema['minimum']\n        if field_schema.get('exclusiveMaximum') == math.inf:\n            del field_schema['exclusiveMaximum']\n        if field_schema.get('maximum') == math.inf:\n            del field_schema['maximum']\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield strict_float_validator if cls.strict else float_validator\n        yield number_size_validator\n        yield number_multiple_validator\n        yield float_finite_validator", "metadata": {"license": "MIT", "len_tokens": 292}}
{"id": "pydantic:pydantic/v1/types.py", "language": "python", "code": "class ConstrainedStr(str):\n    strip_whitespace = False\n    to_upper = False\n    to_lower = False\n    min_length: OptionalInt = None\n    max_length: OptionalInt = None\n    curtail_length: OptionalInt = None\n    regex: Optional[Union[str, Pattern[str]]] = None\n    strict = False\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(\n            field_schema,\n            minLength=cls.min_length,\n            maxLength=cls.max_length,\n            pattern=cls.regex and cls._get_pattern(cls.regex),\n        )\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield strict_str_validator if cls.strict else str_validator\n        yield constr_strip_whitespace\n        yield constr_upper\n        yield constr_lower\n        yield constr_length_validator\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: Union[str]) -> Union[str]:\n        if cls.curtail_length and len(value) > cls.curtail_length:\n            value = value[: cls.curtail_length]\n\n        if cls.regex:\n            if not re.match(cls.regex, value):\n                raise errors.StrRegexError(pattern=cls._get_pattern(cls.regex))\n\n        return value\n\n    @staticmethod\n    def _get_pattern(regex: Union[str, Pattern[str]]) -> str:\n        return regex if isinstance(regex, str) else regex.pattern", "metadata": {"license": "MIT", "len_tokens": 306}}
{"id": "pydantic:pydantic/v1/types.py", "language": "python", "code": "class ConstrainedSet(set):  # type: ignore\n    # Needed for pydantic to detect that this is a set\n    __origin__ = set\n    __args__: Set[Type[T]]  # type: ignore\n\n    min_items: Optional[int] = None\n    max_items: Optional[int] = None\n    item_type: Type[T]  # type: ignore\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.set_length_validator\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(field_schema, minItems=cls.min_items, maxItems=cls.max_items)\n\n    @classmethod\n    def set_length_validator(cls, v: 'Optional[Set[T]]') -> 'Optional[Set[T]]':\n        if v is None:\n            return None\n\n        v = set_validator(v)\n        v_len = len(v)\n\n        if cls.min_items is not None and v_len < cls.min_items:\n            raise errors.SetMinLengthError(limit_value=cls.min_items)\n\n        if cls.max_items is not None and v_len > cls.max_items:\n            raise errors.SetMaxLengthError(limit_value=cls.max_items)\n\n        return v", "metadata": {"license": "MIT", "len_tokens": 266}}
{"id": "pydantic:pydantic/v1/types.py", "language": "python", "code": "class ConstrainedFrozenSet(frozenset):  # type: ignore\n    # Needed for pydantic to detect that this is a set\n    __origin__ = frozenset\n    __args__: FrozenSet[Type[T]]  # type: ignore\n\n    min_items: Optional[int] = None\n    max_items: Optional[int] = None\n    item_type: Type[T]  # type: ignore\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.frozenset_length_validator\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(field_schema, minItems=cls.min_items, maxItems=cls.max_items)\n\n    @classmethod\n    def frozenset_length_validator(cls, v: 'Optional[FrozenSet[T]]') -> 'Optional[FrozenSet[T]]':\n        if v is None:\n            return None\n\n        v = frozenset_validator(v)\n        v_len = len(v)\n\n        if cls.min_items is not None and v_len < cls.min_items:\n            raise errors.FrozenSetMinLengthError(limit_value=cls.min_items)\n\n        if cls.max_items is not None and v_len > cls.max_items:\n            raise errors.FrozenSetMaxLengthError(limit_value=cls.max_items)\n\n        return v", "metadata": {"license": "MIT", "len_tokens": 286}}
{"id": "pydantic:pydantic/v1/types.py", "language": "python", "code": "class ConstrainedList(list):  # type: ignore\n    # Needed for pydantic to detect that this is a list\n    __origin__ = list\n    __args__: Tuple[Type[T], ...]  # type: ignore\n\n    min_items: Optional[int] = None\n    max_items: Optional[int] = None\n    unique_items: Optional[bool] = None\n    item_type: Type[T]  # type: ignore\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.list_length_validator\n        if cls.unique_items:\n            yield cls.unique_items_validator\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(field_schema, minItems=cls.min_items, maxItems=cls.max_items, uniqueItems=cls.unique_items)\n\n    @classmethod\n    def list_length_validator(cls, v: 'Optional[List[T]]') -> 'Optional[List[T]]':\n        if v is None:\n            return None\n\n        v = list_validator(v)\n        v_len = len(v)\n\n        if cls.min_items is not None and v_len < cls.min_items:\n            raise errors.ListMinLengthError(limit_value=cls.min_items)\n\n        if cls.max_items is not None and v_len > cls.max_items:\n            raise errors.ListMaxLengthError(limit_value=cls.max_items)\n\n        return v\n\n    @classmethod\n    def unique_items_validator(cls, v: 'Optional[List[T]]') -> 'Optional[List[T]]':\n        if v is None:\n            return None\n\n        for i, value in enumerate(v, start=1):\n            if value in v[i:]:\n                raise errors.ListUniqueItemsError()\n\n        return v", "metadata": {"license": "MIT", "len_tokens": 366}}
{"id": "pydantic:pydantic/v1/types.py", "language": "python", "code": "class ConstrainedDecimal(Decimal, metaclass=ConstrainedNumberMeta):\n    gt: OptionalIntFloatDecimal = None\n    ge: OptionalIntFloatDecimal = None\n    lt: OptionalIntFloatDecimal = None\n    le: OptionalIntFloatDecimal = None\n    max_digits: OptionalInt = None\n    decimal_places: OptionalInt = None\n    multiple_of: OptionalIntFloatDecimal = None\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(\n            field_schema,\n            exclusiveMinimum=cls.gt,\n            exclusiveMaximum=cls.lt,\n            minimum=cls.ge,\n            maximum=cls.le,\n            multipleOf=cls.multiple_of,\n        )\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield decimal_validator\n        yield number_size_validator\n        yield number_multiple_validator\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: Decimal) -> Decimal:\n        try:\n            normalized_value = value.normalize()\n        except InvalidOperation:\n            normalized_value = value\n        digit_tuple, exponent = normalized_value.as_tuple()[1:]\n        if exponent in {'F', 'n', 'N'}:\n            raise errors.DecimalIsNotFiniteError()\n\n        if exponent >= 0:\n            # A positive exponent adds that many trailing zeros.\n            digits = len(digit_tuple) + exponent\n            decimals = 0\n        else:\n            # If the absolute value of the negative exponent is larger than the\n            # number of digits, then it's the same as the number of digits,\n            # because it'll consume all of the digits in digit_tuple and then\n            # add abs(exponent) - len(digit_tuple) leading zeros after the\n            # decimal point.\n            if abs(exponent) > len(digit_tuple):\n                digits = decimals = abs(exponent)\n            else:\n                digits = len(digit_tuple)\n                decimals = abs(exponent)\n        whole_digits = digits - decimals\n\n        if cls.max_digits is not None and digits > cls.max_digits:\n            raise errors.DecimalMaxDigitsError(max_digits=cls.max_digits)\n\n        if cls.decimal_places is not None and decimals > cls.decimal_places:\n            raise errors.DecimalMaxPlacesError(decimal_places=cls.decimal_places)\n\n        if cls.max_digits is not None and cls.decimal_places is not None:\n            expected = cls.max_digits - cls.decimal_places\n            if whole_digits > expected:\n                raise errors.DecimalWholeDigitsError(whole_digits=expected)\n\n        return value", "metadata": {"license": "MIT", "len_tokens": 533}}
{"id": "pydantic:pydantic/v1/types.py", "language": "python", "code": "class SecretField(abc.ABC):\n    \"\"\"\n    Note: this should be implemented as a generic like `SecretField(ABC, Generic[T])`,\n          the `__init__()` should be part of the abstract class and the\n          `get_secret_value()` method should use the generic `T` type.\n\n          However Cython doesn't support very well generics at the moment and\n          the generated code fails to be imported (see\n          https://github.com/cython/cython/issues/2753).\n    \"\"\"\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, self.__class__) and self.get_secret_value() == other.get_secret_value()\n\n    def __str__(self) -> str:\n        return '**********' if self.get_secret_value() else ''\n\n    def __hash__(self) -> int:\n        return hash(self.get_secret_value())\n\n    @abc.abstractmethod\n    def get_secret_value(self) -> Any:  # pragma: no cover\n        ...", "metadata": {"license": "MIT", "len_tokens": 208}}
{"id": "pydantic:pydantic/v1/types.py", "language": "python", "code": "class SecretStr(SecretField):\n    min_length: OptionalInt = None\n    max_length: OptionalInt = None\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(\n            field_schema,\n            type='string',\n            writeOnly=True,\n            format='password',\n            minLength=cls.min_length,\n            maxLength=cls.max_length,\n        )\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.validate\n        yield constr_length_validator\n\n    @classmethod\n    def validate(cls, value: Any) -> 'SecretStr':\n        if isinstance(value, cls):\n            return value\n        value = str_validator(value)\n        return cls(value)\n\n    def __init__(self, value: str):\n        self._secret_value = value\n\n    def __repr__(self) -> str:\n        return f\"SecretStr('{self}')\"\n\n    def __len__(self) -> int:\n        return len(self._secret_value)\n\n    def display(self) -> str:\n        warnings.warn('`secret_str.display()` is deprecated, use `str(secret_str)` instead', DeprecationWarning)\n        return str(self)\n\n    def get_secret_value(self) -> str:\n        return self._secret_value", "metadata": {"license": "MIT", "len_tokens": 271}}
{"id": "pydantic:pydantic/v1/types.py", "language": "python", "code": "class SecretBytes(SecretField):\n    min_length: OptionalInt = None\n    max_length: OptionalInt = None\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        update_not_none(\n            field_schema,\n            type='string',\n            writeOnly=True,\n            format='password',\n            minLength=cls.min_length,\n            maxLength=cls.max_length,\n        )\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.validate\n        yield constr_length_validator\n\n    @classmethod\n    def validate(cls, value: Any) -> 'SecretBytes':\n        if isinstance(value, cls):\n            return value\n        value = bytes_validator(value)\n        return cls(value)\n\n    def __init__(self, value: bytes):\n        self._secret_value = value\n\n    def __repr__(self) -> str:\n        return f\"SecretBytes(b'{self}')\"\n\n    def __len__(self) -> int:\n        return len(self._secret_value)\n\n    def display(self) -> str:\n        warnings.warn('`secret_bytes.display()` is deprecated, use `str(secret_bytes)` instead', DeprecationWarning)\n        return str(self)\n\n    def get_secret_value(self) -> bytes:\n        return self._secret_value", "metadata": {"license": "MIT", "len_tokens": 272}}
{"id": "pydantic:pydantic/v1/types.py", "language": "python", "code": "class PaymentCardNumber(str):\n    \"\"\"\n    Based on: https://en.wikipedia.org/wiki/Payment_card_number\n    \"\"\"\n\n    strip_whitespace: ClassVar[bool] = True\n    min_length: ClassVar[int] = 12\n    max_length: ClassVar[int] = 19\n    bin: str\n    last4: str\n    brand: PaymentCardBrand\n\n    def __init__(self, card_number: str):\n        self.bin = card_number[:6]\n        self.last4 = card_number[-4:]\n        self.brand = self._get_brand(card_number)\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield str_validator\n        yield constr_strip_whitespace\n        yield constr_length_validator\n        yield cls.validate_digits\n        yield cls.validate_luhn_check_digit\n        yield cls\n        yield cls.validate_length_for_brand\n\n    @property\n    def masked(self) -> str:\n        num_masked = len(self) - 10  # len(bin) + len(last4) == 10\n        return f'{self.bin}{\"*\" * num_masked}{self.last4}'\n\n    @classmethod\n    def validate_digits(cls, card_number: str) -> str:\n        if not card_number.isdigit():\n            raise errors.NotDigitError\n        return card_number\n\n    @classmethod\n    def validate_luhn_check_digit(cls, card_number: str) -> str:\n        \"\"\"\n        Based on: https://en.wikipedia.org/wiki/Luhn_algorithm\n        \"\"\"\n        sum_ = int(card_number[-1])\n        length = len(card_number)\n        parity = length % 2\n        for i in range(length - 1):\n            digit = int(card_number[i])\n            if i % 2 == parity:\n                digit *= 2\n            if digit > 9:\n                digit -= 9\n            sum_ += digit\n        valid = sum_ % 10 == 0\n        if not valid:\n            raise errors.LuhnValidationError\n        return card_number\n\n    @classmethod\n    def validate_length_for_brand(cls, card_number: 'PaymentCardNumber') -> 'PaymentCardNumber':\n        \"\"\"\n        Validate length based on BIN for major brands:\n        https://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN)\n        \"\"\"\n        required_length: Union[None, int, str] = None\n        if card_number.brand in PaymentCardBrand.mastercard:\n            required_length = 16\n            valid = len(card_number) == required_length\n        elif card_number.brand == PaymentCardBrand.visa:\n            required_length = '13, 16 or 19'\n            valid = len(card_number) in {13, 16, 19}\n        elif card_number.brand == PaymentCardBrand.amex:\n            required_length = 15\n            valid = len(card_number) == required_length\n        else:\n            valid = True\n        if not valid:\n            raise errors.InvalidLengthForBrand(brand=card_number.brand, required_length=required_length)\n        return card_number\n\n    @staticmethod\n    def _get_brand(card_number: str) -> PaymentCardBrand:\n        if card_number[0] == '4':\n            brand = PaymentCardBrand.visa\n        elif 51 <= int(card_number[:2]) <= 55:\n            brand = PaymentCardBrand.mastercard\n        elif card_number[:2] in {'34', '37'}:\n            brand = PaymentCardBrand.amex\n        else:\n            brand = PaymentCardBrand.other\n        return brand", "metadata": {"license": "MIT", "len_tokens": 743}}
{"id": "pydantic:pydantic/v1/types.py", "language": "python", "code": "class ByteSize(int):\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v: StrIntFloat) -> 'ByteSize':\n        try:\n            return cls(int(v))\n        except ValueError:\n            pass\n\n        str_match = byte_string_re.match(str(v))\n        if str_match is None:\n            raise errors.InvalidByteSize()\n\n        scalar, unit = str_match.groups()\n        if unit is None:\n            unit = 'b'\n\n        try:\n            unit_mult = BYTE_SIZES[unit.lower()]\n        except KeyError:\n            raise errors.InvalidByteSizeUnit(unit=unit)\n\n        return cls(int(float(scalar) * unit_mult))\n\n    def human_readable(self, decimal: bool = False) -> str:\n        if decimal:\n            divisor = 1000\n            units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n            final_unit = 'EB'\n        else:\n            divisor = 1024\n            units = ['B', 'KiB', 'MiB', 'GiB', 'TiB', 'PiB']\n            final_unit = 'EiB'\n\n        num = float(self)\n        for unit in units:\n            if abs(num) < divisor:\n                return f'{num:0.1f}{unit}'\n            num /= divisor\n\n        return f'{num:0.1f}{final_unit}'\n\n    def to(self, unit: str) -> float:\n        try:\n            unit_div = BYTE_SIZES[unit.lower()]\n        except KeyError:\n            raise errors.InvalidByteSizeUnit(unit=unit)\n\n        return self / unit_div", "metadata": {"license": "MIT", "len_tokens": 353}}
{"id": "pydantic:pydantic/v1/types.py", "language": "python", "code": "def validate(cls, value: Decimal) -> Decimal:\n        try:\n            normalized_value = value.normalize()\n        except InvalidOperation:\n            normalized_value = value\n        digit_tuple, exponent = normalized_value.as_tuple()[1:]\n        if exponent in {'F', 'n', 'N'}:\n            raise errors.DecimalIsNotFiniteError()\n\n        if exponent >= 0:\n            # A positive exponent adds that many trailing zeros.\n            digits = len(digit_tuple) + exponent\n            decimals = 0\n        else:\n            # If the absolute value of the negative exponent is larger than the\n            # number of digits, then it's the same as the number of digits,\n            # because it'll consume all of the digits in digit_tuple and then\n            # add abs(exponent) - len(digit_tuple) leading zeros after the\n            # decimal point.\n            if abs(exponent) > len(digit_tuple):\n                digits = decimals = abs(exponent)\n            else:\n                digits = len(digit_tuple)\n                decimals = abs(exponent)\n        whole_digits = digits - decimals\n\n        if cls.max_digits is not None and digits > cls.max_digits:\n            raise errors.DecimalMaxDigitsError(max_digits=cls.max_digits)\n\n        if cls.decimal_places is not None and decimals > cls.decimal_places:\n            raise errors.DecimalMaxPlacesError(decimal_places=cls.decimal_places)\n\n        if cls.max_digits is not None and cls.decimal_places is not None:\n            expected = cls.max_digits - cls.decimal_places\n            if whole_digits > expected:\n                raise errors.DecimalWholeDigitsError(whole_digits=expected)\n\n        return value", "metadata": {"license": "MIT", "len_tokens": 335}}
{"id": "pydantic:pydantic/v1/types.py", "language": "python", "code": "def validate_length_for_brand(cls, card_number: 'PaymentCardNumber') -> 'PaymentCardNumber':\n        \"\"\"\n        Validate length based on BIN for major brands:\n        https://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN)\n        \"\"\"\n        required_length: Union[None, int, str] = None\n        if card_number.brand in PaymentCardBrand.mastercard:\n            required_length = 16\n            valid = len(card_number) == required_length\n        elif card_number.brand == PaymentCardBrand.visa:\n            required_length = '13, 16 or 19'\n            valid = len(card_number) in {13, 16, 19}\n        elif card_number.brand == PaymentCardBrand.amex:\n            required_length = 15\n            valid = len(card_number) == required_length\n        else:\n            valid = True\n        if not valid:\n            raise errors.InvalidLengthForBrand(brand=card_number.brand, required_length=required_length)\n        return card_number", "metadata": {"license": "MIT", "len_tokens": 210}}
{"id": "pydantic:pydantic/v1/class_validators.py", "language": "python", "code": "def validator(\n    *fields: str,\n    pre: bool = False,\n    each_item: bool = False,\n    always: bool = False,\n    check_fields: bool = True,\n    whole: Optional[bool] = None,\n    allow_reuse: bool = False,\n) -> Callable[[AnyCallable], 'AnyClassMethod']:\n    \"\"\"\n    Decorate methods on the class indicating that they should be used to validate fields\n    :param fields: which field(s) the method should be called on\n    :param pre: whether or not this validator should be called before the standard validators (else after)\n    :param each_item: for complex objects (sets, lists etc.) whether to validate individual elements rather than the\n      whole object\n    :param always: whether this method and other validators should be called even if the value is missing\n    :param check_fields: whether to check that the fields actually exist on the model\n    :param allow_reuse: whether to track and raise an error if another validator refers to the decorated function\n    \"\"\"\n    if not fields:\n        raise ConfigError('validator with no fields specified')\n    elif isinstance(fields[0], FunctionType):\n        raise ConfigError(\n            \"validators should be used with fields and keyword arguments, not bare. \"  # noqa: Q000\n            \"E.g. usage should be `@validator('<field_name>', ...)`\"\n        )\n    elif not all(isinstance(field, str) for field in fields):\n        raise ConfigError(\n            \"validator fields should be passed as separate string args. \"  # noqa: Q000\n            \"E.g. usage should be `@validator('<field_name_1>', '<field_name_2>', ...)`\"\n        )\n\n    if whole is not None:\n        warnings.warn(\n            'The \"whole\" keyword argument is deprecated, use \"each_item\" (inverse meaning, default False) instead',\n            DeprecationWarning,\n        )\n        assert each_item is False, '\"each_item\" and \"whole\" conflict, remove \"whole\"'\n        each_item = not whole\n\n    def dec(f: AnyCallable) -> 'AnyClassMethod':\n        f_cls = _prepare_validator(f, allow_reuse)\n        setattr(\n            f_cls,\n            VALIDATOR_CONFIG_KEY,\n            (\n                fields,\n                Validator(func=f_cls.__func__, pre=pre, each_item=each_item, always=always, check_fields=check_fields),\n            ),\n        )\n        return f_cls\n\n    return dec", "metadata": {"license": "MIT", "len_tokens": 518}}
{"id": "pydantic:pydantic/v1/class_validators.py", "language": "python", "code": "def root_validator(\n    _func: Optional[AnyCallable] = None, *, pre: bool = False, allow_reuse: bool = False, skip_on_failure: bool = False\n) -> Union['AnyClassMethod', Callable[[AnyCallable], 'AnyClassMethod']]:\n    \"\"\"\n    Decorate methods on a model indicating that they should be used to validate (and perhaps modify) data either\n    before or after standard model parsing/validation is performed.\n    \"\"\"\n    if _func:\n        f_cls = _prepare_validator(_func, allow_reuse)\n        setattr(\n            f_cls, ROOT_VALIDATOR_CONFIG_KEY, Validator(func=f_cls.__func__, pre=pre, skip_on_failure=skip_on_failure)\n        )\n        return f_cls\n\n    def dec(f: AnyCallable) -> 'AnyClassMethod':\n        f_cls = _prepare_validator(f, allow_reuse)\n        setattr(\n            f_cls, ROOT_VALIDATOR_CONFIG_KEY, Validator(func=f_cls.__func__, pre=pre, skip_on_failure=skip_on_failure)\n        )\n        return f_cls\n\n    return dec", "metadata": {"license": "MIT", "len_tokens": 223}}
{"id": "pydantic:pydantic/v1/class_validators.py", "language": "python", "code": "class ValidatorGroup:\n    def __init__(self, validators: 'ValidatorListDict') -> None:\n        self.validators = validators\n        self.used_validators = {'*'}\n\n    def get_validators(self, name: str) -> Optional[Dict[str, Validator]]:\n        self.used_validators.add(name)\n        validators = self.validators.get(name, [])\n        if name != ROOT_KEY:\n            validators += self.validators.get('*', [])\n        if validators:\n            return {getattr(v.func, '__name__', f'<No __name__: id:{id(v.func)}>'): v for v in validators}\n        else:\n            return None\n\n    def check_for_unused(self) -> None:\n        unused_validators = set(\n            chain.from_iterable(\n                (\n                    getattr(v.func, '__name__', f'<No __name__: id:{id(v.func)}>')\n                    for v in self.validators[f]\n                    if v.check_fields\n                )\n                for f in (self.validators.keys() - self.used_validators)\n            )\n        )\n        if unused_validators:\n            fn = ', '.join(unused_validators)\n            raise ConfigError(\n                f\"Validators defined with incorrect fields: {fn} \"  # noqa: Q000\n                f\"(use check_fields=False if you're inheriting from the model and intended this)\"\n            )", "metadata": {"license": "MIT", "len_tokens": 274}}
{"id": "pydantic:pydantic/v1/class_validators.py", "language": "python", "code": "def extract_root_validators(namespace: Dict[str, Any]) -> Tuple[List[AnyCallable], List[Tuple[bool, AnyCallable]]]:\n    from inspect import signature\n\n    pre_validators: List[AnyCallable] = []\n    post_validators: List[Tuple[bool, AnyCallable]] = []\n    for name, value in namespace.items():\n        validator_config: Optional[Validator] = getattr(value, ROOT_VALIDATOR_CONFIG_KEY, None)\n        if validator_config:\n            sig = signature(validator_config.func)\n            args = list(sig.parameters.keys())\n            if args[0] == 'self':\n                raise ConfigError(\n                    f'Invalid signature for root validator {name}: {sig}, \"self\" not permitted as first argument, '\n                    f'should be: (cls, values).'\n                )\n            if len(args) != 2:\n                raise ConfigError(f'Invalid signature for root validator {name}: {sig}, should be: (cls, values).')\n            # check function signature\n            if validator_config.pre:\n                pre_validators.append(validator_config.func)\n            else:\n                post_validators.append((validator_config.skip_on_failure, validator_config.func))\n    return pre_validators, post_validators", "metadata": {"license": "MIT", "len_tokens": 251}}
{"id": "pydantic:pydantic/v1/class_validators.py", "language": "python", "code": "def make_generic_validator(validator: AnyCallable) -> 'ValidatorCallable':\n    \"\"\"\n    Make a generic function which calls a validator with the right arguments.\n\n    Unfortunately other approaches (eg. return a partial of a function that builds the arguments) is slow,\n    hence this laborious way of doing things.\n\n    It's done like this so validators don't all need **kwargs in their signature, eg. any combination of\n    the arguments \"values\", \"fields\" and/or \"config\" are permitted.\n    \"\"\"\n    from inspect import signature\n\n    if not isinstance(validator, (partial, partialmethod)):\n        # This should be the default case, so overhead is reduced\n        sig = signature(validator)\n        args = list(sig.parameters.keys())\n    else:\n        # Fix the generated argument lists of partial methods\n        sig = signature(validator.func)\n        args = [\n            k\n            for k in signature(validator.func).parameters.keys()\n            if k not in validator.args | validator.keywords.keys()\n        ]\n\n    first_arg = args.pop(0)\n    if first_arg == 'self':\n        raise ConfigError(\n            f'Invalid signature for validator {validator}: {sig}, \"self\" not permitted as first argument, '\n            f'should be: (cls, value, values, config, field), \"values\", \"config\" and \"field\" are all optional.'\n        )\n    elif first_arg == 'cls':\n        # assume the second argument is value\n        return wraps(validator)(_generic_validator_cls(validator, sig, set(args[1:])))\n    else:\n        # assume the first argument was value which has already been removed\n        return wraps(validator)(_generic_validator_basic(validator, sig, set(args)))", "metadata": {"license": "MIT", "len_tokens": 357}}
{"id": "pydantic:pydantic/v1/class_validators.py", "language": "python", "code": "def _generic_validator_cls(validator: AnyCallable, sig: 'Signature', args: Set[str]) -> 'ValidatorCallable':\n    # assume the first argument is value\n    has_kwargs = False\n    if 'kwargs' in args:\n        has_kwargs = True\n        args -= {'kwargs'}\n\n    if not args.issubset(all_kwargs):\n        raise ConfigError(\n            f'Invalid signature for validator {validator}: {sig}, should be: '\n            f'(cls, value, values, config, field), \"values\", \"config\" and \"field\" are all optional.'\n        )\n\n    if has_kwargs:\n        return lambda cls, v, values, field, config: validator(cls, v, values=values, field=field, config=config)\n    elif args == set():\n        return lambda cls, v, values, field, config: validator(cls, v)\n    elif args == {'values'}:\n        return lambda cls, v, values, field, config: validator(cls, v, values=values)\n    elif args == {'field'}:\n        return lambda cls, v, values, field, config: validator(cls, v, field=field)\n    elif args == {'config'}:\n        return lambda cls, v, values, field, config: validator(cls, v, config=config)\n    elif args == {'values', 'field'}:\n        return lambda cls, v, values, field, config: validator(cls, v, values=values, field=field)\n    elif args == {'values', 'config'}:\n        return lambda cls, v, values, field, config: validator(cls, v, values=values, config=config)\n    elif args == {'field', 'config'}:\n        return lambda cls, v, values, field, config: validator(cls, v, field=field, config=config)\n    else:\n        # args == {'values', 'field', 'config'}\n        return lambda cls, v, values, field, config: validator(cls, v, values=values, field=field, config=config)", "metadata": {"license": "MIT", "len_tokens": 424}}
{"id": "pydantic:pydantic/v1/class_validators.py", "language": "python", "code": "def _generic_validator_basic(validator: AnyCallable, sig: 'Signature', args: Set[str]) -> 'ValidatorCallable':\n    has_kwargs = False\n    if 'kwargs' in args:\n        has_kwargs = True\n        args -= {'kwargs'}\n\n    if not args.issubset(all_kwargs):\n        raise ConfigError(\n            f'Invalid signature for validator {validator}: {sig}, should be: '\n            f'(value, values, config, field), \"values\", \"config\" and \"field\" are all optional.'\n        )\n\n    if has_kwargs:\n        return lambda cls, v, values, field, config: validator(v, values=values, field=field, config=config)\n    elif args == set():\n        return lambda cls, v, values, field, config: validator(v)\n    elif args == {'values'}:\n        return lambda cls, v, values, field, config: validator(v, values=values)\n    elif args == {'field'}:\n        return lambda cls, v, values, field, config: validator(v, field=field)\n    elif args == {'config'}:\n        return lambda cls, v, values, field, config: validator(v, config=config)\n    elif args == {'values', 'field'}:\n        return lambda cls, v, values, field, config: validator(v, values=values, field=field)\n    elif args == {'values', 'config'}:\n        return lambda cls, v, values, field, config: validator(v, values=values, config=config)\n    elif args == {'field', 'config'}:\n        return lambda cls, v, values, field, config: validator(v, field=field, config=config)\n    else:\n        # args == {'values', 'field', 'config'}\n        return lambda cls, v, values, field, config: validator(v, values=values, field=field, config=config)", "metadata": {"license": "MIT", "len_tokens": 395}}
{"id": "pydantic:pydantic/v1/networks.py", "language": "python", "code": "class MultiHostDsn(AnyUrl):\n    __slots__ = AnyUrl.__slots__ + ('hosts',)\n\n    def __init__(self, *args: Any, hosts: Optional[List['HostParts']] = None, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n        self.hosts = hosts\n\n    @staticmethod\n    def _match_url(url: str) -> Optional[Match[str]]:\n        return multi_host_url_regex().match(url)\n\n    @classmethod\n    def validate_parts(cls, parts: 'Parts', validate_port: bool = True) -> 'Parts':\n        return super().validate_parts(parts, validate_port=False)\n\n    @classmethod\n    def _build_url(cls, m: Match[str], url: str, parts: 'Parts') -> 'MultiHostDsn':\n        hosts_parts: List['HostParts'] = []\n        host_re = host_regex()\n        for host in m.groupdict()['hosts'].split(','):\n            d: Parts = host_re.match(host).groupdict()  # type: ignore\n            host, tld, host_type, rebuild = cls.validate_host(d)\n            port = d.get('port')\n            cls._validate_port(port)\n            hosts_parts.append(\n                {\n                    'host': host,\n                    'host_type': host_type,\n                    'tld': tld,\n                    'rebuild': rebuild,\n                    'port': port,\n                }\n            )\n\n        if len(hosts_parts) > 1:\n            return cls(\n                None if any([hp['rebuild'] for hp in hosts_parts]) else url,\n                scheme=parts['scheme'],\n                user=parts['user'],\n                password=parts['password'],\n                path=parts['path'],\n                query=parts['query'],\n                fragment=parts['fragment'],\n                host_type=None,\n                hosts=hosts_parts,\n            )\n        else:\n            # backwards compatibility with single host\n            host_part = hosts_parts[0]\n            return cls(\n                None if host_part['rebuild'] else url,\n                scheme=parts['scheme'],\n                user=parts['user'],\n                password=parts['password'],\n                host=host_part['host'],\n                tld=host_part['tld'],\n                host_type=host_part['host_type'],\n                port=host_part.get('port'),\n                path=parts['path'],\n                query=parts['query'],\n                fragment=parts['fragment'],\n            )", "metadata": {"license": "MIT", "len_tokens": 501}}
{"id": "pydantic:pydantic/v1/networks.py", "language": "python", "code": "class NameEmail(Representation):\n    __slots__ = 'name', 'email'\n\n    def __init__(self, name: str, email: str):\n        self.name = name\n        self.email = email\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, NameEmail) and (self.name, self.email) == (other.name, other.email)\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:\n        field_schema.update(type='string', format='name-email')\n\n    @classmethod\n    def __get_validators__(cls) -> 'CallableGenerator':\n        import_email_validator()\n\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: Any) -> 'NameEmail':\n        if value.__class__ == cls:\n            return value\n        value = str_validator(value)\n        return cls(*validate_email(value))\n\n    def __str__(self) -> str:\n        return f'{self.name} <{self.email}>'", "metadata": {"license": "MIT", "len_tokens": 215}}
{"id": "pydantic:pydantic/v1/networks.py", "language": "python", "code": "def validate_email(value: Union[str]) -> Tuple[str, str]:\n    \"\"\"\n    Email address validation using https://pypi.org/project/email-validator/\n    Notes:\n    * raw ip address (literal) domain parts are not allowed.\n    * \"John Doe <local_part@domain.com>\" style \"pretty\" email addresses are processed\n    * spaces are striped from the beginning and end of addresses but no error is raised\n    \"\"\"\n    if email_validator is None:\n        import_email_validator()\n\n    if len(value) > MAX_EMAIL_LENGTH:\n        raise errors.EmailError()\n\n    m = pretty_email_regex.fullmatch(value)\n    name: Union[str, None] = None\n    if m:\n        name, value = m.groups()\n    email = value.strip()\n    try:\n        parts = email_validator.validate_email(email, check_deliverability=False)\n    except email_validator.EmailNotValidError as e:\n        raise errors.EmailError from e\n\n    if hasattr(parts, 'normalized'):\n        # email-validator >= 2\n        email = parts.normalized\n        assert email is not None\n        name = name or parts.local_part\n        return name, email\n    else:\n        # email-validator >1, <2\n        at_index = email.index('@')\n        local_part = email[:at_index]  # RFC 5321, local part must be case-sensitive.\n        global_part = email[at_index:].lower()\n\n        return name or local_part, local_part + global_part", "metadata": {"license": "MIT", "len_tokens": 305}}
{"id": "pydantic:pydantic/v1/networks.py", "language": "python", "code": "def build(\n        cls,\n        *,\n        scheme: str,\n        user: Optional[str] = None,\n        password: Optional[str] = None,\n        host: str,\n        port: Optional[str] = None,\n        path: Optional[str] = None,\n        query: Optional[str] = None,\n        fragment: Optional[str] = None,\n        **_kwargs: str,\n    ) -> str:\n        parts = Parts(\n            scheme=scheme,\n            user=user,\n            password=password,\n            host=host,\n            port=port,\n            path=path,\n            query=query,\n            fragment=fragment,\n            **_kwargs,  # type: ignore[misc]\n        )\n\n        url = scheme + '://'\n        if user:\n            url += user\n        if password:\n            url += ':' + password\n        if user or password:\n            url += '@'\n        url += host\n        if port and ('port' not in cls.hidden_parts or cls.get_default_parts(parts).get('port') != port):\n            url += ':' + port\n        if path:\n            url += path\n        if query:\n            url += '?' + query\n        if fragment:\n            url += '#' + fragment\n        return url", "metadata": {"license": "MIT", "len_tokens": 252}}
{"id": "pydantic:pydantic/v1/networks.py", "language": "python", "code": "def validate_host(cls, parts: 'Parts') -> Tuple[str, Optional[str], str, bool]:\n        tld, host_type, rebuild = None, None, False\n        for f in ('domain', 'ipv4', 'ipv6'):\n            host = parts[f]  # type: ignore[literal-required]\n            if host:\n                host_type = f\n                break\n\n        if host is None:\n            if cls.host_required:\n                raise errors.UrlHostError()\n        elif host_type == 'domain':\n            is_international = False\n            d = ascii_domain_regex().fullmatch(host)\n            if d is None:\n                d = int_domain_regex().fullmatch(host)\n                if d is None:\n                    raise errors.UrlHostError()\n                is_international = True\n\n            tld = d.group('tld')\n            if tld is None and not is_international:\n                d = int_domain_regex().fullmatch(host)\n                assert d is not None\n                tld = d.group('tld')\n                is_international = True\n\n            if tld is not None:\n                tld = tld[1:]\n            elif cls.tld_required:\n                raise errors.UrlHostTldError()\n\n            if is_international:\n                host_type = 'int_domain'\n                rebuild = True\n                host = host.encode('idna').decode('ascii')\n                if tld is not None:\n                    tld = tld.encode('idna').decode('ascii')\n\n        return host, tld, host_type, rebuild", "metadata": {"license": "MIT", "len_tokens": 317}}
{"id": "pydantic:pydantic/v1/networks.py", "language": "python", "code": "def _build_url(cls, m: Match[str], url: str, parts: 'Parts') -> 'MultiHostDsn':\n        hosts_parts: List['HostParts'] = []\n        host_re = host_regex()\n        for host in m.groupdict()['hosts'].split(','):\n            d: Parts = host_re.match(host).groupdict()  # type: ignore\n            host, tld, host_type, rebuild = cls.validate_host(d)\n            port = d.get('port')\n            cls._validate_port(port)\n            hosts_parts.append(\n                {\n                    'host': host,\n                    'host_type': host_type,\n                    'tld': tld,\n                    'rebuild': rebuild,\n                    'port': port,\n                }\n            )\n\n        if len(hosts_parts) > 1:\n            return cls(\n                None if any([hp['rebuild'] for hp in hosts_parts]) else url,\n                scheme=parts['scheme'],\n                user=parts['user'],\n                password=parts['password'],\n                path=parts['path'],\n                query=parts['query'],\n                fragment=parts['fragment'],\n                host_type=None,\n                hosts=hosts_parts,\n            )\n        else:\n            # backwards compatibility with single host\n            host_part = hosts_parts[0]\n            return cls(\n                None if host_part['rebuild'] else url,\n                scheme=parts['scheme'],\n                user=parts['user'],\n                password=parts['password'],\n                host=host_part['host'],\n                tld=host_part['tld'],\n                host_type=host_part['host_type'],\n                port=host_part.get('port'),\n                path=parts['path'],\n                query=parts['query'],\n                fragment=parts['fragment'],\n            )", "metadata": {"license": "MIT", "len_tokens": 360}}
{"id": "pydantic:pydantic/v1/utils.py", "language": "python", "code": "def generate_model_signature(\n    init: Callable[..., None], fields: Dict[str, 'ModelField'], config: Type['BaseConfig']\n) -> 'Signature':\n    \"\"\"\n    Generate signature for model based on its fields\n    \"\"\"\n    from inspect import Parameter, Signature, signature\n\n    from pydantic.v1.config import Extra\n\n    present_params = signature(init).parameters.values()\n    merged_params: Dict[str, Parameter] = {}\n    var_kw = None\n    use_var_kw = False\n\n    for param in islice(present_params, 1, None):  # skip self arg\n        if param.kind is param.VAR_KEYWORD:\n            var_kw = param\n            continue\n        merged_params[param.name] = param\n\n    if var_kw:  # if custom init has no var_kw, fields which are not declared in it cannot be passed through\n        allow_names = config.allow_population_by_field_name\n        for field_name, field in fields.items():\n            param_name = field.alias\n            if field_name in merged_params or param_name in merged_params:\n                continue\n            elif not is_valid_identifier(param_name):\n                if allow_names and is_valid_identifier(field_name):\n                    param_name = field_name\n                else:\n                    use_var_kw = True\n                    continue\n\n            # TODO: replace annotation with actual expected types once #1055 solved\n            kwargs = {'default': field.default} if not field.required else {}\n            merged_params[param_name] = Parameter(\n                param_name, Parameter.KEYWORD_ONLY, annotation=field.annotation, **kwargs\n            )\n\n    if config.extra is Extra.allow:\n        use_var_kw = True\n\n    if var_kw and use_var_kw:\n        # Make sure the parameter for extra kwargs\n        # does not have the same name as a field\n        default_model_signature = [\n            ('__pydantic_self__', Parameter.POSITIONAL_OR_KEYWORD),\n            ('data', Parameter.VAR_KEYWORD),\n        ]\n        if [(p.name, p.kind) for p in present_params] == default_model_signature:\n            # if this is the standard model signature, use extra_data as the extra args name\n            var_kw_name = 'extra_data'\n        else:\n            # else start from var_kw\n            var_kw_name = var_kw.name\n\n        # generate a name that's definitely unique\n        while var_kw_name in fields:\n            var_kw_name += '_'\n        merged_params[var_kw_name] = var_kw.replace(name=var_kw_name)\n\n    return Signature(parameters=list(merged_params.values()), return_annotation=None)", "metadata": {"license": "MIT", "len_tokens": 527}}
{"id": "pydantic:pydantic/v1/utils.py", "language": "python", "code": "class Representation:\n    \"\"\"\n    Mixin to provide __str__, __repr__, and __pretty__ methods. See #884 for more details.\n\n    __pretty__ is used by [devtools](https://python-devtools.helpmanual.io/) to provide human readable representations\n    of objects.\n    \"\"\"\n\n    __slots__: Tuple[str, ...] = tuple()\n\n    def __repr_args__(self) -> 'ReprArgs':\n        \"\"\"\n        Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n\n        Can either return:\n        * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n        * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n        \"\"\"\n        attrs = ((s, getattr(self, s)) for s in self.__slots__)\n        return [(a, v) for a, v in attrs if v is not None]\n\n    def __repr_name__(self) -> str:\n        \"\"\"\n        Name of the instance's class, used in __repr__.\n        \"\"\"\n        return self.__class__.__name__\n\n    def __repr_str__(self, join_str: str) -> str:\n        return join_str.join(repr(v) if a is None else f'{a}={v!r}' for a, v in self.__repr_args__())\n\n    def __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, None, None]:\n        \"\"\"\n        Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n        \"\"\"\n        yield self.__repr_name__() + '('\n        yield 1\n        for name, value in self.__repr_args__():\n            if name is not None:\n                yield name + '='\n            yield fmt(value)\n            yield ','\n            yield 0\n        yield -1\n        yield ')'\n\n    def __str__(self) -> str:\n        return self.__repr_str__(' ')\n\n    def __repr__(self) -> str:\n        return f'{self.__repr_name__()}({self.__repr_str__(\", \")})'\n\n    def __rich_repr__(self) -> 'RichReprResult':\n        \"\"\"Get fields for Rich library\"\"\"\n        for name, field_repr in self.__repr_args__():\n            if name is None:\n                yield field_repr\n            else:\n                yield name, field_repr", "metadata": {"license": "MIT", "len_tokens": 531}}
{"id": "pydantic:pydantic/v1/utils.py", "language": "python", "code": "class GetterDict(Representation):\n    \"\"\"\n    Hack to make object's smell just enough like dicts for validate_model.\n\n    We can't inherit from Mapping[str, Any] because it upsets cython so we have to implement all methods ourselves.\n    \"\"\"\n\n    __slots__ = ('_obj',)\n\n    def __init__(self, obj: Any):\n        self._obj = obj\n\n    def __getitem__(self, key: str) -> Any:\n        try:\n            return getattr(self._obj, key)\n        except AttributeError as e:\n            raise KeyError(key) from e\n\n    def get(self, key: Any, default: Any = None) -> Any:\n        return getattr(self._obj, key, default)\n\n    def extra_keys(self) -> Set[Any]:\n        \"\"\"\n        We don't want to get any other attributes of obj if the model didn't explicitly ask for them\n        \"\"\"\n        return set()\n\n    def keys(self) -> List[Any]:\n        \"\"\"\n        Keys of the pseudo dictionary, uses a list not set so order information can be maintained like python\n        dictionaries.\n        \"\"\"\n        return list(self)\n\n    def values(self) -> List[Any]:\n        return [self[k] for k in self]\n\n    def items(self) -> Iterator[Tuple[str, Any]]:\n        for k in self:\n            yield k, self.get(k)\n\n    def __iter__(self) -> Iterator[str]:\n        for name in dir(self._obj):\n            if not name.startswith('_'):\n                yield name\n\n    def __len__(self) -> int:\n        return sum(1 for _ in self)\n\n    def __contains__(self, item: Any) -> bool:\n        return item in self.keys()\n\n    def __eq__(self, other: Any) -> bool:\n        return dict(self) == dict(other.items())\n\n    def __repr_args__(self) -> 'ReprArgs':\n        return [(None, dict(self))]\n\n    def __repr_name__(self) -> str:\n        return f'GetterDict[{display_as_type(self._obj)}]'", "metadata": {"license": "MIT", "len_tokens": 421}}
{"id": "pydantic:pydantic/v1/utils.py", "language": "python", "code": "def get_discriminator_alias_and_values(tp: Any, discriminator_key: str) -> Tuple[str, Tuple[str, ...]]:\n    \"\"\"\n    Get alias and all valid values in the `Literal` type of the discriminator field\n    `tp` can be a `BaseModel` class or directly an `Annotated` `Union` of many.\n    \"\"\"\n    is_root_model = getattr(tp, '__custom_root_type__', False)\n\n    if get_origin(tp) is Annotated:\n        tp = get_args(tp)[0]\n\n    if hasattr(tp, '__pydantic_model__'):\n        tp = tp.__pydantic_model__\n\n    if is_union(get_origin(tp)):\n        alias, all_values = _get_union_alias_and_all_values(tp, discriminator_key)\n        return alias, tuple(v for values in all_values for v in values)\n    elif is_root_model:\n        union_type = tp.__fields__[ROOT_KEY].type_\n        alias, all_values = _get_union_alias_and_all_values(union_type, discriminator_key)\n\n        if len(set(all_values)) > 1:\n            raise ConfigError(\n                f'Field {discriminator_key!r} is not the same for all submodels of {display_as_type(tp)!r}'\n            )\n\n        return alias, all_values[0]\n\n    else:\n        try:\n            t_discriminator_type = tp.__fields__[discriminator_key].type_\n        except AttributeError as e:\n            raise TypeError(f'Type {tp.__name__!r} is not a valid `BaseModel` or `dataclass`') from e\n        except KeyError as e:\n            raise ConfigError(f'Model {tp.__name__!r} needs a discriminator field for key {discriminator_key!r}') from e\n\n        if not is_literal_type(t_discriminator_type):\n            raise ConfigError(f'Field {discriminator_key!r} of model {tp.__name__!r} needs to be a `Literal`')\n\n        return tp.__fields__[discriminator_key].alias, all_literal_values(t_discriminator_type)", "metadata": {"license": "MIT", "len_tokens": 426}}
{"id": "pydantic:pydantic/v1/utils.py", "language": "python", "code": "def _normalize_indexes(self, items: 'MappingIntStrAny', v_length: int) -> 'DictIntStrAny':\n        \"\"\"\n        :param items: dict or set of indexes which will be normalized\n        :param v_length: length of sequence indexes of which will be\n\n        >>> self._normalize_indexes({0: True, -2: True, -1: True}, 4)\n        {0: True, 2: True, 3: True}\n        >>> self._normalize_indexes({'__all__': True}, 4)\n        {0: True, 1: True, 2: True, 3: True}\n        \"\"\"\n\n        normalized_items: 'DictIntStrAny' = {}\n        all_items = None\n        for i, v in items.items():\n            if not (isinstance(v, Mapping) or isinstance(v, AbstractSet) or self.is_true(v)):\n                raise TypeError(f'Unexpected type of exclude value for index \"{i}\" {v.__class__}')\n            if i == '__all__':\n                all_items = self._coerce_value(v)\n                continue\n            if not isinstance(i, int):\n                raise TypeError(\n                    'Excluding fields from a sequence of sub-models or dicts must be performed index-wise: '\n                    'expected integer keys or keyword \"__all__\"'\n                )\n            normalized_i = v_length + i if i < 0 else i\n            normalized_items[normalized_i] = self.merge(v, normalized_items.get(normalized_i))\n\n        if not all_items:\n            return normalized_items\n        if self.is_true(all_items):\n            for i in range(v_length):\n                normalized_items.setdefault(i, ...)\n            return normalized_items\n        for i in range(v_length):\n            normalized_item = normalized_items.setdefault(i, {})\n            if not self.is_true(normalized_item):\n                normalized_items[i] = self.merge(all_items, normalized_item)\n        return normalized_items", "metadata": {"license": "MIT", "len_tokens": 399}}
{"id": "pydantic:pydantic/v1/utils.py", "language": "python", "code": "def merge(cls, base: Any, override: Any, intersect: bool = False) -> Any:\n        \"\"\"\n        Merge a ``base`` item with an ``override`` item.\n\n        Both ``base`` and ``override`` are converted to dictionaries if possible.\n        Sets are converted to dictionaries with the sets entries as keys and\n        Ellipsis as values.\n\n        Each key-value pair existing in ``base`` is merged with ``override``,\n        while the rest of the key-value pairs are updated recursively with this function.\n\n        Merging takes place based on the \"union\" of keys if ``intersect`` is\n        set to ``False`` (default) and on the intersection of keys if\n        ``intersect`` is set to ``True``.\n        \"\"\"\n        override = cls._coerce_value(override)\n        base = cls._coerce_value(base)\n        if override is None:\n            return base\n        if cls.is_true(base) or base is None:\n            return override\n        if cls.is_true(override):\n            return base if intersect else override\n\n        # intersection or union of keys while preserving ordering:\n        if intersect:\n            merge_keys = [k for k in base if k in override] + [k for k in override if k in base]\n        else:\n            merge_keys = list(base) + [k for k in override if k not in base]\n\n        merged: 'DictIntStrAny' = {}\n        for k in merge_keys:\n            merged_item = cls.merge(base.get(k), override.get(k), intersect=intersect)\n            if merged_item is not None:\n                merged[k] = merged_item\n\n        return merged", "metadata": {"license": "MIT", "len_tokens": 341}}
{"id": "pydantic:pydantic/v1/annotated_types.py", "language": "python", "code": "import sys\nfrom typing import TYPE_CHECKING, Any, Dict, FrozenSet, NamedTuple, Type\n\nfrom pydantic.v1.fields import Required\nfrom pydantic.v1.main import BaseModel, create_model\nfrom pydantic.v1.typing import is_typeddict, is_typeddict_special\n\nif TYPE_CHECKING:\n    from typing_extensions import TypedDict\n\nif sys.version_info < (3, 11):\n\n    def is_legacy_typeddict(typeddict_cls: Type['TypedDict']) -> bool:  # type: ignore[valid-type]\n        return is_typeddict(typeddict_cls) and type(typeddict_cls).__module__ == 'typing'\n\nelse:\n\n    def is_legacy_typeddict(_: Any) -> Any:\n        return False\n\n\ndef create_model_from_typeddict(\n    # Mypy bug: `Type[TypedDict]` is resolved as `Any` https://github.com/python/mypy/issues/11030\n    typeddict_cls: Type['TypedDict'],  # type: ignore[valid-type]\n    **kwargs: Any,\n) -> Type['BaseModel']:\n    \"\"\"\n    Create a `BaseModel` based on the fields of a `TypedDict`.\n    Since `typing.TypedDict` in Python 3.8 does not store runtime information about optional keys,\n    we raise an error if this happens (see https://bugs.python.org/issue38834).\n    \"\"\"\n    field_definitions: Dict[str, Any]\n\n    # Best case scenario: with python 3.9+ or when `TypedDict` is imported from `typing_extensions`\n    if not hasattr(typeddict_cls, '__required_keys__'):\n        raise TypeError(\n            'You should use `typing_extensions.TypedDict` instead of `typing.TypedDict` with Python < 3.9.2. '\n            'Without it, there is no way to differentiate required and optional fields when subclassed.'\n        )\n\n    if is_legacy_typeddict(typeddict_cls) and any(\n        is_typeddict_special(t) for t in typeddict_cls.__annotations__.values()\n    ):\n        raise TypeError(\n            'You should use `typing_extensions.TypedDict` instead of `typing.TypedDict` with Python < 3.11. '\n            'Without it, there is no way to reflect Required/NotRequired keys.'\n        )\n\n    required_keys: FrozenSet[str] = typeddict_cls.__required_keys__  # type: ignore[attr-defined]\n    field_definitions = {\n        field_name: (field_type, Required if field_name in required_keys else None)\n        for field_name, field_type in typeddict_cls.__annotations__.items()\n    }\n\n    return create_model(typeddict_cls.__name__, **kwargs, **field_definitions)\n\n\ndef create_model_from_namedtuple(namedtuple_cls: Type['NamedTuple'], **kwargs: Any) -> Type['BaseModel']:\n    \"\"\"\n    Create a `BaseModel` based on the fields of a named tuple.\n    A named tuple can be created with `typing.NamedTuple` and declared annotations\n    but also with `collections.namedtuple`, in this case we consider all fields\n    to have type `Any`.\n    \"\"\"\n    # With python 3.10+, `__annotations__` always exists but can be empty hence the `getattr... or...` logic\n    namedtuple_annotations: Dict[str, Type[Any]] = getattr(namedtuple_cls, '__annotations__', None) or {\n        k: Any for k in namedtuple_cls._fields\n    }\n    field_definitions: Dict[str, Any] = {\n        field_name: (field_type, Required) for field_name, field_type in namedtuple_annotations.items()\n    }\n    return create_model(namedtuple_cls.__name__, **kwargs, **field_definitions)\n", "metadata": {"license": "MIT", "len_tokens": 798}}
{"id": "pydantic:pydantic/v1/annotated_types.py", "language": "python", "code": "def create_model_from_typeddict(\n    # Mypy bug: `Type[TypedDict]` is resolved as `Any` https://github.com/python/mypy/issues/11030\n    typeddict_cls: Type['TypedDict'],  # type: ignore[valid-type]\n    **kwargs: Any,\n) -> Type['BaseModel']:\n    \"\"\"\n    Create a `BaseModel` based on the fields of a `TypedDict`.\n    Since `typing.TypedDict` in Python 3.8 does not store runtime information about optional keys,\n    we raise an error if this happens (see https://bugs.python.org/issue38834).\n    \"\"\"\n    field_definitions: Dict[str, Any]\n\n    # Best case scenario: with python 3.9+ or when `TypedDict` is imported from `typing_extensions`\n    if not hasattr(typeddict_cls, '__required_keys__'):\n        raise TypeError(\n            'You should use `typing_extensions.TypedDict` instead of `typing.TypedDict` with Python < 3.9.2. '\n            'Without it, there is no way to differentiate required and optional fields when subclassed.'\n        )\n\n    if is_legacy_typeddict(typeddict_cls) and any(\n        is_typeddict_special(t) for t in typeddict_cls.__annotations__.values()\n    ):\n        raise TypeError(\n            'You should use `typing_extensions.TypedDict` instead of `typing.TypedDict` with Python < 3.11. '\n            'Without it, there is no way to reflect Required/NotRequired keys.'\n        )\n\n    required_keys: FrozenSet[str] = typeddict_cls.__required_keys__  # type: ignore[attr-defined]\n    field_definitions = {\n        field_name: (field_type, Required if field_name in required_keys else None)\n        for field_name, field_type in typeddict_cls.__annotations__.items()\n    }\n\n    return create_model(typeddict_cls.__name__, **kwargs, **field_definitions)", "metadata": {"license": "MIT", "len_tokens": 419}}
{"id": "pydantic:pydantic/v1/annotated_types.py", "language": "python", "code": "def create_model_from_namedtuple(namedtuple_cls: Type['NamedTuple'], **kwargs: Any) -> Type['BaseModel']:\n    \"\"\"\n    Create a `BaseModel` based on the fields of a named tuple.\n    A named tuple can be created with `typing.NamedTuple` and declared annotations\n    but also with `collections.namedtuple`, in this case we consider all fields\n    to have type `Any`.\n    \"\"\"\n    # With python 3.10+, `__annotations__` always exists but can be empty hence the `getattr... or...` logic\n    namedtuple_annotations: Dict[str, Type[Any]] = getattr(namedtuple_cls, '__annotations__', None) or {\n        k: Any for k in namedtuple_cls._fields\n    }\n    field_definitions: Dict[str, Any] = {\n        field_name: (field_type, Required) for field_name, field_type in namedtuple_annotations.items()\n    }\n    return create_model(namedtuple_cls.__name__, **kwargs, **field_definitions)", "metadata": {"license": "MIT", "len_tokens": 212}}
{"id": "pydantic:pydantic/v1/dataclasses.py", "language": "python", "code": "def dataclass(\n    _cls: Optional[Type[_T]] = None,\n    *,\n    init: bool = True,\n    repr: bool = True,\n    eq: bool = True,\n    order: bool = False,\n    unsafe_hash: bool = False,\n    frozen: bool = False,\n    config: Union[ConfigDict, Type[object], None] = None,\n    validate_on_init: Optional[bool] = None,\n    use_proxy: Optional[bool] = None,\n    kw_only: bool = False,\n) -> Union[Callable[[Type[_T]], 'DataclassClassOrWrapper'], 'DataclassClassOrWrapper']:\n    \"\"\"\n    Like the python standard lib dataclasses but with type validation.\n    The result is either a pydantic dataclass that will validate input data\n    or a wrapper that will trigger validation around a stdlib dataclass\n    to avoid modifying it directly\n    \"\"\"\n    the_config = get_config(config)\n\n    def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''  # needs to be done before generating dataclass\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(  # type: ignore\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls\n\n    if _cls is None:\n        return wrap\n\n    return wrap(_cls)", "metadata": {"license": "MIT", "len_tokens": 534}}
{"id": "pydantic:pydantic/v1/dataclasses.py", "language": "python", "code": "class DataclassProxy:\n    __slots__ = '__dataclass__'\n\n    def __init__(self, dc_cls: Type['Dataclass']) -> None:\n        object.__setattr__(self, '__dataclass__', dc_cls)\n\n    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n        with set_validation(self.__dataclass__, True):\n            return self.__dataclass__(*args, **kwargs)\n\n    def __getattr__(self, name: str) -> Any:\n        return getattr(self.__dataclass__, name)\n\n    def __setattr__(self, __name: str, __value: Any) -> None:\n        return setattr(self.__dataclass__, __name, __value)\n\n    def __instancecheck__(self, instance: Any) -> bool:\n        return isinstance(instance, self.__dataclass__)\n\n    def __copy__(self) -> 'DataclassProxy':\n        return DataclassProxy(copy.copy(self.__dataclass__))\n\n    def __deepcopy__(self, memo: Any) -> 'DataclassProxy':\n        return DataclassProxy(copy.deepcopy(self.__dataclass__, memo))", "metadata": {"license": "MIT", "len_tokens": 235}}
{"id": "pydantic:pydantic/v1/dataclasses.py", "language": "python", "code": "def create_pydantic_model_from_dataclass(\n    dc_cls: Type['Dataclass'],\n    config: Type[Any] = BaseConfig,\n    dc_cls_doc: Optional[str] = None,\n) -> Type['BaseModel']:\n    field_definitions: Dict[str, Any] = {}\n    for field in dataclasses.fields(dc_cls):\n        default: Any = Undefined\n        default_factory: Optional['NoArgAnyCallable'] = None\n        field_info: FieldInfo\n\n        if field.default is not dataclasses.MISSING:\n            default = field.default\n        elif field.default_factory is not dataclasses.MISSING:\n            default_factory = field.default_factory\n        else:\n            default = Required\n\n        if isinstance(default, FieldInfo):\n            field_info = default\n            dc_cls.__pydantic_has_field_info_default__ = True\n        else:\n            field_info = Field(default=default, default_factory=default_factory, **field.metadata)\n\n        field_definitions[field.name] = (field.type, field_info)\n\n    validators = gather_all_validators(dc_cls)\n    model: Type['BaseModel'] = create_model(\n        dc_cls.__name__,\n        __config__=config,\n        __module__=dc_cls.__module__,\n        __validators__=validators,\n        __cls_kwargs__={'__resolve_forward_refs__': False},\n        **field_definitions,\n    )\n    model.__doc__ = dc_cls_doc if dc_cls_doc is not None else dc_cls.__doc__ or ''\n    return model", "metadata": {"license": "MIT", "len_tokens": 309}}
{"id": "pydantic:pydantic/v1/dataclasses.py", "language": "python", "code": "def _dataclass_validate_values(self: 'Dataclass') -> None:\n    # validation errors can occur if this function is called twice on an already initialised dataclass.\n    # for example if Extra.forbid is enabled, it would consider __pydantic_initialised__ an invalid extra property\n    if getattr(self, '__pydantic_initialised__'):\n        return\n    if getattr(self, '__pydantic_has_field_info_default__', False):\n        # We need to remove `FieldInfo` values since they are not valid as input\n        # It's ok to do that because they are obviously the default values!\n        input_data = {\n            k: v\n            for k, v in self.__dict__.items()\n            if not (isinstance(v, FieldInfo) or _is_field_cached_property(self, k))\n        }\n    else:\n        input_data = {k: v for k, v in self.__dict__.items() if not _is_field_cached_property(self, k)}\n    d, _, validation_error = validate_model(self.__pydantic_model__, input_data, cls=self.__class__)\n    if validation_error:\n        raise validation_error\n    self.__dict__.update(d)\n    object.__setattr__(self, '__pydantic_initialised__', True)", "metadata": {"license": "MIT", "len_tokens": 266}}
{"id": "pydantic:pydantic/v1/dataclasses.py", "language": "python", "code": "def is_builtin_dataclass(_cls: Type[Any]) -> bool:\n    \"\"\"\n    Whether a class is a stdlib dataclass\n    (useful to discriminated a pydantic dataclass that is actually a wrapper around a stdlib dataclass)\n\n    we check that\n    - `_cls` is a dataclass\n    - `_cls` is not a processed pydantic dataclass (with a basemodel attached)\n    - `_cls` is not a pydantic dataclass inheriting directly from a stdlib dataclass\n    e.g.\n    ```\n    @dataclasses.dataclass\n    class A:\n        x: int\n\n    @pydantic.dataclasses.dataclass\n    class B(A):\n        y: int\n    ```\n    In this case, when we first check `B`, we make an extra check and look at the annotations ('y'),\n    which won't be a superset of all the dataclass fields (only the stdlib fields i.e. 'x')\n    \"\"\"\n    return (\n        dataclasses.is_dataclass(_cls)\n        and not hasattr(_cls, '__pydantic_model__')\n        and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls, '__annotations__', {})))\n    )", "metadata": {"license": "MIT", "len_tokens": 261}}
{"id": "pydantic:pydantic/v1/dataclasses.py", "language": "python", "code": "class Dataclass:\n        # stdlib attributes\n        __dataclass_fields__: ClassVar[Dict[str, Any]]\n        __dataclass_params__: ClassVar[Any]  # in reality `dataclasses._DataclassParams`\n        __post_init__: ClassVar[Callable[..., None]]\n\n        # Added by pydantic\n        __pydantic_run_validation__: ClassVar[bool]\n        __post_init_post_parse__: ClassVar[Callable[..., None]]\n        __pydantic_initialised__: ClassVar[bool]\n        __pydantic_model__: ClassVar[Type[BaseModel]]\n        __pydantic_validate_values__: ClassVar[Callable[['Dataclass'], None]]\n        __pydantic_has_field_info_default__: ClassVar[bool]  # whether a `pydantic.Field` is used as default value\n\n        def __init__(self, *args: object, **kwargs: object) -> None:\n            pass\n\n        @classmethod\n        def __get_validators__(cls: Type['Dataclass']) -> 'CallableGenerator':\n            pass\n\n        @classmethod\n        def __validate__(cls: Type['DataclassT'], v: Any) -> 'DataclassT':\n            pass", "metadata": {"license": "MIT", "len_tokens": 248}}
{"id": "pydantic:pydantic/v1/dataclasses.py", "language": "python", "code": "def wrap(cls: Type[Any]) -> 'DataclassClassOrWrapper':\n        should_use_proxy = (\n            use_proxy\n            if use_proxy is not None\n            else (\n                is_builtin_dataclass(cls)\n                and (cls.__bases__[0] is object or set(dir(cls)) == set(dir(cls.__bases__[0])))\n            )\n        )\n        if should_use_proxy:\n            dc_cls_doc = ''\n            dc_cls = DataclassProxy(cls)\n            default_validate_on_init = False\n        else:\n            dc_cls_doc = cls.__doc__ or ''  # needs to be done before generating dataclass\n            if sys.version_info >= (3, 10):\n                dc_cls = dataclasses.dataclass(\n                    cls,\n                    init=init,\n                    repr=repr,\n                    eq=eq,\n                    order=order,\n                    unsafe_hash=unsafe_hash,\n                    frozen=frozen,\n                    kw_only=kw_only,\n                )\n            else:\n                dc_cls = dataclasses.dataclass(  # type: ignore\n                    cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen\n                )\n            default_validate_on_init = True\n\n        should_validate_on_init = default_validate_on_init if validate_on_init is None else validate_on_init\n        _add_pydantic_validation_attributes(cls, the_config, should_validate_on_init, dc_cls_doc)\n        dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__: cls})\n        return dc_cls", "metadata": {"license": "MIT", "len_tokens": 317}}
{"id": "pydantic:pydantic/v1/dataclasses.py", "language": "python", "code": "def new_init(self: 'Dataclass', *args: Any, **kwargs: Any) -> None:\n            handle_extra_init(self, *args, **kwargs)\n\n            if self.__class__.__pydantic_run_validation__:\n                self.__pydantic_validate_values__()\n\n            if hasattr(self, '__post_init_post_parse__'):\n                # We need to find again the initvars. To do that we use `__dataclass_fields__` instead of\n                # public method `dataclasses.fields`\n\n                # get all initvars and their default values\n                initvars_and_values: Dict[str, Any] = {}\n                for i, f in enumerate(self.__class__.__dataclass_fields__.values()):\n                    if f._field_type is dataclasses._FIELD_INITVAR:  # type: ignore[attr-defined]\n                        try:\n                            # set arg value by default\n                            initvars_and_values[f.name] = args[i]\n                        except IndexError:\n                            initvars_and_values[f.name] = kwargs.get(f.name, f.default)\n\n                self.__post_init_post_parse__(**initvars_and_values)", "metadata": {"license": "MIT", "len_tokens": 225}}
{"id": "pydantic:pydantic/v1/parse.py", "language": "python", "code": "import json\nimport pickle\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Callable, Union\n\nfrom pydantic.v1.types import StrBytes\n\n\nclass Protocol(str, Enum):\n    json = 'json'\n    pickle = 'pickle'\n\n\ndef load_str_bytes(\n    b: StrBytes,\n    *,\n    content_type: str = None,\n    encoding: str = 'utf8',\n    proto: Protocol = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n) -> Any:\n    if proto is None and content_type:\n        if content_type.endswith(('json', 'javascript')):\n            pass\n        elif allow_pickle and content_type.endswith('pickle'):\n            proto = Protocol.pickle\n        else:\n            raise TypeError(f'Unknown content-type: {content_type}')\n\n    proto = proto or Protocol.json\n\n    if proto == Protocol.json:\n        if isinstance(b, bytes):\n            b = b.decode(encoding)\n        return json_loads(b)\n    elif proto == Protocol.pickle:\n        if not allow_pickle:\n            raise RuntimeError('Trying to decode with pickle with allow_pickle=False')\n        bb = b if isinstance(b, bytes) else b.encode()\n        return pickle.loads(bb)\n    else:\n        raise TypeError(f'Unknown protocol: {proto}')\n\n\ndef load_file(\n    path: Union[str, Path],\n    *,\n    content_type: str = None,\n    encoding: str = 'utf8',\n    proto: Protocol = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n) -> Any:\n    path = Path(path)\n    b = path.read_bytes()\n    if content_type is None:\n        if path.suffix in ('.js', '.json'):\n            proto = Protocol.json\n        elif path.suffix == '.pkl':\n            proto = Protocol.pickle\n\n    return load_str_bytes(\n        b, proto=proto, content_type=content_type, encoding=encoding, allow_pickle=allow_pickle, json_loads=json_loads\n    )\n", "metadata": {"license": "MIT", "len_tokens": 437}}
{"id": "pydantic:pydantic/v1/parse.py", "language": "python", "code": "def load_str_bytes(\n    b: StrBytes,\n    *,\n    content_type: str = None,\n    encoding: str = 'utf8',\n    proto: Protocol = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n) -> Any:\n    if proto is None and content_type:\n        if content_type.endswith(('json', 'javascript')):\n            pass\n        elif allow_pickle and content_type.endswith('pickle'):\n            proto = Protocol.pickle\n        else:\n            raise TypeError(f'Unknown content-type: {content_type}')\n\n    proto = proto or Protocol.json\n\n    if proto == Protocol.json:\n        if isinstance(b, bytes):\n            b = b.decode(encoding)\n        return json_loads(b)\n    elif proto == Protocol.pickle:\n        if not allow_pickle:\n            raise RuntimeError('Trying to decode with pickle with allow_pickle=False')\n        bb = b if isinstance(b, bytes) else b.encode()\n        return pickle.loads(bb)\n    else:\n        raise TypeError(f'Unknown protocol: {proto}')", "metadata": {"license": "MIT", "len_tokens": 223}}
{"id": "pydantic:pydantic/v1/typing.py", "language": "python", "code": "def resolve_annotations(raw_annotations: Dict[str, Type[Any]], module_name: Optional[str]) -> Dict[str, Type[Any]]:\n    \"\"\"\n    Partially taken from typing.get_type_hints.\n\n    Resolve string or ForwardRef annotations into type objects if possible.\n    \"\"\"\n    base_globals: Optional[Dict[str, Any]] = None\n    if module_name:\n        try:\n            module = sys.modules[module_name]\n        except KeyError:\n            # happens occasionally, see https://github.com/pydantic/pydantic/issues/2363\n            pass\n        else:\n            base_globals = module.__dict__\n\n    annotations = {}\n    for name, value in raw_annotations.items():\n        if isinstance(value, str):\n            if (3, 10) > sys.version_info >= (3, 9, 8) or sys.version_info >= (3, 10, 1):\n                value = ForwardRef(value, is_argument=False, is_class=True)\n            else:\n                value = ForwardRef(value, is_argument=False)\n        try:\n            if sys.version_info >= (3, 13):\n                value = _eval_type(value, base_globals, None, type_params=())\n            else:\n                value = _eval_type(value, base_globals, None)\n        except NameError:\n            # this is ok, it can be fixed with update_forward_refs\n            pass\n        annotations[name] = value\n    return annotations", "metadata": {"license": "MIT", "len_tokens": 290}}
{"id": "pydantic:pydantic/v1/typing.py", "language": "python", "code": "def update_model_forward_refs(\n    model: Type[Any],\n    fields: Iterable['ModelField'],\n    json_encoders: Dict[Union[Type[Any], str, ForwardRef], AnyCallable],\n    localns: 'DictStrAny',\n    exc_to_suppress: Tuple[Type[BaseException], ...] = (),\n) -> None:\n    \"\"\"\n    Try to update model fields ForwardRefs based on model and localns.\n    \"\"\"\n    if model.__module__ in sys.modules:\n        globalns = sys.modules[model.__module__].__dict__.copy()\n    else:\n        globalns = {}\n\n    globalns.setdefault(model.__name__, model)\n\n    for f in fields:\n        try:\n            update_field_forward_refs(f, globalns=globalns, localns=localns)\n        except exc_to_suppress:\n            pass\n\n    for key in set(json_encoders.keys()):\n        if isinstance(key, str):\n            fr: ForwardRef = ForwardRef(key)\n        elif isinstance(key, ForwardRef):\n            fr = key\n        else:\n            continue\n\n        try:\n            new_key = evaluate_forwardref(fr, globalns, localns or None)\n        except exc_to_suppress:  # pragma: no cover\n            continue\n\n        json_encoders[new_key] = json_encoders.pop(key)", "metadata": {"license": "MIT", "len_tokens": 269}}
{"id": "pydantic:pydantic/v1/typing.py", "language": "python", "code": "def _generic_get_args(tp: Type[Any]) -> Tuple[Any, ...]:\n        \"\"\"\n        In python 3.9, `typing.Dict`, `typing.List`, ...\n        do have an empty `__args__` by default (instead of the generic ~T for example).\n        In order to still support `Dict` for example and consider it as `Dict[Any, Any]`,\n        we retrieve the `_nparams` value that tells us how many parameters it needs.\n        \"\"\"\n        if hasattr(tp, '_nparams'):\n            return (Any,) * tp._nparams\n        # Special case for `tuple[()]`, which used to return ((),) with `typing.Tuple`\n        # in python 3.10- but now returns () for `tuple` and `Tuple`.\n        # This will probably be clarified in pydantic v2\n        try:\n            if tp == Tuple[()] or sys.version_info >= (3, 9) and tp == tuple[()]:  # type: ignore[misc]\n                return ((),)\n        # there is a TypeError when compiled with cython\n        except TypeError:  # pragma: no cover\n            pass\n        return ()", "metadata": {"license": "MIT", "len_tokens": 248}}
{"id": "pydantic:pydantic/v1/typing.py", "language": "python", "code": "def convert_generics(tp: Type[Any]) -> Type[Any]:\n        \"\"\"\n        Recursively searches for `str` type hints and replaces them with ForwardRef.\n\n        Examples::\n            convert_generics(list['Hero']) == list[ForwardRef('Hero')]\n            convert_generics(dict['Hero', 'Team']) == dict[ForwardRef('Hero'), ForwardRef('Team')]\n            convert_generics(typing.Dict['Hero', 'Team']) == typing.Dict[ForwardRef('Hero'), ForwardRef('Team')]\n            convert_generics(list[str | 'Hero'] | int) == list[str | ForwardRef('Hero')] | int\n        \"\"\"\n        origin = get_origin(tp)\n        if not origin or not hasattr(tp, '__args__'):\n            return tp\n\n        args = get_args(tp)\n\n        # typing.Annotated needs special treatment\n        if origin is Annotated:\n            return Annotated[(convert_generics(args[0]), *args[1:])]  # type: ignore\n\n        # recursively replace `str` instances inside of `GenericAlias` with `ForwardRef(arg)`\n        converted = tuple(\n            ForwardRef(arg) if isinstance(arg, str) and isinstance(tp, TypingGenericAlias) else convert_generics(arg)\n            for arg in args\n        )\n\n        if converted == args:\n            return tp\n        elif isinstance(tp, TypingGenericAlias):\n            return TypingGenericAlias(origin, converted)\n        elif isinstance(tp, TypesUnionType):\n            # recreate types.UnionType (PEP604, Python >= 3.10)\n            return functools.reduce(operator.or_, converted)  # type: ignore\n        else:\n            try:\n                setattr(tp, '__args__', converted)\n            except AttributeError:\n                pass\n            return tp", "metadata": {"license": "MIT", "len_tokens": 360}}
{"id": "pydantic:pydantic/v1/main.py", "language": "python", "code": "def create_model(\n    __model_name: str,\n    *,\n    __config__: Optional[Type[BaseConfig]] = None,\n    __base__: Union[None, Type['Model'], Tuple[Type['Model'], ...]] = None,\n    __module__: str = __name__,\n    __validators__: Dict[str, 'AnyClassMethod'] = None,\n    __cls_kwargs__: Dict[str, Any] = None,\n    __slots__: Optional[Tuple[str, ...]] = None,\n    **field_definitions: Any,\n) -> Type['Model']:\n    \"\"\"\n    Dynamically create a model.\n    :param __model_name: name of the created model\n    :param __config__: config class to use for the new model\n    :param __base__: base class for the new model to inherit from\n    :param __module__: module of the created model\n    :param __validators__: a dict of method names and @validator class methods\n    :param __cls_kwargs__: a dict for class creation\n    :param __slots__: Deprecated, `__slots__` should not be passed to `create_model`\n    :param field_definitions: fields of the model (or extra fields if a base is supplied)\n        in the format `<name>=(<type>, <default default>)` or `<name>=<default value>, e.g.\n        `foobar=(str, ...)` or `foobar=123`, or, for complex use-cases, in the format\n        `<name>=<Field>` or `<name>=(<type>, <FieldInfo>)`, e.g.\n        `foo=Field(datetime, default_factory=datetime.utcnow, alias='bar')` or\n        `foo=(str, FieldInfo(title='Foo'))`\n    \"\"\"\n    if __slots__ is not None:\n        # __slots__ will be ignored from here on\n        warnings.warn('__slots__ should not be passed to create_model', RuntimeWarning)\n\n    if __base__ is not None:\n        if __config__ is not None:\n            raise ConfigError('to avoid confusion __config__ and __base__ cannot be used together')\n        if not isinstance(__base__, tuple):\n            __base__ = (__base__,)\n    else:\n        __base__ = (cast(Type['Model'], BaseModel),)\n\n    __cls_kwargs__ = __cls_kwargs__ or {}\n\n    fields = {}\n    annotations = {}\n\n    for f_name, f_def in field_definitions.items():\n        if not is_valid_field(f_name):\n            warnings.warn(f'fields may not start with an underscore, ignoring \"{f_name}\"', RuntimeWarning)\n        if isinstance(f_def, tuple):\n            try:\n                f_annotation, f_value = f_def\n            except ValueError as e:\n                raise ConfigError(\n                    'field definitions should either be a tuple of (<type>, <default>) or just a '\n                    'default value, unfortunately this means tuples as '\n                    'default values are not allowed'\n                ) from e\n        else:\n            f_annotation, f_value = None, f_def\n\n        if f_annotation:\n            annotations[f_name] = f_annotation\n        fields[f_name] = f_value\n\n    namespace: 'DictStrAny' = {'__annotations__': annotations, '__module__': __module__}\n    if __validators__:\n        namespace.update(__validators__)\n    namespace.update(fields)\n    if __config__:\n        namespace['Config'] = inherit_config(__config__, BaseConfig)\n    resolved_bases = resolve_bases(__base__)\n    meta, ns, kwds = prepare_class(__model_name, resolved_bases, kwds=__cls_kwargs__)\n    if resolved_bases is not __base__:\n        ns['__orig_bases__'] = __base__\n    namespace.update(ns)\n    return meta(__model_name, resolved_bases, namespace, **kwds)", "metadata": {"license": "MIT", "len_tokens": 797}}
{"id": "pydantic:pydantic/v1/main.py", "language": "python", "code": "def validate_model(  # noqa: C901 (ignore complexity)\n    model: Type[BaseModel], input_data: 'DictStrAny', cls: 'ModelOrDc' = None\n) -> Tuple['DictStrAny', 'SetStr', Optional[ValidationError]]:\n    \"\"\"\n    validate data against a model.\n    \"\"\"\n    values = {}\n    errors = []\n    # input_data names, possibly alias\n    names_used = set()\n    # field names, never aliases\n    fields_set = set()\n    config = model.__config__\n    check_extra = config.extra is not Extra.ignore\n    cls_ = cls or model\n\n    for validator in model.__pre_root_validators__:\n        try:\n            input_data = validator(cls_, input_data)\n        except (ValueError, TypeError, AssertionError) as exc:\n            return {}, set(), ValidationError([ErrorWrapper(exc, loc=ROOT_KEY)], cls_)\n\n    for name, field in model.__fields__.items():\n        value = input_data.get(field.alias, _missing)\n        using_name = False\n        if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n            value = input_data.get(field.name, _missing)\n            using_name = True\n\n        if value is _missing:\n            if field.required:\n                errors.append(ErrorWrapper(MissingError(), loc=field.alias))\n                continue\n\n            value = field.get_default()\n\n            if not config.validate_all and not field.validate_always:\n                values[name] = value\n                continue\n        else:\n            fields_set.add(name)\n            if check_extra:\n                names_used.add(field.name if using_name else field.alias)\n\n        v_, errors_ = field.validate(value, values, loc=field.alias, cls=cls_)\n        if isinstance(errors_, ErrorWrapper):\n            errors.append(errors_)\n        elif isinstance(errors_, list):\n            errors.extend(errors_)\n        else:\n            values[name] = v_\n\n    if check_extra:\n        if isinstance(input_data, GetterDict):\n            extra = input_data.extra_keys() - names_used\n        else:\n            extra = input_data.keys() - names_used\n        if extra:\n            fields_set |= extra\n            if config.extra is Extra.allow:\n                for f in extra:\n                    values[f] = input_data[f]\n            else:\n                for f in sorted(extra):\n                    errors.append(ErrorWrapper(ExtraError(), loc=f))\n\n    for skip_on_failure, validator in model.__post_root_validators__:\n        if skip_on_failure and errors:\n            continue\n        try:\n            values = validator(cls_, values)\n        except (ValueError, TypeError, AssertionError) as exc:\n            errors.append(ErrorWrapper(exc, loc=ROOT_KEY))\n\n    if errors:\n        return values, fields_set, ValidationError(errors, cls_)\n    else:\n        return values, fields_set, None", "metadata": {"license": "MIT", "len_tokens": 587}}
{"id": "pydantic:pydantic/v1/main.py", "language": "python", "code": "def __setattr__(self, name, value):  # noqa: C901 (ignore complexity)\n        if name in self.__private_attributes__ or name in DUNDER_ATTRIBUTES:\n            return object_setattr(self, name, value)\n\n        if self.__config__.extra is not Extra.allow and name not in self.__fields__:\n            raise ValueError(f'\"{self.__class__.__name__}\" object has no field \"{name}\"')\n        elif not self.__config__.allow_mutation or self.__config__.frozen:\n            raise TypeError(f'\"{self.__class__.__name__}\" is immutable and does not support item assignment')\n        elif name in self.__fields__ and self.__fields__[name].final:\n            raise TypeError(\n                f'\"{self.__class__.__name__}\" object \"{name}\" field is final and does not support reassignment'\n            )\n        elif self.__config__.validate_assignment:\n            new_values = {**self.__dict__, name: value}\n\n            for validator in self.__pre_root_validators__:\n                try:\n                    new_values = validator(self.__class__, new_values)\n                except (ValueError, TypeError, AssertionError) as exc:\n                    raise ValidationError([ErrorWrapper(exc, loc=ROOT_KEY)], self.__class__)\n\n            known_field = self.__fields__.get(name, None)\n            if known_field:\n                # We want to\n                # - make sure validators are called without the current value for this field inside `values`\n                # - keep other values (e.g. submodels) untouched (using `BaseModel.dict()` will change them into dicts)\n                # - keep the order of the fields\n                if not known_field.field_info.allow_mutation:\n                    raise TypeError(f'\"{known_field.name}\" has allow_mutation set to False and cannot be assigned')\n                dict_without_original_value = {k: v for k, v in self.__dict__.items() if k != name}\n                value, error_ = known_field.validate(value, dict_without_original_value, loc=name, cls=self.__class__)\n                if error_:\n                    raise ValidationError([error_], self.__class__)\n                else:\n                    new_values[name] = value\n\n            errors = []\n            for skip_on_failure, validator in self.__post_root_validators__:\n                if skip_on_failure and errors:\n                    continue\n                try:\n                    new_values = validator(self.__class__, new_values)\n                except (ValueError, TypeError, AssertionError) as exc:\n                    errors.append(ErrorWrapper(exc, loc=ROOT_KEY))\n            if errors:\n                raise ValidationError(errors, self.__class__)\n\n            # update the whole __dict__ as other values than just `value`\n            # may be changed (e.g. with `root_validator`)\n            object_setattr(self, '__dict__', new_values)\n        else:\n            self.__dict__[name] = value\n\n        self.__fields_set__.add(name)", "metadata": {"license": "MIT", "len_tokens": 608}}
{"id": "pydantic:pydantic/v1/main.py", "language": "python", "code": "def dict(\n        self,\n        *,\n        include: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        exclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        by_alias: bool = False,\n        skip_defaults: Optional[bool] = None,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n    ) -> 'DictStrAny':\n        \"\"\"\n        Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n\n        \"\"\"\n        if skip_defaults is not None:\n            warnings.warn(\n                f'{self.__class__.__name__}.dict(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\n                DeprecationWarning,\n            )\n            exclude_unset = skip_defaults\n\n        return dict(\n            self._iter(\n                to_dict=True,\n                by_alias=by_alias,\n                include=include,\n                exclude=exclude,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n        )", "metadata": {"license": "MIT", "len_tokens": 236}}
{"id": "pydantic:pydantic/v1/main.py", "language": "python", "code": "def json(\n        self,\n        *,\n        include: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        exclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        by_alias: bool = False,\n        skip_defaults: Optional[bool] = None,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n        encoder: Optional[Callable[[Any], Any]] = None,\n        models_as_dict: bool = True,\n        **dumps_kwargs: Any,\n    ) -> str:\n        \"\"\"\n        Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n\n        `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n        \"\"\"\n        if skip_defaults is not None:\n            warnings.warn(\n                f'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\n                DeprecationWarning,\n            )\n            exclude_unset = skip_defaults\n        encoder = cast(Callable[[Any], Any], encoder or self.__json_encoder__)\n\n        # We don't directly call `self.dict()`, which does exactly this with `to_dict=True`\n        # because we want to be able to keep raw `BaseModel` instances and not as `dict`.\n        # This allows users to write custom JSON encoders for given `BaseModel` classes.\n        data = dict(\n            self._iter(\n                to_dict=models_as_dict,\n                by_alias=by_alias,\n                include=include,\n                exclude=exclude,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n        )\n        if self.__custom_root_type__:\n            data = data[ROOT_KEY]\n        return self.__config__.json_dumps(data, default=encoder, **dumps_kwargs)", "metadata": {"license": "MIT", "len_tokens": 416}}
{"id": "pydantic:pydantic/v1/main.py", "language": "python", "code": "def construct(cls: Type['Model'], _fields_set: Optional['SetStr'] = None, **values: Any) -> 'Model':\n        \"\"\"\n        Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n        Default values are respected, but no other validation is performed.\n        Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n        \"\"\"\n        m = cls.__new__(cls)\n        fields_values: Dict[str, Any] = {}\n        for name, field in cls.__fields__.items():\n            if field.alt_alias and field.alias in values:\n                fields_values[name] = values[field.alias]\n            elif name in values:\n                fields_values[name] = values[name]\n            elif not field.required:\n                fields_values[name] = field.get_default()\n        fields_values.update(values)\n        object_setattr(m, '__dict__', fields_values)\n        if _fields_set is None:\n            _fields_set = set(values.keys())\n        object_setattr(m, '__fields_set__', _fields_set)\n        m._init_private_attributes()\n        return m", "metadata": {"license": "MIT", "len_tokens": 235}}
{"id": "pydantic:pydantic/v1/main.py", "language": "python", "code": "def copy(\n        self: 'Model',\n        *,\n        include: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        exclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        update: Optional['DictStrAny'] = None,\n        deep: bool = False,\n    ) -> 'Model':\n        \"\"\"\n        Duplicate a model, optionally choose which fields to include, exclude and change.\n\n        :param include: fields to include in new model\n        :param exclude: fields to exclude from new model, as with values this takes precedence over include\n        :param update: values to change/add in the new model. Note: the data is not validated before creating\n            the new model: you should trust this data\n        :param deep: set to `True` to make a deep copy of the model\n        :return: new model instance\n        \"\"\"\n\n        values = dict(\n            self._iter(to_dict=False, by_alias=False, include=include, exclude=exclude, exclude_unset=False),\n            **(update or {}),\n        )\n\n        # new `__fields_set__` can have unset optional fields with a set value in `update` kwarg\n        if update:\n            fields_set = self.__fields_set__ | update.keys()\n        else:\n            fields_set = set(self.__fields_set__)\n\n        return self._copy_and_set_values(values, fields_set, deep=deep)", "metadata": {"license": "MIT", "len_tokens": 303}}
{"id": "pydantic:pydantic/v1/main.py", "language": "python", "code": "def validate(cls: Type['Model'], value: Any) -> 'Model':\n        if isinstance(value, cls):\n            copy_on_model_validation = cls.__config__.copy_on_model_validation\n            # whether to deep or shallow copy the model on validation, None means do not copy\n            deep_copy: Optional[bool] = None\n            if copy_on_model_validation not in {'deep', 'shallow', 'none'}:\n                # Warn about deprecated behavior\n                warnings.warn(\n                    \"`copy_on_model_validation` should be a string: 'deep', 'shallow' or 'none'\", DeprecationWarning\n                )\n                if copy_on_model_validation:\n                    deep_copy = False\n\n            if copy_on_model_validation == 'shallow':\n                # shallow copy\n                deep_copy = False\n            elif copy_on_model_validation == 'deep':\n                # deep copy\n                deep_copy = True\n\n            if deep_copy is None:\n                return value\n            else:\n                return value._copy_and_set_values(value.__dict__, value.__fields_set__, deep=deep_copy)\n\n        value = cls._enforce_dict_if_root(value)\n\n        if isinstance(value, dict):\n            return cls(**value)\n        elif cls.__config__.orm_mode:\n            return cls.from_orm(value)\n        else:\n            try:\n                value_as_dict = dict(value)\n            except (TypeError, ValueError) as e:\n                raise DictError() from e\n            return cls(**value_as_dict)", "metadata": {"license": "MIT", "len_tokens": 304}}
{"id": "pydantic:pydantic/v1/main.py", "language": "python", "code": "def _get_value(\n        cls,\n        v: Any,\n        to_dict: bool,\n        by_alias: bool,\n        include: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']],\n        exclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']],\n        exclude_unset: bool,\n        exclude_defaults: bool,\n        exclude_none: bool,\n    ) -> Any:\n        if isinstance(v, BaseModel):\n            if to_dict:\n                v_dict = v.dict(\n                    by_alias=by_alias,\n                    exclude_unset=exclude_unset,\n                    exclude_defaults=exclude_defaults,\n                    include=include,\n                    exclude=exclude,\n                    exclude_none=exclude_none,\n                )\n                if ROOT_KEY in v_dict:\n                    return v_dict[ROOT_KEY]\n                return v_dict\n            else:\n                return v.copy(include=include, exclude=exclude)\n\n        value_exclude = ValueItems(v, exclude) if exclude else None\n        value_include = ValueItems(v, include) if include else None\n\n        if isinstance(v, dict):\n            return {\n                k_: cls._get_value(\n                    v_,\n                    to_dict=to_dict,\n                    by_alias=by_alias,\n                    exclude_unset=exclude_unset,\n                    exclude_defaults=exclude_defaults,\n                    include=value_include and value_include.for_element(k_),\n                    exclude=value_exclude and value_exclude.for_element(k_),\n                    exclude_none=exclude_none,\n                )\n                for k_, v_ in v.items()\n                if (not value_exclude or not value_exclude.is_excluded(k_))\n                and (not value_include or value_include.is_included(k_))\n            }\n\n        elif sequence_like(v):\n            seq_args = (\n                cls._get_value(\n                    v_,\n                    to_dict=to_dict,\n                    by_alias=by_alias,\n                    exclude_unset=exclude_unset,\n                    exclude_defaults=exclude_defaults,\n                    include=value_include and value_include.for_element(i),\n                    exclude=value_exclude and value_exclude.for_element(i),\n                    exclude_none=exclude_none,\n                )\n                for i, v_ in enumerate(v)\n                if (not value_exclude or not value_exclude.is_excluded(i))\n                and (not value_include or value_include.is_included(i))\n            )\n\n            return v.__class__(*seq_args) if is_namedtuple(v.__class__) else v.__class__(seq_args)\n\n        elif isinstance(v, Enum) and getattr(cls.Config, 'use_enum_values', False):\n            return v.value\n\n        else:\n            return v", "metadata": {"license": "MIT", "len_tokens": 526}}
{"id": "pydantic:pydantic/v1/main.py", "language": "python", "code": "def _iter(\n        self,\n        to_dict: bool = False,\n        by_alias: bool = False,\n        include: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        exclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny']] = None,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n    ) -> 'TupleGenerator':\n        # Merge field set excludes with explicit exclude parameter with explicit overriding field set options.\n        # The extra \"is not None\" guards are not logically necessary but optimizes performance for the simple case.\n        if exclude is not None or self.__exclude_fields__ is not None:\n            exclude = ValueItems.merge(self.__exclude_fields__, exclude)\n\n        if include is not None or self.__include_fields__ is not None:\n            include = ValueItems.merge(self.__include_fields__, include, intersect=True)\n\n        allowed_keys = self._calculate_keys(\n            include=include, exclude=exclude, exclude_unset=exclude_unset  # type: ignore\n        )\n        if allowed_keys is None and not (to_dict or by_alias or exclude_unset or exclude_defaults or exclude_none):\n            # huge boost for plain _iter()\n            yield from self.__dict__.items()\n            return\n\n        value_exclude = ValueItems(self, exclude) if exclude is not None else None\n        value_include = ValueItems(self, include) if include is not None else None\n\n        for field_key, v in self.__dict__.items():\n            if (allowed_keys is not None and field_key not in allowed_keys) or (exclude_none and v is None):\n                continue\n\n            if exclude_defaults:\n                model_field = self.__fields__.get(field_key)\n                if not getattr(model_field, 'required', True) and getattr(model_field, 'default', _missing) == v:\n                    continue\n\n            if by_alias and field_key in self.__fields__:\n                dict_key = self.__fields__[field_key].alias\n            else:\n                dict_key = field_key\n\n            if to_dict or value_include or value_exclude:\n                v = self._get_value(\n                    v,\n                    to_dict=to_dict,\n                    by_alias=by_alias,\n                    include=value_include and value_include.for_element(field_key),\n                    exclude=value_exclude and value_exclude.for_element(field_key),\n                    exclude_unset=exclude_unset,\n                    exclude_defaults=exclude_defaults,\n                    exclude_none=exclude_none,\n                )\n            yield dict_key, v", "metadata": {"license": "MIT", "len_tokens": 531}}
{"id": "pydantic:pydantic/v1/datetime_parse.py", "language": "python", "code": "def parse_time(value: Union[time, StrBytesIntFloat]) -> time:\n    \"\"\"\n    Parse a time/string and return a datetime.time.\n\n    Raise ValueError if the input is well formatted but not a valid time.\n    Raise ValueError if the input isn't well formatted, in particular if it contains an offset.\n    \"\"\"\n    if isinstance(value, time):\n        return value\n\n    number = get_numeric(value, 'time')\n    if number is not None:\n        if number >= 86400:\n            # doesn't make sense since the time time loop back around to 0\n            raise errors.TimeError()\n        return (datetime.min + timedelta(seconds=number)).time()\n\n    if isinstance(value, bytes):\n        value = value.decode()\n\n    match = time_re.match(value)  # type: ignore\n    if match is None:\n        raise errors.TimeError()\n\n    kw = match.groupdict()\n    if kw['microsecond']:\n        kw['microsecond'] = kw['microsecond'].ljust(6, '0')\n\n    tzinfo = _parse_timezone(kw.pop('tzinfo'), errors.TimeError)\n    kw_: Dict[str, Union[None, int, timezone]] = {k: int(v) for k, v in kw.items() if v is not None}\n    kw_['tzinfo'] = tzinfo\n\n    try:\n        return time(**kw_)  # type: ignore\n    except ValueError:\n        raise errors.TimeError()", "metadata": {"license": "MIT", "len_tokens": 301}}
{"id": "pydantic:pydantic/v1/datetime_parse.py", "language": "python", "code": "def parse_datetime(value: Union[datetime, StrBytesIntFloat]) -> datetime:\n    \"\"\"\n    Parse a datetime/int/float/string and return a datetime.datetime.\n\n    This function supports time zone offsets. When the input contains one,\n    the output uses a timezone with a fixed offset from UTC.\n\n    Raise ValueError if the input is well formatted but not a valid datetime.\n    Raise ValueError if the input isn't well formatted.\n    \"\"\"\n    if isinstance(value, datetime):\n        return value\n\n    number = get_numeric(value, 'datetime')\n    if number is not None:\n        return from_unix_seconds(number)\n\n    if isinstance(value, bytes):\n        value = value.decode()\n\n    match = datetime_re.match(value)  # type: ignore\n    if match is None:\n        raise errors.DateTimeError()\n\n    kw = match.groupdict()\n    if kw['microsecond']:\n        kw['microsecond'] = kw['microsecond'].ljust(6, '0')\n\n    tzinfo = _parse_timezone(kw.pop('tzinfo'), errors.DateTimeError)\n    kw_: Dict[str, Union[None, int, timezone]] = {k: int(v) for k, v in kw.items() if v is not None}\n    kw_['tzinfo'] = tzinfo\n\n    try:\n        return datetime(**kw_)  # type: ignore\n    except ValueError:\n        raise errors.DateTimeError()", "metadata": {"license": "MIT", "len_tokens": 291}}
{"id": "pydantic:pydantic/v1/datetime_parse.py", "language": "python", "code": "def parse_duration(value: StrBytesIntFloat) -> timedelta:\n    \"\"\"\n    Parse a duration int/float/string and return a datetime.timedelta.\n\n    The preferred format for durations in Django is '%d %H:%M:%S.%f'.\n\n    Also supports ISO 8601 representation.\n    \"\"\"\n    if isinstance(value, timedelta):\n        return value\n\n    if isinstance(value, (int, float)):\n        # below code requires a string\n        value = f'{value:f}'\n    elif isinstance(value, bytes):\n        value = value.decode()\n\n    try:\n        match = standard_duration_re.match(value) or iso8601_duration_re.match(value)\n    except TypeError:\n        raise TypeError('invalid type; expected timedelta, string, bytes, int or float')\n\n    if not match:\n        raise errors.DurationError()\n\n    kw = match.groupdict()\n    sign = -1 if kw.pop('sign', '+') == '-' else 1\n    if kw.get('microseconds'):\n        kw['microseconds'] = kw['microseconds'].ljust(6, '0')\n\n    if kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-'):\n        kw['microseconds'] = '-' + kw['microseconds']\n\n    kw_ = {k: float(v) for k, v in kw.items() if v is not None}\n\n    return sign * timedelta(**kw_)", "metadata": {"license": "MIT", "len_tokens": 288}}
{"id": "pydantic:pydantic/v1/generics.py", "language": "python", "code": "def __parameterized_bases__(cls, typevars_map: Parametrization) -> Iterator[Type[Any]]:\n        \"\"\"\n        Returns unbound bases of cls parameterised to given type variables\n\n        :param typevars_map: Dictionary of type applications for binding subclasses.\n            Given a generic class `Model` with 2 type variables [S, T]\n            and a concrete model `Model[str, int]`,\n            the value `{S: str, T: int}` would be passed to `typevars_map`.\n        :return: an iterator of generic sub classes, parameterised by `typevars_map`\n            and other assigned parameters of `cls`\n\n        e.g.:\n        ```\n        class A(GenericModel, Generic[T]):\n            ...\n\n        class B(A[V], Generic[V]):\n            ...\n\n        assert A[int] in B.__parameterized_bases__({V: int})\n        ```\n        \"\"\"\n\n        def build_base_model(\n            base_model: Type[GenericModel], mapped_types: Parametrization\n        ) -> Iterator[Type[GenericModel]]:\n            base_parameters = tuple(mapped_types[param] for param in base_model.__parameters__)\n            parameterized_base = base_model.__class_getitem__(base_parameters)\n            if parameterized_base is base_model or parameterized_base is cls:\n                # Avoid duplication in MRO\n                return\n            yield parameterized_base\n\n        for base_model in cls.__bases__:\n            if not issubclass(base_model, GenericModel):\n                # not a class that can be meaningfully parameterized\n                continue\n            elif not getattr(base_model, '__parameters__', None):\n                # base_model is \"GenericModel\"  (and has no __parameters__)\n                # or\n                # base_model is already concrete, and will be included transitively via cls.\n                continue\n            elif cls in _assigned_parameters:\n                if base_model in _assigned_parameters:\n                    # cls is partially parameterised but not from base_model\n                    # e.g. cls = B[S], base_model = A[S]\n                    # B[S][int] should subclass A[int],  (and will be transitively via B[int])\n                    # but it's not viable to consistently subclass types with arbitrary construction\n                    # So don't attempt to include A[S][int]\n                    continue\n                else:  # base_model not in _assigned_parameters:\n                    # cls is partially parameterized, base_model is original generic\n                    # e.g.  cls = B[str, T], base_model = B[S, T]\n                    # Need to determine the mapping for the base_model parameters\n                    mapped_types: Parametrization = {\n                        key: typevars_map.get(value, value) for key, value in _assigned_parameters[cls].items()\n                    }\n                    yield from build_base_model(base_model, mapped_types)\n            else:\n                # cls is base generic, so base_class has a distinct base\n                # can construct the Parameterised base model using typevars_map directly\n                yield from build_base_model(base_model, typevars_map)", "metadata": {"license": "MIT", "len_tokens": 624}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def schema(\n    models: Sequence[Union[Type['BaseModel'], Type['Dataclass']]],\n    *,\n    by_alias: bool = True,\n    title: Optional[str] = None,\n    description: Optional[str] = None,\n    ref_prefix: Optional[str] = None,\n    ref_template: str = default_ref_template,\n) -> Dict[str, Any]:\n    \"\"\"\n    Process a list of models and generate a single JSON Schema with all of them defined in the ``definitions``\n    top-level JSON key, including their sub-models.\n\n    :param models: a list of models to include in the generated JSON Schema\n    :param by_alias: generate the schemas using the aliases defined, if any\n    :param title: title for the generated schema that includes the definitions\n    :param description: description for the generated schema\n    :param ref_prefix: the JSON Pointer prefix for schema references with ``$ref``, if None, will be set to the\n      default of ``#/definitions/``. Update it if you want the schemas to reference the definitions somewhere\n      else, e.g. for OpenAPI use ``#/components/schemas/``. The resulting generated schemas will still be at the\n      top-level key ``definitions``, so you can extract them from there. But all the references will have the set\n      prefix.\n    :param ref_template: Use a ``string.format()`` template for ``$ref`` instead of a prefix. This can be useful\n      for references that cannot be represented by ``ref_prefix`` such as a definition stored in another file. For\n      a sibling json file in a ``/schemas`` directory use ``\"/schemas/${model}.json#\"``.\n    :return: dict with the JSON Schema with a ``definitions`` top-level key including the schema definitions for\n      the models and sub-models passed in ``models``.\n    \"\"\"\n    clean_models = [get_model(model) for model in models]\n    flat_models = get_flat_models_from_models(clean_models)\n    model_name_map = get_model_name_map(flat_models)\n    definitions = {}\n    output_schema: Dict[str, Any] = {}\n    if title:\n        output_schema['title'] = title\n    if description:\n        output_schema['description'] = description\n    for model in clean_models:\n        m_schema, m_definitions, m_nested_models = model_process_schema(\n            model,\n            by_alias=by_alias,\n            model_name_map=model_name_map,\n            ref_prefix=ref_prefix,\n            ref_template=ref_template,\n        )\n        definitions.update(m_definitions)\n        model_name = model_name_map[model]\n        definitions[model_name] = m_schema\n    if definitions:\n        output_schema['definitions'] = definitions\n    return output_schema", "metadata": {"license": "MIT", "len_tokens": 571}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def model_schema(\n    model: Union[Type['BaseModel'], Type['Dataclass']],\n    by_alias: bool = True,\n    ref_prefix: Optional[str] = None,\n    ref_template: str = default_ref_template,\n) -> Dict[str, Any]:\n    \"\"\"\n    Generate a JSON Schema for one model. With all the sub-models defined in the ``definitions`` top-level\n    JSON key.\n\n    :param model: a Pydantic model (a class that inherits from BaseModel)\n    :param by_alias: generate the schemas using the aliases defined, if any\n    :param ref_prefix: the JSON Pointer prefix for schema references with ``$ref``, if None, will be set to the\n      default of ``#/definitions/``. Update it if you want the schemas to reference the definitions somewhere\n      else, e.g. for OpenAPI use ``#/components/schemas/``. The resulting generated schemas will still be at the\n      top-level key ``definitions``, so you can extract them from there. But all the references will have the set\n      prefix.\n    :param ref_template: Use a ``string.format()`` template for ``$ref`` instead of a prefix. This can be useful for\n      references that cannot be represented by ``ref_prefix`` such as a definition stored in another file. For a\n      sibling json file in a ``/schemas`` directory use ``\"/schemas/${model}.json#\"``.\n    :return: dict with the JSON Schema for the passed ``model``\n    \"\"\"\n    model = get_model(model)\n    flat_models = get_flat_models_from_model(model)\n    model_name_map = get_model_name_map(flat_models)\n    model_name = model_name_map[model]\n    m_schema, m_definitions, nested_models = model_process_schema(\n        model, by_alias=by_alias, model_name_map=model_name_map, ref_prefix=ref_prefix, ref_template=ref_template\n    )\n    if model_name in nested_models:\n        # model_name is in Nested models, it has circular references\n        m_definitions[model_name] = m_schema\n        m_schema = get_schema_ref(model_name, ref_prefix, ref_template, False)\n    if m_definitions:\n        m_schema.update({'definitions': m_definitions})\n    return m_schema", "metadata": {"license": "MIT", "len_tokens": 477}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def get_field_info_schema(field: ModelField, schema_overrides: bool = False) -> Tuple[Dict[str, Any], bool]:\n    # If no title is explicitly set, we don't set title in the schema for enums.\n    # The behaviour is the same as `BaseModel` reference, where the default title\n    # is in the definitions part of the schema.\n    schema_: Dict[str, Any] = {}\n    if field.field_info.title or not lenient_issubclass(field.type_, Enum):\n        schema_['title'] = field.field_info.title or field.alias.title().replace('_', ' ')\n\n    if field.field_info.title:\n        schema_overrides = True\n\n    if field.field_info.description:\n        schema_['description'] = field.field_info.description\n        schema_overrides = True\n\n    if not field.required and field.default is not None and not is_callable_type(field.outer_type_):\n        schema_['default'] = encode_default(field.default)\n        schema_overrides = True\n\n    return schema_, schema_overrides", "metadata": {"license": "MIT", "len_tokens": 215}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def field_schema(\n    field: ModelField,\n    *,\n    by_alias: bool = True,\n    model_name_map: Dict[TypeModelOrEnum, str],\n    ref_prefix: Optional[str] = None,\n    ref_template: str = default_ref_template,\n    known_models: Optional[TypeModelSet] = None,\n) -> Tuple[Dict[str, Any], Dict[str, Any], Set[str]]:\n    \"\"\"\n    Process a Pydantic field and return a tuple with a JSON Schema for it as the first item.\n    Also return a dictionary of definitions with models as keys and their schemas as values. If the passed field\n    is a model and has sub-models, and those sub-models don't have overrides (as ``title``, ``default``, etc), they\n    will be included in the definitions and referenced in the schema instead of included recursively.\n\n    :param field: a Pydantic ``ModelField``\n    :param by_alias: use the defined alias (if any) in the returned schema\n    :param model_name_map: used to generate the JSON Schema references to other models included in the definitions\n    :param ref_prefix: the JSON Pointer prefix to use for references to other schemas, if None, the default of\n      #/definitions/ will be used\n    :param ref_template: Use a ``string.format()`` template for ``$ref`` instead of a prefix. This can be useful for\n      references that cannot be represented by ``ref_prefix`` such as a definition stored in another file. For a\n      sibling json file in a ``/schemas`` directory use ``\"/schemas/${model}.json#\"``.\n    :param known_models: used to solve circular references\n    :return: tuple of the schema for this field and additional definitions\n    \"\"\"\n    s, schema_overrides = get_field_info_schema(field)\n\n    validation_schema = get_field_schema_validations(field)\n    if validation_schema:\n        s.update(validation_schema)\n        schema_overrides = True\n\n    f_schema, f_definitions, f_nested_models = field_type_schema(\n        field,\n        by_alias=by_alias,\n        model_name_map=model_name_map,\n        schema_overrides=schema_overrides,\n        ref_prefix=ref_prefix,\n        ref_template=ref_template,\n        known_models=known_models or set(),\n    )\n\n    # $ref will only be returned when there are no schema_overrides\n    if '$ref' in f_schema:\n        return f_schema, f_definitions, f_nested_models\n    else:\n        s.update(f_schema)\n        return s, f_definitions, f_nested_models", "metadata": {"license": "MIT", "len_tokens": 540}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def get_field_schema_validations(field: ModelField) -> Dict[str, Any]:\n    \"\"\"\n    Get the JSON Schema validation keywords for a ``field`` with an annotation of\n    a Pydantic ``FieldInfo`` with validation arguments.\n    \"\"\"\n    f_schema: Dict[str, Any] = {}\n\n    if lenient_issubclass(field.type_, Enum):\n        # schema is already updated by `enum_process_schema`; just update with field extra\n        if field.field_info.extra:\n            f_schema.update(field.field_info.extra)\n        return f_schema\n\n    if lenient_issubclass(field.type_, (str, bytes)):\n        for attr_name, t, keyword in _str_types_attrs:\n            attr = getattr(field.field_info, attr_name, None)\n            if isinstance(attr, t):\n                f_schema[keyword] = attr\n    if lenient_issubclass(field.type_, numeric_types) and not issubclass(field.type_, bool):\n        for attr_name, t, keyword in _numeric_types_attrs:\n            attr = getattr(field.field_info, attr_name, None)\n            if isinstance(attr, t):\n                f_schema[keyword] = attr\n    if field.field_info is not None and field.field_info.const:\n        f_schema['const'] = field.default\n    if field.field_info.extra:\n        f_schema.update(field.field_info.extra)\n    modify_schema = getattr(field.outer_type_, '__modify_schema__', None)\n    if modify_schema:\n        _apply_modify_schema(modify_schema, field, f_schema)\n    return f_schema", "metadata": {"license": "MIT", "len_tokens": 319}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def get_model_name_map(unique_models: TypeModelSet) -> Dict[TypeModelOrEnum, str]:\n    \"\"\"\n    Process a set of models and generate unique names for them to be used as keys in the JSON Schema\n    definitions. By default the names are the same as the class name. But if two models in different Python\n    modules have the same name (e.g. \"users.Model\" and \"items.Model\"), the generated names will be\n    based on the Python module path for those conflicting models to prevent name collisions.\n\n    :param unique_models: a Python set of models\n    :return: dict mapping models to names\n    \"\"\"\n    name_model_map = {}\n    conflicting_names: Set[str] = set()\n    for model in unique_models:\n        model_name = normalize_name(model.__name__)\n        if model_name in conflicting_names:\n            model_name = get_long_model_name(model)\n            name_model_map[model_name] = model\n        elif model_name in name_model_map:\n            conflicting_names.add(model_name)\n            conflicting_model = name_model_map.pop(model_name)\n            name_model_map[get_long_model_name(conflicting_model)] = conflicting_model\n            name_model_map[get_long_model_name(model)] = model\n        else:\n            name_model_map[model_name] = model\n    return {v: k for k, v in name_model_map.items()}", "metadata": {"license": "MIT", "len_tokens": 282}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def get_flat_models_from_model(model: Type['BaseModel'], known_models: Optional[TypeModelSet] = None) -> TypeModelSet:\n    \"\"\"\n    Take a single ``model`` and generate a set with itself and all the sub-models in the tree. I.e. if you pass\n    model ``Foo`` (subclass of Pydantic ``BaseModel``) as ``model``, and it has a field of type ``Bar`` (also\n    subclass of ``BaseModel``) and that model ``Bar`` has a field of type ``Baz`` (also subclass of ``BaseModel``),\n    the return value will be ``set([Foo, Bar, Baz])``.\n\n    :param model: a Pydantic ``BaseModel`` subclass\n    :param known_models: used to solve circular references\n    :return: a set with the initial model and all its sub-models\n    \"\"\"\n    known_models = known_models or set()\n    flat_models: TypeModelSet = set()\n    flat_models.add(model)\n    known_models |= flat_models\n    fields = cast(Sequence[ModelField], model.__fields__.values())\n    flat_models |= get_flat_models_from_fields(fields, known_models=known_models)\n    return flat_models", "metadata": {"license": "MIT", "len_tokens": 259}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def get_flat_models_from_field(field: ModelField, known_models: TypeModelSet) -> TypeModelSet:\n    \"\"\"\n    Take a single Pydantic ``ModelField`` (from a model) that could have been declared as a subclass of BaseModel\n    (so, it could be a submodel), and generate a set with its model and all the sub-models in the tree.\n    I.e. if you pass a field that was declared to be of type ``Foo`` (subclass of BaseModel) as ``field``, and that\n    model ``Foo`` has a field of type ``Bar`` (also subclass of ``BaseModel``) and that model ``Bar`` has a field of\n    type ``Baz`` (also subclass of ``BaseModel``), the return value will be ``set([Foo, Bar, Baz])``.\n\n    :param field: a Pydantic ``ModelField``\n    :param known_models: used to solve circular references\n    :return: a set with the model used in the declaration for this field, if any, and all its sub-models\n    \"\"\"\n    from pydantic.v1.main import BaseModel\n\n    flat_models: TypeModelSet = set()\n\n    field_type = field.type_\n    if lenient_issubclass(getattr(field_type, '__pydantic_model__', None), BaseModel):\n        field_type = field_type.__pydantic_model__\n\n    if field.sub_fields and not lenient_issubclass(field_type, BaseModel):\n        flat_models |= get_flat_models_from_fields(field.sub_fields, known_models=known_models)\n    elif lenient_issubclass(field_type, BaseModel) and field_type not in known_models:\n        flat_models |= get_flat_models_from_model(field_type, known_models=known_models)\n    elif lenient_issubclass(field_type, Enum):\n        flat_models.add(field_type)\n    return flat_models", "metadata": {"license": "MIT", "len_tokens": 400}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def get_flat_models_from_fields(fields: Sequence[ModelField], known_models: TypeModelSet) -> TypeModelSet:\n    \"\"\"\n    Take a list of Pydantic  ``ModelField``s (from a model) that could have been declared as subclasses of ``BaseModel``\n    (so, any of them could be a submodel), and generate a set with their models and all the sub-models in the tree.\n    I.e. if you pass a the fields of a model ``Foo`` (subclass of ``BaseModel``) as ``fields``, and on of them has a\n    field of type ``Bar`` (also subclass of ``BaseModel``) and that model ``Bar`` has a field of type ``Baz`` (also\n    subclass of ``BaseModel``), the return value will be ``set([Foo, Bar, Baz])``.\n\n    :param fields: a list of Pydantic ``ModelField``s\n    :param known_models: used to solve circular references\n    :return: a set with any model declared in the fields, and all their sub-models\n    \"\"\"\n    flat_models: TypeModelSet = set()\n    for field in fields:\n        flat_models |= get_flat_models_from_field(field, known_models=known_models)\n    return flat_models", "metadata": {"license": "MIT", "len_tokens": 271}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def model_process_schema(\n    model: TypeModelOrEnum,\n    *,\n    by_alias: bool = True,\n    model_name_map: Dict[TypeModelOrEnum, str],\n    ref_prefix: Optional[str] = None,\n    ref_template: str = default_ref_template,\n    known_models: Optional[TypeModelSet] = None,\n    field: Optional[ModelField] = None,\n) -> Tuple[Dict[str, Any], Dict[str, Any], Set[str]]:\n    \"\"\"\n    Used by ``model_schema()``, you probably should be using that function.\n\n    Take a single ``model`` and generate its schema. Also return additional schema definitions, from sub-models. The\n    sub-models of the returned schema will be referenced, but their definitions will not be included in the schema. All\n    the definitions are returned as the second value.\n    \"\"\"\n    from inspect import getdoc, signature\n\n    known_models = known_models or set()\n    if lenient_issubclass(model, Enum):\n        model = cast(Type[Enum], model)\n        s = enum_process_schema(model, field=field)\n        return s, {}, set()\n    model = cast(Type['BaseModel'], model)\n    s = {'title': model.__config__.title or model.__name__}\n    doc = getdoc(model)\n    if doc:\n        s['description'] = doc\n    known_models.add(model)\n    m_schema, m_definitions, nested_models = model_type_schema(\n        model,\n        by_alias=by_alias,\n        model_name_map=model_name_map,\n        ref_prefix=ref_prefix,\n        ref_template=ref_template,\n        known_models=known_models,\n    )\n    s.update(m_schema)\n    schema_extra = model.__config__.schema_extra\n    if callable(schema_extra):\n        if len(signature(schema_extra).parameters) == 1:\n            schema_extra(s)\n        else:\n            schema_extra(s, model)\n    else:\n        s.update(schema_extra)\n    return s, m_definitions, nested_models", "metadata": {"license": "MIT", "len_tokens": 416}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def model_type_schema(\n    model: Type['BaseModel'],\n    *,\n    by_alias: bool,\n    model_name_map: Dict[TypeModelOrEnum, str],\n    ref_template: str,\n    ref_prefix: Optional[str] = None,\n    known_models: TypeModelSet,\n) -> Tuple[Dict[str, Any], Dict[str, Any], Set[str]]:\n    \"\"\"\n    You probably should be using ``model_schema()``, this function is indirectly used by that function.\n\n    Take a single ``model`` and generate the schema for its type only, not including additional\n    information as title, etc. Also return additional schema definitions, from sub-models.\n    \"\"\"\n    properties = {}\n    required = []\n    definitions: Dict[str, Any] = {}\n    nested_models: Set[str] = set()\n    for k, f in model.__fields__.items():\n        try:\n            f_schema, f_definitions, f_nested_models = field_schema(\n                f,\n                by_alias=by_alias,\n                model_name_map=model_name_map,\n                ref_prefix=ref_prefix,\n                ref_template=ref_template,\n                known_models=known_models,\n            )\n        except SkipField as skip:\n            warnings.warn(skip.message, UserWarning)\n            continue\n        definitions.update(f_definitions)\n        nested_models.update(f_nested_models)\n        if by_alias:\n            properties[f.alias] = f_schema\n            if f.required:\n                required.append(f.alias)\n        else:\n            properties[k] = f_schema\n            if f.required:\n                required.append(k)\n    if ROOT_KEY in properties:\n        out_schema = properties[ROOT_KEY]\n        out_schema['title'] = model.__config__.title or model.__name__\n    else:\n        out_schema = {'type': 'object', 'properties': properties}\n        if required:\n            out_schema['required'] = required\n    if model.__config__.extra == 'forbid':\n        out_schema['additionalProperties'] = False\n    return out_schema, definitions, nested_models", "metadata": {"license": "MIT", "len_tokens": 412}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def enum_process_schema(enum: Type[Enum], *, field: Optional[ModelField] = None) -> Dict[str, Any]:\n    \"\"\"\n    Take a single `enum` and generate its schema.\n\n    This is similar to the `model_process_schema` function, but applies to ``Enum`` objects.\n    \"\"\"\n    import inspect\n\n    schema_: Dict[str, Any] = {\n        'title': enum.__name__,\n        # Python assigns all enums a default docstring value of 'An enumeration', so\n        # all enums will have a description field even if not explicitly provided.\n        'description': inspect.cleandoc(enum.__doc__ or 'An enumeration.'),\n        # Add enum values and the enum field type to the schema.\n        'enum': [item.value for item in cast(Iterable[Enum], enum)],\n    }\n\n    add_field_type_to_schema(enum, schema_)\n\n    modify_schema = getattr(enum, '__modify_schema__', None)\n    if modify_schema:\n        _apply_modify_schema(modify_schema, field, schema_)\n\n    return schema_", "metadata": {"license": "MIT", "len_tokens": 220}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def multitypes_literal_field_for_schema(values: Tuple[Any, ...], field: ModelField) -> ModelField:\n    \"\"\"\n    To support `Literal` with values of different types, we split it into multiple `Literal` with same type\n    e.g. `Literal['qwe', 'asd', 1, 2]` becomes `Union[Literal['qwe', 'asd'], Literal[1, 2]]`\n    \"\"\"\n    literal_distinct_types = defaultdict(list)\n    for v in values:\n        literal_distinct_types[v.__class__].append(v)\n    distinct_literals = (Literal[tuple(same_type_values)] for same_type_values in literal_distinct_types.values())\n\n    return ModelField(\n        name=field.name,\n        type_=Union[tuple(distinct_literals)],  # type: ignore\n        class_validators=field.class_validators,\n        model_config=field.model_config,\n        default=field.default,\n        required=field.required,\n        alias=field.alias,\n        field_info=field.field_info,\n    )", "metadata": {"license": "MIT", "len_tokens": 217}}
{"id": "pydantic:pydantic/v1/schema.py", "language": "python", "code": "def get_annotation_from_field_info(\n    annotation: Any, field_info: FieldInfo, field_name: str, validate_assignment: bool = False\n) -> Type[Any]:\n    \"\"\"\n    Get an annotation with validation implemented for numbers and strings based on the field_info.\n    :param annotation: an annotation from a field specification, as ``str``, ``ConstrainedStr``\n    :param field_info: an instance of FieldInfo, possibly with declarations for validations and JSON Schema\n    :param field_name: name of the field for use in error messages\n    :param validate_assignment: default False, flag for BaseModel Config value of validate_assignment\n    :return: the same ``annotation`` if unmodified or a new annotation with validation in place\n    \"\"\"\n    constraints = field_info.get_constraints()\n    used_constraints: Set[str] = set()\n    if constraints:\n        annotation, used_constraints = get_annotation_with_constraints(annotation, field_info)\n    if validate_assignment:\n        used_constraints.add('allow_mutation')\n\n    unused_constraints = constraints - used_constraints\n    if unused_constraints:\n        raise ValueError(\n            f'On field \"{field_name}\" the following field constraints are set but not enforced: '\n            f'{\", \".join(unused_constraints)}. '\n            f'\\nFor more details see https://docs.pydantic.dev/usage/schema/#unenforced-field-constraints'\n        )\n\n    return annotation", "metadata": {"license": "MIT", "len_tokens": 288}}
{"id": "pydantic:pydantic/_internal/_schema_generation_shared.py", "language": "python", "code": "class GenerateJsonSchemaHandler(GetJsonSchemaHandler):\n    \"\"\"JsonSchemaHandler implementation that doesn't do ref unwrapping by default.\n\n    This is used for any Annotated metadata so that we don't end up with conflicting\n    modifications to the definition schema.\n\n    Used internally by Pydantic, please do not rely on this implementation.\n    See `GetJsonSchemaHandler` for the handler API.\n    \"\"\"\n\n    def __init__(self, generate_json_schema: GenerateJsonSchema, handler_override: HandlerOverride | None) -> None:\n        self.generate_json_schema = generate_json_schema\n        self.handler = handler_override or generate_json_schema.generate_inner\n        self.mode = generate_json_schema.mode\n\n    def __call__(self, core_schema: CoreSchemaOrField, /) -> JsonSchemaValue:\n        return self.handler(core_schema)\n\n    def resolve_ref_schema(self, maybe_ref_json_schema: JsonSchemaValue) -> JsonSchemaValue:\n        \"\"\"Resolves `$ref` in the json schema.\n\n        This returns the input json schema if there is no `$ref` in json schema.\n\n        Args:\n            maybe_ref_json_schema: The input json schema that may contains `$ref`.\n\n        Returns:\n            Resolved json schema.\n\n        Raises:\n            LookupError: If it can't find the definition for `$ref`.\n        \"\"\"\n        if '$ref' not in maybe_ref_json_schema:\n            return maybe_ref_json_schema\n        ref = maybe_ref_json_schema['$ref']\n        json_schema = self.generate_json_schema.get_schema_from_definitions(ref)\n        if json_schema is None:\n            raise LookupError(\n                f'Could not find a ref for {ref}.'\n                ' Maybe you tried to call resolve_ref_schema from within a recursive model?'\n            )\n        return json_schema", "metadata": {"license": "MIT", "len_tokens": 360}}
{"id": "pydantic:pydantic/_internal/_schema_generation_shared.py", "language": "python", "code": "class CallbackGetCoreSchemaHandler(GetCoreSchemaHandler):\n    \"\"\"Wrapper to use an arbitrary function as a `GetCoreSchemaHandler`.\n\n    Used internally by Pydantic, please do not rely on this implementation.\n    See `GetCoreSchemaHandler` for the handler API.\n    \"\"\"\n\n    def __init__(\n        self,\n        handler: Callable[[Any], core_schema.CoreSchema],\n        generate_schema: GenerateSchema,\n        ref_mode: Literal['to-def', 'unpack'] = 'to-def',\n    ) -> None:\n        self._handler = handler\n        self._generate_schema = generate_schema\n        self._ref_mode = ref_mode\n\n    def __call__(self, source_type: Any, /) -> core_schema.CoreSchema:\n        schema = self._handler(source_type)\n        if self._ref_mode == 'to-def':\n            ref = schema.get('ref')\n            if ref is not None:\n                return self._generate_schema.defs.create_definition_reference_schema(schema)\n            return schema\n        else:  # ref_mode = 'unpack'\n            return self.resolve_ref_schema(schema)\n\n    def _get_types_namespace(self) -> NamespacesTuple:\n        return self._generate_schema._types_namespace\n\n    def generate_schema(self, source_type: Any, /) -> core_schema.CoreSchema:\n        return self._generate_schema.generate_schema(source_type)\n\n    @property\n    def field_name(self) -> str | None:\n        return self._generate_schema.field_name_stack.get()\n\n    def resolve_ref_schema(self, maybe_ref_schema: core_schema.CoreSchema) -> core_schema.CoreSchema:\n        \"\"\"Resolves reference in the core schema.\n\n        Args:\n            maybe_ref_schema: The input core schema that may contains reference.\n\n        Returns:\n            Resolved core schema.\n\n        Raises:\n            LookupError: If it can't find the definition for reference.\n        \"\"\"\n        if maybe_ref_schema['type'] == 'definition-ref':\n            ref = maybe_ref_schema['schema_ref']\n            definition = self._generate_schema.defs.get_schema_from_ref(ref)\n            if definition is None:\n                raise LookupError(\n                    f'Could not find a ref for {ref}.'\n                    ' Maybe you tried to call resolve_ref_schema from within a recursive model?'\n                )\n            return definition\n        elif maybe_ref_schema['type'] == 'definitions':\n            return self.resolve_ref_schema(maybe_ref_schema['schema'])\n        return maybe_ref_schema", "metadata": {"license": "MIT", "len_tokens": 496}}
{"id": "pydantic:pydantic/_internal/_docs_extraction.py", "language": "python", "code": "class DocstringVisitor(ast.NodeVisitor):\n    def __init__(self) -> None:\n        super().__init__()\n\n        self.target: str | None = None\n        self.attrs: dict[str, str] = {}\n        self.previous_node_type: type[ast.AST] | None = None\n\n    def visit(self, node: ast.AST) -> Any:\n        node_result = super().visit(node)\n        self.previous_node_type = type(node)\n        return node_result\n\n    def visit_AnnAssign(self, node: ast.AnnAssign) -> Any:\n        if isinstance(node.target, ast.Name):\n            self.target = node.target.id\n\n    def visit_Expr(self, node: ast.Expr) -> Any:\n        if (\n            isinstance(node.value, ast.Constant)\n            and isinstance(node.value.value, str)\n            and self.previous_node_type is ast.AnnAssign\n        ):\n            docstring = inspect.cleandoc(node.value.value)\n            if self.target:\n                self.attrs[self.target] = docstring\n            self.target = None", "metadata": {"license": "MIT", "len_tokens": 215}}
{"id": "pydantic:pydantic/_internal/_docs_extraction.py", "language": "python", "code": "def _extract_source_from_frame(cls: type[Any]) -> list[str] | None:\n    frame = inspect.currentframe()\n\n    while frame:\n        if inspect.getmodule(frame) is inspect.getmodule(cls):\n            lnum = frame.f_lineno\n            try:\n                lines, _ = inspect.findsource(frame)\n            except OSError:  # pragma: no cover\n                # Source can't be retrieved (maybe because running in an interactive terminal),\n                # we don't want to error here.\n                pass\n            else:\n                block_lines = inspect.getblock(lines[lnum - 1 :])\n                dedent_source = _dedent_source_lines(block_lines)\n                try:\n                    block_tree = ast.parse(dedent_source)\n                except SyntaxError:\n                    pass\n                else:\n                    stmt = block_tree.body[0]\n                    if isinstance(stmt, ast.FunctionDef) and stmt.name == 'dedent_workaround':\n                        # `_dedent_source_lines` wrapped the class around the workaround function\n                        stmt = stmt.body[0]\n                    if isinstance(stmt, ast.ClassDef) and stmt.name == cls.__name__:\n                        return block_lines\n\n        frame = frame.f_back", "metadata": {"license": "MIT", "len_tokens": 239}}
{"id": "pydantic:pydantic/_internal/_docs_extraction.py", "language": "python", "code": "def extract_docstrings_from_cls(cls: type[Any], use_inspect: bool = False) -> dict[str, str]:\n    \"\"\"Map model attributes and their corresponding docstring.\n\n    Args:\n        cls: The class of the Pydantic model to inspect.\n        use_inspect: Whether to skip usage of frames to find the object and use\n            the `inspect` module instead.\n\n    Returns:\n        A mapping containing attribute names and their corresponding docstring.\n    \"\"\"\n    if use_inspect or sys.version_info >= (3, 13):\n        # On Python < 3.13, `inspect.getsourcelines()` might not work as expected\n        # if two classes have the same name in the same source file.\n        # On Python 3.13+, it will use the new `__firstlineno__` class attribute,\n        # making it way more robust.\n        try:\n            source, _ = inspect.getsourcelines(cls)\n        except OSError:  # pragma: no cover\n            return {}\n    else:\n        # TODO remove this implementation when we drop support for Python 3.12:\n        source = _extract_source_from_frame(cls)\n\n    if not source:\n        return {}\n\n    dedent_source = _dedent_source_lines(source)\n\n    visitor = DocstringVisitor()\n    visitor.visit(ast.parse(dedent_source))\n    return visitor.attrs", "metadata": {"license": "MIT", "len_tokens": 284}}
{"id": "pydantic:pydantic/_internal/_core_utils.py", "language": "python", "code": "def get_type_ref(type_: Any, args_override: tuple[type[Any], ...] | None = None) -> str:\n    \"\"\"Produces the ref to be used for this type by pydantic_core's core schemas.\n\n    This `args_override` argument was added for the purpose of creating valid recursive references\n    when creating generic models without needing to create a concrete class.\n    \"\"\"\n    origin = get_origin(type_) or type_\n\n    args = get_args(type_) if is_generic_alias(type_) else (args_override or ())\n    generic_metadata = getattr(type_, '__pydantic_generic_metadata__', None)\n    if generic_metadata:\n        origin = generic_metadata['origin'] or origin\n        args = generic_metadata['args'] or args\n\n    module_name = getattr(origin, '__module__', '<No __module__>')\n    if typing_objects.is_typealiastype(origin):\n        type_ref = f'{module_name}.{origin.__name__}:{id(origin)}'\n    else:\n        try:\n            qualname = getattr(origin, '__qualname__', f'<No __qualname__: {origin}>')\n        except Exception:\n            qualname = getattr(origin, '__qualname__', '<No __qualname__>')\n        type_ref = f'{module_name}.{qualname}:{id(origin)}'\n\n    arg_refs: list[str] = []\n    for arg in args:\n        if isinstance(arg, str):\n            # Handle string literals as a special case; we may be able to remove this special handling if we\n            # wrap them in a ForwardRef at some point.\n            arg_ref = f'{arg}:str-{id(arg)}'\n        else:\n            arg_ref = f'{_repr.display_as_type(arg)}:{id(arg)}'\n        arg_refs.append(arg_ref)\n    if arg_refs:\n        type_ref = f'{type_ref}[{\",\".join(arg_refs)}]'\n    return type_ref", "metadata": {"license": "MIT", "len_tokens": 387}}
{"id": "pydantic:pydantic/_internal/_core_utils.py", "language": "python", "code": "def _clean_schema_for_pretty_print(obj: Any, strip_metadata: bool = True) -> Any:  # pragma: no cover\n    \"\"\"A utility function to remove irrelevant information from a core schema.\"\"\"\n    if isinstance(obj, Mapping):\n        new_dct = {}\n        for k, v in obj.items():\n            if k == 'metadata' and strip_metadata:\n                new_metadata = {}\n\n                for meta_k, meta_v in v.items():\n                    if meta_k in ('pydantic_js_functions', 'pydantic_js_annotation_functions'):\n                        new_metadata['js_metadata'] = '<stripped>'\n                    else:\n                        new_metadata[meta_k] = _clean_schema_for_pretty_print(meta_v, strip_metadata=strip_metadata)\n\n                if list(new_metadata.keys()) == ['js_metadata']:\n                    new_metadata = {'<stripped>'}\n\n                new_dct[k] = new_metadata\n            # Remove some defaults:\n            elif k in ('custom_init', 'root_model') and not v:\n                continue\n            else:\n                new_dct[k] = _clean_schema_for_pretty_print(v, strip_metadata=strip_metadata)\n\n        return new_dct\n    elif isinstance(obj, Sequence) and not isinstance(obj, str):\n        return [_clean_schema_for_pretty_print(v, strip_metadata=strip_metadata) for v in obj]\n    else:\n        return obj", "metadata": {"license": "MIT", "len_tokens": 281}}
{"id": "pydantic:pydantic/_internal/_core_utils.py", "language": "python", "code": "def pretty_print_core_schema(\n    val: Any,\n    *,\n    console: Console | None = None,\n    max_depth: int | None = None,\n    strip_metadata: bool = True,\n) -> None:  # pragma: no cover\n    \"\"\"Pretty-print a core schema using the `rich` library.\n\n    Args:\n        val: The core schema to print, or a Pydantic model/dataclass/type adapter\n            (in which case the cached core schema is fetched and printed).\n        console: A rich console to use when printing. Defaults to the global rich console instance.\n        max_depth: The number of nesting levels which may be printed.\n        strip_metadata: Whether to strip metadata in the output. If `True` any known core metadata\n            attributes will be stripped (but custom attributes are kept). Defaults to `True`.\n    \"\"\"\n    # lazy import:\n    from rich.pretty import pprint\n\n    # circ. imports:\n    from pydantic import BaseModel, TypeAdapter\n    from pydantic.dataclasses import is_pydantic_dataclass\n\n    if (inspect.isclass(val) and issubclass(val, BaseModel)) or is_pydantic_dataclass(val):\n        val = val.__pydantic_core_schema__\n    if isinstance(val, TypeAdapter):\n        val = val.core_schema\n    cleaned_schema = _clean_schema_for_pretty_print(val, strip_metadata=strip_metadata)\n\n    pprint(cleaned_schema, console=console, max_depth=max_depth)", "metadata": {"license": "MIT", "len_tokens": 306}}
{"id": "pydantic:pydantic/_internal/_validate_call.py", "language": "python", "code": "class ValidateCallWrapper:\n    \"\"\"This is a wrapper around a function that validates the arguments passed to it, and optionally the return value.\"\"\"\n\n    __slots__ = (\n        'function',\n        'validate_return',\n        'schema_type',\n        'module',\n        'qualname',\n        'ns_resolver',\n        'config_wrapper',\n        '__pydantic_complete__',\n        '__pydantic_validator__',\n        '__return_pydantic_validator__',\n    )\n\n    def __init__(\n        self,\n        function: ValidateCallSupportedTypes,\n        config: ConfigDict | None,\n        validate_return: bool,\n        parent_namespace: MappingNamespace | None,\n    ) -> None:\n        self.function = function\n        self.validate_return = validate_return\n        if isinstance(function, partial):\n            self.schema_type = function.func\n            self.module = function.func.__module__\n        else:\n            self.schema_type = function\n            self.module = function.__module__\n        self.qualname = extract_function_qualname(function)\n\n        self.ns_resolver = NsResolver(\n            namespaces_tuple=ns_for_function(self.schema_type, parent_namespace=parent_namespace)\n        )\n        self.config_wrapper = ConfigWrapper(config)\n        if not self.config_wrapper.defer_build:\n            self._create_validators()\n        else:\n            self.__pydantic_complete__ = False\n\n    def _create_validators(self) -> None:\n        gen_schema = GenerateSchema(self.config_wrapper, self.ns_resolver)\n        schema = gen_schema.clean_schema(gen_schema.generate_schema(self.function))\n        core_config = self.config_wrapper.core_config(title=self.qualname)\n\n        self.__pydantic_validator__ = create_schema_validator(\n            schema,\n            self.schema_type,\n            self.module,\n            self.qualname,\n            'validate_call',\n            core_config,\n            self.config_wrapper.plugin_settings,\n        )\n        if self.validate_return:\n            signature = inspect.signature(self.function)\n            return_type = signature.return_annotation if signature.return_annotation is not signature.empty else Any\n            gen_schema = GenerateSchema(self.config_wrapper, self.ns_resolver)\n            schema = gen_schema.clean_schema(gen_schema.generate_schema(return_type))\n            validator = create_schema_validator(\n                schema,\n                self.schema_type,\n                self.module,\n                self.qualname,\n                'validate_call',\n                core_config,\n                self.config_wrapper.plugin_settings,\n            )\n            if inspect.iscoroutinefunction(self.function):\n\n                async def return_val_wrapper(aw: Awaitable[Any]) -> None:\n                    return validator.validate_python(await aw)\n\n                self.__return_pydantic_validator__ = return_val_wrapper\n            else:\n                self.__return_pydantic_validator__ = validator.validate_python\n        else:\n            self.__return_pydantic_validator__ = None\n\n        self.__pydantic_complete__ = True\n\n    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n        if not self.__pydantic_complete__:\n            self._create_validators()\n\n        res = self.__pydantic_validator__.validate_python(pydantic_core.ArgsKwargs(args, kwargs))\n        if self.__return_pydantic_validator__:\n            return self.__return_pydantic_validator__(res)\n        else:\n            return res", "metadata": {"license": "MIT", "len_tokens": 660}}
{"id": "pydantic:pydantic/_internal/_validate_call.py", "language": "python", "code": "def _create_validators(self) -> None:\n        gen_schema = GenerateSchema(self.config_wrapper, self.ns_resolver)\n        schema = gen_schema.clean_schema(gen_schema.generate_schema(self.function))\n        core_config = self.config_wrapper.core_config(title=self.qualname)\n\n        self.__pydantic_validator__ = create_schema_validator(\n            schema,\n            self.schema_type,\n            self.module,\n            self.qualname,\n            'validate_call',\n            core_config,\n            self.config_wrapper.plugin_settings,\n        )\n        if self.validate_return:\n            signature = inspect.signature(self.function)\n            return_type = signature.return_annotation if signature.return_annotation is not signature.empty else Any\n            gen_schema = GenerateSchema(self.config_wrapper, self.ns_resolver)\n            schema = gen_schema.clean_schema(gen_schema.generate_schema(return_type))\n            validator = create_schema_validator(\n                schema,\n                self.schema_type,\n                self.module,\n                self.qualname,\n                'validate_call',\n                core_config,\n                self.config_wrapper.plugin_settings,\n            )\n            if inspect.iscoroutinefunction(self.function):\n\n                async def return_val_wrapper(aw: Awaitable[Any]) -> None:\n                    return validator.validate_python(await aw)\n\n                self.__return_pydantic_validator__ = return_val_wrapper\n            else:\n                self.__return_pydantic_validator__ = validator.validate_python\n        else:\n            self.__return_pydantic_validator__ = None\n\n        self.__pydantic_complete__ = True", "metadata": {"license": "MIT", "len_tokens": 294}}
{"id": "pydantic:pydantic/_internal/_decorators_v1.py", "language": "python", "code": "def make_generic_v1_field_validator(validator: V1Validator) -> core_schema.WithInfoValidatorFunction:\n    \"\"\"Wrap a V1 style field validator for V2 compatibility.\n\n    Args:\n        validator: The V1 style field validator.\n\n    Returns:\n        A wrapped V2 style field validator.\n\n    Raises:\n        PydanticUserError: If the signature is not supported or the parameters are\n            not available in Pydantic V2.\n    \"\"\"\n    sig = signature(validator)\n\n    needs_values_kw = False\n\n    for param_num, (param_name, parameter) in enumerate(sig.parameters.items()):\n        if can_be_keyword(parameter) and param_name in ('field', 'config'):\n            raise PydanticUserError(\n                'The `field` and `config` parameters are not available in Pydantic V2, '\n                'please use the `info` parameter instead.',\n                code='validator-field-config-info',\n            )\n        if parameter.kind is Parameter.VAR_KEYWORD:\n            needs_values_kw = True\n        elif can_be_keyword(parameter) and param_name == 'values':\n            needs_values_kw = True\n        elif can_be_positional(parameter) and param_num == 0:\n            # value\n            continue\n        elif parameter.default is Parameter.empty:  # ignore params with defaults e.g. bound by functools.partial\n            raise PydanticUserError(\n                f'Unsupported signature for V1 style validator {validator}: {sig} is not supported.',\n                code='validator-v1-signature',\n            )\n\n    if needs_values_kw:\n        # (v, **kwargs), (v, values, **kwargs), (v, *, values, **kwargs) or (v, *, values)\n        val1 = cast(V1ValidatorWithValues, validator)\n\n        def wrapper1(value: Any, info: core_schema.ValidationInfo) -> Any:\n            return val1(value, values=info.data)\n\n        return wrapper1\n    else:\n        val2 = cast(V1OnlyValueValidator, validator)\n\n        def wrapper2(value: Any, _: core_schema.ValidationInfo) -> Any:\n            return val2(value)\n\n        return wrapper2", "metadata": {"license": "MIT", "len_tokens": 443}}
{"id": "pydantic:pydantic/_internal/_decorators_v1.py", "language": "python", "code": "def make_v1_generic_root_validator(\n    validator: V1RootValidatorFunction, pre: bool\n) -> V2CoreBeforeRootValidator | V2CoreAfterRootValidator:\n    \"\"\"Wrap a V1 style root validator for V2 compatibility.\n\n    Args:\n        validator: The V1 style field validator.\n        pre: Whether the validator is a pre validator.\n\n    Returns:\n        A wrapped V2 style validator.\n    \"\"\"\n    if pre is True:\n        # mode='before' for pydantic-core\n        def _wrapper1(values: RootValidatorValues, _: core_schema.ValidationInfo) -> RootValidatorValues:\n            return validator(values)\n\n        return _wrapper1\n\n    # mode='after' for pydantic-core\n    def _wrapper2(fields_tuple: RootValidatorFieldsTuple, _: core_schema.ValidationInfo) -> RootValidatorFieldsTuple:\n        if len(fields_tuple) == 2:\n            # dataclass, this is easy\n            values, init_vars = fields_tuple\n            values = validator(values)\n            return values, init_vars\n        else:\n            # ugly hack: to match v1 behaviour, we merge values and model_extra, then split them up based on fields\n            # afterwards\n            model_dict, model_extra, fields_set = fields_tuple\n            if model_extra:\n                fields = set(model_dict.keys())\n                model_dict.update(model_extra)\n                model_dict_new = validator(model_dict)\n                for k in list(model_dict_new.keys()):\n                    if k not in fields:\n                        model_extra[k] = model_dict_new.pop(k)\n            else:\n                model_dict_new = validator(model_dict)\n            return model_dict_new, model_extra, fields_set\n\n    return _wrapper2", "metadata": {"license": "MIT", "len_tokens": 350}}
{"id": "pydantic:pydantic/_internal/_typing_extra.py", "language": "python", "code": "def is_classvar_annotation(tp: Any, /) -> bool:\n    \"\"\"Return whether the provided argument represents a class variable annotation.\n\n    Although not explicitly stated by the typing specification, `ClassVar` can be used\n    inside `Annotated` and as such, this function checks for this specific scenario.\n\n    Because this function is used to detect class variables before evaluating forward references\n    (or because evaluation failed), we also implement a naive regex match implementation. This is\n    required because class variables are inspected before fields are collected, so we try to be\n    as accurate as possible.\n    \"\"\"\n    if typing_objects.is_classvar(tp):\n        return True\n\n    origin = get_origin(tp)\n\n    if typing_objects.is_classvar(origin):\n        return True\n\n    if typing_objects.is_annotated(origin):\n        annotated_type = tp.__origin__\n        if typing_objects.is_classvar(annotated_type) or typing_objects.is_classvar(get_origin(annotated_type)):\n            return True\n\n    str_ann: str | None = None\n    if isinstance(tp, typing.ForwardRef):\n        str_ann = tp.__forward_arg__\n    if isinstance(tp, str):\n        str_ann = tp\n\n    if str_ann is not None and _classvar_re.match(str_ann):\n        # stdlib dataclasses do something similar, although a bit more advanced\n        # (see `dataclass._is_type`).\n        return True\n\n    return False", "metadata": {"license": "MIT", "len_tokens": 295}}
{"id": "pydantic:pydantic/_internal/_typing_extra.py", "language": "python", "code": "def parent_frame_namespace(*, parent_depth: int = 2, force: bool = False) -> dict[str, Any] | None:\n    \"\"\"Fetch the local namespace of the parent frame where this function is called.\n\n    Using this function is mostly useful to resolve forward annotations pointing to members defined in a local namespace,\n    such as assignments inside a function. Using the standard library tools, it is currently not possible to resolve\n    such annotations:\n\n    ```python {lint=\"skip\" test=\"skip\"}\n    from typing import get_type_hints\n\n    def func() -> None:\n        Alias = int\n\n        class C:\n            a: 'Alias'\n\n        # Raises a `NameError: 'Alias' is not defined`\n        get_type_hints(C)\n    ```\n\n    Pydantic uses this function when a Pydantic model is being defined to fetch the parent frame locals. However,\n    this only allows us to fetch the parent frame namespace and not other parents (e.g. a model defined in a function,\n    itself defined in another function). Inspecting the next outer frames (using `f_back`) is not reliable enough\n    (see https://discuss.python.org/t/20659).\n\n    Because this function is mostly used to better resolve forward annotations, nothing is returned if the parent frame's\n    code object is defined at the module level. In this case, the locals of the frame will be the same as the module\n    globals where the class is defined (see `_namespace_utils.get_module_ns_of`). However, if you still want to fetch\n    the module globals (e.g. when rebuilding a model, where the frame where the rebuild call is performed might contain\n    members that you want to use for forward annotations evaluation), you can use the `force` parameter.\n\n    Args:\n        parent_depth: The depth at which to get the frame. Defaults to 2, meaning the parent frame where this function\n            is called will be used.\n        force: Whether to always return the frame locals, even if the frame's code object is defined at the module level.\n\n    Returns:\n        The locals of the namespace, or `None` if it was skipped as per the described logic.\n    \"\"\"\n    frame = sys._getframe(parent_depth)\n\n    if frame.f_code.co_name.startswith('<generic parameters of'):\n        # As `parent_frame_namespace` is mostly called in `ModelMetaclass.__new__`,\n        # the parent frame can be the annotation scope if the PEP 695 generic syntax is used.\n        # (see https://docs.python.org/3/reference/executionmodel.html#annotation-scopes,\n        # https://docs.python.org/3/reference/compound_stmts.html#generic-classes).\n        # In this case, the code name is set to `<generic parameters of MyClass>`,\n        # and we need to skip this frame as it is irrelevant.\n        frame = cast(types.FrameType, frame.f_back)  # guaranteed to not be `None`\n\n    # note, we don't copy frame.f_locals here (or during the last return call), because we don't expect the namespace to be\n    # modified down the line if this becomes a problem, we could implement some sort of frozen mapping structure to enforce this.\n    if force:\n        return frame.f_locals\n\n    # If either of the following conditions are true, the class is defined at the top module level.\n    # To better understand why we need both of these checks, see\n    # https://github.com/pydantic/pydantic/pull/10113#discussion_r1714981531.\n    if frame.f_back is None or frame.f_code.co_name == '<module>':\n        return None\n\n    return frame.f_locals", "metadata": {"license": "MIT", "len_tokens": 770}}
{"id": "pydantic:pydantic/_internal/_typing_extra.py", "language": "python", "code": "def get_model_type_hints(\n    obj: type[BaseModel],\n    *,\n    ns_resolver: NsResolver | None = None,\n) -> dict[str, tuple[Any, bool]]:\n    \"\"\"Collect annotations from a Pydantic model class, including those from parent classes.\n\n    Args:\n        obj: The Pydantic model to inspect.\n        ns_resolver: A namespace resolver instance to use. Defaults to an empty instance.\n\n    Returns:\n        A dictionary mapping annotation names to a two-tuple: the first element is the evaluated\n        type or the original annotation if a `NameError` occurred, the second element is a boolean\n        indicating if whether the evaluation succeeded.\n    \"\"\"\n    hints: dict[str, Any] | dict[str, tuple[Any, bool]] = {}\n    ns_resolver = ns_resolver or NsResolver()\n\n    for base in reversed(obj.__mro__):\n        # For Python 3.14, we could also use `Format.VALUE` and pass the globals/locals\n        # from the ns_resolver, but we want to be able to know which specific field failed\n        # to evaluate:\n        ann = safe_get_annotations(base)\n\n        if not ann:\n            continue\n\n        with ns_resolver.push(base):\n            globalns, localns = ns_resolver.types_namespace\n            for name, value in ann.items():\n                if name.startswith('_'):\n                    # For private attributes, we only need the annotation to detect the `ClassVar` special form.\n                    # For this reason, we still try to evaluate it, but we also catch any possible exception (on\n                    # top of the `NameError`s caught in `try_eval_type`) that could happen so that users are free\n                    # to use any kind of forward annotation for private fields (e.g. circular imports, new typing\n                    # syntax, etc).\n                    try:\n                        hints[name] = try_eval_type(value, globalns, localns)\n                    except Exception:\n                        hints[name] = (value, False)\n                else:\n                    hints[name] = try_eval_type(value, globalns, localns)\n    return hints", "metadata": {"license": "MIT", "len_tokens": 438}}
{"id": "pydantic:pydantic/_internal/_typing_extra.py", "language": "python", "code": "def get_cls_type_hints(\n    obj: type[Any],\n    *,\n    ns_resolver: NsResolver | None = None,\n) -> dict[str, Any]:\n    \"\"\"Collect annotations from a class, including those from parent classes.\n\n    Args:\n        obj: The class to inspect.\n        ns_resolver: A namespace resolver instance to use. Defaults to an empty instance.\n    \"\"\"\n    hints: dict[str, Any] = {}\n    ns_resolver = ns_resolver or NsResolver()\n\n    for base in reversed(obj.__mro__):\n        # For Python 3.14, we could also use `Format.VALUE` and pass the globals/locals\n        # from the ns_resolver, but we want to be able to know which specific field failed\n        # to evaluate:\n        ann = safe_get_annotations(base)\n\n        if not ann:\n            continue\n\n        with ns_resolver.push(base):\n            globalns, localns = ns_resolver.types_namespace\n            for name, value in ann.items():\n                hints[name] = eval_type(value, globalns, localns)\n    return hints", "metadata": {"license": "MIT", "len_tokens": 226}}
{"id": "pydantic:pydantic/_internal/_typing_extra.py", "language": "python", "code": "def eval_type_backport(\n    value: Any,\n    globalns: GlobalsNamespace | None = None,\n    localns: MappingNamespace | None = None,\n    type_params: tuple[Any, ...] | None = None,\n) -> Any:\n    \"\"\"An enhanced version of `typing._eval_type` which will fall back to using the `eval_type_backport`\n    package if it's installed to let older Python versions use newer typing constructs.\n\n    Specifically, this transforms `X | Y` into `typing.Union[X, Y]` and `list[X]` into `typing.List[X]`\n    (as well as all the types made generic in PEP 585) if the original syntax is not supported in the\n    current Python version.\n\n    This function will also display a helpful error if the value passed fails to evaluate.\n    \"\"\"\n    try:\n        return _eval_type_backport(value, globalns, localns, type_params)\n    except TypeError as e:\n        if 'Unable to evaluate type annotation' in str(e):\n            raise\n\n        # If it is a `TypeError` and value isn't a `ForwardRef`, it would have failed during annotation definition.\n        # Thus we assert here for type checking purposes:\n        assert isinstance(value, typing.ForwardRef)\n\n        message = f'Unable to evaluate type annotation {value.__forward_arg__!r}.'\n        if sys.version_info >= (3, 11):\n            e.add_note(message)\n            raise\n        else:\n            raise TypeError(message) from e\n    except RecursionError as e:\n        # TODO ideally recursion errors should be checked in `eval_type` above, but `eval_type_backport`\n        # is used directly in some places.\n        message = (\n            \"If you made use of an implicit recursive type alias (e.g. `MyType = list['MyType']), \"\n            'consider using PEP 695 type aliases instead. For more details, refer to the documentation: '\n            f'https://docs.pydantic.dev/{version_short()}/concepts/types/#named-recursive-types'\n        )\n        if sys.version_info >= (3, 11):\n            e.add_note(message)\n            raise\n        else:\n            raise RecursionError(f'{e.args[0]}\\n{message}')", "metadata": {"license": "MIT", "len_tokens": 476}}
{"id": "pydantic:pydantic/_internal/_typing_extra.py", "language": "python", "code": "def _eval_type_backport(\n    value: Any,\n    globalns: GlobalsNamespace | None = None,\n    localns: MappingNamespace | None = None,\n    type_params: tuple[Any, ...] | None = None,\n) -> Any:\n    try:\n        return _eval_type(value, globalns, localns, type_params)\n    except TypeError as e:\n        if not (isinstance(value, typing.ForwardRef) and is_backport_fixable_error(e)):\n            raise\n\n        try:\n            from eval_type_backport import eval_type_backport\n        except ImportError:\n            raise TypeError(\n                f'Unable to evaluate type annotation {value.__forward_arg__!r}. If you are making use '\n                'of the new typing syntax (unions using `|` since Python 3.10 or builtins subscripting '\n                'since Python 3.9), you should either replace the use of new syntax with the existing '\n                '`typing` constructs or install the `eval_type_backport` package.'\n            ) from e\n\n        return eval_type_backport(\n            value,\n            globalns,\n            localns,  # pyright: ignore[reportArgumentType], waiting on a new `eval_type_backport` release.\n            try_default=False,\n        )", "metadata": {"license": "MIT", "len_tokens": 268}}
{"id": "pydantic:pydantic/_internal/_typing_extra.py", "language": "python", "code": "def _eval_type(\n    value: Any,\n    globalns: GlobalsNamespace | None = None,\n    localns: MappingNamespace | None = None,\n    type_params: tuple[Any, ...] | None = None,\n) -> Any:\n    if sys.version_info >= (3, 14):\n        # Starting in 3.14, `_eval_type()` does *not* apply `_type_convert()`\n        # anymore. This means the `None` -> `type(None)` conversion does not apply:\n        evaluated = typing._eval_type(  # type: ignore\n            value,\n            globalns,\n            localns,\n            type_params=type_params,\n            # This is relevant when evaluating types from `TypedDict` classes, where string annotations\n            # are automatically converted to `ForwardRef` instances with a module set. In this case,\n            # Our `globalns` is irrelevant and we need to indicate `typing._eval_type()` that it should\n            # infer it from the `ForwardRef.__forward_module__` attribute instead (`typing.get_type_hints()`\n            # does the same). Note that this would probably be unnecessary if we properly iterated over the\n            # `__orig_bases__` for TypedDicts in `get_cls_type_hints()`:\n            prefer_fwd_module=True,\n        )\n        if evaluated is None:\n            evaluated = type(None)\n        return evaluated\n    elif sys.version_info >= (3, 13):\n        return typing._eval_type(  # type: ignore\n            value, globalns, localns, type_params=type_params\n        )\n    else:\n        return typing._eval_type(  # type: ignore\n            value, globalns, localns\n        )", "metadata": {"license": "MIT", "len_tokens": 361}}
{"id": "pydantic:pydantic/_internal/_typing_extra.py", "language": "python", "code": "def get_function_type_hints(\n    function: Callable[..., Any],\n    *,\n    include_keys: set[str] | None = None,\n    globalns: GlobalsNamespace | None = None,\n    localns: MappingNamespace | None = None,\n) -> dict[str, Any]:\n    \"\"\"Return type hints for a function.\n\n    This is similar to the `typing.get_type_hints` function, with a few differences:\n    - Support `functools.partial` by using the underlying `func` attribute.\n    - Do not wrap type annotation of a parameter with `Optional` if it has a default value of `None`\n      (related bug: https://github.com/python/cpython/issues/90353, only fixed in 3.11+).\n    \"\"\"\n    try:\n        if isinstance(function, partial):\n            annotations = function.func.__annotations__\n        else:\n            annotations = function.__annotations__\n    except AttributeError:\n        # Some functions (e.g. builtins) don't have annotations:\n        return {}\n\n    if globalns is None:\n        globalns = get_module_ns_of(function)\n    type_params: tuple[Any, ...] | None = None\n    if localns is None:\n        # If localns was specified, it is assumed to already contain type params. This is because\n        # Pydantic has more advanced logic to do so (see `_namespace_utils.ns_for_function`).\n        type_params = getattr(function, '__type_params__', ())\n\n    type_hints = {}\n    for name, value in annotations.items():\n        if include_keys is not None and name not in include_keys:\n            continue\n        if value is None:\n            value = NoneType\n        elif isinstance(value, str):\n            value = _make_forward_ref(value)\n\n        type_hints[name] = eval_type_backport(value, globalns, localns, type_params)\n\n    return type_hints", "metadata": {"license": "MIT", "len_tokens": 389}}
{"id": "pydantic:pydantic/_internal/_typing_extra.py", "language": "python", "code": "def _make_forward_ref(\n        arg: Any,\n        is_argument: bool = True,\n        *,\n        is_class: bool = False,\n    ) -> typing.ForwardRef:\n        \"\"\"Wrapper for ForwardRef that accounts for the `is_class` argument missing in older versions.\n        The `module` argument is omitted as it breaks <3.9.8, =3.10.0 and isn't used in the calls below.\n\n        See https://github.com/python/cpython/pull/28560 for some background.\n        The backport happened on 3.9.8, see:\n        https://github.com/pydantic/pydantic/discussions/6244#discussioncomment-6275458,\n        and on 3.10.1 for the 3.10 branch, see:\n        https://github.com/pydantic/pydantic/issues/6912\n\n        Implemented as EAFP with memory.\n        \"\"\"\n        return typing.ForwardRef(arg, is_argument)", "metadata": {"license": "MIT", "len_tokens": 203}}
{"id": "pydantic:pydantic/_internal/_validators.py", "language": "python", "code": "def sequence_validator(\n    input_value: Sequence[Any],\n    /,\n    validator: core_schema.ValidatorFunctionWrapHandler,\n) -> Sequence[Any]:\n    \"\"\"Validator for `Sequence` types, isinstance(v, Sequence) has already been called.\"\"\"\n    value_type = type(input_value)\n\n    # We don't accept any plain string as a sequence\n    # Relevant issue: https://github.com/pydantic/pydantic/issues/5595\n    if issubclass(value_type, (str, bytes)):\n        raise PydanticCustomError(\n            'sequence_str',\n            \"'{type_name}' instances are not allowed as a Sequence value\",\n            {'type_name': value_type.__name__},\n        )\n\n    # TODO: refactor sequence validation to validate with either a list or a tuple\n    # schema, depending on the type of the value.\n    # Additionally, we should be able to remove one of either this validator or the\n    # SequenceValidator in _std_types_schema.py (preferably this one, while porting over some logic).\n    # Effectively, a refactor for sequence validation is needed.\n    if value_type is tuple:\n        input_value = list(input_value)\n\n    v_list = validator(input_value)\n\n    # the rest of the logic is just re-creating the original type from `v_list`\n    if value_type is list:\n        return v_list\n    elif issubclass(value_type, range):\n        # return the list as we probably can't re-create the range\n        return v_list\n    elif value_type is tuple:\n        return tuple(v_list)\n    else:\n        # best guess at how to re-create the original type, more custom construction logic might be required\n        return value_type(v_list)", "metadata": {"license": "MIT", "len_tokens": 355}}
{"id": "pydantic:pydantic/_internal/_validators.py", "language": "python", "code": "def _import_string_logic(dotted_path: str) -> Any:\n    \"\"\"Inspired by uvicorn  dotted paths should include a colon before the final item if that item is not a module.\n    (This is necessary to distinguish between a submodule and an attribute when there is a conflict.).\n\n    If the dotted path does not include a colon and the final item is not a valid module, importing as an attribute\n    rather than a submodule will be attempted automatically.\n\n    So, for example, the following values of `dotted_path` result in the following returned values:\n    * 'collections': <module 'collections'>\n    * 'collections.abc': <module 'collections.abc'>\n    * 'collections.abc:Mapping': <class 'collections.abc.Mapping'>\n    * `collections.abc.Mapping`: <class 'collections.abc.Mapping'> (though this is a bit slower than the previous line)\n\n    An error will be raised under any of the following scenarios:\n    * `dotted_path` contains more than one colon (e.g., 'collections:abc:Mapping')\n    * the substring of `dotted_path` before the colon is not a valid module in the environment (e.g., '123:Mapping')\n    * the substring of `dotted_path` after the colon is not an attribute of the module (e.g., 'collections:abc123')\n    \"\"\"\n    from importlib import import_module\n\n    components = dotted_path.strip().split(':')\n    if len(components) > 2:\n        raise ImportError(f\"Import strings should have at most one ':'; received {dotted_path!r}\")\n\n    module_path = components[0]\n    if not module_path:\n        raise ImportError(f'Import strings should have a nonempty module name; received {dotted_path!r}')\n\n    try:\n        module = import_module(module_path)\n    except ModuleNotFoundError as e:\n        if '.' in module_path:\n            # Check if it would be valid if the final item was separated from its module with a `:`\n            maybe_module_path, maybe_attribute = dotted_path.strip().rsplit('.', 1)\n            try:\n                return _import_string_logic(f'{maybe_module_path}:{maybe_attribute}')\n            except ImportError:\n                pass\n            raise ImportError(f'No module named {module_path!r}') from e\n        raise e\n\n    if len(components) > 1:\n        attribute = components[1]\n        try:\n            return getattr(module, attribute)\n        except AttributeError as e:\n            raise ImportError(f'cannot import name {attribute!r} from {module_path!r}') from e\n    else:\n        return module", "metadata": {"license": "MIT", "len_tokens": 552}}
{"id": "pydantic:pydantic/_internal/_validators.py", "language": "python", "code": "def _extract_decimal_digits_info(decimal: Decimal) -> tuple[int, int]:\n    \"\"\"Compute the total number of digits and decimal places for a given [`Decimal`][decimal.Decimal] instance.\n\n    This function handles both normalized and non-normalized Decimal instances.\n    Example: Decimal('1.230') -> 4 digits, 3 decimal places\n\n    Args:\n        decimal (Decimal): The decimal number to analyze.\n\n    Returns:\n        tuple[int, int]: A tuple containing the number of decimal places and total digits.\n\n    Though this could be divided into two separate functions, the logic is easier to follow if we couple the computation\n    of the number of decimals and digits together.\n    \"\"\"\n    try:\n        decimal_tuple = decimal.as_tuple()\n\n        assert isinstance(decimal_tuple.exponent, int)\n\n        exponent = decimal_tuple.exponent\n        num_digits = len(decimal_tuple.digits)\n\n        if exponent >= 0:\n            # A positive exponent adds that many trailing zeros\n            # Ex: digit_tuple=(1, 2, 3), exponent=2 -> 12300 -> 0 decimal places, 5 digits\n            num_digits += exponent\n            decimal_places = 0\n        else:\n            # If the absolute value of the negative exponent is larger than the\n            # number of digits, then it's the same as the number of digits,\n            # because it'll consume all the digits in digit_tuple and then\n            # add abs(exponent) - len(digit_tuple) leading zeros after the decimal point.\n            # Ex: digit_tuple=(1, 2, 3), exponent=-2 -> 1.23 -> 2 decimal places, 3 digits\n            # Ex: digit_tuple=(1, 2, 3), exponent=-4 -> 0.0123 -> 4 decimal places, 4 digits\n            decimal_places = abs(exponent)\n            num_digits = max(num_digits, decimal_places)\n\n        return decimal_places, num_digits\n    except (AssertionError, AttributeError):\n        raise TypeError(f'Unable to extract decimal digits info from supplied value {decimal}')", "metadata": {"license": "MIT", "len_tokens": 432}}
{"id": "pydantic:pydantic/_internal/_validators.py", "language": "python", "code": "def get_defaultdict_default_default_factory(values_source_type: Any) -> Callable[[], Any]:\n    FieldInfo = import_cached_field_info()\n\n    values_type_origin = get_origin(values_source_type)\n\n    def infer_default() -> Callable[[], Any]:\n        allowed_default_types: dict[Any, Any] = {\n            tuple: tuple,\n            collections.abc.Sequence: tuple,\n            collections.abc.MutableSequence: list,\n            list: list,\n            typing.Sequence: list,\n            set: set,\n            typing.MutableSet: set,\n            collections.abc.MutableSet: set,\n            collections.abc.Set: frozenset,\n            typing.MutableMapping: dict,\n            typing.Mapping: dict,\n            collections.abc.Mapping: dict,\n            collections.abc.MutableMapping: dict,\n            float: float,\n            int: int,\n            str: str,\n            bool: bool,\n        }\n        values_type = values_type_origin or values_source_type\n        instructions = 'set using `DefaultDict[..., Annotated[..., Field(default_factory=...)]]`'\n        if typing_objects.is_typevar(values_type):\n\n            def type_var_default_factory() -> None:\n                raise RuntimeError(\n                    'Generic defaultdict cannot be used without a concrete value type or an'\n                    ' explicit default factory, ' + instructions\n                )\n\n            return type_var_default_factory\n        elif values_type not in allowed_default_types:\n            # a somewhat subjective set of types that have reasonable default values\n            allowed_msg = ', '.join([t.__name__ for t in set(allowed_default_types.values())])\n            raise PydanticSchemaGenerationError(\n                f'Unable to infer a default factory for keys of type {values_source_type}.'\n                f' Only {allowed_msg} are supported, other types require an explicit default factory'\n                ' ' + instructions\n            )\n        return allowed_default_types[values_type]\n\n    # Assume Annotated[..., Field(...)]\n    if typing_objects.is_annotated(values_type_origin):\n        field_info = next((v for v in get_args(values_source_type) if isinstance(v, FieldInfo)), None)\n    else:\n        field_info = None\n    if field_info and field_info.default_factory:\n        # Assume the default factory does not take any argument:\n        default_default_factory = cast(Callable[[], Any], field_info.default_factory)\n    else:\n        default_default_factory = infer_default()\n    return default_default_factory", "metadata": {"license": "MIT", "len_tokens": 491}}
{"id": "pydantic:pydantic/_internal/_validators.py", "language": "python", "code": "def infer_default() -> Callable[[], Any]:\n        allowed_default_types: dict[Any, Any] = {\n            tuple: tuple,\n            collections.abc.Sequence: tuple,\n            collections.abc.MutableSequence: list,\n            list: list,\n            typing.Sequence: list,\n            set: set,\n            typing.MutableSet: set,\n            collections.abc.MutableSet: set,\n            collections.abc.Set: frozenset,\n            typing.MutableMapping: dict,\n            typing.Mapping: dict,\n            collections.abc.Mapping: dict,\n            collections.abc.MutableMapping: dict,\n            float: float,\n            int: int,\n            str: str,\n            bool: bool,\n        }\n        values_type = values_type_origin or values_source_type\n        instructions = 'set using `DefaultDict[..., Annotated[..., Field(default_factory=...)]]`'\n        if typing_objects.is_typevar(values_type):\n\n            def type_var_default_factory() -> None:\n                raise RuntimeError(\n                    'Generic defaultdict cannot be used without a concrete value type or an'\n                    ' explicit default factory, ' + instructions\n                )\n\n            return type_var_default_factory\n        elif values_type not in allowed_default_types:\n            # a somewhat subjective set of types that have reasonable default values\n            allowed_msg = ', '.join([t.__name__ for t in set(allowed_default_types.values())])\n            raise PydanticSchemaGenerationError(\n                f'Unable to infer a default factory for keys of type {values_source_type}.'\n                f' Only {allowed_msg} are supported, other types require an explicit default factory'\n                ' ' + instructions\n            )\n        return allowed_default_types[values_type]", "metadata": {"license": "MIT", "len_tokens": 341}}
{"id": "pydantic:pydantic/_internal/_serializers.py", "language": "python", "code": "from __future__ import annotations\n\nimport collections\nimport collections.abc\nimport typing\nfrom typing import Any\n\nfrom pydantic_core import PydanticOmit, core_schema\n\nSEQUENCE_ORIGIN_MAP: dict[Any, Any] = {\n    typing.Deque: collections.deque,  # noqa: UP006\n    collections.deque: collections.deque,\n    list: list,\n    typing.List: list,  # noqa: UP006\n    tuple: tuple,\n    typing.Tuple: tuple,  # noqa: UP006\n    set: set,\n    typing.AbstractSet: set,\n    typing.Set: set,  # noqa: UP006\n    frozenset: frozenset,\n    typing.FrozenSet: frozenset,  # noqa: UP006\n    typing.Sequence: list,\n    typing.MutableSequence: list,\n    typing.MutableSet: set,\n    # this doesn't handle subclasses of these\n    # parametrized typing.Set creates one of these\n    collections.abc.MutableSet: set,\n    collections.abc.Set: frozenset,\n}\n\n\ndef serialize_sequence_via_list(\n    v: Any, handler: core_schema.SerializerFunctionWrapHandler, info: core_schema.SerializationInfo\n) -> Any:\n    items: list[Any] = []\n\n    mapped_origin = SEQUENCE_ORIGIN_MAP.get(type(v), None)\n    if mapped_origin is None:\n        # we shouldn't hit this branch, should probably add a serialization error or something\n        return v\n\n    for index, item in enumerate(v):\n        try:\n            v = handler(item, index)\n        except PydanticOmit:  # noqa: PERF203\n            pass\n        else:\n            items.append(v)\n\n    if info.mode_is_json():\n        return items\n    else:\n        return mapped_origin(items)\n", "metadata": {"license": "MIT", "len_tokens": 372}}
{"id": "pydantic:pydantic/_internal/_namespace_utils.py", "language": "python", "code": "class LazyLocalNamespace(Mapping[str, Any]):\n    \"\"\"A lazily evaluated mapping, to be used as the `locals` argument during annotations evaluation.\n\n    While the [`eval`][eval] function expects a mapping as the `locals` argument, it only\n    performs `__getitem__` calls. The [`Mapping`][collections.abc.Mapping] abstract base class\n    is fully implemented only for type checking purposes.\n\n    Args:\n        *namespaces: The namespaces to consider, in ascending order of priority.\n\n    Example:\n        ```python {lint=\"skip\" test=\"skip\"}\n        ns = LazyLocalNamespace({'a': 1, 'b': 2}, {'a': 3})\n        ns['a']\n        #> 3\n        ns['b']\n        #> 2\n        ```\n    \"\"\"\n\n    def __init__(self, *namespaces: MappingNamespace) -> None:\n        self._namespaces = namespaces\n\n    @cached_property\n    def data(self) -> dict[str, Any]:\n        return {k: v for ns in self._namespaces for k, v in ns.items()}\n\n    def __len__(self) -> int:\n        return len(self.data)\n\n    def __getitem__(self, key: str) -> Any:\n        return self.data[key]\n\n    def __contains__(self, key: object) -> bool:\n        return key in self.data\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self.data)", "metadata": {"license": "MIT", "len_tokens": 308}}
{"id": "pydantic:pydantic/_internal/_namespace_utils.py", "language": "python", "code": "def ns_for_function(obj: Callable[..., Any], parent_namespace: MappingNamespace | None = None) -> NamespacesTuple:\n    \"\"\"Return the global and local namespaces to be used when evaluating annotations for the provided function.\n\n    The global namespace will be the `__dict__` attribute of the module the function was defined in.\n    The local namespace will contain the `__type_params__` introduced by PEP 695.\n\n    Args:\n        obj: The object to use when building namespaces.\n        parent_namespace: Optional namespace to be added with the lowest priority in the local namespace.\n            If the passed function is a method, the `parent_namespace` will be the namespace of the class\n            the method is defined in. Thus, we also fetch type `__type_params__` from there (i.e. the\n            class-scoped type variables).\n    \"\"\"\n    locals_list: list[MappingNamespace] = []\n    if parent_namespace is not None:\n        locals_list.append(parent_namespace)\n\n    # Get the `__type_params__` attribute introduced by PEP 695.\n    # Note that the `typing._eval_type` function expects type params to be\n    # passed as a separate argument. However, internally, `_eval_type` calls\n    # `ForwardRef._evaluate` which will merge type params with the localns,\n    # essentially mimicking what we do here.\n    type_params: tuple[_TypeVarLike, ...] = getattr(obj, '__type_params__', ())\n    if parent_namespace is not None:\n        # We also fetch type params from the parent namespace. If present, it probably\n        # means the function was defined in a class. This is to support the following:\n        # https://github.com/python/cpython/issues/124089.\n        type_params += parent_namespace.get('__type_params__', ())\n\n    locals_list.append({t.__name__: t for t in type_params})\n\n    # What about short-circuiting to `obj.__globals__`?\n    globalns = get_module_ns_of(obj)\n\n    return NamespacesTuple(globalns, LazyLocalNamespace(*locals_list))", "metadata": {"license": "MIT", "len_tokens": 435}}
{"id": "pydantic:pydantic/_internal/_namespace_utils.py", "language": "python", "code": "def types_namespace(self) -> NamespacesTuple:\n        \"\"\"The current global and local namespaces to be used for annotations evaluation.\"\"\"\n        if not self._types_stack:\n            # TODO: should we merge the parent namespace here?\n            # This is relevant for TypeAdapter, where there are no types on the stack, and we might\n            # need access to the parent_ns. Right now, we sidestep this in `type_adapter.py` by passing\n            # locals to both parent_ns and the base_ns_tuple, but this is a bit hacky.\n            # we might consider something like:\n            # if self._parent_ns is not None:\n            #     # Hacky workarounds, see class docstring:\n            #     # An optional parent namespace that will be added to the locals with the lowest priority\n            #     locals_list: list[MappingNamespace] = [self._parent_ns, self._base_ns_tuple.locals]\n            #     return NamespacesTuple(self._base_ns_tuple.globals, LazyLocalNamespace(*locals_list))\n            return self._base_ns_tuple\n\n        typ = self._types_stack[-1]\n\n        globalns = get_module_ns_of(typ)\n\n        locals_list: list[MappingNamespace] = []\n        # Hacky workarounds, see class docstring:\n        # An optional parent namespace that will be added to the locals with the lowest priority\n        if self._parent_ns is not None:\n            locals_list.append(self._parent_ns)\n        if len(self._types_stack) > 1:\n            first_type = self._types_stack[0]\n            locals_list.append({first_type.__name__: first_type})\n\n        # Adding `__type_params__` *before* `vars(typ)`, as the latter takes priority\n        # (see https://github.com/python/cpython/pull/120272).\n        # TODO `typ.__type_params__` when we drop support for Python 3.11:\n        type_params: tuple[_TypeVarLike, ...] = getattr(typ, '__type_params__', ())\n        if type_params:\n            # Adding `__type_params__` is mostly useful for generic classes defined using\n            # PEP 695 syntax *and* using forward annotations (see the example in\n            # https://github.com/python/cpython/issues/114053). For TypeAliasType instances,\n            # it is way less common, but still required if using a string annotation in the alias\n            # value, e.g. `type A[T] = 'T'` (which is not necessary in most cases).\n            locals_list.append({t.__name__: t for t in type_params})\n\n        # TypeAliasType instances don't have a `__dict__` attribute, so the check\n        # is necessary:\n        if hasattr(typ, '__dict__'):\n            locals_list.append(vars(typ))\n\n        # The `len(self._types_stack) > 1` check above prevents this from being added twice:\n        locals_list.append({typ.__name__: typ})\n\n        return NamespacesTuple(globalns, LazyLocalNamespace(*locals_list))", "metadata": {"license": "MIT", "len_tokens": 642}}
{"id": "pydantic:pydantic/_internal/_repr.py", "language": "python", "code": "class Representation:\n    # Mixin to provide `__str__`, `__repr__`, and `__pretty__` and `__rich_repr__` methods.\n    # `__pretty__` is used by [devtools](https://python-devtools.helpmanual.io/).\n    # `__rich_repr__` is used by [rich](https://rich.readthedocs.io/en/stable/pretty.html).\n    # (this is not a docstring to avoid adding a docstring to classes which inherit from Representation)\n\n    __slots__ = ()\n\n    def __repr_args__(self) -> ReprArgs:\n        \"\"\"Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n\n        Can either return:\n        * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n        * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n        \"\"\"\n        attrs_names = cast(Collection[str], self.__slots__)\n        if not attrs_names and hasattr(self, '__dict__'):\n            attrs_names = self.__dict__.keys()\n        attrs = ((s, getattr(self, s)) for s in attrs_names)\n        return [(a, v if v is not self else self.__repr_recursion__(v)) for a, v in attrs if v is not None]\n\n    def __repr_name__(self) -> str:\n        \"\"\"Name of the instance's class, used in __repr__.\"\"\"\n        return self.__class__.__name__\n\n    def __repr_recursion__(self, object: Any) -> str:\n        \"\"\"Returns the string representation of a recursive object.\"\"\"\n        # This is copied over from the stdlib `pprint` module:\n        return f'<Recursion on {type(object).__name__} with id={id(object)}>'\n\n    def __repr_str__(self, join_str: str) -> str:\n        return join_str.join(repr(v) if a is None else f'{a}={v!r}' for a, v in self.__repr_args__())\n\n    def __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any]:\n        \"\"\"Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\"\"\"\n        yield self.__repr_name__() + '('\n        yield 1\n        for name, value in self.__repr_args__():\n            if name is not None:\n                yield name + '='\n            yield fmt(value)\n            yield ','\n            yield 0\n        yield -1\n        yield ')'\n\n    def __rich_repr__(self) -> RichReprResult:\n        \"\"\"Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\"\"\"\n        for name, field_repr in self.__repr_args__():\n            if name is None:\n                yield field_repr\n            else:\n                yield name, field_repr\n\n    def __str__(self) -> str:\n        return self.__repr_str__(' ')\n\n    def __repr__(self) -> str:\n        return f'{self.__repr_name__()}({self.__repr_str__(\", \")})'", "metadata": {"license": "MIT", "len_tokens": 686}}
{"id": "pydantic:pydantic/_internal/_repr.py", "language": "python", "code": "def display_as_type(obj: Any) -> str:\n    \"\"\"Pretty representation of a type, should be as close as possible to the original type definition string.\n\n    Takes some logic from `typing._type_repr`.\n    \"\"\"\n    if isinstance(obj, (types.FunctionType, types.BuiltinFunctionType)):\n        return obj.__name__\n    elif obj is ...:\n        return '...'\n    elif isinstance(obj, Representation):\n        return repr(obj)\n    elif isinstance(obj, ForwardRef) or typing_objects.is_typealiastype(obj):\n        return str(obj)\n\n    if not isinstance(obj, (_typing_extra.typing_base, _typing_extra.WithArgsTypes, type)):\n        obj = obj.__class__\n\n    if is_union_origin(typing_extensions.get_origin(obj)):\n        args = ', '.join(map(display_as_type, typing_extensions.get_args(obj)))\n        return f'Union[{args}]'\n    elif isinstance(obj, _typing_extra.WithArgsTypes):\n        if typing_objects.is_literal(typing_extensions.get_origin(obj)):\n            args = ', '.join(map(repr, typing_extensions.get_args(obj)))\n        else:\n            args = ', '.join(map(display_as_type, typing_extensions.get_args(obj)))\n        try:\n            return f'{obj.__qualname__}[{args}]'\n        except AttributeError:\n            return str(obj).replace('typing.', '').replace('typing_extensions.', '')  # handles TypeAliasType in 3.12\n    elif isinstance(obj, type):\n        return obj.__qualname__\n    else:\n        return repr(obj).replace('typing.', '').replace('typing_extensions.', '')", "metadata": {"license": "MIT", "len_tokens": 322}}
{"id": "pydantic:pydantic/_internal/_known_annotated_metadata.py", "language": "python", "code": "def expand_grouped_metadata(annotations: Iterable[Any]) -> Iterable[Any]:\n    \"\"\"Expand the annotations.\n\n    Args:\n        annotations: An iterable of annotations.\n\n    Returns:\n        An iterable of expanded annotations.\n\n    Example:\n        ```python\n        from annotated_types import Ge, Len\n\n        from pydantic._internal._known_annotated_metadata import expand_grouped_metadata\n\n        print(list(expand_grouped_metadata([Ge(4), Len(5)])))\n        #> [Ge(ge=4), MinLen(min_length=5)]\n        ```\n    \"\"\"\n    import annotated_types as at\n\n    FieldInfo = import_cached_field_info()\n\n    for annotation in annotations:\n        if isinstance(annotation, at.GroupedMetadata):\n            yield from annotation\n        elif isinstance(annotation, FieldInfo):\n            yield from annotation.metadata\n            # this is a bit problematic in that it results in duplicate metadata\n            # all of our \"consumers\" can handle it, but it is not ideal\n            # we probably should split up FieldInfo into:\n            # - annotated types metadata\n            # - individual metadata known only to Pydantic\n            annotation = copy(annotation)\n            annotation.metadata = []\n            yield annotation\n        else:\n            yield annotation", "metadata": {"license": "MIT", "len_tokens": 252}}
{"id": "pydantic:pydantic/_internal/_known_annotated_metadata.py", "language": "python", "code": "def collect_known_metadata(annotations: Iterable[Any]) -> tuple[dict[str, Any], list[Any]]:\n    \"\"\"Split `annotations` into known metadata and unknown annotations.\n\n    Args:\n        annotations: An iterable of annotations.\n\n    Returns:\n        A tuple contains a dict of known metadata and a list of unknown annotations.\n\n    Example:\n        ```python\n        from annotated_types import Gt, Len\n\n        from pydantic._internal._known_annotated_metadata import collect_known_metadata\n\n        print(collect_known_metadata([Gt(1), Len(42), ...]))\n        #> ({'gt': 1, 'min_length': 42}, [Ellipsis])\n        ```\n    \"\"\"\n    annotations = expand_grouped_metadata(annotations)\n\n    res: dict[str, Any] = {}\n    remaining: list[Any] = []\n\n    for annotation in annotations:\n        # isinstance(annotation, PydanticMetadata) also covers ._fields:_PydanticGeneralMetadata\n        if isinstance(annotation, PydanticMetadata):\n            res.update(annotation.__dict__)\n        # we don't use dataclasses.asdict because that recursively calls asdict on the field values\n        elif (annotation_type := type(annotation)) in (at_to_constraint_map := _get_at_to_constraint_map()):\n            constraint = at_to_constraint_map[annotation_type]\n            res[constraint] = getattr(annotation, constraint)\n        elif isinstance(annotation, type) and issubclass(annotation, PydanticMetadata):\n            # also support PydanticMetadata classes being used without initialisation,\n            # e.g. `Annotated[int, Strict]` as well as `Annotated[int, Strict()]`\n            res.update({k: v for k, v in vars(annotation).items() if not k.startswith('_')})\n        else:\n            remaining.append(annotation)\n    # Nones can sneak in but pydantic-core will reject them\n    # it'd be nice to clean things up so we don't put in None (we probably don't _need_ to, it was just easier)\n    # but this is simple enough to kick that can down the road\n    res = {k: v for k, v in res.items() if v is not None}\n    return res, remaining", "metadata": {"license": "MIT", "len_tokens": 452}}
{"id": "pydantic:pydantic/_internal/_model_construction.py", "language": "python", "code": "def set_default_hash_func(cls: type[BaseModel], bases: tuple[type[Any], ...]) -> None:\n    base_hash_func = get_attribute_from_bases(bases, '__hash__')\n    new_hash_func = make_hash_func(cls)\n    if base_hash_func in {None, object.__hash__} or getattr(base_hash_func, '__code__', None) == new_hash_func.__code__:\n        # If `__hash__` is some default, we generate a hash function.\n        # It will be `None` if not overridden from BaseModel.\n        # It may be `object.__hash__` if there is another\n        # parent class earlier in the bases which doesn't override `__hash__` (e.g. `typing.Generic`).\n        # It may be a value set by `set_default_hash_func` if `cls` is a subclass of another frozen model.\n        # In the last case we still need a new hash function to account for new `model_fields`.\n        cls.__hash__ = new_hash_func", "metadata": {"license": "MIT", "len_tokens": 215}}
{"id": "pydantic:pydantic/_internal/_model_construction.py", "language": "python", "code": "def set_model_fields(\n    cls: type[BaseModel],\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n) -> None:\n    \"\"\"Collect and set `cls.__pydantic_fields__` and `cls.__class_vars__`.\n\n    Args:\n        cls: BaseModel or dataclass.\n        config_wrapper: The config wrapper instance.\n        ns_resolver: Namespace resolver to use when getting model annotations.\n    \"\"\"\n    typevars_map = get_model_typevars_map(cls)\n    fields, pydantic_extra_info, class_vars = collect_model_fields(\n        cls, config_wrapper, ns_resolver, typevars_map=typevars_map\n    )\n\n    cls.__pydantic_fields__ = fields\n    cls.__pydantic_extra_info__ = pydantic_extra_info\n    cls.__class_vars__.update(class_vars)\n\n    for k in class_vars:\n        # Class vars should not be private attributes\n        #     We remove them _here_ and not earlier because we rely on inspecting the class to determine its classvars,\n        #     but private attributes are determined by inspecting the namespace _prior_ to class creation.\n        #     In the case that a classvar with a leading-'_' is defined via a ForwardRef (e.g., when using\n        #     `__future__.annotations`), we want to remove the private attribute which was detected _before_ we knew it\n        #     evaluated to a classvar\n\n        value = cls.__private_attributes__.pop(k, None)\n        if value is not None and value.default is not PydanticUndefined:\n            setattr(cls, k, value.default)", "metadata": {"license": "MIT", "len_tokens": 339}}
{"id": "pydantic:pydantic/_internal/_model_construction.py", "language": "python", "code": "class _DeprecatedFieldDescriptor:\n    \"\"\"Read-only data descriptor used to emit a runtime deprecation warning before accessing a deprecated field.\n\n    Attributes:\n        msg: The deprecation message to be emitted.\n        wrapped_property: The property instance if the deprecated field is a computed field, or `None`.\n        field_name: The name of the field being deprecated.\n    \"\"\"\n\n    field_name: str\n\n    def __init__(self, msg: str, wrapped_property: property | None = None) -> None:\n        self.msg = msg\n        self.wrapped_property = wrapped_property\n\n    def __set_name__(self, cls: type[BaseModel], name: str) -> None:\n        self.field_name = name\n\n    def __get__(self, obj: BaseModel | None, obj_type: type[BaseModel] | None = None) -> Any:\n        if obj is None:\n            if self.wrapped_property is not None:\n                return self.wrapped_property.__get__(None, obj_type)\n            raise AttributeError(self.field_name)\n\n        warnings.warn(self.msg, DeprecationWarning, stacklevel=2)\n\n        if self.wrapped_property is not None:\n            return self.wrapped_property.__get__(obj, obj_type)\n        return obj.__dict__[self.field_name]\n\n    # Defined to make it a data descriptor and take precedence over the instance's dictionary.\n    # Note that it will not be called when setting a value on a model instance\n    # as `BaseModel.__setattr__` is defined and takes priority.\n    def __set__(self, obj: Any, value: Any) -> NoReturn:\n        raise AttributeError(self.field_name)", "metadata": {"license": "MIT", "len_tokens": 345}}
{"id": "pydantic:pydantic/_internal/_model_construction.py", "language": "python", "code": "class _PydanticWeakRef:\n    \"\"\"Wrapper for `weakref.ref` that enables `pickle` serialization.\n\n    Cloudpickle fails to serialize `weakref.ref` objects due to an arcane error related\n    to abstract base classes (`abc.ABC`). This class works around the issue by wrapping\n    `weakref.ref` instead of subclassing it.\n\n    See https://github.com/pydantic/pydantic/issues/6763 for context.\n\n    Semantics:\n        - If not pickled, behaves the same as a `weakref.ref`.\n        - If pickled along with the referenced object, the same `weakref.ref` behavior\n          will be maintained between them after unpickling.\n        - If pickled without the referenced object, after unpickling the underlying\n          reference will be cleared (`__call__` will always return `None`).\n    \"\"\"\n\n    def __init__(self, obj: Any):\n        if obj is None:\n            # The object will be `None` upon deserialization if the serialized weakref\n            # had lost its underlying object.\n            self._wr = None\n        else:\n            self._wr = weakref.ref(obj)\n\n    def __call__(self) -> Any:\n        if self._wr is None:\n            return None\n        else:\n            return self._wr()\n\n    def __reduce__(self) -> tuple[Callable, tuple[weakref.ReferenceType | None]]:\n        return _PydanticWeakRef, (self(),)", "metadata": {"license": "MIT", "len_tokens": 310}}
{"id": "pydantic:pydantic/_internal/_signature.py", "language": "python", "code": "def _process_param_defaults(param: Parameter) -> Parameter:\n    \"\"\"Modify the signature for a parameter in a dataclass where the default value is a FieldInfo instance.\n\n    Args:\n        param (Parameter): The parameter\n\n    Returns:\n        Parameter: The custom processed parameter\n    \"\"\"\n    from ..fields import FieldInfo\n\n    param_default = param.default\n    if isinstance(param_default, FieldInfo):\n        annotation = param.annotation\n        # Replace the annotation if appropriate\n        # inspect does \"clever\" things to show annotations as strings because we have\n        # `from __future__ import annotations` in main, we don't want that\n        if annotation == 'Any':\n            annotation = Any\n\n        # Replace the field default\n        default = param_default.default\n        if default is PydanticUndefined:\n            if param_default.default_factory is PydanticUndefined:\n                default = Signature.empty\n            else:\n                # this is used by dataclasses to indicate a factory exists:\n                default = dataclasses._HAS_DEFAULT_FACTORY  # type: ignore\n        return param.replace(\n            annotation=annotation, name=_field_name_for_signature(param.name, param_default), default=default\n        )\n    return param", "metadata": {"license": "MIT", "len_tokens": 248}}
{"id": "pydantic:pydantic/_internal/_signature.py", "language": "python", "code": "def _generate_signature_parameters(  # noqa: C901 (ignore complexity, could use a refactor)\n    init: Callable[..., None],\n    fields: dict[str, FieldInfo],\n    validate_by_name: bool,\n    extra: ExtraValues | None,\n) -> dict[str, Parameter]:\n    \"\"\"Generate a mapping of parameter names to Parameter objects for a pydantic BaseModel or dataclass.\"\"\"\n    from itertools import islice\n\n    present_params = signature(init).parameters.values()\n    merged_params: dict[str, Parameter] = {}\n    var_kw = None\n    use_var_kw = False\n\n    for param in islice(present_params, 1, None):  # skip self arg\n        # inspect does \"clever\" things to show annotations as strings because we have\n        # `from __future__ import annotations` in main, we don't want that\n        if fields.get(param.name):\n            # exclude params with init=False\n            if getattr(fields[param.name], 'init', True) is False:\n                continue\n            param = param.replace(name=_field_name_for_signature(param.name, fields[param.name]))\n        if param.annotation == 'Any':\n            param = param.replace(annotation=Any)\n        if param.kind is param.VAR_KEYWORD:\n            var_kw = param\n            continue\n        merged_params[param.name] = param\n\n    if var_kw:  # if custom init has no var_kw, fields which are not declared in it cannot be passed through\n        allow_names = validate_by_name\n        for field_name, field in fields.items():\n            # when alias is a str it should be used for signature generation\n            param_name = _field_name_for_signature(field_name, field)\n\n            if field_name in merged_params or param_name in merged_params:\n                continue\n\n            if not is_valid_identifier(param_name):\n                if allow_names:\n                    param_name = field_name\n                else:\n                    use_var_kw = True\n                    continue\n\n            if field.is_required():\n                default = Parameter.empty\n            elif field.default_factory is not None:\n                # Mimics stdlib dataclasses:\n                default = _HAS_DEFAULT_FACTORY\n            else:\n                default = field.default\n            merged_params[param_name] = Parameter(\n                param_name,\n                Parameter.KEYWORD_ONLY,\n                annotation=field.rebuild_annotation(),\n                default=default,\n            )\n\n    if extra == 'allow':\n        use_var_kw = True\n\n    if var_kw and use_var_kw:\n        # Make sure the parameter for extra kwargs\n        # does not have the same name as a field\n        default_model_signature = [\n            ('self', Parameter.POSITIONAL_ONLY),\n            ('data', Parameter.VAR_KEYWORD),\n        ]\n        if [(p.name, p.kind) for p in present_params] == default_model_signature:\n            # if this is the standard model signature, use extra_data as the extra args name\n            var_kw_name = 'extra_data'\n        else:\n            # else start from var_kw\n            var_kw_name = var_kw.name\n\n        # generate a name that's definitely unique\n        while var_kw_name in fields:\n            var_kw_name += '_'\n        merged_params[var_kw_name] = var_kw.replace(name=var_kw_name)\n\n    return merged_params", "metadata": {"license": "MIT", "len_tokens": 669}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def check_decorator_fields_exist(decorators: Iterable[AnyFieldDecorator], fields: Iterable[str]) -> None:\n    \"\"\"Check if the defined fields in decorators exist in `fields` param.\n\n    It ignores the check for a decorator if the decorator has `*` as field or `check_fields=False`.\n\n    Args:\n        decorators: An iterable of decorators.\n        fields: An iterable of fields name.\n\n    Raises:\n        PydanticUserError: If one of the field names does not exist in `fields` param.\n    \"\"\"\n    fields = set(fields)\n    for dec in decorators:\n        if '*' in dec.info.fields:\n            continue\n        if dec.info.check_fields is False:\n            continue\n        for field in dec.info.fields:\n            if field not in fields:\n                raise PydanticUserError(\n                    f'Decorators defined with incorrect fields: {dec.cls_ref}.{dec.cls_var_name}'\n                    \" (use check_fields=False if you're inheriting from the model and intended this)\",\n                    code='decorator-missing-field',\n                )", "metadata": {"license": "MIT", "len_tokens": 217}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def apply_each_item_validators(\n    schema: core_schema.CoreSchema,\n    each_item_validators: list[Decorator[ValidatorDecoratorInfo]],\n) -> core_schema.CoreSchema:\n    # This V1 compatibility shim should eventually be removed\n\n    # fail early if each_item_validators is empty\n    if not each_item_validators:\n        return schema\n\n    # push down any `each_item=True` validators\n    # note that this won't work for any Annotated types that get wrapped by a function validator\n    # but that's okay because that didn't exist in V1\n    if schema['type'] == 'nullable':\n        schema['schema'] = apply_each_item_validators(schema['schema'], each_item_validators)\n        return schema\n    elif schema['type'] == 'tuple':\n        if (variadic_item_index := schema.get('variadic_item_index')) is not None:\n            schema['items_schema'][variadic_item_index] = apply_validators(\n                schema['items_schema'][variadic_item_index],\n                each_item_validators,\n            )\n    elif is_list_like_schema_with_items_schema(schema):\n        inner_schema = schema.get('items_schema', core_schema.any_schema())\n        schema['items_schema'] = apply_validators(inner_schema, each_item_validators)\n    elif schema['type'] == 'dict':\n        inner_schema = schema.get('values_schema', core_schema.any_schema())\n        schema['values_schema'] = apply_validators(inner_schema, each_item_validators)\n    else:\n        raise TypeError(\n            f'`@validator(..., each_item=True)` cannot be applied to fields with a schema of {schema[\"type\"]}'\n        )\n    return schema", "metadata": {"license": "MIT", "len_tokens": 343}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _add_custom_serialization_from_json_encoders(\n    json_encoders: JsonEncoders | None, tp: Any, schema: CoreSchema\n) -> CoreSchema:\n    \"\"\"Iterate over the json_encoders and add the first matching encoder to the schema.\n\n    Args:\n        json_encoders: A dictionary of types and their encoder functions.\n        tp: The type to check for a matching encoder.\n        schema: The schema to add the encoder to.\n    \"\"\"\n    if not json_encoders:\n        return schema\n    if 'serialization' in schema:\n        return schema\n    # Check the class type and its superclasses for a matching encoder\n    # Decimal.__class__.__mro__ (and probably other cases) doesn't include Decimal itself\n    # if the type is a GenericAlias (e.g. from list[int]) we need to use __class__ instead of .__mro__\n    for base in (tp, *getattr(tp, '__mro__', tp.__class__.__mro__)[:-1]):\n        encoder = json_encoders.get(base)\n        if encoder is None:\n            continue\n\n        warnings.warn(\n            f'`json_encoders` is deprecated. See https://docs.pydantic.dev/{version_short()}/concepts/serialization/#custom-serializers for alternatives',\n            PydanticDeprecatedSince20,\n        )\n\n        # TODO: in theory we should check that the schema accepts a serialization key\n        schema['serialization'] = core_schema.plain_serializer_function_ser_schema(encoder, when_used='json')\n        return schema\n\n    return schema", "metadata": {"license": "MIT", "len_tokens": 327}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def apply_validators(\n    schema: core_schema.CoreSchema,\n    validators: Iterable[Decorator[RootValidatorDecoratorInfo]]\n    | Iterable[Decorator[ValidatorDecoratorInfo]]\n    | Iterable[Decorator[FieldValidatorDecoratorInfo]],\n) -> core_schema.CoreSchema:\n    \"\"\"Apply validators to a schema.\n\n    Args:\n        schema: The schema to apply validators on.\n        validators: An iterable of validators.\n        field_name: The name of the field if validators are being applied to a model field.\n\n    Returns:\n        The updated schema.\n    \"\"\"\n    for validator in validators:\n        # Actually, type could be 'field' or 'model', but this is only used for deprecated\n        # decorators, so let's not worry about it.\n        info_arg = inspect_validator(validator.func, mode=validator.info.mode, type='field')\n        val_type = 'with-info' if info_arg else 'no-info'\n\n        schema = _VALIDATOR_F_MATCH[(validator.info.mode, val_type)](validator.func, schema)\n    return schema", "metadata": {"license": "MIT", "len_tokens": 211}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def apply_model_validators(\n    schema: core_schema.CoreSchema,\n    validators: Iterable[Decorator[ModelValidatorDecoratorInfo]],\n    mode: Literal['inner', 'outer', 'all'],\n) -> core_schema.CoreSchema:\n    \"\"\"Apply model validators to a schema.\n\n    If mode == 'inner', only \"before\" validators are applied\n    If mode == 'outer', validators other than \"before\" are applied\n    If mode == 'all', all validators are applied\n\n    Args:\n        schema: The schema to apply validators on.\n        validators: An iterable of validators.\n        mode: The validator mode.\n\n    Returns:\n        The updated schema.\n    \"\"\"\n    ref: str | None = schema.pop('ref', None)  # type: ignore\n    for validator in validators:\n        if mode == 'inner' and validator.info.mode != 'before':\n            continue\n        if mode == 'outer' and validator.info.mode == 'before':\n            continue\n        info_arg = inspect_validator(validator.func, mode=validator.info.mode, type='model')\n        if validator.info.mode == 'wrap':\n            if info_arg:\n                schema = core_schema.with_info_wrap_validator_function(function=validator.func, schema=schema)\n            else:\n                schema = core_schema.no_info_wrap_validator_function(function=validator.func, schema=schema)\n        elif validator.info.mode == 'before':\n            if info_arg:\n                schema = core_schema.with_info_before_validator_function(function=validator.func, schema=schema)\n            else:\n                schema = core_schema.no_info_before_validator_function(function=validator.func, schema=schema)\n        else:\n            assert validator.info.mode == 'after'\n            if info_arg:\n                schema = core_schema.with_info_after_validator_function(function=validator.func, schema=schema)\n            else:\n                schema = core_schema.no_info_after_validator_function(function=validator.func, schema=schema)\n    if ref:\n        schema['ref'] = ref  # type: ignore\n    return schema", "metadata": {"license": "MIT", "len_tokens": 406}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _extract_get_pydantic_json_schema(tp: Any) -> GetJsonSchemaFunction | None:\n    \"\"\"Extract `__get_pydantic_json_schema__` from a type, handling the deprecated `__modify_schema__`.\"\"\"\n    js_modify_function = getattr(tp, '__get_pydantic_json_schema__', None)\n\n    if hasattr(tp, '__modify_schema__'):\n        BaseModel = import_cached_base_model()\n\n        has_custom_v2_modify_js_func = (\n            js_modify_function is not None\n            and BaseModel.__get_pydantic_json_schema__.__func__  # type: ignore\n            not in (js_modify_function, getattr(js_modify_function, '__func__', None))\n        )\n\n        if not has_custom_v2_modify_js_func:\n            cls_name = getattr(tp, '__name__', None)\n            raise PydanticUserError(\n                f'The `__modify_schema__` method is not supported in Pydantic v2. '\n                f'Use `__get_pydantic_json_schema__` instead{f\" in class `{cls_name}`\" if cls_name else \"\"}.',\n                code='custom-json-schema',\n            )\n\n    if (origin := get_origin(tp)) is not None:\n        # Generic aliases proxy attribute access to the origin, *except* dunder attributes,\n        # such as `__get_pydantic_json_schema__`, hence the explicit check.\n        return _extract_get_pydantic_json_schema(origin)\n\n    if js_modify_function is None:\n        return None\n\n    return js_modify_function", "metadata": {"license": "MIT", "len_tokens": 319}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _enum_schema(self, enum_type: type[Enum]) -> CoreSchema:\n        cases: list[Any] = list(enum_type.__members__.values())\n\n        enum_ref = get_type_ref(enum_type)\n        description = None if not enum_type.__doc__ else inspect.cleandoc(enum_type.__doc__)\n        if (\n            description == 'An enumeration.'\n        ):  # This is the default value provided by enum.EnumMeta.__new__; don't use it\n            description = None\n        js_updates = {'title': enum_type.__name__, 'description': description}\n        js_updates = {k: v for k, v in js_updates.items() if v is not None}\n\n        sub_type: Literal['str', 'int', 'float'] | None = None\n        if issubclass(enum_type, int):\n            sub_type = 'int'\n            value_ser_type: core_schema.SerSchema = core_schema.simple_ser_schema('int')\n        elif issubclass(enum_type, str):\n            # this handles `StrEnum` (3.11 only), and also `Foobar(str, Enum)`\n            sub_type = 'str'\n            value_ser_type = core_schema.simple_ser_schema('str')\n        elif issubclass(enum_type, float):\n            sub_type = 'float'\n            value_ser_type = core_schema.simple_ser_schema('float')\n        else:\n            # TODO this is an ugly hack, how do we trigger an Any schema for serialization?\n            value_ser_type = core_schema.plain_serializer_function_ser_schema(lambda x: x)\n\n        if cases:\n\n            def get_json_schema(schema: CoreSchema, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n                json_schema = handler(schema)\n                original_schema = handler.resolve_ref_schema(json_schema)\n                original_schema.update(js_updates)\n                return json_schema\n\n            # we don't want to add the missing to the schema if it's the default one\n            default_missing = getattr(enum_type._missing_, '__func__', None) is Enum._missing_.__func__  # pyright: ignore[reportFunctionMemberAccess]\n            enum_schema = core_schema.enum_schema(\n                enum_type,\n                cases,\n                sub_type=sub_type,\n                missing=None if default_missing else enum_type._missing_,\n                ref=enum_ref,\n                metadata={'pydantic_js_functions': [get_json_schema]},\n            )\n\n            if self._config_wrapper.use_enum_values:\n                enum_schema = core_schema.no_info_after_validator_function(\n                    attrgetter('value'), enum_schema, serialization=value_ser_type\n                )\n\n            return enum_schema\n\n        else:\n\n            def get_json_schema_no_cases(_, handler: GetJsonSchemaHandler) -> JsonSchemaValue:\n                json_schema = handler(core_schema.enum_schema(enum_type, cases, sub_type=sub_type, ref=enum_ref))\n                original_schema = handler.resolve_ref_schema(json_schema)\n                original_schema.update(js_updates)\n                return json_schema\n\n            # Use an isinstance check for enums with no cases.\n            # The most important use case for this is creating TypeVar bounds for generics that should\n            # be restricted to enums. This is more consistent than it might seem at first, since you can only\n            # subclass enum.Enum (or subclasses of enum.Enum) if all parent classes have no cases.\n            # We use the get_json_schema function when an Enum subclass has been declared with no cases\n            # so that we can still generate a valid json schema.\n            return core_schema.is_instance_schema(\n                enum_type,\n                metadata={'pydantic_js_functions': [get_json_schema_no_cases]},\n            )", "metadata": {"license": "MIT", "len_tokens": 735}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _ip_schema(self, tp: Any) -> CoreSchema:\n        from ._validators import IP_VALIDATOR_LOOKUP, IpType\n\n        ip_type_json_schema_format: dict[type[IpType], str] = {\n            IPv4Address: 'ipv4',\n            IPv4Network: 'ipv4network',\n            IPv4Interface: 'ipv4interface',\n            IPv6Address: 'ipv6',\n            IPv6Network: 'ipv6network',\n            IPv6Interface: 'ipv6interface',\n        }\n\n        def ser_ip(ip: Any, info: core_schema.SerializationInfo) -> str | IpType:\n            if not isinstance(ip, (tp, str)):\n                raise PydanticSerializationUnexpectedValue(\n                    f\"Expected `{tp}` but got `{type(ip)}` with value `'{ip}'` - serialized value may not be as expected.\"\n                )\n            if info.mode == 'python':\n                return ip\n            return str(ip)\n\n        return core_schema.lax_or_strict_schema(\n            lax_schema=core_schema.no_info_plain_validator_function(IP_VALIDATOR_LOOKUP[tp]),\n            strict_schema=core_schema.json_or_python_schema(\n                json_schema=core_schema.no_info_after_validator_function(tp, core_schema.str_schema()),\n                python_schema=core_schema.is_instance_schema(tp),\n            ),\n            serialization=core_schema.plain_serializer_function_ser_schema(ser_ip, info_arg=True, when_used='always'),\n            metadata={\n                'pydantic_js_functions': [lambda _1, _2: {'type': 'string', 'format': ip_type_json_schema_format[tp]}]\n            },\n        )", "metadata": {"license": "MIT", "len_tokens": 333}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _path_schema(self, tp: Any, path_type: Any) -> CoreSchema:\n        if tp is os.PathLike and (path_type not in {str, bytes} and not typing_objects.is_any(path_type)):\n            raise PydanticUserError(\n                '`os.PathLike` can only be used with `str`, `bytes` or `Any`', code='schema-for-unknown-type'\n            )\n\n        path_constructor = pathlib.PurePath if tp is os.PathLike else tp\n        strict_inner_schema = (\n            core_schema.bytes_schema(strict=True) if (path_type is bytes) else core_schema.str_schema(strict=True)\n        )\n        lax_inner_schema = core_schema.bytes_schema() if (path_type is bytes) else core_schema.str_schema()\n\n        def path_validator(input_value: str | bytes) -> os.PathLike[Any]:  # type: ignore\n            try:\n                if path_type is bytes:\n                    if isinstance(input_value, bytes):\n                        try:\n                            input_value = input_value.decode()\n                        except UnicodeDecodeError as e:\n                            raise PydanticCustomError('bytes_type', 'Input must be valid bytes') from e\n                    else:\n                        raise PydanticCustomError('bytes_type', 'Input must be bytes')\n                elif not isinstance(input_value, str):\n                    raise PydanticCustomError('path_type', 'Input is not a valid path')\n\n                return path_constructor(input_value)  # type: ignore\n            except TypeError as e:\n                raise PydanticCustomError('path_type', 'Input is not a valid path') from e\n\n        def ser_path(path: Any, info: core_schema.SerializationInfo) -> str | os.PathLike[Any]:\n            if not isinstance(path, (tp, str)):\n                raise PydanticSerializationUnexpectedValue(\n                    f\"Expected `{tp}` but got `{type(path)}` with value `'{path}'` - serialized value may not be as expected.\"\n                )\n            if info.mode == 'python':\n                return path\n            return str(path)\n\n        instance_schema = core_schema.json_or_python_schema(\n            json_schema=core_schema.no_info_after_validator_function(path_validator, lax_inner_schema),\n            python_schema=core_schema.is_instance_schema(tp),\n        )\n\n        schema = core_schema.lax_or_strict_schema(\n            lax_schema=core_schema.union_schema(\n                [\n                    instance_schema,\n                    core_schema.no_info_after_validator_function(path_validator, strict_inner_schema),\n                ],\n                custom_error_type='path_type',\n                custom_error_message=f'Input is not a valid path for {tp}',\n            ),\n            strict_schema=instance_schema,\n            serialization=core_schema.plain_serializer_function_ser_schema(ser_path, info_arg=True, when_used='always'),\n            metadata={'pydantic_js_functions': [lambda source, handler: {**handler(source), 'format': 'path'}]},\n        )\n        return schema", "metadata": {"license": "MIT", "len_tokens": 595}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _deque_schema(self, items_type: Any) -> CoreSchema:\n        from ._serializers import serialize_sequence_via_list\n        from ._validators import deque_validator\n\n        item_type_schema = self.generate_schema(items_type)\n\n        # we have to use a lax list schema here, because we need to validate the deque's\n        # items via a list schema, but it's ok if the deque itself is not a list\n        list_schema = core_schema.list_schema(item_type_schema, strict=False)\n\n        check_instance = core_schema.json_or_python_schema(\n            json_schema=list_schema,\n            python_schema=core_schema.is_instance_schema(collections.deque, cls_repr='Deque'),\n        )\n\n        lax_schema = core_schema.no_info_wrap_validator_function(deque_validator, list_schema)\n\n        return core_schema.lax_or_strict_schema(\n            lax_schema=lax_schema,\n            strict_schema=core_schema.chain_schema([check_instance, lax_schema]),\n            serialization=core_schema.wrap_serializer_function_ser_schema(\n                serialize_sequence_via_list, schema=item_type_schema, info_arg=True\n            ),\n        )", "metadata": {"license": "MIT", "len_tokens": 224}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _mapping_schema(self, tp: Any, keys_type: Any, values_type: Any) -> CoreSchema:\n        from ._validators import MAPPING_ORIGIN_MAP, defaultdict_validator, get_defaultdict_default_default_factory\n\n        mapped_origin = MAPPING_ORIGIN_MAP[tp]\n        keys_schema = self.generate_schema(keys_type)\n        with warnings.catch_warnings():\n            # We kind of abused `Field()` default factories to be able to specify\n            # the `defaultdict`'s `default_factory`. As a consequence, we get warnings\n            # as normally `FieldInfo.default_factory` is unsupported in the context where\n            # `Field()` is used and our only solution is to ignore them (note that this might\n            # wrongfully ignore valid warnings, e.g. if the `value_type` is a PEP 695 type alias\n            # with unsupported metadata).\n            warnings.simplefilter('ignore', category=UnsupportedFieldAttributeWarning)\n            values_schema = self.generate_schema(values_type)\n        dict_schema = core_schema.dict_schema(keys_schema, values_schema, strict=False)\n\n        if mapped_origin is dict:\n            schema = dict_schema\n        else:\n            check_instance = core_schema.json_or_python_schema(\n                json_schema=dict_schema,\n                python_schema=core_schema.is_instance_schema(mapped_origin),\n            )\n\n            if tp is collections.defaultdict:\n                default_default_factory = get_defaultdict_default_default_factory(values_type)\n                coerce_instance_wrap = partial(\n                    core_schema.no_info_wrap_validator_function,\n                    partial(defaultdict_validator, default_default_factory=default_default_factory),\n                )\n            else:\n                coerce_instance_wrap = partial(core_schema.no_info_after_validator_function, mapped_origin)\n\n            lax_schema = coerce_instance_wrap(dict_schema)\n            strict_schema = core_schema.chain_schema([check_instance, lax_schema])\n\n            schema = core_schema.lax_or_strict_schema(\n                lax_schema=lax_schema,\n                strict_schema=strict_schema,\n                serialization=core_schema.wrap_serializer_function_ser_schema(\n                    lambda v, h: h(v), schema=dict_schema, info_arg=False\n                ),\n            )\n\n        return schema", "metadata": {"license": "MIT", "len_tokens": 434}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def generate_schema(\n        self,\n        obj: Any,\n    ) -> core_schema.CoreSchema:\n        \"\"\"Generate core schema.\n\n        Args:\n            obj: The object to generate core schema for.\n\n        Returns:\n            The generated core schema.\n\n        Raises:\n            PydanticUndefinedAnnotation:\n                If it is not possible to evaluate forward reference.\n            PydanticSchemaGenerationError:\n                If it is not possible to generate pydantic-core schema.\n            TypeError:\n                - If `alias_generator` returns a disallowed type (must be str, AliasPath or AliasChoices).\n                - If V1 style validator with `each_item=True` applied on a wrong field.\n            PydanticUserError:\n                - If `typing.TypedDict` is used instead of `typing_extensions.TypedDict` on Python < 3.12.\n                - If `__modify_schema__` method is used instead of `__get_pydantic_json_schema__`.\n        \"\"\"\n        schema = self._generate_schema_from_get_schema_method(obj, obj)\n\n        if schema is None:\n            schema = self._generate_schema_inner(obj)\n\n        metadata_js_function = _extract_get_pydantic_json_schema(obj)\n        if metadata_js_function is not None:\n            metadata_schema = resolve_original_schema(schema, self.defs)\n            if metadata_schema:\n                self._add_js_function(metadata_schema, metadata_js_function)\n\n        schema = _add_custom_serialization_from_json_encoders(self._config_wrapper.json_encoders, obj, schema)\n\n        return schema", "metadata": {"license": "MIT", "len_tokens": 310}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _generate_schema_from_get_schema_method(self, obj: Any, source: Any) -> core_schema.CoreSchema | None:\n        BaseModel_ = import_cached_base_model()\n\n        get_schema = getattr(obj, '__get_pydantic_core_schema__', None)\n        is_base_model_get_schema = (\n            getattr(get_schema, '__func__', None) is BaseModel_.__get_pydantic_core_schema__.__func__  # pyright: ignore[reportFunctionMemberAccess]\n        )\n\n        if (\n            get_schema is not None\n            # BaseModel.__get_pydantic_core_schema__ is defined for backwards compatibility,\n            # to allow existing code to call `super().__get_pydantic_core_schema__` in Pydantic\n            # model that overrides `__get_pydantic_core_schema__`. However, it raises a deprecation\n            # warning stating that the method will be removed, and during the core schema gen we actually\n            # don't call the method:\n            and not is_base_model_get_schema\n        ):\n            # Some referenceable types might have a `__get_pydantic_core_schema__` method\n            # defined on it by users (e.g. on a dataclass). This generally doesn't play well\n            # as these types are already recognized by the `GenerateSchema` class and isn't ideal\n            # as we might end up calling `get_schema_or_ref` (expensive) on types that are actually\n            # not referenceable:\n            with self.defs.get_schema_or_ref(obj) as (_, maybe_schema):\n                if maybe_schema is not None:\n                    return maybe_schema\n\n            if obj is source:\n                ref_mode = 'unpack'\n            else:\n                ref_mode = 'to-def'\n            schema = get_schema(\n                source, CallbackGetCoreSchemaHandler(self._generate_schema_inner, self, ref_mode=ref_mode)\n            )\n            if schema['type'] == 'definitions':\n                schema = self.defs.unpack_definitions(schema)\n\n            ref = get_ref(schema)\n            if ref:\n                return self.defs.create_definition_reference_schema(schema)\n\n            # Note: if schema is of type `'definition-ref'`, we might want to copy it as a\n            # safety measure (because these are inlined in place -- i.e. mutated directly)\n            return schema\n\n        if get_schema is None and (validators := getattr(obj, '__get_validators__', None)) is not None:\n            from pydantic.v1 import BaseModel as BaseModelV1\n\n            if issubclass(obj, BaseModelV1):\n                warnings.warn(\n                    f'Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `{obj.__name__}` to V2.',\n                    UserWarning,\n                )\n            else:\n                warnings.warn(\n                    '`__get_validators__` is deprecated and will be removed, use `__get_pydantic_core_schema__` instead.',\n                    PydanticDeprecatedSince20,\n                )\n            return core_schema.chain_schema([core_schema.with_info_plain_validator_function(v) for v in validators()])", "metadata": {"license": "MIT", "len_tokens": 645}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _resolve_forward_ref(self, obj: Any) -> Any:\n        # we assume that types_namespace has the target of forward references in its scope,\n        # but this could fail, for example, if calling Validator on an imported type which contains\n        # forward references to other types only defined in the module from which it was imported\n        # `Validator(SomeImportedTypeAliasWithAForwardReference)`\n        # or the equivalent for BaseModel\n        # class Model(BaseModel):\n        #   x: SomeImportedTypeAliasWithAForwardReference\n        try:\n            obj = _typing_extra.eval_type_backport(obj, *self._types_namespace)\n        except NameError as e:\n            raise PydanticUndefinedAnnotation.from_name_error(e) from e\n\n        # if obj is still a ForwardRef, it means we can't evaluate it, raise PydanticUndefinedAnnotation\n        if isinstance(obj, ForwardRef):\n            raise PydanticUndefinedAnnotation(obj.__forward_arg__, f'Unable to evaluate forward reference {obj}')\n\n        if self._typevars_map:\n            obj = replace_types(obj, self._typevars_map)\n\n        return obj", "metadata": {"license": "MIT", "len_tokens": 233}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _get_args_resolving_forward_refs(self, obj: Any, required: bool = False) -> tuple[Any, ...] | None:\n        args = get_args(obj)\n        if args:\n            if isinstance(obj, GenericAlias):\n                # PEP 585 generic aliases don't convert args to ForwardRefs, unlike `typing.List/Dict` etc.\n                # This was fixed in https://github.com/python/cpython/pull/30900 (Python 3.11).\n                # TODO: this shouldn't be necessary (probably even this `_get_args_resolving_forward_refs()` function)\n                # once we drop support for Python 3.10 *or* if we implement our own `typing._eval_type()` implementation.\n                args = (_typing_extra._make_forward_ref(a) if isinstance(a, str) else a for a in args)\n            args = tuple(self._resolve_forward_ref(a) if isinstance(a, ForwardRef) else a for a in args)\n        elif required:  # pragma: no cover\n            raise TypeError(f'Expected {obj} to have generic parameters but it had none')\n        return args", "metadata": {"license": "MIT", "len_tokens": 232}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _match_generic_type(self, obj: Any, origin: Any) -> CoreSchema:  # noqa: C901\n        # Need to handle generic dataclasses before looking for the schema properties because attribute accesses\n        # on _GenericAlias delegate to the origin type, so lose the information about the concrete parametrization\n        # As a result, currently, there is no way to cache the schema for generic dataclasses. This may be possible\n        # to resolve by modifying the value returned by `Generic.__class_getitem__`, but that is a dangerous game.\n        if dataclasses.is_dataclass(origin):\n            return self._dataclass_schema(obj, origin)  # pyright: ignore[reportArgumentType]\n        if _typing_extra.is_namedtuple(origin):\n            return self._namedtuple_schema(obj, origin)\n\n        schema = self._generate_schema_from_get_schema_method(origin, obj)\n        if schema is not None:\n            return schema\n\n        if typing_objects.is_typealiastype(origin):\n            return self._type_alias_type_schema(obj)\n        elif is_union_origin(origin):\n            return self._union_schema(obj)\n        elif origin in TUPLE_TYPES:\n            return self._tuple_schema(obj)\n        elif origin in LIST_TYPES:\n            return self._list_schema(self._get_first_arg_or_any(obj))\n        elif origin in SET_TYPES:\n            return self._set_schema(self._get_first_arg_or_any(obj))\n        elif origin in FROZEN_SET_TYPES:\n            return self._frozenset_schema(self._get_first_arg_or_any(obj))\n        elif origin in DICT_TYPES:\n            return self._dict_schema(*self._get_first_two_args_or_any(obj))\n        elif origin in PATH_TYPES:\n            return self._path_schema(origin, self._get_first_arg_or_any(obj))\n        elif origin in DEQUE_TYPES:\n            return self._deque_schema(self._get_first_arg_or_any(obj))\n        elif origin in MAPPING_TYPES:\n            return self._mapping_schema(origin, *self._get_first_two_args_or_any(obj))\n        elif origin in COUNTER_TYPES:\n            return self._mapping_schema(origin, self._get_first_arg_or_any(obj), int)\n        elif is_typeddict(origin):\n            return self._typed_dict_schema(obj, origin)\n        elif origin in TYPE_TYPES:\n            return self._subclass_schema(obj)\n        elif origin in SEQUENCE_TYPES:\n            return self._sequence_schema(self._get_first_arg_or_any(obj))\n        elif origin in ITERABLE_TYPES:\n            return self._iterable_schema(obj)\n        elif origin in PATTERN_TYPES:\n            return self._pattern_schema(obj)\n\n        if self._arbitrary_types:\n            return self._arbitrary_type_schema(origin)\n        return self._unknown_type_schema(obj)", "metadata": {"license": "MIT", "len_tokens": 562}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _common_field_schema(  # C901\n        self, name: str, field_info: FieldInfo, decorators: DecoratorInfos\n    ) -> tuple[CoreSchema, dict[str, Any]]:\n        source_type, annotations = field_info.annotation, field_info.metadata\n\n        def set_discriminator(schema: CoreSchema) -> CoreSchema:\n            schema = self._apply_discriminator_to_union(schema, field_info.discriminator)\n            return schema\n\n        # Convert `@field_validator` decorators to `Before/After/Plain/WrapValidator` instances:\n        validators_from_decorators = [\n            _mode_to_validator[decorator.info.mode]._from_decorator(decorator)\n            for decorator in filter_field_decorator_info_by_field(decorators.field_validators.values(), name)\n        ]\n\n        with self.field_name_stack.push(name):\n            if field_info.discriminator is not None:\n                schema = self._apply_annotations(\n                    source_type, annotations + validators_from_decorators, transform_inner_schema=set_discriminator\n                )\n            else:\n                schema = self._apply_annotations(\n                    source_type,\n                    annotations + validators_from_decorators,\n                )\n\n        # This V1 compatibility shim should eventually be removed\n        # push down any `each_item=True` validators\n        # note that this won't work for any Annotated types that get wrapped by a function validator\n        # but that's okay because that didn't exist in V1\n        this_field_validators = filter_field_decorator_info_by_field(decorators.validators.values(), name)\n        if _validators_require_validate_default(this_field_validators):\n            field_info.validate_default = True\n        each_item_validators = [v for v in this_field_validators if v.info.each_item is True]\n        this_field_validators = [v for v in this_field_validators if v not in each_item_validators]\n        schema = apply_each_item_validators(schema, each_item_validators)\n\n        schema = apply_validators(schema, this_field_validators)\n\n        # the default validator needs to go outside of any other validators\n        # so that it is the topmost validator for the field validator\n        # which uses it to check if the field has a default value or not\n        if not field_info.is_required():\n            schema = wrap_default(field_info, schema)\n\n        schema = self._apply_field_serializers(\n            schema, filter_field_decorator_info_by_field(decorators.field_serializers.values(), name)\n        )\n\n        pydantic_js_updates, pydantic_js_extra = _extract_json_schema_info_from_field_info(field_info)\n        core_metadata: dict[str, Any] = {}\n        update_core_metadata(\n            core_metadata, pydantic_js_updates=pydantic_js_updates, pydantic_js_extra=pydantic_js_extra\n        )\n\n        return schema, core_metadata", "metadata": {"license": "MIT", "len_tokens": 579}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _union_schema(self, union_type: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a Union.\"\"\"\n        args = self._get_args_resolving_forward_refs(union_type, required=True)\n        choices: list[CoreSchema] = []\n        nullable = False\n        for arg in args:\n            if arg is None or arg is _typing_extra.NoneType:\n                nullable = True\n            else:\n                choices.append(self.generate_schema(arg))\n\n        if len(choices) == 1:\n            s = choices[0]\n        else:\n            choices_with_tags: list[CoreSchema | tuple[CoreSchema, str]] = []\n            for choice in choices:\n                tag = cast(CoreMetadata, choice.get('metadata', {})).get('pydantic_internal_union_tag_key')\n                if tag is not None:\n                    choices_with_tags.append((choice, tag))\n                else:\n                    choices_with_tags.append(choice)\n            s = core_schema.union_schema(choices_with_tags)\n\n        if nullable:\n            s = core_schema.nullable_schema(s)\n        return s", "metadata": {"license": "MIT", "len_tokens": 220}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _namedtuple_schema(self, namedtuple_cls: Any, origin: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a NamedTuple.\"\"\"\n        with (\n            self.model_type_stack.push(namedtuple_cls),\n            self.defs.get_schema_or_ref(namedtuple_cls) as (\n                namedtuple_ref,\n                maybe_schema,\n            ),\n        ):\n            if maybe_schema is not None:\n                return maybe_schema\n            typevars_map = get_standard_typevars_map(namedtuple_cls)\n            if origin is not None:\n                namedtuple_cls = origin\n\n            try:\n                annotations = _typing_extra.get_cls_type_hints(namedtuple_cls, ns_resolver=self._ns_resolver)\n            except NameError as e:\n                raise PydanticUndefinedAnnotation.from_name_error(e) from e\n            if not annotations:\n                # annotations is empty, happens if namedtuple_cls defined via collections.namedtuple(...)\n                annotations: dict[str, Any] = dict.fromkeys(namedtuple_cls._fields, Any)\n\n            if typevars_map:\n                annotations = {\n                    field_name: replace_types(annotation, typevars_map)\n                    for field_name, annotation in annotations.items()\n                }\n\n            arguments_schema = core_schema.arguments_schema(\n                [\n                    self._generate_parameter_schema(\n                        field_name,\n                        annotation,\n                        source=AnnotationSource.NAMED_TUPLE,\n                        default=namedtuple_cls._field_defaults.get(field_name, Parameter.empty),\n                    )\n                    for field_name, annotation in annotations.items()\n                ],\n                metadata={'pydantic_js_prefer_positional_arguments': True},\n            )\n            schema = core_schema.call_schema(arguments_schema, namedtuple_cls, ref=namedtuple_ref)\n            return self.defs.create_definition_reference_schema(schema)", "metadata": {"license": "MIT", "len_tokens": 353}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _generate_parameter_schema(\n        self,\n        name: str,\n        annotation: type[Any],\n        source: AnnotationSource,\n        default: Any = Parameter.empty,\n        mode: Literal['positional_only', 'positional_or_keyword', 'keyword_only'] | None = None,\n    ) -> core_schema.ArgumentsParameter:\n        \"\"\"Generate the definition of a field in a namedtuple or a parameter in a function signature.\n\n        This definition is meant to be used for the `'arguments'` core schema, which will be replaced\n        in V3 by the `'arguments-v3`'.\n        \"\"\"\n        FieldInfo = import_cached_field_info()\n\n        if default is Parameter.empty:\n            field = FieldInfo.from_annotation(annotation, _source=source)\n        else:\n            field = FieldInfo.from_annotated_attribute(annotation, default, _source=source)\n\n        assert field.annotation is not None, 'field.annotation should not be None when generating a schema'\n        update_field_from_config(self._config_wrapper, name, field)\n\n        with self.field_name_stack.push(name):\n            schema = self._apply_annotations(\n                field.annotation,\n                [field],\n                # Because we pass `field` as metadata above (required for attributes relevant for\n                # JSON Scheme generation), we need to ignore the potential warnings about `FieldInfo`\n                # attributes that will not be used:\n                check_unsupported_field_info_attributes=False,\n            )\n\n        if not field.is_required():\n            schema = wrap_default(field, schema)\n\n        parameter_schema = core_schema.arguments_parameter(\n            name,\n            schema,\n            mode=mode,\n            alias=_convert_to_aliases(field.validation_alias),\n        )\n\n        return parameter_schema", "metadata": {"license": "MIT", "len_tokens": 343}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _generate_parameter_v3_schema(\n        self,\n        name: str,\n        annotation: Any,\n        source: AnnotationSource,\n        mode: Literal[\n            'positional_only',\n            'positional_or_keyword',\n            'keyword_only',\n            'var_args',\n            'var_kwargs_uniform',\n            'var_kwargs_unpacked_typed_dict',\n        ],\n        default: Any = Parameter.empty,\n    ) -> core_schema.ArgumentsV3Parameter:\n        \"\"\"Generate the definition of a parameter in a function signature.\n\n        This definition is meant to be used for the `'arguments-v3'` core schema, which will replace\n        the `'arguments`' schema in V3.\n        \"\"\"\n        FieldInfo = import_cached_field_info()\n\n        if default is Parameter.empty:\n            field = FieldInfo.from_annotation(annotation, _source=source)\n        else:\n            field = FieldInfo.from_annotated_attribute(annotation, default, _source=source)\n        update_field_from_config(self._config_wrapper, name, field)\n\n        with self.field_name_stack.push(name):\n            schema = self._apply_annotations(\n                field.annotation,\n                [field],\n                # Because we pass `field` as metadata above (required for attributes relevant for\n                # JSON Scheme generation), we need to ignore the potential warnings about `FieldInfo`\n                # attributes that will not be used:\n                check_unsupported_field_info_attributes=False,\n            )\n\n        if not field.is_required():\n            schema = wrap_default(field, schema)\n\n        parameter_schema = core_schema.arguments_v3_parameter(\n            name=name,\n            schema=schema,\n            mode=mode,\n            alias=_convert_to_aliases(field.validation_alias),\n        )\n\n        return parameter_schema", "metadata": {"license": "MIT", "len_tokens": 346}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _tuple_schema(self, tuple_type: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a Tuple, e.g. `tuple[int, str]` or `tuple[int, ...]`.\"\"\"\n        # TODO: do we really need to resolve type vars here?\n        typevars_map = get_standard_typevars_map(tuple_type)\n        params = self._get_args_resolving_forward_refs(tuple_type)\n\n        if typevars_map and params:\n            params = tuple(replace_types(param, typevars_map) for param in params)\n\n        # NOTE: subtle difference: `tuple[()]` gives `params=()`, whereas `typing.Tuple[()]` gives `params=((),)`\n        # This is only true for <3.11, on Python 3.11+ `typing.Tuple[()]` gives `params=()`\n        if not params:\n            if tuple_type in TUPLE_TYPES:\n                return core_schema.tuple_schema([core_schema.any_schema()], variadic_item_index=0)\n            else:\n                # special case for `tuple[()]` which means `tuple[]` - an empty tuple\n                return core_schema.tuple_schema([])\n        elif params[-1] is Ellipsis:\n            if len(params) == 2:\n                return core_schema.tuple_schema([self.generate_schema(params[0])], variadic_item_index=0)\n            else:\n                # TODO: something like https://github.com/pydantic/pydantic/issues/5952\n                raise ValueError('Variable tuples can only have one type')\n        elif len(params) == 1 and params[0] == ():\n            # special case for `tuple[()]` which means `tuple[]` - an empty tuple\n            # NOTE: This conditional can be removed when we drop support for Python 3.10.\n            return core_schema.tuple_schema([])\n        else:\n            return core_schema.tuple_schema([self.generate_schema(param) for param in params])", "metadata": {"license": "MIT", "len_tokens": 406}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _subclass_schema(self, type_: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a type, e.g. `type[int]`.\"\"\"\n        type_param = self._get_first_arg_or_any(type_)\n\n        # Assume `type[Annotated[<typ>, ...]]` is equivalent to `type[<typ>]`:\n        type_param = _typing_extra.annotated_type(type_param) or type_param\n\n        if typing_objects.is_any(type_param):\n            return self._type_schema()\n        elif typing_objects.is_typealiastype(type_param):\n            return self.generate_schema(type[type_param.__value__])\n        elif typing_objects.is_typevar(type_param):\n            if type_param.__bound__:\n                if is_union_origin(get_origin(type_param.__bound__)):\n                    return self._union_is_subclass_schema(type_param.__bound__)\n                return core_schema.is_subclass_schema(type_param.__bound__)\n            elif type_param.__constraints__:\n                return core_schema.union_schema([self.generate_schema(type[c]) for c in type_param.__constraints__])\n            else:\n                return self._type_schema()\n        elif is_union_origin(get_origin(type_param)):\n            return self._union_is_subclass_schema(type_param)\n        else:\n            if typing_objects.is_self(type_param):\n                type_param = self._resolve_self_type(type_param)\n            if _typing_extra.is_generic_alias(type_param):\n                raise PydanticUserError(\n                    'Subscripting `type[]` with an already parametrized type is not supported. '\n                    f'Instead of using type[{type_param!r}], use type[{_repr.display_as_type(get_origin(type_param))}].',\n                    code=None,\n                )\n            if not inspect.isclass(type_param):\n                # when using type[None], this doesn't type convert to type[NoneType], and None isn't a class\n                # so we handle it manually here\n                if type_param is None:\n                    return core_schema.is_subclass_schema(_typing_extra.NoneType)\n                raise TypeError(f'Expected a class, got {type_param!r}')\n            return core_schema.is_subclass_schema(type_param)", "metadata": {"license": "MIT", "len_tokens": 437}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _sequence_schema(self, items_type: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a Sequence, e.g. `Sequence[int]`.\"\"\"\n        from ._serializers import serialize_sequence_via_list\n\n        item_type_schema = self.generate_schema(items_type)\n        list_schema = core_schema.list_schema(item_type_schema)\n\n        json_schema = smart_deepcopy(list_schema)\n        python_schema = core_schema.is_instance_schema(typing.Sequence, cls_repr='Sequence')\n        if not typing_objects.is_any(items_type):\n            from ._validators import sequence_validator\n\n            python_schema = core_schema.chain_schema(\n                [python_schema, core_schema.no_info_wrap_validator_function(sequence_validator, list_schema)],\n            )\n\n        serialization = core_schema.wrap_serializer_function_ser_schema(\n            serialize_sequence_via_list, schema=item_type_schema, info_arg=True\n        )\n        return core_schema.json_or_python_schema(\n            json_schema=json_schema, python_schema=python_schema, serialization=serialization\n        )", "metadata": {"license": "MIT", "len_tokens": 203}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _pattern_schema(self, pattern_type: Any) -> core_schema.CoreSchema:\n        from . import _validators\n\n        metadata = {'pydantic_js_functions': [lambda _1, _2: {'type': 'string', 'format': 'regex'}]}\n        ser = core_schema.plain_serializer_function_ser_schema(\n            attrgetter('pattern'), when_used='json', return_schema=core_schema.str_schema()\n        )\n        if pattern_type is typing.Pattern or pattern_type is re.Pattern:\n            # bare type\n            return core_schema.no_info_plain_validator_function(\n                _validators.pattern_either_validator, serialization=ser, metadata=metadata\n            )\n\n        param = self._get_args_resolving_forward_refs(\n            pattern_type,\n            required=True,\n        )[0]\n        if param is str:\n            return core_schema.no_info_plain_validator_function(\n                _validators.pattern_str_validator, serialization=ser, metadata=metadata\n            )\n        elif param is bytes:\n            return core_schema.no_info_plain_validator_function(\n                _validators.pattern_bytes_validator, serialization=ser, metadata=metadata\n            )\n        else:\n            raise PydanticSchemaGenerationError(f'Unable to generate pydantic-core schema for {pattern_type!r}.')", "metadata": {"license": "MIT", "len_tokens": 255}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _arguments_schema(\n        self, function: ValidateCallSupportedTypes, parameters_callback: ParametersCallback | None = None\n    ) -> core_schema.ArgumentsSchema:\n        \"\"\"Generate schema for a Signature.\"\"\"\n        mode_lookup: dict[_ParameterKind, Literal['positional_only', 'positional_or_keyword', 'keyword_only']] = {\n            Parameter.POSITIONAL_ONLY: 'positional_only',\n            Parameter.POSITIONAL_OR_KEYWORD: 'positional_or_keyword',\n            Parameter.KEYWORD_ONLY: 'keyword_only',\n        }\n\n        sig = signature(function)\n        globalns, localns = self._types_namespace\n        type_hints = _typing_extra.get_function_type_hints(function, globalns=globalns, localns=localns)\n\n        arguments_list: list[core_schema.ArgumentsParameter] = []\n        var_args_schema: core_schema.CoreSchema | None = None\n        var_kwargs_schema: core_schema.CoreSchema | None = None\n        var_kwargs_mode: core_schema.VarKwargsMode | None = None\n\n        for i, (name, p) in enumerate(sig.parameters.items()):\n            if p.annotation is sig.empty:\n                annotation = typing.cast(Any, Any)\n            else:\n                annotation = type_hints[name]\n\n            if parameters_callback is not None:\n                result = parameters_callback(i, name, annotation)\n                if result == 'skip':\n                    continue\n\n            parameter_mode = mode_lookup.get(p.kind)\n            if parameter_mode is not None:\n                arg_schema = self._generate_parameter_schema(\n                    name, annotation, AnnotationSource.FUNCTION, p.default, parameter_mode\n                )\n                arguments_list.append(arg_schema)\n            elif p.kind == Parameter.VAR_POSITIONAL:\n                var_args_schema = self.generate_schema(annotation)\n            else:\n                assert p.kind == Parameter.VAR_KEYWORD, p.kind\n\n                unpack_type = _typing_extra.unpack_type(annotation)\n                if unpack_type is not None:\n                    origin = get_origin(unpack_type) or unpack_type\n                    if not is_typeddict(origin):\n                        raise PydanticUserError(\n                            f'Expected a `TypedDict` class inside `Unpack[...]`, got {unpack_type!r}',\n                            code='unpack-typed-dict',\n                        )\n                    non_pos_only_param_names = {\n                        name for name, p in sig.parameters.items() if p.kind != Parameter.POSITIONAL_ONLY\n                    }\n                    overlapping_params = non_pos_only_param_names.intersection(origin.__annotations__)\n                    if overlapping_params:\n                        raise PydanticUserError(\n                            f'Typed dictionary {origin.__name__!r} overlaps with parameter'\n                            f'{\"s\" if len(overlapping_params) >= 2 else \"\"} '\n                            f'{\", \".join(repr(p) for p in sorted(overlapping_params))}',\n                            code='overlapping-unpack-typed-dict',\n                        )\n\n                    var_kwargs_mode = 'unpacked-typed-dict'\n                    var_kwargs_schema = self._typed_dict_schema(unpack_type, get_origin(unpack_type))\n                else:\n                    var_kwargs_mode = 'uniform'\n                    var_kwargs_schema = self.generate_schema(annotation)\n\n        return core_schema.arguments_schema(\n            arguments_list,\n            var_args_schema=var_args_schema,\n            var_kwargs_mode=var_kwargs_mode,\n            var_kwargs_schema=var_kwargs_schema,\n            validate_by_name=self._config_wrapper.validate_by_name,\n        )", "metadata": {"license": "MIT", "len_tokens": 694}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _arguments_v3_schema(\n        self, function: ValidateCallSupportedTypes, parameters_callback: ParametersCallback | None = None\n    ) -> core_schema.ArgumentsV3Schema:\n        mode_lookup: dict[\n            _ParameterKind, Literal['positional_only', 'positional_or_keyword', 'var_args', 'keyword_only']\n        ] = {\n            Parameter.POSITIONAL_ONLY: 'positional_only',\n            Parameter.POSITIONAL_OR_KEYWORD: 'positional_or_keyword',\n            Parameter.VAR_POSITIONAL: 'var_args',\n            Parameter.KEYWORD_ONLY: 'keyword_only',\n        }\n\n        sig = signature(function)\n        globalns, localns = self._types_namespace\n        type_hints = _typing_extra.get_function_type_hints(function, globalns=globalns, localns=localns)\n\n        parameters_list: list[core_schema.ArgumentsV3Parameter] = []\n\n        for i, (name, p) in enumerate(sig.parameters.items()):\n            if parameters_callback is not None:\n                result = parameters_callback(i, name, p.annotation)\n                if result == 'skip':\n                    continue\n\n            if p.annotation is Parameter.empty:\n                annotation = typing.cast(Any, Any)\n            else:\n                annotation = type_hints[name]\n\n            parameter_mode = mode_lookup.get(p.kind)\n            if parameter_mode is None:\n                assert p.kind == Parameter.VAR_KEYWORD, p.kind\n\n                unpack_type = _typing_extra.unpack_type(annotation)\n                if unpack_type is not None:\n                    origin = get_origin(unpack_type) or unpack_type\n                    if not is_typeddict(origin):\n                        raise PydanticUserError(\n                            f'Expected a `TypedDict` class inside `Unpack[...]`, got {unpack_type!r}',\n                            code='unpack-typed-dict',\n                        )\n                    non_pos_only_param_names = {\n                        name for name, p in sig.parameters.items() if p.kind != Parameter.POSITIONAL_ONLY\n                    }\n                    overlapping_params = non_pos_only_param_names.intersection(origin.__annotations__)\n                    if overlapping_params:\n                        raise PydanticUserError(\n                            f'Typed dictionary {origin.__name__!r} overlaps with parameter'\n                            f'{\"s\" if len(overlapping_params) >= 2 else \"\"} '\n                            f'{\", \".join(repr(p) for p in sorted(overlapping_params))}',\n                            code='overlapping-unpack-typed-dict',\n                        )\n                    parameter_mode = 'var_kwargs_unpacked_typed_dict'\n                    annotation = unpack_type\n                else:\n                    parameter_mode = 'var_kwargs_uniform'\n\n            parameters_list.append(\n                self._generate_parameter_v3_schema(\n                    name, annotation, AnnotationSource.FUNCTION, parameter_mode, default=p.default\n                )\n            )\n\n        return core_schema.arguments_v3_schema(\n            parameters_list,\n            validate_by_name=self._config_wrapper.validate_by_name,\n        )", "metadata": {"license": "MIT", "len_tokens": 595}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _computed_field_schema(\n        self,\n        d: Decorator[ComputedFieldInfo],\n        field_serializers: dict[str, Decorator[FieldSerializerDecoratorInfo]],\n    ) -> core_schema.ComputedField:\n        if d.info.return_type is not PydanticUndefined:\n            return_type = d.info.return_type\n        else:\n            try:\n                # Do not pass in globals as the function could be defined in a different module.\n                # Instead, let `get_callable_return_type` infer the globals to use, but still pass\n                # in locals that may contain a parent/rebuild namespace:\n                return_type = _decorators.get_callable_return_type(d.func, localns=self._types_namespace.locals)\n            except NameError as e:\n                raise PydanticUndefinedAnnotation.from_name_error(e) from e\n        if return_type is PydanticUndefined:\n            raise PydanticUserError(\n                'Computed field is missing return type annotation or specifying `return_type`'\n                ' to the `@computed_field` decorator (e.g. `@computed_field(return_type=int | str)`)',\n                code='model-field-missing-annotation',\n            )\n\n        return_type = replace_types(return_type, self._typevars_map)\n        # Create a new ComputedFieldInfo so that different type parametrizations of the same\n        # generic model's computed field can have different return types.\n        d.info = dataclasses.replace(d.info, return_type=return_type)\n        return_type_schema = self.generate_schema(return_type)\n        # Apply serializers to computed field if there exist\n        return_type_schema = self._apply_field_serializers(\n            return_type_schema,\n            filter_field_decorator_info_by_field(field_serializers.values(), d.cls_var_name),\n        )\n\n        pydantic_js_updates, pydantic_js_extra = _extract_json_schema_info_from_field_info(d.info)\n        core_metadata: dict[str, Any] = {}\n        update_core_metadata(\n            core_metadata,\n            pydantic_js_updates={'readOnly': True, **(pydantic_js_updates if pydantic_js_updates else {})},\n            pydantic_js_extra=pydantic_js_extra,\n        )\n        return core_schema.computed_field(\n            d.cls_var_name, return_schema=return_type_schema, alias=d.info.alias, metadata=core_metadata\n        )", "metadata": {"license": "MIT", "len_tokens": 480}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _apply_annotations(\n        self,\n        source_type: Any,\n        annotations: list[Any],\n        transform_inner_schema: Callable[[CoreSchema], CoreSchema] = lambda x: x,\n        check_unsupported_field_info_attributes: bool = True,\n    ) -> CoreSchema:\n        \"\"\"Apply arguments from `Annotated` or from `FieldInfo` to a schema.\n\n        This gets called by `GenerateSchema._annotated_schema` but differs from it in that it does\n        not expect `source_type` to be an `Annotated` object, it expects it to be  the first argument of that\n        (in other words, `GenerateSchema._annotated_schema` just unpacks `Annotated`, this process it).\n        \"\"\"\n        annotations = list(_known_annotated_metadata.expand_grouped_metadata(annotations))\n\n        pydantic_js_annotation_functions: list[GetJsonSchemaFunction] = []\n\n        def inner_handler(obj: Any) -> CoreSchema:\n            schema = self._generate_schema_from_get_schema_method(obj, source_type)\n\n            if schema is None:\n                schema = self._generate_schema_inner(obj)\n\n            metadata_js_function = _extract_get_pydantic_json_schema(obj)\n            if metadata_js_function is not None:\n                metadata_schema = resolve_original_schema(schema, self.defs)\n                if metadata_schema is not None:\n                    self._add_js_function(metadata_schema, metadata_js_function)\n            return transform_inner_schema(schema)\n\n        get_inner_schema = CallbackGetCoreSchemaHandler(inner_handler, self)\n\n        for annotation in annotations:\n            if annotation is None:\n                continue\n            get_inner_schema = self._get_wrapped_inner_schema(\n                get_inner_schema,\n                annotation,\n                pydantic_js_annotation_functions,\n                check_unsupported_field_info_attributes=check_unsupported_field_info_attributes,\n            )\n\n        schema = get_inner_schema(source_type)\n        if pydantic_js_annotation_functions:\n            core_metadata = schema.setdefault('metadata', {})\n            update_core_metadata(core_metadata, pydantic_js_annotation_functions=pydantic_js_annotation_functions)\n        return _add_custom_serialization_from_json_encoders(self._config_wrapper.json_encoders, source_type, schema)", "metadata": {"license": "MIT", "len_tokens": 441}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _apply_single_annotation(\n        self,\n        schema: core_schema.CoreSchema,\n        metadata: Any,\n        check_unsupported_field_info_attributes: bool = True,\n    ) -> core_schema.CoreSchema:\n        FieldInfo = import_cached_field_info()\n\n        if isinstance(metadata, FieldInfo):\n            if (\n                check_unsupported_field_info_attributes\n                # HACK: we don't want to emit the warning for `FieldInfo` subclasses, because FastAPI does weird manipulations\n                # with its subclasses and their annotations:\n                and type(metadata) is FieldInfo\n            ):\n                for attr, value in (unsupported_attributes := self._get_unsupported_field_info_attributes(metadata)):\n                    warnings.warn(\n                        f'The {attr!r} attribute with value {value!r} was provided to the `Field()` function, '\n                        f'which has no effect in the context it was used. {attr!r} is field-specific metadata, '\n                        'and can only be attached to a model field using `Annotated` metadata or by assignment. '\n                        'This may have happened because an `Annotated` type alias using the `type` statement was '\n                        'used, or if the `Field()` function was attached to a single member of a union type.',\n                        category=UnsupportedFieldAttributeWarning,\n                    )\n\n                if (\n                    metadata.default_factory_takes_validated_data\n                    and self.model_type_stack.get() is None\n                    and 'defaut_factory' not in unsupported_attributes\n                ):\n                    warnings.warn(\n                        \"A 'default_factory' taking validated data as an argument was provided to the `Field()` function, \"\n                        'but no validated data is available in the context it was used.',\n                        category=UnsupportedFieldAttributeWarning,\n                    )\n\n            for field_metadata in metadata.metadata:\n                schema = self._apply_single_annotation(schema, field_metadata)\n\n            if metadata.discriminator is not None:\n                schema = self._apply_discriminator_to_union(schema, metadata.discriminator)\n            return schema\n\n        if schema['type'] == 'nullable':\n            # for nullable schemas, metadata is automatically applied to the inner schema\n            inner = schema.get('schema', core_schema.any_schema())\n            inner = self._apply_single_annotation(inner, metadata)\n            if inner:\n                schema['schema'] = inner\n            return schema\n\n        original_schema = schema\n        ref = schema.get('ref')\n        if ref is not None:\n            schema = schema.copy()\n            new_ref = ref + f'_{repr(metadata)}'\n            if (existing := self.defs.get_schema_from_ref(new_ref)) is not None:\n                return existing\n            schema['ref'] = new_ref  # pyright: ignore[reportGeneralTypeIssues]\n        elif schema['type'] == 'definition-ref':\n            ref = schema['schema_ref']\n            if (referenced_schema := self.defs.get_schema_from_ref(ref)) is not None:\n                schema = referenced_schema.copy()\n                new_ref = ref + f'_{repr(metadata)}'\n                if (existing := self.defs.get_schema_from_ref(new_ref)) is not None:\n                    return existing\n                schema['ref'] = new_ref  # pyright: ignore[reportGeneralTypeIssues]\n\n        maybe_updated_schema = _known_annotated_metadata.apply_known_metadata(metadata, schema)\n\n        if maybe_updated_schema is not None:\n            return maybe_updated_schema\n        return original_schema", "metadata": {"license": "MIT", "len_tokens": 695}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _get_unsupported_field_info_attributes(self, field_info: FieldInfo) -> list[tuple[str, Any]]:\n        \"\"\"Get the list of unsupported `FieldInfo` attributes when not directly used in `Annotated` for field annotations.\"\"\"\n        unused_metadata: list[tuple[str, Any]] = []\n        for unused_metadata_name, unset_value in UNSUPPORTED_STANDALONE_FIELDINFO_ATTRIBUTES:\n            if (\n                (unused_metadata_value := getattr(field_info, unused_metadata_name)) is not unset_value\n                # `default` and `default_factory` can still be used with a type adapter, so only include them\n                # if used with a model-like class:\n                and (\n                    unused_metadata_name not in ('default', 'default_factory')\n                    or self.model_type_stack.get() is not None\n                )\n                # Setting `alias` will set `validation/serialization_alias` as well, so we want to avoid duplicate warnings:\n                and (\n                    unused_metadata_name not in ('validation_alias', 'serialization_alias')\n                    or 'alias' not in field_info._attributes_set\n                )\n            ):\n                unused_metadata.append((unused_metadata_name, unused_metadata_value))\n\n        return unused_metadata", "metadata": {"license": "MIT", "len_tokens": 243}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _get_wrapped_inner_schema(\n        self,\n        get_inner_schema: GetCoreSchemaHandler,\n        annotation: Any,\n        pydantic_js_annotation_functions: list[GetJsonSchemaFunction],\n        check_unsupported_field_info_attributes: bool = False,\n    ) -> CallbackGetCoreSchemaHandler:\n        annotation_get_schema: GetCoreSchemaFunction | None = getattr(annotation, '__get_pydantic_core_schema__', None)\n\n        def new_handler(source: Any) -> core_schema.CoreSchema:\n            if annotation_get_schema is not None:\n                schema = annotation_get_schema(source, get_inner_schema)\n            else:\n                schema = get_inner_schema(source)\n                schema = self._apply_single_annotation(\n                    schema,\n                    annotation,\n                    check_unsupported_field_info_attributes=check_unsupported_field_info_attributes,\n                )\n                schema = self._apply_single_annotation_json_schema(schema, annotation)\n\n            metadata_js_function = _extract_get_pydantic_json_schema(annotation)\n            if metadata_js_function is not None:\n                pydantic_js_annotation_functions.append(metadata_js_function)\n            return schema\n\n        return CallbackGetCoreSchemaHandler(new_handler, self)", "metadata": {"license": "MIT", "len_tokens": 232}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _apply_field_serializers(\n        self,\n        schema: core_schema.CoreSchema,\n        serializers: list[Decorator[FieldSerializerDecoratorInfo]],\n    ) -> core_schema.CoreSchema:\n        \"\"\"Apply field serializers to a schema.\"\"\"\n        if serializers:\n            schema = copy(schema)\n            if schema['type'] == 'definitions':\n                inner_schema = schema['schema']\n                schema['schema'] = self._apply_field_serializers(inner_schema, serializers)\n                return schema\n            elif 'ref' in schema:\n                schema = self.defs.create_definition_reference_schema(schema)\n\n            # use the last serializer to make it easy to override a serializer set on a parent model\n            serializer = serializers[-1]\n            is_field_serializer, info_arg = inspect_field_serializer(serializer.func, serializer.info.mode)\n\n            if serializer.info.return_type is not PydanticUndefined:\n                return_type = serializer.info.return_type\n            else:\n                try:\n                    # Do not pass in globals as the function could be defined in a different module.\n                    # Instead, let `get_callable_return_type` infer the globals to use, but still pass\n                    # in locals that may contain a parent/rebuild namespace:\n                    return_type = _decorators.get_callable_return_type(\n                        serializer.func, localns=self._types_namespace.locals\n                    )\n                except NameError as e:\n                    raise PydanticUndefinedAnnotation.from_name_error(e) from e\n\n            if return_type is PydanticUndefined:\n                return_schema = None\n            else:\n                return_schema = self.generate_schema(return_type)\n\n            if serializer.info.mode == 'wrap':\n                schema['serialization'] = core_schema.wrap_serializer_function_ser_schema(\n                    serializer.func,\n                    is_field_serializer=is_field_serializer,\n                    info_arg=info_arg,\n                    return_schema=return_schema,\n                    when_used=serializer.info.when_used,\n                )\n            else:\n                assert serializer.info.mode == 'plain'\n                schema['serialization'] = core_schema.plain_serializer_function_ser_schema(\n                    serializer.func,\n                    is_field_serializer=is_field_serializer,\n                    info_arg=info_arg,\n                    return_schema=return_schema,\n                    when_used=serializer.info.when_used,\n                )\n        return schema", "metadata": {"license": "MIT", "len_tokens": 445}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def _apply_model_serializers(\n        self, schema: core_schema.CoreSchema, serializers: Iterable[Decorator[ModelSerializerDecoratorInfo]]\n    ) -> core_schema.CoreSchema:\n        \"\"\"Apply model serializers to a schema.\"\"\"\n        ref: str | None = schema.pop('ref', None)  # type: ignore\n        if serializers:\n            serializer = list(serializers)[-1]\n            info_arg = inspect_model_serializer(serializer.func, serializer.info.mode)\n\n            if serializer.info.return_type is not PydanticUndefined:\n                return_type = serializer.info.return_type\n            else:\n                try:\n                    # Do not pass in globals as the function could be defined in a different module.\n                    # Instead, let `get_callable_return_type` infer the globals to use, but still pass\n                    # in locals that may contain a parent/rebuild namespace:\n                    return_type = _decorators.get_callable_return_type(\n                        serializer.func, localns=self._types_namespace.locals\n                    )\n                except NameError as e:\n                    raise PydanticUndefinedAnnotation.from_name_error(e) from e\n\n            if return_type is PydanticUndefined:\n                return_schema = None\n            else:\n                return_schema = self.generate_schema(return_type)\n\n            if serializer.info.mode == 'wrap':\n                ser_schema: core_schema.SerSchema = core_schema.wrap_serializer_function_ser_schema(\n                    serializer.func,\n                    info_arg=info_arg,\n                    return_schema=return_schema,\n                    when_used=serializer.info.when_used,\n                )\n            else:\n                # plain\n                ser_schema = core_schema.plain_serializer_function_ser_schema(\n                    serializer.func,\n                    info_arg=info_arg,\n                    return_schema=return_schema,\n                    when_used=serializer.info.when_used,\n                )\n            schema['serialization'] = ser_schema\n        if ref:\n            schema['ref'] = ref  # type: ignore\n        return schema", "metadata": {"license": "MIT", "len_tokens": 383}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def get_schema_or_ref(self, tp: Any, /) -> Generator[tuple[str, core_schema.DefinitionReferenceSchema | None]]:\n        \"\"\"Get a definition for `tp` if one exists.\n\n        If a definition exists, a tuple of `(ref_string, CoreSchema)` is returned.\n        If no definition exists yet, a tuple of `(ref_string, None)` is returned.\n\n        Note that the returned `CoreSchema` will always be a `DefinitionReferenceSchema`,\n        not the actual definition itself.\n\n        This should be called for any type that can be identified by reference.\n        This includes any recursive types.\n\n        At present the following types can be named/recursive:\n\n        - Pydantic model\n        - Pydantic and stdlib dataclasses\n        - Typed dictionaries\n        - Named tuples\n        - `TypeAliasType` instances\n        - Enums\n        \"\"\"\n        ref = get_type_ref(tp)\n        # return the reference if we're either (1) in a cycle or (2) it the reference was already encountered:\n        if ref in self._recursively_seen or ref in self._definitions:\n            yield (ref, core_schema.definition_reference_schema(ref))\n        else:\n            self._recursively_seen.add(ref)\n            try:\n                yield (ref, None)\n            finally:\n                self._recursively_seen.discard(ref)", "metadata": {"license": "MIT", "len_tokens": 278}}
{"id": "pydantic:pydantic/_internal/_generate_schema.py", "language": "python", "code": "def finalize_schema(self, schema: CoreSchema) -> CoreSchema:\n        \"\"\"Finalize the core schema.\n\n        This traverses the core schema and referenced definitions, replaces `'definition-ref'` schemas\n        by the referenced definition if possible, and applies deferred discriminators.\n        \"\"\"\n        definitions = self._definitions\n        try:\n            gather_result = gather_schemas_for_cleaning(\n                schema,\n                definitions=definitions,\n            )\n        except MissingDefinitionError as e:\n            raise InvalidSchemaError from e\n\n        remaining_defs: dict[str, CoreSchema] = {}\n\n        # Note: this logic doesn't play well when core schemas with deferred discriminator metadata\n        # and references are encountered. See the `test_deferred_discriminated_union_and_references()` test.\n        for ref, inlinable_def_ref in gather_result['collected_references'].items():\n            if inlinable_def_ref is not None and (inlining_behavior := _inlining_behavior(inlinable_def_ref)) != 'keep':\n                if inlining_behavior == 'inline':\n                    # `ref` was encountered, and only once:\n                    #  - `inlinable_def_ref` is a `'definition-ref'` schema and is guaranteed to be\n                    #    the only one. Transform it into the definition it points to.\n                    #  - Do not store the definition in the `remaining_defs`.\n                    inlinable_def_ref.clear()  # pyright: ignore[reportAttributeAccessIssue]\n                    inlinable_def_ref.update(self._resolve_definition(ref, definitions))  # pyright: ignore\n                elif inlining_behavior == 'preserve_metadata':\n                    # `ref` was encountered, and only once, but contains discriminator metadata.\n                    # We will do the same thing as if `inlining_behavior` was `'inline'`, but make\n                    # sure to keep the metadata for the deferred discriminator application logic below.\n                    meta = inlinable_def_ref.pop('metadata')\n                    inlinable_def_ref.clear()  # pyright: ignore[reportAttributeAccessIssue]\n                    inlinable_def_ref.update(self._resolve_definition(ref, definitions))  # pyright: ignore\n                    inlinable_def_ref['metadata'] = meta\n            else:\n                # `ref` was encountered, at least two times (or only once, but with metadata or a serialization schema):\n                # - Do not inline the `'definition-ref'` schemas (they are not provided in the gather result anyway).\n                # - Store the the definition in the `remaining_defs`\n                remaining_defs[ref] = self._resolve_definition(ref, definitions)\n\n        for cs in gather_result['deferred_discriminator_schemas']:\n            discriminator: str | None = cs['metadata'].pop('pydantic_internal_union_discriminator', None)  # pyright: ignore[reportTypedDictNotRequiredAccess]\n            if discriminator is None:\n                # This can happen in rare scenarios, when a deferred schema is present multiple times in the\n                # gather result (e.g. when using the `Sequence` type -- see `test_sequence_discriminated_union()`).\n                # In this case, a previous loop iteration applied the discriminator and so we can just skip it here.\n                continue\n            applied = _discriminated_union.apply_discriminator(cs.copy(), discriminator, remaining_defs)\n            # Mutate the schema directly to have the discriminator applied\n            cs.clear()  # pyright: ignore[reportAttributeAccessIssue]\n            cs.update(applied)  # pyright: ignore\n\n        if remaining_defs:\n            schema = core_schema.definitions_schema(schema=schema, definitions=[*remaining_defs.values()])\n        return schema", "metadata": {"license": "MIT", "len_tokens": 752}}
{"id": "pydantic:pydantic/_internal/_schema_gather.py", "language": "python", "code": "class GatherContext:\n    \"\"\"The current context used during core schema traversing.\n\n    Context instances should only be used during schema traversing.\n    \"\"\"\n\n    definitions: dict[str, CoreSchema]\n    \"\"\"The available definitions.\"\"\"\n\n    deferred_discriminator_schemas: list[CoreSchema] = field(init=False, default_factory=list)\n    \"\"\"The list of core schemas having the discriminator application deferred.\n\n    Internally, these core schemas have a specific key set in the core metadata dict.\n    \"\"\"\n\n    collected_references: dict[str, DefinitionReferenceSchema | None] = field(init=False, default_factory=dict)\n    \"\"\"The collected definition references.\n\n    If a definition reference schema can be inlined, it means that there is\n    only one in the whole core schema. As such, it is stored as the value.\n    Otherwise, the value is set to `None`.\n\n    During schema traversing, definition reference schemas can be added as candidates, or removed\n    (by setting the value to `None`).\n    \"\"\"", "metadata": {"license": "MIT", "len_tokens": 207}}
{"id": "pydantic:pydantic/_internal/_schema_gather.py", "language": "python", "code": "def gather_schemas_for_cleaning(schema: CoreSchema, definitions: dict[str, CoreSchema]) -> GatherResult:\n    \"\"\"Traverse the core schema and definitions and return the necessary information for schema cleaning.\n\n    During the core schema traversing, any `'definition-ref'` schema is:\n\n    - Validated: the reference must point to an existing definition. If this is not the case, a\n      `MissingDefinitionError` exception is raised.\n    - Stored in the context: the actual reference is stored in the context. Depending on whether\n      the `'definition-ref'` schema is encountered more that once, the schema itself is also\n      saved in the context to be inlined (i.e. replaced by the definition it points to).\n    \"\"\"\n    context = GatherContext(definitions)\n    traverse_schema(schema, context)\n\n    return {\n        'collected_references': context.collected_references,\n        'deferred_discriminator_schemas': context.deferred_discriminator_schemas,\n    }", "metadata": {"license": "MIT", "len_tokens": 204}}
{"id": "pydantic:pydantic/_internal/_discriminated_union.py", "language": "python", "code": "def apply_discriminator(\n    schema: core_schema.CoreSchema,\n    discriminator: str | Discriminator,\n    definitions: dict[str, core_schema.CoreSchema] | None = None,\n) -> core_schema.CoreSchema:\n    \"\"\"Applies the discriminator and returns a new core schema.\n\n    Args:\n        schema: The input schema.\n        discriminator: The name of the field which will serve as the discriminator.\n        definitions: A mapping of schema ref to schema.\n\n    Returns:\n        The new core schema.\n\n    Raises:\n        TypeError:\n            - If `discriminator` is used with invalid union variant.\n            - If `discriminator` is used with `Union` type with one variant.\n            - If `discriminator` value mapped to multiple choices.\n        MissingDefinitionForUnionRef:\n            If the definition for ref is missing.\n        PydanticUserError:\n            - If a model in union doesn't have a discriminator field.\n            - If discriminator field has a non-string alias.\n            - If discriminator fields have different aliases.\n            - If discriminator field not of type `Literal`.\n    \"\"\"\n    from ..types import Discriminator\n\n    if isinstance(discriminator, Discriminator):\n        if isinstance(discriminator.discriminator, str):\n            discriminator = discriminator.discriminator\n        else:\n            return discriminator._convert_schema(schema)\n\n    return _ApplyInferredDiscriminator(discriminator, definitions or {}).apply(schema)", "metadata": {"license": "MIT", "len_tokens": 285}}
{"id": "pydantic:pydantic/_internal/_discriminated_union.py", "language": "python", "code": "def apply(self, schema: core_schema.CoreSchema) -> core_schema.CoreSchema:\n        \"\"\"Return a new CoreSchema based on `schema` that uses a tagged-union with the discriminator provided\n        to this class.\n\n        Args:\n            schema: The input schema.\n\n        Returns:\n            The new core schema.\n\n        Raises:\n            TypeError:\n                - If `discriminator` is used with invalid union variant.\n                - If `discriminator` is used with `Union` type with one variant.\n                - If `discriminator` value mapped to multiple choices.\n            ValueError:\n                If the definition for ref is missing.\n            PydanticUserError:\n                - If a model in union doesn't have a discriminator field.\n                - If discriminator field has a non-string alias.\n                - If discriminator fields have different aliases.\n                - If discriminator field not of type `Literal`.\n        \"\"\"\n        assert not self._used\n        schema = self._apply_to_root(schema)\n        if self._should_be_nullable and not self._is_nullable:\n            schema = core_schema.nullable_schema(schema)\n        self._used = True\n        return schema", "metadata": {"license": "MIT", "len_tokens": 233}}
{"id": "pydantic:pydantic/_internal/_discriminated_union.py", "language": "python", "code": "def _apply_to_root(self, schema: core_schema.CoreSchema) -> core_schema.CoreSchema:\n        \"\"\"This method handles the outer-most stage of recursion over the input schema:\n        unwrapping nullable or definitions schemas, and calling the `_handle_choice`\n        method iteratively on the choices extracted (recursively) from the possibly-wrapped union.\n        \"\"\"\n        if schema['type'] == 'nullable':\n            self._is_nullable = True\n            wrapped = self._apply_to_root(schema['schema'])\n            nullable_wrapper = schema.copy()\n            nullable_wrapper['schema'] = wrapped\n            return nullable_wrapper\n\n        if schema['type'] == 'definitions':\n            wrapped = self._apply_to_root(schema['schema'])\n            definitions_wrapper = schema.copy()\n            definitions_wrapper['schema'] = wrapped\n            return definitions_wrapper\n\n        if schema['type'] != 'union':\n            # If the schema is not a union, it probably means it just had a single member and\n            # was flattened by pydantic_core.\n            # However, it still may make sense to apply the discriminator to this schema,\n            # as a way to get discriminated-union-style error messages, so we allow this here.\n            schema = core_schema.union_schema([schema])\n\n        # Reverse the choices list before extending the stack so that they get handled in the order they occur\n        choices_schemas = [v[0] if isinstance(v, tuple) else v for v in schema['choices'][::-1]]\n        self._choices_to_handle.extend(choices_schemas)\n        while self._choices_to_handle:\n            choice = self._choices_to_handle.pop()\n            self._handle_choice(choice)\n\n        if self._discriminator_alias is not None and self._discriminator_alias != self.discriminator:\n            # * We need to annotate `discriminator` as a union here to handle both branches of this conditional\n            # * We need to annotate `discriminator` as list[list[str | int]] and not list[list[str]] due to the\n            #   invariance of list, and because list[list[str | int]] is the type of the discriminator argument\n            #   to tagged_union_schema below\n            # * See the docstring of pydantic_core.core_schema.tagged_union_schema for more details about how to\n            #   interpret the value of the discriminator argument to tagged_union_schema. (The list[list[str]] here\n            #   is the appropriate way to provide a list of fallback attributes to check for a discriminator value.)\n            discriminator: str | list[list[str | int]] = [[self.discriminator], [self._discriminator_alias]]\n        else:\n            discriminator = self.discriminator\n        return core_schema.tagged_union_schema(\n            choices=self._tagged_union_choices,\n            discriminator=discriminator,\n            custom_error_type=schema.get('custom_error_type'),\n            custom_error_message=schema.get('custom_error_message'),\n            custom_error_context=schema.get('custom_error_context'),\n            strict=False,\n            from_attributes=True,\n            ref=schema.get('ref'),\n            metadata=schema.get('metadata'),\n            serialization=schema.get('serialization'),\n        )", "metadata": {"license": "MIT", "len_tokens": 645}}
{"id": "pydantic:pydantic/_internal/_discriminated_union.py", "language": "python", "code": "def _handle_choice(self, choice: core_schema.CoreSchema) -> None:\n        \"\"\"This method handles the \"middle\" stage of recursion over the input schema.\n        Specifically, it is responsible for handling each choice of the outermost union\n        (and any \"coalesced\" choices obtained from inner unions).\n\n        Here, \"handling\" entails:\n        * Coalescing nested unions and compatible tagged-unions\n        * Tracking the presence of 'none' and 'nullable' schemas occurring as choices\n        * Validating that each allowed discriminator value maps to a unique choice\n        * Updating the _tagged_union_choices mapping that will ultimately be used to build the TaggedUnionSchema.\n        \"\"\"\n        if choice['type'] == 'definition-ref':\n            if choice['schema_ref'] not in self.definitions:\n                raise MissingDefinitionForUnionRef(choice['schema_ref'])\n\n        if choice['type'] == 'none':\n            self._should_be_nullable = True\n        elif choice['type'] == 'definitions':\n            self._handle_choice(choice['schema'])\n        elif choice['type'] == 'nullable':\n            self._should_be_nullable = True\n            self._handle_choice(choice['schema'])  # unwrap the nullable schema\n        elif choice['type'] == 'union':\n            # Reverse the choices list before extending the stack so that they get handled in the order they occur\n            choices_schemas = [v[0] if isinstance(v, tuple) else v for v in choice['choices'][::-1]]\n            self._choices_to_handle.extend(choices_schemas)\n        elif choice['type'] not in {\n            'model',\n            'typed-dict',\n            'tagged-union',\n            'lax-or-strict',\n            'dataclass',\n            'dataclass-args',\n            'definition-ref',\n        } and not _core_utils.is_function_with_inner_schema(choice):\n            # We should eventually handle 'definition-ref' as well\n            err_str = f'The core schema type {choice[\"type\"]!r} is not a valid discriminated union variant.'\n            if choice['type'] == 'list':\n                err_str += (\n                    ' If you are making use of a list of union types, make sure the discriminator is applied to the '\n                    'union type and not the list (e.g. `list[Annotated[<T> | <U>, Field(discriminator=...)]]`).'\n                )\n            raise TypeError(err_str)\n        else:\n            if choice['type'] == 'tagged-union' and self._is_discriminator_shared(choice):\n                # In this case, this inner tagged-union is compatible with the outer tagged-union,\n                # and its choices can be coalesced into the outer TaggedUnionSchema.\n                subchoices = [x for x in choice['choices'].values() if not isinstance(x, (str, int))]\n                # Reverse the choices list before extending the stack so that they get handled in the order they occur\n                self._choices_to_handle.extend(subchoices[::-1])\n                return\n\n            inferred_discriminator_values = self._infer_discriminator_values_for_choice(choice, source_name=None)\n            self._set_unique_choice_for_values(choice, inferred_discriminator_values)", "metadata": {"license": "MIT", "len_tokens": 664}}
{"id": "pydantic:pydantic/_internal/_discriminated_union.py", "language": "python", "code": "def _infer_discriminator_values_for_choice(  # noqa C901\n        self, choice: core_schema.CoreSchema, source_name: str | None\n    ) -> list[str | int]:\n        \"\"\"This function recurses over `choice`, extracting all discriminator values that should map to this choice.\n\n        `model_name` is accepted for the purpose of producing useful error messages.\n        \"\"\"\n        if choice['type'] == 'definitions':\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=source_name)\n\n        elif _core_utils.is_function_with_inner_schema(choice):\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=source_name)\n\n        elif choice['type'] == 'lax-or-strict':\n            return sorted(\n                set(\n                    self._infer_discriminator_values_for_choice(choice['lax_schema'], source_name=None)\n                    + self._infer_discriminator_values_for_choice(choice['strict_schema'], source_name=None)\n                )\n            )\n\n        elif choice['type'] == 'tagged-union':\n            values: list[str | int] = []\n            # Ignore str/int \"choices\" since these are just references to other choices\n            subchoices = [x for x in choice['choices'].values() if not isinstance(x, (str, int))]\n            for subchoice in subchoices:\n                subchoice_values = self._infer_discriminator_values_for_choice(subchoice, source_name=None)\n                values.extend(subchoice_values)\n            return values\n\n        elif choice['type'] == 'union':\n            values = []\n            for subchoice in choice['choices']:\n                subchoice_schema = subchoice[0] if isinstance(subchoice, tuple) else subchoice\n                subchoice_values = self._infer_discriminator_values_for_choice(subchoice_schema, source_name=None)\n                values.extend(subchoice_values)\n            return values\n\n        elif choice['type'] == 'nullable':\n            self._should_be_nullable = True\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=None)\n\n        elif choice['type'] == 'model':\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=choice['cls'].__name__)\n\n        elif choice['type'] == 'dataclass':\n            return self._infer_discriminator_values_for_choice(choice['schema'], source_name=choice['cls'].__name__)\n\n        elif choice['type'] == 'model-fields':\n            return self._infer_discriminator_values_for_model_choice(choice, source_name=source_name)\n\n        elif choice['type'] == 'dataclass-args':\n            return self._infer_discriminator_values_for_dataclass_choice(choice, source_name=source_name)\n\n        elif choice['type'] == 'typed-dict':\n            return self._infer_discriminator_values_for_typed_dict_choice(choice, source_name=source_name)\n\n        elif choice['type'] == 'definition-ref':\n            schema_ref = choice['schema_ref']\n            if schema_ref not in self.definitions:\n                raise MissingDefinitionForUnionRef(schema_ref)\n            return self._infer_discriminator_values_for_choice(self.definitions[schema_ref], source_name=source_name)\n        else:\n            err_str = f'The core schema type {choice[\"type\"]!r} is not a valid discriminated union variant.'\n            if choice['type'] == 'list':\n                err_str += (\n                    ' If you are making use of a list of union types, make sure the discriminator is applied to the '\n                    'union type and not the list (e.g. `list[Annotated[<T> | <U>, Field(discriminator=...)]]`).'\n                )\n            raise TypeError(err_str)", "metadata": {"license": "MIT", "len_tokens": 758}}
{"id": "pydantic:pydantic/_internal/_discriminated_union.py", "language": "python", "code": "def _infer_discriminator_values_for_field(self, field: CoreSchemaField, source: str) -> list[str | int]:\n        if field['type'] == 'computed-field':\n            # This should never occur as a discriminator, as it is only relevant to serialization\n            return []\n        alias = field.get('validation_alias', self.discriminator)\n        if not isinstance(alias, str):\n            raise PydanticUserError(\n                f'Alias {alias!r} is not supported in a discriminated union', code='discriminator-alias-type'\n            )\n        if self._discriminator_alias is None:\n            self._discriminator_alias = alias\n        elif self._discriminator_alias != alias:\n            raise PydanticUserError(\n                f'Aliases for discriminator {self.discriminator!r} must be the same '\n                f'(got {alias}, {self._discriminator_alias})',\n                code='discriminator-alias',\n            )\n        return self._infer_discriminator_values_for_inner_schema(field['schema'], source)", "metadata": {"license": "MIT", "len_tokens": 213}}
{"id": "pydantic:pydantic/_internal/_discriminated_union.py", "language": "python", "code": "def _infer_discriminator_values_for_inner_schema(\n        self, schema: core_schema.CoreSchema, source: str\n    ) -> list[str | int]:\n        \"\"\"When inferring discriminator values for a field, we typically extract the expected values from a literal\n        schema. This function does that, but also handles nested unions and defaults.\n        \"\"\"\n        if schema['type'] == 'literal':\n            return schema['expected']\n\n        elif schema['type'] == 'union':\n            # Generally when multiple values are allowed they should be placed in a single `Literal`, but\n            # we add this case to handle the situation where a field is annotated as a `Union` of `Literal`s.\n            # For example, this lets us handle `Union[Literal['key'], Union[Literal['Key'], Literal['KEY']]]`\n            values: list[Any] = []\n            for choice in schema['choices']:\n                choice_schema = choice[0] if isinstance(choice, tuple) else choice\n                choice_values = self._infer_discriminator_values_for_inner_schema(choice_schema, source)\n                values.extend(choice_values)\n            return values\n\n        elif schema['type'] == 'default':\n            # This will happen if the field has a default value; we ignore it while extracting the discriminator values\n            return self._infer_discriminator_values_for_inner_schema(schema['schema'], source)\n\n        elif schema['type'] == 'function-after':\n            # After validators don't affect the discriminator values\n            return self._infer_discriminator_values_for_inner_schema(schema['schema'], source)\n\n        elif schema['type'] in {'function-before', 'function-wrap', 'function-plain'}:\n            validator_type = repr(schema['type'].split('-')[1])\n            raise PydanticUserError(\n                f'Cannot use a mode={validator_type} validator in the'\n                f' discriminator field {self.discriminator!r} of {source}',\n                code='discriminator-validator',\n            )\n\n        else:\n            raise PydanticUserError(\n                f'{source} needs field {self.discriminator!r} to be of type `Literal`',\n                code='discriminator-needs-literal',\n            )", "metadata": {"license": "MIT", "len_tokens": 446}}
{"id": "pydantic:pydantic/_internal/_discriminated_union.py", "language": "python", "code": "def _set_unique_choice_for_values(self, choice: core_schema.CoreSchema, values: Sequence[str | int]) -> None:\n        \"\"\"This method updates `self.tagged_union_choices` so that all provided (discriminator) `values` map to the\n        provided `choice`, validating that none of these values already map to another (different) choice.\n        \"\"\"\n        for discriminator_value in values:\n            if discriminator_value in self._tagged_union_choices:\n                # It is okay if `value` is already in tagged_union_choices as long as it maps to the same value.\n                # Because tagged_union_choices may map values to other values, we need to walk the choices dict\n                # until we get to a \"real\" choice, and confirm that is equal to the one assigned.\n                existing_choice = self._tagged_union_choices[discriminator_value]\n                if existing_choice != choice:\n                    raise TypeError(\n                        f'Value {discriminator_value!r} for discriminator '\n                        f'{self.discriminator!r} mapped to multiple choices'\n                    )\n            else:\n                self._tagged_union_choices[discriminator_value] = choice", "metadata": {"license": "MIT", "len_tokens": 236}}
{"id": "pydantic:pydantic/_internal/_decorators.py", "language": "python", "code": "class PydanticDescriptorProxy(Generic[ReturnType]):\n    \"\"\"Wrap a classmethod, staticmethod, property or unbound function\n    and act as a descriptor that allows us to detect decorated items\n    from the class' attributes.\n\n    This class' __get__ returns the wrapped item's __get__ result,\n    which makes it transparent for classmethods and staticmethods.\n\n    Attributes:\n        wrapped: The decorator that has to be wrapped.\n        decorator_info: The decorator info.\n        shim: A wrapper function to wrap V1 style function.\n    \"\"\"\n\n    wrapped: DecoratedType[ReturnType]\n    decorator_info: DecoratorInfo\n    shim: Callable[[Callable[..., Any]], Callable[..., Any]] | None = None\n\n    def __post_init__(self):\n        for attr in 'setter', 'deleter':\n            if hasattr(self.wrapped, attr):\n                f = partial(self._call_wrapped_attr, name=attr)\n                setattr(self, attr, f)\n\n    def _call_wrapped_attr(self, func: Callable[[Any], None], *, name: str) -> PydanticDescriptorProxy[ReturnType]:\n        self.wrapped = getattr(self.wrapped, name)(func)\n        if isinstance(self.wrapped, property):\n            # update ComputedFieldInfo.wrapped_property\n            from ..fields import ComputedFieldInfo\n\n            if isinstance(self.decorator_info, ComputedFieldInfo):\n                self.decorator_info.wrapped_property = self.wrapped\n        return self\n\n    def __get__(self, obj: object | None, obj_type: type[object] | None = None) -> PydanticDescriptorProxy[ReturnType]:\n        try:\n            return self.wrapped.__get__(obj, obj_type)  # pyright: ignore[reportReturnType]\n        except AttributeError:\n            # not a descriptor, e.g. a partial object\n            return self.wrapped  # type: ignore[return-value]\n\n    def __set_name__(self, instance: Any, name: str) -> None:\n        if hasattr(self.wrapped, '__set_name__'):\n            self.wrapped.__set_name__(instance, name)  # pyright: ignore[reportFunctionMemberAccess]\n\n    def __getattr__(self, name: str, /) -> Any:\n        \"\"\"Forward checks for __isabstractmethod__ and such.\"\"\"\n        return getattr(self.wrapped, name)", "metadata": {"license": "MIT", "len_tokens": 494}}
{"id": "pydantic:pydantic/_internal/_decorators.py", "language": "python", "code": "class Decorator(Generic[DecoratorInfoType]):\n    \"\"\"A generic container class to join together the decorator metadata\n    (metadata from decorator itself, which we have when the\n    decorator is called but not when we are building the core-schema)\n    and the bound function (which we have after the class itself is created).\n\n    Attributes:\n        cls_ref: The class ref.\n        cls_var_name: The decorated function name.\n        func: The decorated function.\n        shim: A wrapper function to wrap V1 style function.\n        info: The decorator info.\n    \"\"\"\n\n    cls_ref: str\n    cls_var_name: str\n    func: Callable[..., Any]\n    shim: Callable[[Any], Any] | None\n    info: DecoratorInfoType\n\n    @staticmethod\n    def build(\n        cls_: Any,\n        *,\n        cls_var_name: str,\n        shim: Callable[[Any], Any] | None,\n        info: DecoratorInfoType,\n    ) -> Decorator[DecoratorInfoType]:\n        \"\"\"Build a new decorator.\n\n        Args:\n            cls_: The class.\n            cls_var_name: The decorated function name.\n            shim: A wrapper function to wrap V1 style function.\n            info: The decorator info.\n\n        Returns:\n            The new decorator instance.\n        \"\"\"\n        func = get_attribute_from_bases(cls_, cls_var_name)\n        if shim is not None:\n            func = shim(func)\n        func = unwrap_wrapped_function(func, unwrap_partial=False)\n        if not callable(func):\n            # TODO most likely this branch can be removed when we drop support for Python 3.12:\n            # This branch will get hit for classmethod properties\n            attribute = get_attribute_from_base_dicts(cls_, cls_var_name)  # prevents the binding call to `__get__`\n            if isinstance(attribute, PydanticDescriptorProxy):\n                func = unwrap_wrapped_function(attribute.wrapped)\n        return Decorator(\n            cls_ref=get_type_ref(cls_),\n            cls_var_name=cls_var_name,\n            func=func,\n            shim=shim,\n            info=info,\n        )\n\n    def bind_to_cls(self, cls: Any) -> Decorator[DecoratorInfoType]:\n        \"\"\"Bind the decorator to a class.\n\n        Args:\n            cls: the class.\n\n        Returns:\n            The new decorator instance.\n        \"\"\"\n        return self.build(\n            cls,\n            cls_var_name=self.cls_var_name,\n            shim=self.shim,\n            info=copy(self.info),\n        )", "metadata": {"license": "MIT", "len_tokens": 512}}
{"id": "pydantic:pydantic/_internal/_decorators.py", "language": "python", "code": "def mro_for_bases(bases: tuple[type[Any], ...]) -> tuple[type[Any], ...]:\n    def merge_seqs(seqs: list[deque[type[Any]]]) -> Iterable[type[Any]]:\n        while True:\n            non_empty = [seq for seq in seqs if seq]\n            if not non_empty:\n                # Nothing left to process, we're done.\n                return\n            candidate: type[Any] | None = None\n            for seq in non_empty:  # Find merge candidates among seq heads.\n                candidate = seq[0]\n                not_head = [s for s in non_empty if candidate in islice(s, 1, None)]\n                if not_head:\n                    # Reject the candidate.\n                    candidate = None\n                else:\n                    break\n            if not candidate:\n                raise TypeError('Inconsistent hierarchy, no C3 MRO is possible')\n            yield candidate\n            for seq in non_empty:\n                # Remove candidate.\n                if seq[0] == candidate:\n                    seq.popleft()\n\n    seqs = [deque(mro(base)) for base in bases] + [deque(bases)]\n    return tuple(merge_seqs(seqs))", "metadata": {"license": "MIT", "len_tokens": 241}}
{"id": "pydantic:pydantic/_internal/_decorators.py", "language": "python", "code": "def get_attribute_from_bases(tp: type[Any] | tuple[type[Any], ...], name: str) -> Any:\n    \"\"\"Get the attribute from the next class in the MRO that has it,\n    aiming to simulate calling the method on the actual class.\n\n    The reason for iterating over the mro instead of just getting\n    the attribute (which would do that for us) is to support TypedDict,\n    which lacks a real __mro__, but can have a virtual one constructed\n    from its bases (as done here).\n\n    Args:\n        tp: The type or class to search for the attribute. If a tuple, this is treated as a set of base classes.\n        name: The name of the attribute to retrieve.\n\n    Returns:\n        Any: The attribute value, if found.\n\n    Raises:\n        AttributeError: If the attribute is not found in any class in the MRO.\n    \"\"\"\n    if isinstance(tp, tuple):\n        for base in mro_for_bases(tp):\n            attribute = base.__dict__.get(name, _sentinel)\n            if attribute is not _sentinel:\n                attribute_get = getattr(attribute, '__get__', None)\n                if attribute_get is not None:\n                    return attribute_get(None, tp)\n                return attribute\n        raise AttributeError(f'{name} not found in {tp}')\n    else:\n        try:\n            return getattr(tp, name)\n        except AttributeError:\n            return get_attribute_from_bases(mro(tp), name)", "metadata": {"license": "MIT", "len_tokens": 308}}
{"id": "pydantic:pydantic/_internal/_decorators.py", "language": "python", "code": "def _decorator_infos_for_class(\n    typ: type[Any],\n    *,\n    collect_to_replace: bool,\n) -> tuple[DecoratorInfos, list[tuple[str, Any]]]:\n    \"\"\"Collect a `DecoratorInfos` for class, without looking into bases.\"\"\"\n    res = DecoratorInfos()\n    to_replace: list[tuple[str, Any]] = []\n\n    for var_name, var_value in vars(typ).items():\n        if isinstance(var_value, PydanticDescriptorProxy):\n            info = var_value.decorator_info\n            if isinstance(info, ValidatorDecoratorInfo):\n                res.validators[var_name] = Decorator.build(typ, cls_var_name=var_name, shim=var_value.shim, info=info)\n            elif isinstance(info, FieldValidatorDecoratorInfo):\n                res.field_validators[var_name] = Decorator.build(\n                    typ, cls_var_name=var_name, shim=var_value.shim, info=info\n                )\n            elif isinstance(info, RootValidatorDecoratorInfo):\n                res.root_validators[var_name] = Decorator.build(\n                    typ, cls_var_name=var_name, shim=var_value.shim, info=info\n                )\n            elif isinstance(info, FieldSerializerDecoratorInfo):\n                res.field_serializers[var_name] = Decorator.build(\n                    typ, cls_var_name=var_name, shim=var_value.shim, info=info\n                )\n            elif isinstance(info, ModelValidatorDecoratorInfo):\n                res.model_validators[var_name] = Decorator.build(\n                    typ, cls_var_name=var_name, shim=var_value.shim, info=info\n                )\n            elif isinstance(info, ModelSerializerDecoratorInfo):\n                res.model_serializers[var_name] = Decorator.build(\n                    typ, cls_var_name=var_name, shim=var_value.shim, info=info\n                )\n            else:\n                from ..fields import ComputedFieldInfo\n\n                isinstance(var_value, ComputedFieldInfo)\n                res.computed_fields[var_name] = Decorator.build(typ, cls_var_name=var_name, shim=None, info=info)\n            if collect_to_replace:\n                to_replace.append((var_name, var_value.wrapped))\n\n    return res, to_replace", "metadata": {"license": "MIT", "len_tokens": 456}}
{"id": "pydantic:pydantic/_internal/_decorators.py", "language": "python", "code": "def inspect_validator(\n    validator: Callable[..., Any], *, mode: FieldValidatorModes, type: Literal['field', 'model']\n) -> bool:\n    \"\"\"Look at a field or model validator function and determine whether it takes an info argument.\n\n    An error is raised if the function has an invalid signature.\n\n    Args:\n        validator: The validator function to inspect.\n        mode: The proposed validator mode.\n        type: The type of validator, either 'field' or 'model'.\n\n    Returns:\n        Whether the validator takes an info argument.\n    \"\"\"\n    try:\n        sig = _signature_no_eval(validator)\n    except (ValueError, TypeError):\n        # `inspect.signature` might not be able to infer a signature, e.g. with C objects.\n        # In this case, we assume no info argument is present:\n        return False\n    n_positional = count_positional_required_params(sig)\n    if mode == 'wrap':\n        if n_positional == 3:\n            return True\n        elif n_positional == 2:\n            return False\n    else:\n        assert mode in {'before', 'after', 'plain'}, f\"invalid mode: {mode!r}, expected 'before', 'after' or 'plain\"\n        if n_positional == 2:\n            return True\n        elif n_positional == 1:\n            return False\n\n    raise PydanticUserError(\n        f'Unrecognized {type} validator function signature for {validator} with `mode={mode}`: {sig}',\n        code='validator-signature',\n    )", "metadata": {"license": "MIT", "len_tokens": 325}}
{"id": "pydantic:pydantic/_internal/_decorators.py", "language": "python", "code": "def inspect_field_serializer(serializer: Callable[..., Any], mode: Literal['plain', 'wrap']) -> tuple[bool, bool]:\n    \"\"\"Look at a field serializer function and determine if it is a field serializer,\n    and whether it takes an info argument.\n\n    An error is raised if the function has an invalid signature.\n\n    Args:\n        serializer: The serializer function to inspect.\n        mode: The serializer mode, either 'plain' or 'wrap'.\n\n    Returns:\n        Tuple of (is_field_serializer, info_arg).\n    \"\"\"\n    try:\n        sig = _signature_no_eval(serializer)\n    except (ValueError, TypeError):\n        # `inspect.signature` might not be able to infer a signature, e.g. with C objects.\n        # In this case, we assume no info argument is present and this is not a method:\n        return (False, False)\n\n    first = next(iter(sig.parameters.values()), None)\n    is_field_serializer = first is not None and first.name == 'self'\n\n    n_positional = count_positional_required_params(sig)\n    if is_field_serializer:\n        # -1 to correct for self parameter\n        info_arg = _serializer_info_arg(mode, n_positional - 1)\n    else:\n        info_arg = _serializer_info_arg(mode, n_positional)\n\n    if info_arg is None:\n        raise PydanticUserError(\n            f'Unrecognized field_serializer function signature for {serializer} with `mode={mode}`:{sig}',\n            code='field-serializer-signature',\n        )\n\n    return is_field_serializer, info_arg", "metadata": {"license": "MIT", "len_tokens": 323}}
{"id": "pydantic:pydantic/_internal/_decorators.py", "language": "python", "code": "def inspect_annotated_serializer(serializer: Callable[..., Any], mode: Literal['plain', 'wrap']) -> bool:\n    \"\"\"Look at a serializer function used via `Annotated` and determine whether it takes an info argument.\n\n    An error is raised if the function has an invalid signature.\n\n    Args:\n        serializer: The serializer function to check.\n        mode: The serializer mode, either 'plain' or 'wrap'.\n\n    Returns:\n        info_arg\n    \"\"\"\n    try:\n        sig = _signature_no_eval(serializer)\n    except (ValueError, TypeError):\n        # `inspect.signature` might not be able to infer a signature, e.g. with C objects.\n        # In this case, we assume no info argument is present:\n        return False\n    info_arg = _serializer_info_arg(mode, count_positional_required_params(sig))\n    if info_arg is None:\n        raise PydanticUserError(\n            f'Unrecognized field_serializer function signature for {serializer} with `mode={mode}`:{sig}',\n            code='field-serializer-signature',\n        )\n    else:\n        return info_arg", "metadata": {"license": "MIT", "len_tokens": 227}}
{"id": "pydantic:pydantic/_internal/_decorators.py", "language": "python", "code": "def inspect_model_serializer(serializer: Callable[..., Any], mode: Literal['plain', 'wrap']) -> bool:\n    \"\"\"Look at a model serializer function and determine whether it takes an info argument.\n\n    An error is raised if the function has an invalid signature.\n\n    Args:\n        serializer: The serializer function to check.\n        mode: The serializer mode, either 'plain' or 'wrap'.\n\n    Returns:\n        `info_arg` - whether the function expects an info argument.\n    \"\"\"\n    if isinstance(serializer, (staticmethod, classmethod)) or not is_instance_method_from_sig(serializer):\n        raise PydanticUserError(\n            '`@model_serializer` must be applied to instance methods', code='model-serializer-instance-method'\n        )\n\n    sig = _signature_no_eval(serializer)\n    info_arg = _serializer_info_arg(mode, count_positional_required_params(sig))\n    if info_arg is None:\n        raise PydanticUserError(\n            f'Unrecognized model_serializer function signature for {serializer} with `mode={mode}`:{sig}',\n            code='model-serializer-signature',\n        )\n    else:\n        return info_arg", "metadata": {"license": "MIT", "len_tokens": 230}}
{"id": "pydantic:pydantic/_internal/_decorators.py", "language": "python", "code": "def unwrap_wrapped_function(\n    func: Any,\n    *,\n    unwrap_partial: bool = True,\n    unwrap_class_static_method: bool = True,\n) -> Any:\n    \"\"\"Recursively unwraps a wrapped function until the underlying function is reached.\n    This handles property, functools.partial, functools.partialmethod, staticmethod, and classmethod.\n\n    Args:\n        func: The function to unwrap.\n        unwrap_partial: If True (default), unwrap partial and partialmethod decorators.\n        unwrap_class_static_method: If True (default), also unwrap classmethod and staticmethod\n            decorators. If False, only unwrap partial and partialmethod decorators.\n\n    Returns:\n        The underlying function of the wrapped function.\n    \"\"\"\n    # Define the types we want to check against as a single tuple.\n    unwrap_types = (\n        (property, cached_property)\n        + ((partial, partialmethod) if unwrap_partial else ())\n        + ((staticmethod, classmethod) if unwrap_class_static_method else ())\n    )\n\n    while isinstance(func, unwrap_types):\n        if unwrap_class_static_method and isinstance(func, (classmethod, staticmethod)):\n            func = func.__func__\n        elif isinstance(func, (partial, partialmethod)):\n            func = func.func\n        elif isinstance(func, property):\n            func = func.fget  # arbitrary choice, convenient for computed fields\n        else:\n            # Make coverage happy as it can only get here in the last possible case\n            assert isinstance(func, cached_property)\n            func = func.func  # type: ignore\n\n    return func", "metadata": {"license": "MIT", "len_tokens": 314}}
{"id": "pydantic:pydantic/_internal/_decorators.py", "language": "python", "code": "def get_callable_return_type(\n    callable_obj: Any,\n    globalns: GlobalsNamespace | None = None,\n    localns: MappingNamespace | None = None,\n) -> Any | PydanticUndefinedType:\n    \"\"\"Get the callable return type.\n\n    Args:\n        callable_obj: The callable to analyze.\n        globalns: The globals namespace to use during type annotation evaluation.\n        localns: The locals namespace to use during type annotation evaluation.\n\n    Returns:\n        The function return type.\n    \"\"\"\n    if isinstance(callable_obj, type):\n        # types are callables, and we assume the return type\n        # is the type itself (e.g. `int()` results in an instance of `int`).\n        return callable_obj\n\n    if not isinstance(callable_obj, _function_like):\n        call_func = getattr(type(callable_obj), '__call__', None)  # noqa: B004\n        if call_func is not None:\n            callable_obj = call_func\n\n    hints = get_function_type_hints(\n        unwrap_wrapped_function(callable_obj),\n        include_keys={'return'},\n        globalns=globalns,\n        localns=localns,\n    )\n    return hints.get('return', PydanticUndefined)", "metadata": {"license": "MIT", "len_tokens": 255}}
{"id": "pydantic:pydantic/_internal/_decorators.py", "language": "python", "code": "def build(\n        cls_: Any,\n        *,\n        cls_var_name: str,\n        shim: Callable[[Any], Any] | None,\n        info: DecoratorInfoType,\n    ) -> Decorator[DecoratorInfoType]:\n        \"\"\"Build a new decorator.\n\n        Args:\n            cls_: The class.\n            cls_var_name: The decorated function name.\n            shim: A wrapper function to wrap V1 style function.\n            info: The decorator info.\n\n        Returns:\n            The new decorator instance.\n        \"\"\"\n        func = get_attribute_from_bases(cls_, cls_var_name)\n        if shim is not None:\n            func = shim(func)\n        func = unwrap_wrapped_function(func, unwrap_partial=False)\n        if not callable(func):\n            # TODO most likely this branch can be removed when we drop support for Python 3.12:\n            # This branch will get hit for classmethod properties\n            attribute = get_attribute_from_base_dicts(cls_, cls_var_name)  # prevents the binding call to `__get__`\n            if isinstance(attribute, PydanticDescriptorProxy):\n                func = unwrap_wrapped_function(attribute.wrapped)\n        return Decorator(\n            cls_ref=get_type_ref(cls_),\n            cls_var_name=cls_var_name,\n            func=func,\n            shim=shim,\n            info=info,\n        )", "metadata": {"license": "MIT", "len_tokens": 272}}
{"id": "pydantic:pydantic/_internal/_decorators.py", "language": "python", "code": "def build(\n        cls,\n        typ: type[Any],\n        # Default to `True` for backwards compatibility:\n        replace_wrapped_methods: bool = True,\n    ) -> Self:\n        \"\"\"Build a `DecoratorInfos` instance for the given model, dataclass or `TypedDict` type.\n\n        Decorators from parent classes are included, including \"bare\" classes (e.g. if `typ`\n        is a Pydantic model, non Pydantic parent model classes are also taken into account).\n        The collection of the decorators happens by respecting the MRO.\n\n        If one of the bases has an `__pydantic_decorators__` attribute set, it is assumed to be\n        a `DecoratorInfos` instance and is used as-is. The `__pydantic_decorators__` attribute\n        is *not* being set on the provided `typ`.\n\n        Args:\n            typ: The model, dataclass or `TypedDict` type to use when building the `DecoratorInfos` instance.\n            replace_wrapped_methods: Whether to replace the decorator's wrapped methods on `typ`.\n                This is useful e.g. for field validators which are initially class methods. This should\n                only be set to `True` if `typ` is a Pydantic model or dataclass (otherwise this results\n                in mutations of classes Pydantic doesn't \"own\").\n        \"\"\"\n        # reminder: dicts are ordered and replacement does not alter the order\n        res = cls()\n        # Iterate over the bases, without the actual `typ`.\n        # `1:-1` because we don't need to include `object`/`TypedDict`:\n        for base in reversed(mro(typ)[1:-1]):\n            existing: DecoratorInfos | None = base.__dict__.get('__pydantic_decorators__')\n            if existing is None:\n                existing, _ = _decorator_infos_for_class(base, collect_to_replace=False)\n            res.validators.update({k: v.bind_to_cls(typ) for k, v in existing.validators.items()})\n            res.field_validators.update({k: v.bind_to_cls(typ) for k, v in existing.field_validators.items()})\n            res.root_validators.update({k: v.bind_to_cls(typ) for k, v in existing.root_validators.items()})\n            res.field_serializers.update({k: v.bind_to_cls(typ) for k, v in existing.field_serializers.items()})\n            res.model_serializers.update({k: v.bind_to_cls(typ) for k, v in existing.model_serializers.items()})\n            res.model_validators.update({k: v.bind_to_cls(typ) for k, v in existing.model_validators.items()})\n            res.computed_fields.update({k: v.bind_to_cls(typ) for k, v in existing.computed_fields.items()})\n\n        decorator_infos, to_replace = _decorator_infos_for_class(typ, collect_to_replace=True)\n\n        res.validators.update(decorator_infos.validators)\n        res.field_validators.update(decorator_infos.field_validators)\n        res.root_validators.update(decorator_infos.root_validators)\n        res.field_serializers.update(decorator_infos.field_serializers)\n        res.model_serializers.update(decorator_infos.model_serializers)\n        res.model_validators.update(decorator_infos.model_validators)\n        res.computed_fields.update(decorator_infos.computed_fields)\n\n        if replace_wrapped_methods and to_replace:\n            for name, value in to_replace:\n                setattr(typ, name, value)\n\n        res._validate()\n        return res", "metadata": {"license": "MIT", "len_tokens": 741}}
{"id": "pydantic:pydantic/_internal/_core_metadata.py", "language": "python", "code": "class CoreMetadata(TypedDict, total=False):\n    \"\"\"A `TypedDict` for holding the metadata dict of the schema.\n\n    Attributes:\n        pydantic_js_functions: List of JSON schema functions that resolve refs during application.\n        pydantic_js_annotation_functions: List of JSON schema functions that don't resolve refs during application.\n        pydantic_js_prefer_positional_arguments: Whether JSON schema generator will\n            prefer positional over keyword arguments for an 'arguments' schema.\n            custom validation function. Only applies to before, plain, and wrap validators.\n        pydantic_js_updates: key / value pair updates to apply to the JSON schema for a type.\n        pydantic_js_extra: WIP, either key/value pair updates to apply to the JSON schema, or a custom callable.\n        pydantic_internal_union_tag_key: Used internally by the `Tag` metadata to specify the tag used for a discriminated union.\n        pydantic_internal_union_discriminator: Used internally to specify the discriminator value for a discriminated union\n            when the discriminator was applied to a `'definition-ref'` schema, and that reference was missing at the time\n            of the annotation application.\n\n    TODO: Perhaps we should move this structure to pydantic-core. At the moment, though,\n    it's easier to iterate on if we leave it in pydantic until we feel there is a semi-stable API.\n\n    TODO: It's unfortunate how functionally oriented JSON schema generation is, especially that which occurs during\n    the core schema generation process. It's inevitable that we need to store some json schema related information\n    on core schemas, given that we generate JSON schemas directly from core schemas. That being said, debugging related\n    issues is quite difficult when JSON schema information is disguised via dynamically defined functions.\n    \"\"\"\n\n    pydantic_js_functions: list[GetJsonSchemaFunction]\n    pydantic_js_annotation_functions: list[GetJsonSchemaFunction]\n    pydantic_js_prefer_positional_arguments: bool\n    pydantic_js_updates: JsonDict\n    pydantic_js_extra: JsonDict | JsonSchemaExtraCallable\n    pydantic_internal_union_tag_key: str\n    pydantic_internal_union_discriminator: str", "metadata": {"license": "MIT", "len_tokens": 457}}
{"id": "pydantic:pydantic/_internal/_core_metadata.py", "language": "python", "code": "def update_core_metadata(\n    core_metadata: Any,\n    /,\n    *,\n    pydantic_js_functions: list[GetJsonSchemaFunction] | None = None,\n    pydantic_js_annotation_functions: list[GetJsonSchemaFunction] | None = None,\n    pydantic_js_updates: JsonDict | None = None,\n    pydantic_js_extra: JsonDict | JsonSchemaExtraCallable | None = None,\n) -> None:\n    from ..json_schema import PydanticJsonSchemaWarning\n\n    \"\"\"Update CoreMetadata instance in place. When we make modifications in this function, they\n    take effect on the `core_metadata` reference passed in as the first (and only) positional argument.\n\n    First, cast to `CoreMetadata`, then finish with a cast to `dict[str, Any]` for core schema compatibility.\n    We do this here, instead of before / after each call to this function so that this typing hack\n    can be easily removed if/when we move `CoreMetadata` to `pydantic-core`.\n\n    For parameter descriptions, see `CoreMetadata` above.\n    \"\"\"\n    core_metadata = cast(CoreMetadata, core_metadata)\n\n    if pydantic_js_functions:\n        core_metadata.setdefault('pydantic_js_functions', []).extend(pydantic_js_functions)\n\n    if pydantic_js_annotation_functions:\n        core_metadata.setdefault('pydantic_js_annotation_functions', []).extend(pydantic_js_annotation_functions)\n\n    if pydantic_js_updates:\n        if (existing_updates := core_metadata.get('pydantic_js_updates')) is not None:\n            core_metadata['pydantic_js_updates'] = {**existing_updates, **pydantic_js_updates}\n        else:\n            core_metadata['pydantic_js_updates'] = pydantic_js_updates\n\n    if pydantic_js_extra is not None:\n        existing_pydantic_js_extra = core_metadata.get('pydantic_js_extra')\n        if existing_pydantic_js_extra is None:\n            core_metadata['pydantic_js_extra'] = pydantic_js_extra\n        if isinstance(existing_pydantic_js_extra, dict):\n            if isinstance(pydantic_js_extra, dict):\n                core_metadata['pydantic_js_extra'] = {**existing_pydantic_js_extra, **pydantic_js_extra}\n            if callable(pydantic_js_extra):\n                warn(\n                    'Composing `dict` and `callable` type `json_schema_extra` is not supported.'\n                    'The `callable` type is being ignored.'\n                    \"If you'd like support for this behavior, please open an issue on pydantic.\",\n                    PydanticJsonSchemaWarning,\n                )\n        if callable(existing_pydantic_js_extra):\n            # if ever there's a case of a callable, we'll just keep the last json schema extra spec\n            core_metadata['pydantic_js_extra'] = pydantic_js_extra", "metadata": {"license": "MIT", "len_tokens": 598}}
{"id": "pydantic:pydantic/_internal/_fields.py", "language": "python", "code": "def _check_protected_namespaces(\n    protected_namespaces: tuple[str | Pattern[str], ...],\n    ann_name: str,\n    bases: tuple[type[Any], ...],\n    cls_name: str,\n) -> None:\n    BaseModel = import_cached_base_model()\n\n    for protected_namespace in protected_namespaces:\n        ns_violation = False\n        if isinstance(protected_namespace, Pattern):\n            ns_violation = protected_namespace.match(ann_name) is not None\n        elif isinstance(protected_namespace, str):\n            ns_violation = ann_name.startswith(protected_namespace)\n\n        if ns_violation:\n            for b in bases:\n                if hasattr(b, ann_name):\n                    if not (issubclass(b, BaseModel) and ann_name in getattr(b, '__pydantic_fields__', {})):\n                        raise ValueError(\n                            f'Field {ann_name!r} conflicts with member {getattr(b, ann_name)}'\n                            f' of protected namespace {protected_namespace!r}.'\n                        )\n            else:\n                valid_namespaces: list[str] = []\n                for pn in protected_namespaces:\n                    if isinstance(pn, Pattern):\n                        if not pn.match(ann_name):\n                            valid_namespaces.append(f're.compile({pn.pattern!r})')\n                    else:\n                        if not ann_name.startswith(pn):\n                            valid_namespaces.append(f\"'{pn}'\")\n\n                valid_namespaces_str = f'({\", \".join(valid_namespaces)}{\",)\" if len(valid_namespaces) == 1 else \")\"}'\n\n                warnings.warn(\n                    f'Field {ann_name!r} in {cls_name!r} conflicts with protected namespace {protected_namespace!r}.\\n\\n'\n                    f\"You may be able to solve this by setting the 'protected_namespaces' configuration to {valid_namespaces_str}.\",\n                    UserWarning,\n                    stacklevel=5,\n                )", "metadata": {"license": "MIT", "len_tokens": 388}}
{"id": "pydantic:pydantic/_internal/_fields.py", "language": "python", "code": "def _apply_alias_generator_to_field_info(\n    alias_generator: Callable[[str], str] | AliasGenerator, field_name: str, field_info: FieldInfo\n):\n    \"\"\"Apply an alias generator to aliases on a `FieldInfo` instance if appropriate.\n\n    Args:\n        alias_generator: A callable that takes a string and returns a string, or an `AliasGenerator` instance.\n        field_name: The name of the field from which to generate the alias.\n        field_info: The `FieldInfo` instance to which the alias generator is (maybe) applied.\n    \"\"\"\n    # Apply an alias_generator if\n    # 1. An alias is not specified\n    # 2. An alias is specified, but the priority is <= 1\n    if (\n        field_info.alias_priority is None\n        or field_info.alias_priority <= 1\n        or field_info.alias is None\n        or field_info.validation_alias is None\n        or field_info.serialization_alias is None\n    ):\n        alias, validation_alias, serialization_alias = None, None, None\n\n        if isinstance(alias_generator, AliasGenerator):\n            alias, validation_alias, serialization_alias = alias_generator.generate_aliases(field_name)\n        elif callable(alias_generator):\n            alias = alias_generator(field_name)\n            if not isinstance(alias, str):\n                raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n\n        # if priority is not set, we set to 1\n        # which supports the case where the alias_generator from a child class is used\n        # to generate an alias for a field in a parent class\n        if field_info.alias_priority is None or field_info.alias_priority <= 1:\n            field_info.alias_priority = 1\n\n        # if the priority is 1, then we set the aliases to the generated alias\n        if field_info.alias_priority == 1:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)\n            field_info.alias = alias\n\n        # if any of the aliases are not set, then we set them to the corresponding generated alias\n        if field_info.alias is None:\n            field_info.alias = alias\n        if field_info.serialization_alias is None:\n            field_info.serialization_alias = get_first_not_none(serialization_alias, alias)\n        if field_info.validation_alias is None:\n            field_info.validation_alias = get_first_not_none(validation_alias, alias)", "metadata": {"license": "MIT", "len_tokens": 513}}
{"id": "pydantic:pydantic/_internal/_fields.py", "language": "python", "code": "def rebuild_model_fields(\n    cls: type[BaseModel],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> tuple[dict[str, FieldInfo], PydanticExtraInfo | None]:\n    \"\"\"Rebuild the (already present) model fields by trying to reevaluate annotations.\n\n    This function should be called whenever a model with incomplete fields is encountered.\n\n    Returns:\n        A two-tuple, the first element being the rebuilt fields, the second element being\n        the rebuild `PydanticExtraInfo` instance, if available.\n\n    Raises:\n        NameError: If one of the annotations failed to evaluate.\n\n    Note:\n        This function *doesn't* mutate the model fields in place, as it can be called during\n        schema generation, where you don't want to mutate other model's fields.\n    \"\"\"\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                new_field = _recreate_field_info(\n                    field_info, ns_resolver=ns_resolver, typevars_map=typevars_map, lenient=False\n                )\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n\n        if cls.__pydantic_extra_info__ is not None and not cls.__pydantic_extra_info__.complete:\n            rebuilt_extra_info = PydanticExtraInfo(\n                annotation=_typing_extra.eval_type(\n                    cls.__pydantic_extra_info__.annotation, *ns_resolver.types_namespace\n                ),\n                complete=True,\n            )\n        else:\n            rebuilt_extra_info = cls.__pydantic_extra_info__\n\n    return rebuilt_fields, rebuilt_extra_info", "metadata": {"license": "MIT", "len_tokens": 398}}
{"id": "pydantic:pydantic/_internal/_fields.py", "language": "python", "code": "def _recreate_field_info(\n    field_info: FieldInfo,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n    *,\n    lenient: bool,\n) -> FieldInfo:\n    FieldInfo_ = import_cached_field_info()\n\n    existing_desc = field_info.description\n    if lenient:\n        ann = _generics.replace_types(field_info._original_annotation, typevars_map)\n        ann, evaluated = _typing_extra.try_eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n    else:\n        # Not the best pattern, maybe we could ship our own `eval_type()`,\n        # that would replace the type variables on the fly during evaluation.\n        ann = _typing_extra.eval_type(\n            field_info._original_annotation,\n            *ns_resolver.types_namespace,\n        )\n        ann = _generics.replace_types(ann, typevars_map)\n        ann = _typing_extra.eval_type(\n            ann,\n            *ns_resolver.types_namespace,\n        )\n        evaluated = True\n\n    if (assign := field_info._original_assignment) is PydanticUndefined:\n        new_field = FieldInfo_.from_annotation(ann, _source=AnnotationSource.CLASS)\n    else:\n        new_field = FieldInfo_.from_annotated_attribute(ann, assign, _source=AnnotationSource.CLASS)\n        new_field._original_assignment = assign\n    new_field._original_annotation = ann\n    # The description might come from the docstring if `use_attribute_docstrings` was `True`:\n    new_field.description = new_field.description if new_field.description is not None else existing_desc\n    if not evaluated:\n        new_field._complete = False\n\n    return new_field", "metadata": {"license": "MIT", "len_tokens": 355}}
{"id": "pydantic:pydantic/_internal/_fields.py", "language": "python", "code": "def rebuild_dataclass_fields(\n    cls: type[PydanticDataclass],\n    *,\n    config_wrapper: ConfigWrapper,\n    ns_resolver: NsResolver,\n    typevars_map: Mapping[TypeVar, Any],\n) -> dict[str, FieldInfo]:\n    \"\"\"Rebuild the (already present) dataclass fields by trying to reevaluate annotations.\n\n    This function should be called whenever a dataclass with incomplete fields is encountered.\n\n    Raises:\n        NameError: If one of the annotations failed to evaluate.\n\n    Note:\n        This function *doesn't* mutate the dataclass fields in place, as it can be called during\n        schema generation, where you don't want to mutate other dataclass's fields.\n    \"\"\"\n    FieldInfo_ = import_cached_field_info()\n\n    rebuilt_fields: dict[str, FieldInfo] = {}\n    with ns_resolver.push(cls):\n        for f_name, field_info in cls.__pydantic_fields__.items():\n            if field_info._complete:\n                rebuilt_fields[f_name] = field_info\n            else:\n                existing_desc = field_info.description\n                ann = _typing_extra.eval_type(\n                    field_info._original_annotation,\n                    *ns_resolver.types_namespace,\n                )\n                ann = _generics.replace_types(ann, typevars_map)\n                new_field = FieldInfo_.from_annotated_attribute(\n                    ann,\n                    field_info._original_assignment,\n                    _source=AnnotationSource.DATACLASS,\n                )\n\n                # The description might come from the docstring if `use_attribute_docstrings` was `True`:\n                new_field.description = new_field.description if new_field.description is not None else existing_desc\n                update_field_from_config(config_wrapper, f_name, new_field)\n                rebuilt_fields[f_name] = new_field\n\n    return rebuilt_fields", "metadata": {"license": "MIT", "len_tokens": 368}}
{"id": "pydantic:pydantic/_internal/_config.py", "language": "python", "code": "def check_deprecated(config_dict: ConfigDict) -> None:\n    \"\"\"Check for deprecated config keys and warn the user.\n\n    Args:\n        config_dict: The input config.\n    \"\"\"\n    deprecated_removed_keys = V2_REMOVED_KEYS & config_dict.keys()\n    deprecated_renamed_keys = V2_RENAMED_KEYS.keys() & config_dict.keys()\n    if deprecated_removed_keys or deprecated_renamed_keys:\n        renamings = {k: V2_RENAMED_KEYS[k] for k in sorted(deprecated_renamed_keys)}\n        renamed_bullets = [f'* {k!r} has been renamed to {v!r}' for k, v in renamings.items()]\n        removed_bullets = [f'* {k!r} has been removed' for k in sorted(deprecated_removed_keys)]\n        message = '\\n'.join(['Valid config keys have changed in V2:'] + renamed_bullets + removed_bullets)\n        warnings.warn(message, UserWarning)", "metadata": {"license": "MIT", "len_tokens": 209}}
{"id": "pydantic:pydantic/_internal/_config.py", "language": "python", "code": "def for_model(\n        cls,\n        bases: tuple[type[Any], ...],\n        namespace: dict[str, Any],\n        raw_annotations: dict[str, Any],\n        kwargs: dict[str, Any],\n    ) -> Self:\n        \"\"\"Build a new `ConfigWrapper` instance for a `BaseModel`.\n\n        The config wrapper built based on (in descending order of priority):\n        - options from `kwargs`\n        - options from the `namespace`\n        - options from the base classes (`bases`)\n\n        Args:\n            bases: A tuple of base classes.\n            namespace: The namespace of the class being created.\n            raw_annotations: The (non-evaluated) annotations of the model.\n            kwargs: The kwargs passed to the class being created.\n\n        Returns:\n            A `ConfigWrapper` instance for `BaseModel`.\n        \"\"\"\n        config_new = ConfigDict()\n        for base in bases:\n            config = getattr(base, 'model_config', None)\n            if config:\n                config_new.update(config.copy())\n\n        config_class_from_namespace = namespace.get('Config')\n        config_dict_from_namespace = namespace.get('model_config')\n\n        if raw_annotations.get('model_config') and config_dict_from_namespace is None:\n            raise PydanticUserError(\n                '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',\n                code='model-config-invalid-field-name',\n            )\n\n        if config_class_from_namespace and config_dict_from_namespace:\n            raise PydanticUserError('\"Config\" and \"model_config\" cannot be used together', code='config-both')\n\n        config_from_namespace = config_dict_from_namespace or prepare_config(config_class_from_namespace)\n\n        config_new.update(config_from_namespace)\n\n        for k in list(kwargs.keys()):\n            if k in config_keys:\n                config_new[k] = kwargs.pop(k)\n\n        return cls(config_new)", "metadata": {"license": "MIT", "len_tokens": 382}}
{"id": "pydantic:pydantic/_internal/_config.py", "language": "python", "code": "def core_config(self, title: str | None) -> core_schema.CoreConfig:\n        \"\"\"Create a pydantic-core config.\n\n        We don't use getattr here since we don't want to populate with defaults.\n\n        Args:\n            title: The title to use if not set in config.\n\n        Returns:\n            A `CoreConfig` object created from config.\n        \"\"\"\n        config = self.config_dict\n\n        if config.get('schema_generator') is not None:\n            warnings.warn(\n                'The `schema_generator` setting has been deprecated since v2.10. This setting no longer has any effect.',\n                PydanticDeprecatedSince210,\n                stacklevel=2,\n            )\n\n        if (populate_by_name := config.get('populate_by_name')) is not None:\n            # We include this patch for backwards compatibility purposes, but this config setting will be deprecated in v3.0, and likely removed in v4.0.\n            # Thus, the above warning and this patch can be removed then as well.\n            if config.get('validate_by_name') is None:\n                config['validate_by_alias'] = True\n                config['validate_by_name'] = populate_by_name\n\n        # We dynamically patch validate_by_name to be True if validate_by_alias is set to False\n        # and validate_by_name is not explicitly set.\n        if config.get('validate_by_alias') is False and config.get('validate_by_name') is None:\n            config['validate_by_name'] = True\n\n        if (not config.get('validate_by_alias', True)) and (not config.get('validate_by_name', False)):\n            raise PydanticUserError(\n                'At least one of `validate_by_alias` or `validate_by_name` must be set to True.',\n                code='validate-by-alias-and-name-false',\n            )\n\n        return core_schema.CoreConfig(\n            **{  # pyright: ignore[reportArgumentType]\n                k: v\n                for k, v in (\n                    ('title', config.get('title') or title or None),\n                    ('extra_fields_behavior', config.get('extra')),\n                    ('allow_inf_nan', config.get('allow_inf_nan')),\n                    ('str_strip_whitespace', config.get('str_strip_whitespace')),\n                    ('str_to_lower', config.get('str_to_lower')),\n                    ('str_to_upper', config.get('str_to_upper')),\n                    ('strict', config.get('strict')),\n                    ('ser_json_timedelta', config.get('ser_json_timedelta')),\n                    ('ser_json_temporal', config.get('ser_json_temporal')),\n                    ('val_temporal_unit', config.get('val_temporal_unit')),\n                    ('ser_json_bytes', config.get('ser_json_bytes')),\n                    ('val_json_bytes', config.get('val_json_bytes')),\n                    ('ser_json_inf_nan', config.get('ser_json_inf_nan')),\n                    ('from_attributes', config.get('from_attributes')),\n                    ('loc_by_alias', config.get('loc_by_alias')),\n                    ('revalidate_instances', config.get('revalidate_instances')),\n                    ('validate_default', config.get('validate_default')),\n                    ('str_max_length', config.get('str_max_length')),\n                    ('str_min_length', config.get('str_min_length')),\n                    ('hide_input_in_errors', config.get('hide_input_in_errors')),\n                    ('coerce_numbers_to_str', config.get('coerce_numbers_to_str')),\n                    ('regex_engine', config.get('regex_engine')),\n                    ('validation_error_cause', config.get('validation_error_cause')),\n                    ('cache_strings', config.get('cache_strings')),\n                    ('validate_by_alias', config.get('validate_by_alias')),\n                    ('validate_by_name', config.get('validate_by_name')),\n                    ('serialize_by_alias', config.get('serialize_by_alias')),\n                    ('url_preserve_empty_path', config.get('url_preserve_empty_path')),\n                )\n                if v is not None\n            }\n        )", "metadata": {"license": "MIT", "len_tokens": 792}}
{"id": "pydantic:pydantic/_internal/_mock_val_ser.py", "language": "python", "code": "class MockCoreSchema(Mapping[str, Any]):\n    \"\"\"Mocker for `pydantic_core.CoreSchema` which optionally attempts to\n    rebuild the thing it's mocking when one of its methods is accessed and raises an error if that fails.\n    \"\"\"\n\n    __slots__ = '_error_message', '_code', '_attempt_rebuild', '_built_memo'\n\n    def __init__(\n        self,\n        error_message: str,\n        *,\n        code: PydanticErrorCodes,\n        attempt_rebuild: Callable[[], CoreSchema | None] | None = None,\n    ) -> None:\n        self._error_message = error_message\n        self._code: PydanticErrorCodes = code\n        self._attempt_rebuild = attempt_rebuild\n        self._built_memo: CoreSchema | None = None\n\n    def __getitem__(self, key: str) -> Any:\n        return self._get_built().__getitem__(key)\n\n    def __len__(self) -> int:\n        return self._get_built().__len__()\n\n    def __iter__(self) -> Iterator[str]:\n        return self._get_built().__iter__()\n\n    def _get_built(self) -> CoreSchema:\n        if self._built_memo is not None:\n            return self._built_memo\n\n        if self._attempt_rebuild:\n            schema = self._attempt_rebuild()\n            if schema is not None:\n                self._built_memo = schema\n                return schema\n        raise PydanticUserError(self._error_message, code=self._code)\n\n    def rebuild(self) -> CoreSchema | None:\n        self._built_memo = None\n        if self._attempt_rebuild:\n            schema = self._attempt_rebuild()\n            if schema is not None:\n                return schema\n            else:\n                raise PydanticUserError(self._error_message, code=self._code)\n        return None", "metadata": {"license": "MIT", "len_tokens": 391}}
{"id": "pydantic:pydantic/_internal/_mock_val_ser.py", "language": "python", "code": "class MockValSer(Generic[ValSer]):\n    \"\"\"Mocker for `pydantic_core.SchemaValidator` or `pydantic_core.SchemaSerializer` which optionally attempts to\n    rebuild the thing it's mocking when one of its methods is accessed and raises an error if that fails.\n    \"\"\"\n\n    __slots__ = '_error_message', '_code', '_val_or_ser', '_attempt_rebuild'\n\n    def __init__(\n        self,\n        error_message: str,\n        *,\n        code: PydanticErrorCodes,\n        val_or_ser: Literal['validator', 'serializer'],\n        attempt_rebuild: Callable[[], ValSer | None] | None = None,\n    ) -> None:\n        self._error_message = error_message\n        self._val_or_ser = SchemaValidator if val_or_ser == 'validator' else SchemaSerializer\n        self._code: PydanticErrorCodes = code\n        self._attempt_rebuild = attempt_rebuild\n\n    def __getattr__(self, item: str) -> None:\n        __tracebackhide__ = True\n        if self._attempt_rebuild:\n            val_ser = self._attempt_rebuild()\n            if val_ser is not None:\n                return getattr(val_ser, item)\n\n        # raise an AttributeError if `item` doesn't exist\n        getattr(self._val_or_ser, item)\n        raise PydanticUserError(self._error_message, code=self._code)\n\n    def rebuild(self) -> ValSer | None:\n        if self._attempt_rebuild:\n            val_ser = self._attempt_rebuild()\n            if val_ser is not None:\n                return val_ser\n            else:\n                raise PydanticUserError(self._error_message, code=self._code)\n        return None", "metadata": {"license": "MIT", "len_tokens": 358}}
{"id": "pydantic:pydantic/_internal/_mock_val_ser.py", "language": "python", "code": "def set_type_adapter_mocks(adapter: TypeAdapter) -> None:\n    \"\"\"Set `core_schema`, `validator` and `serializer` to mock core types on a type adapter instance.\n\n    Args:\n        adapter: The type adapter instance to set the mocks on\n    \"\"\"\n    type_repr = str(adapter._type)\n    undefined_type_error_message = (\n        f'`TypeAdapter[{type_repr}]` is not fully defined; you should define `{type_repr}` and all referenced types,'\n        f' then call `.rebuild()` on the instance.'\n    )\n\n    def attempt_rebuild_fn(attr_fn: Callable[[TypeAdapter], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if adapter.rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(adapter)\n            return None\n\n        return handler\n\n    adapter.core_schema = MockCoreSchema(  # pyright: ignore[reportAttributeAccessIssue]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        attempt_rebuild=attempt_rebuild_fn(lambda ta: ta.core_schema),\n    )\n    adapter.validator = MockValSer(  # pyright: ignore[reportAttributeAccessIssue]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='validator',\n        attempt_rebuild=attempt_rebuild_fn(lambda ta: ta.validator),\n    )\n    adapter.serializer = MockValSer(  # pyright: ignore[reportAttributeAccessIssue]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='serializer',\n        attempt_rebuild=attempt_rebuild_fn(lambda ta: ta.serializer),\n    )", "metadata": {"license": "MIT", "len_tokens": 356}}
{"id": "pydantic:pydantic/_internal/_mock_val_ser.py", "language": "python", "code": "def set_model_mocks(cls: type[BaseModel], undefined_name: str = 'all referenced types') -> None:\n    \"\"\"Set `__pydantic_core_schema__`, `__pydantic_validator__` and `__pydantic_serializer__` to mock core types on a model.\n\n    Args:\n        cls: The model class to set the mocks on\n        undefined_name: Name of the undefined thing, used in error messages\n    \"\"\"\n    undefined_type_error_message = (\n        f'`{cls.__name__}` is not fully defined; you should define {undefined_name},'\n        f' then call `{cls.__name__}.model_rebuild()`.'\n    )\n\n    def attempt_rebuild_fn(attr_fn: Callable[[type[BaseModel]], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if cls.model_rebuild(raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None\n\n        return handler\n\n    cls.__pydantic_core_schema__ = MockCoreSchema(  # pyright: ignore[reportAttributeAccessIssue]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_core_schema__),\n    )\n    cls.__pydantic_validator__ = MockValSer(  # pyright: ignore[reportAttributeAccessIssue]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='validator',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_validator__),\n    )\n    cls.__pydantic_serializer__ = MockValSer(  # pyright: ignore[reportAttributeAccessIssue]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='serializer',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_serializer__),\n    )", "metadata": {"license": "MIT", "len_tokens": 415}}
{"id": "pydantic:pydantic/_internal/_mock_val_ser.py", "language": "python", "code": "def set_dataclass_mocks(cls: type[PydanticDataclass], undefined_name: str = 'all referenced types') -> None:\n    \"\"\"Set `__pydantic_validator__` and `__pydantic_serializer__` to `MockValSer`s on a dataclass.\n\n    Args:\n        cls: The model class to set the mocks on\n        undefined_name: Name of the undefined thing, used in error messages\n    \"\"\"\n    from ..dataclasses import rebuild_dataclass\n\n    undefined_type_error_message = (\n        f'`{cls.__name__}` is not fully defined; you should define {undefined_name},'\n        f' then call `pydantic.dataclasses.rebuild_dataclass({cls.__name__})`.'\n    )\n\n    def attempt_rebuild_fn(attr_fn: Callable[[type[PydanticDataclass]], T]) -> Callable[[], T | None]:\n        def handler() -> T | None:\n            if rebuild_dataclass(cls, raise_errors=False, _parent_namespace_depth=5) is not False:\n                return attr_fn(cls)\n            return None\n\n        return handler\n\n    cls.__pydantic_core_schema__ = MockCoreSchema(  # pyright: ignore[reportAttributeAccessIssue]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_core_schema__),\n    )\n    cls.__pydantic_validator__ = MockValSer(  # pyright: ignore[reportAttributeAccessIssue]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='validator',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_validator__),\n    )\n    cls.__pydantic_serializer__ = MockValSer(  # pyright: ignore[reportAttributeAccessIssue]\n        undefined_type_error_message,\n        code='class-not-fully-defined',\n        val_or_ser='serializer',\n        attempt_rebuild=attempt_rebuild_fn(lambda c: c.__pydantic_serializer__),\n    )", "metadata": {"license": "MIT", "len_tokens": 431}}
{"id": "pydantic:pydantic/_internal/_generics.py", "language": "python", "code": "def create_generic_submodel(\n    model_name: str, origin: type[BaseModel], args: tuple[Any, ...], params: tuple[Any, ...]\n) -> type[BaseModel]:\n    \"\"\"Dynamically create a submodel of a provided (generic) BaseModel.\n\n    This is used when producing concrete parametrizations of generic models. This function\n    only *creates* the new subclass; the schema/validators/serialization must be updated to\n    reflect a concrete parametrization elsewhere.\n\n    Args:\n        model_name: The name of the newly created model.\n        origin: The base class for the new model to inherit from.\n        args: A tuple of generic metadata arguments.\n        params: A tuple of generic metadata parameters.\n\n    Returns:\n        The created submodel.\n    \"\"\"\n    namespace: dict[str, Any] = {'__module__': origin.__module__}\n    bases = (origin,)\n    meta, ns, kwds = prepare_class(model_name, bases)\n    namespace.update(ns)\n    created_model = meta(\n        model_name,\n        bases,\n        namespace,\n        __pydantic_generic_metadata__={\n            'origin': origin,\n            'args': args,\n            'parameters': params,\n        },\n        __pydantic_reset_parent_namespace__=False,\n        **kwds,\n    )\n\n    model_module, called_globally = _get_caller_frame_info(depth=3)\n    if called_globally:  # create global reference and therefore allow pickling\n        object_by_reference = None\n        reference_name = model_name\n        reference_module_globals = sys.modules[created_model.__module__].__dict__\n        while object_by_reference is not created_model:\n            object_by_reference = reference_module_globals.setdefault(reference_name, created_model)\n            reference_name += '_'\n\n    return created_model", "metadata": {"license": "MIT", "len_tokens": 374}}
{"id": "pydantic:pydantic/_internal/_generics.py", "language": "python", "code": "def get_model_typevars_map(cls: type[BaseModel]) -> dict[TypeVar, Any]:\n    \"\"\"Package a generic BaseModel's typevars and concrete parametrization (if present) into a dictionary compatible\n    with the `replace_types` function.\n\n    Since BaseModel.__class_getitem__ does not produce a typing._GenericAlias, and the BaseModel generic info is\n    stored in the __pydantic_generic_metadata__ attribute, we need special handling here.\n    \"\"\"\n    # TODO: This could be unified with `get_standard_typevars_map` if we stored the generic metadata\n    #   in the __origin__, __args__, and __parameters__ attributes of the model.\n    generic_metadata = cls.__pydantic_generic_metadata__\n    origin = generic_metadata['origin']\n    args = generic_metadata['args']\n    if not args:\n        # No need to go into `iter_contained_typevars`:\n        return {}\n    return dict(zip(iter_contained_typevars(origin), args))", "metadata": {"license": "MIT", "len_tokens": 207}}
{"id": "pydantic:pydantic/_internal/_generics.py", "language": "python", "code": "def map_generic_model_arguments(cls: type[BaseModel], args: tuple[Any, ...]) -> dict[TypeVar, Any]:\n    \"\"\"Return a mapping between the parameters of a generic model and the provided arguments during parameterization.\n\n    Raises:\n        TypeError: If the number of arguments does not match the parameters (i.e. if providing too few or too many arguments).\n\n    Example:\n        ```python {test=\"skip\" lint=\"skip\"}\n        class Model[T, U, V = int](BaseModel): ...\n\n        map_generic_model_arguments(Model, (str, bytes))\n        #> {T: str, U: bytes, V: int}\n\n        map_generic_model_arguments(Model, (str,))\n        #> TypeError: Too few arguments for <class '__main__.Model'>; actual 1, expected at least 2\n\n        map_generic_model_arguments(Model, (str, bytes, int, complex))\n        #> TypeError: Too many arguments for <class '__main__.Model'>; actual 4, expected 3\n        ```\n\n    Note:\n        This function is analogous to the private `typing._check_generic_specialization` function.\n    \"\"\"\n    parameters = cls.__pydantic_generic_metadata__['parameters']\n    expected_len = len(parameters)\n    typevars_map: dict[TypeVar, Any] = {}\n\n    _missing = object()\n    for parameter, argument in zip_longest(parameters, args, fillvalue=_missing):\n        if parameter is _missing:\n            raise TypeError(f'Too many arguments for {cls}; actual {len(args)}, expected {expected_len}')\n\n        if argument is _missing:\n            param = cast(TypeVar, parameter)\n            try:\n                has_default = param.has_default()  # pyright: ignore[reportAttributeAccessIssue]\n            except AttributeError:\n                # Happens if using `typing.TypeVar` (and not `typing_extensions`) on Python < 3.13.\n                has_default = False\n            if has_default:\n                # The default might refer to other type parameters. For an example, see:\n                # https://typing.python.org/en/latest/spec/generics.html#type-parameters-as-parameters-to-generics\n                typevars_map[param] = replace_types(param.__default__, typevars_map)  # pyright: ignore[reportAttributeAccessIssue]\n            else:\n                expected_len -= sum(hasattr(p, 'has_default') and p.has_default() for p in parameters)  # pyright: ignore[reportAttributeAccessIssue]\n                raise TypeError(f'Too few arguments for {cls}; actual {len(args)}, expected at least {expected_len}')\n        else:\n            param = cast(TypeVar, parameter)\n            typevars_map[param] = argument\n\n    return typevars_map", "metadata": {"license": "MIT", "len_tokens": 570}}
{"id": "pydantic:pydantic/_internal/_generics.py", "language": "python", "code": "def generic_recursion_self_type(\n    origin: type[BaseModel], args: tuple[Any, ...]\n) -> Iterator[PydanticRecursiveRef | None]:\n    \"\"\"This contextmanager should be placed around the recursive calls used to build a generic type,\n    and accept as arguments the generic origin type and the type arguments being passed to it.\n\n    If the same origin and arguments are observed twice, it implies that a self-reference placeholder\n    can be used while building the core schema, and will produce a schema_ref that will be valid in the\n    final parent schema.\n    \"\"\"\n    previously_seen_type_refs = _generic_recursion_cache.get()\n    if previously_seen_type_refs is None:\n        previously_seen_type_refs = set()\n        token = _generic_recursion_cache.set(previously_seen_type_refs)\n    else:\n        token = None\n\n    try:\n        type_ref = get_type_ref(origin, args_override=args)\n        if type_ref in previously_seen_type_refs:\n            self_type = PydanticRecursiveRef(type_ref=type_ref)\n            yield self_type\n        else:\n            previously_seen_type_refs.add(type_ref)\n            yield\n            previously_seen_type_refs.remove(type_ref)\n    finally:\n        if token:\n            _generic_recursion_cache.reset(token)", "metadata": {"license": "MIT", "len_tokens": 258}}
{"id": "pydantic:pydantic/_internal/_generics.py", "language": "python", "code": "def get_cached_generic_type_early(parent: type[BaseModel], typevar_values: Any) -> type[BaseModel] | None:\n    \"\"\"The use of a two-stage cache lookup approach was necessary to have the highest performance possible for\n    repeated calls to `__class_getitem__` on generic types (which may happen in tighter loops during runtime),\n    while still ensuring that certain alternative parametrizations ultimately resolve to the same type.\n\n    As a concrete example, this approach was necessary to make Model[List[T]][int] equal to Model[List[int]].\n    The approach could be modified to not use two different cache keys at different points, but the\n    _early_cache_key is optimized to be as quick to compute as possible (for repeated-access speed), and the\n    _late_cache_key is optimized to be as \"correct\" as possible, so that two types that will ultimately be the\n    same after resolving the type arguments will always produce cache hits.\n\n    If we wanted to move to only using a single cache key per type, we would either need to always use the\n    slower/more computationally intensive logic associated with _late_cache_key, or would need to accept\n    that Model[List[T]][int] is a different type than Model[List[T]][int]. Because we rely on subclass relationships\n    during validation, I think it is worthwhile to ensure that types that are functionally equivalent are actually\n    equal.\n    \"\"\"\n    return _GENERIC_TYPES_CACHE.get(_early_cache_key(parent, typevar_values))", "metadata": {"license": "MIT", "len_tokens": 314}}
{"id": "pydantic:pydantic/_internal/_generics.py", "language": "python", "code": "def _union_orderings_key(typevar_values: Any) -> Any:\n    \"\"\"This is intended to help differentiate between Union types with the same arguments in different order.\n\n    Thanks to caching internal to the `typing` module, it is not possible to distinguish between\n    List[Union[int, float]] and List[Union[float, int]] (and similarly for other \"parent\" origins besides List)\n    because `typing` considers Union[int, float] to be equal to Union[float, int].\n\n    However, you _can_ distinguish between (top-level) Union[int, float] vs. Union[float, int].\n    Because we parse items as the first Union type that is successful, we get slightly more consistent behavior\n    if we make an effort to distinguish the ordering of items in a union. It would be best if we could _always_\n    get the exact-correct order of items in the union, but that would require a change to the `typing` module itself.\n    (See https://github.com/python/cpython/issues/86483 for reference.)\n    \"\"\"\n    if isinstance(typevar_values, tuple):\n        return tuple(_union_orderings_key(value) for value in typevar_values)\n    elif typing_objects.is_union(typing_extensions.get_origin(typevar_values)):\n        return get_args(typevar_values)\n    else:\n        return ()", "metadata": {"license": "MIT", "len_tokens": 278}}
{"id": "pydantic:pydantic/_internal/_generics.py", "language": "python", "code": "def _late_cache_key(origin: type[BaseModel], args: tuple[Any, ...], typevar_values: Any) -> GenericTypesCacheKey:\n    \"\"\"This is intended for use later in the process of creating a new type, when we have more information\n    about the exact args that will be passed. If it turns out that a different set of inputs to\n    __class_getitem__ resulted in the same inputs to the generic type creation process, we can still\n    return the cached type, and update the cache with the _early_cache_key as well.\n    \"\"\"\n    # The _union_orderings_key is placed at the start here to ensure there cannot be a collision with an\n    # _early_cache_key, as that function will always produce a BaseModel subclass as the first item in the key,\n    # whereas this function will always produce a tuple as the first item in the key.\n    return _union_orderings_key(typevar_values), origin, args", "metadata": {"license": "MIT", "len_tokens": 200}}
{"id": "pydantic:pydantic/_internal/_dataclasses.py", "language": "python", "code": "def as_dataclass_field(pydantic_field: FieldInfo) -> dataclasses.Field[Any]:\n    field_args: dict[str, Any] = {'default': pydantic_field}\n\n    # Needed because if `doc` is set, the dataclass slots will be a dict (field name -> doc) instead of a tuple:\n    if sys.version_info >= (3, 14) and pydantic_field.description is not None:\n        field_args['doc'] = pydantic_field.description\n\n    # Needed as the stdlib dataclass module processes kw_only in a specific way during class construction:\n    if sys.version_info >= (3, 10) and pydantic_field.kw_only:\n        field_args['kw_only'] = True\n\n    # Needed as the stdlib dataclass modules generates `__repr__()` during class construction:\n    if pydantic_field.repr is not True:\n        field_args['repr'] = pydantic_field.repr\n\n    return dataclasses.field(**field_args)", "metadata": {"license": "MIT", "len_tokens": 208}}
{"id": "pydantic:pydantic/_internal/_dataclasses.py", "language": "python", "code": "class PydanticDataclass(StandardDataclass, Protocol):\n        \"\"\"A protocol containing attributes only available once a class has been decorated as a Pydantic dataclass.\n\n        Attributes:\n            __pydantic_config__: Pydantic-specific configuration settings for the dataclass.\n            __pydantic_complete__: Whether dataclass building is completed, or if there are still undefined fields.\n            __pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n            __pydantic_decorators__: Metadata containing the decorators defined on the dataclass.\n            __pydantic_fields__: Metadata about the fields defined on the dataclass.\n            __pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the dataclass.\n            __pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the dataclass.\n        \"\"\"\n\n        __pydantic_config__: ClassVar[ConfigDict]\n        __pydantic_complete__: ClassVar[bool]\n        __pydantic_core_schema__: ClassVar[core_schema.CoreSchema]\n        __pydantic_decorators__: ClassVar[_decorators.DecoratorInfos]\n        __pydantic_fields__: ClassVar[dict[str, FieldInfo]]\n        __pydantic_serializer__: ClassVar[SchemaSerializer]\n        __pydantic_validator__: ClassVar[SchemaValidator | PluggableSchemaValidator]\n\n        @classmethod\n        def __pydantic_fields_complete__(cls) -> bool: ...", "metadata": {"license": "MIT", "len_tokens": 312}}
{"id": "pydantic:pydantic/_internal/_utils.py", "language": "python", "code": "def smart_deepcopy(obj: Obj) -> Obj:\n    \"\"\"Return type as is for immutable built-in types\n    Use obj.copy() for built-in empty collections\n    Use copy.deepcopy() for non-empty collections and unknown objects.\n    \"\"\"\n    if obj is MISSING:\n        return obj  # pyright: ignore[reportReturnType]\n    obj_type = obj.__class__\n    if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n        return obj  # fastest case: obj is immutable and not collection therefore will not be copied anyway\n    try:\n        if not obj and obj_type in BUILTIN_COLLECTIONS:\n            # faster way for empty collections, no need to copy its members\n            return obj if obj_type is tuple else obj.copy()  # tuple doesn't have copy method  # type: ignore\n    except (TypeError, ValueError, RuntimeError):\n        # do we really dare to catch ALL errors? Seems a bit risky\n        pass\n\n    return deepcopy(obj)", "metadata": {"license": "MIT", "len_tokens": 205}}
{"id": "pydantic:pydantic/_internal/_utils.py", "language": "python", "code": "class SafeGetItemProxy:\n    \"\"\"Wrapper redirecting `__getitem__` to `get` with a sentinel value as default\n\n    This makes is safe to use in `operator.itemgetter` when some keys may be missing\n    \"\"\"\n\n    # Define __slots__manually for performances\n    # @dataclasses.dataclass() only support slots=True in python>=3.10\n    __slots__ = ('wrapped',)\n\n    wrapped: Mapping[str, Any]\n\n    def __getitem__(self, key: str, /) -> Any:\n        return self.wrapped.get(key, _SENTINEL)\n\n    # required to pass the object to operator.itemgetter() instances due to a quirk of typeshed\n    # https://github.com/python/mypy/issues/13713\n    # https://github.com/python/typeshed/pull/8785\n    # Since this is typing-only, hide it in a typing.TYPE_CHECKING block\n    if TYPE_CHECKING:\n\n        def __contains__(self, key: str, /) -> bool:\n            return self.wrapped.__contains__(key)", "metadata": {"license": "MIT", "len_tokens": 227}}
{"id": "pydantic:pydantic/_internal/_utils.py", "language": "python", "code": "class deprecated_instance_property(Generic[_ModelT, _RT]):\n    \"\"\"A decorator exposing the decorated class method as a property, with a warning on instance access.\n\n    This decorator takes a class method defined on the `BaseModel` class and transforms it into\n    an attribute. The attribute can be accessed on both the class and instances of the class. If accessed\n    via an instance, a deprecation warning is emitted stating that instance access will be removed in V3.\n    \"\"\"\n\n    def __init__(self, fget: Callable[[type[_ModelT]], _RT], /) -> None:\n        # Note: fget should be a classmethod:\n        self.fget = fget\n\n    @overload\n    def __get__(self, instance: None, objtype: type[_ModelT]) -> _RT: ...\n    @overload\n    @deprecated(\n        'Accessing this attribute on the instance is deprecated, and will be removed in Pydantic V3. '\n        'Instead, you should access this attribute from the model class.',\n        category=None,\n    )\n    def __get__(self, instance: _ModelT, objtype: type[_ModelT]) -> _RT: ...\n    def __get__(self, instance: _ModelT | None, objtype: type[_ModelT]) -> _RT:\n        if instance is not None:\n            # fmt: off\n            attr_name = (\n                self.fget.__name__\n                if sys.version_info >= (3, 10)\n                else self.fget.__func__.__name__  # pyright: ignore[reportFunctionMemberAccess]\n            )\n            # fmt: on\n            warnings.warn(\n                f'Accessing the {attr_name!r} attribute on the instance is deprecated. '\n                'Instead, you should access this attribute from the model class.',\n                category=PydanticDeprecatedSince211,\n                stacklevel=2,\n            )\n        return self.fget.__get__(instance, objtype)()", "metadata": {"license": "MIT", "len_tokens": 414}}
{"id": "pydantic:pydantic/_internal/_utils.py", "language": "python", "code": "def _normalize_indexes(self, items: MappingIntStrAny, v_length: int) -> dict[int | str, Any]:\n        \"\"\":param items: dict or set of indexes which will be normalized\n        :param v_length: length of sequence indexes of which will be\n\n        >>> self._normalize_indexes({0: True, -2: True, -1: True}, 4)\n        {0: True, 2: True, 3: True}\n        >>> self._normalize_indexes({'__all__': True}, 4)\n        {0: True, 1: True, 2: True, 3: True}\n        \"\"\"\n        normalized_items: dict[int | str, Any] = {}\n        all_items = None\n        for i, v in items.items():\n            if not (isinstance(v, Mapping) or isinstance(v, AbstractSet) or self.is_true(v)):\n                raise TypeError(f'Unexpected type of exclude value for index \"{i}\" {v.__class__}')\n            if i == '__all__':\n                all_items = self._coerce_value(v)\n                continue\n            if not isinstance(i, int):\n                raise TypeError(\n                    'Excluding fields from a sequence of sub-models or dicts must be performed index-wise: '\n                    'expected integer keys or keyword \"__all__\"'\n                )\n            normalized_i = v_length + i if i < 0 else i\n            normalized_items[normalized_i] = self.merge(v, normalized_items.get(normalized_i))\n\n        if not all_items:\n            return normalized_items\n        if self.is_true(all_items):\n            for i in range(v_length):\n                normalized_items.setdefault(i, ...)\n            return normalized_items\n        for i in range(v_length):\n            normalized_item = normalized_items.setdefault(i, {})\n            if not self.is_true(normalized_item):\n                normalized_items[i] = self.merge(all_items, normalized_item)\n        return normalized_items", "metadata": {"license": "MIT", "len_tokens": 399}}
{"id": "pydantic:pydantic/_internal/_utils.py", "language": "python", "code": "def merge(cls, base: Any, override: Any, intersect: bool = False) -> Any:\n        \"\"\"Merge a `base` item with an `override` item.\n\n        Both `base` and `override` are converted to dictionaries if possible.\n        Sets are converted to dictionaries with the sets entries as keys and\n        Ellipsis as values.\n\n        Each key-value pair existing in `base` is merged with `override`,\n        while the rest of the key-value pairs are updated recursively with this function.\n\n        Merging takes place based on the \"union\" of keys if `intersect` is\n        set to `False` (default) and on the intersection of keys if\n        `intersect` is set to `True`.\n        \"\"\"\n        override = cls._coerce_value(override)\n        base = cls._coerce_value(base)\n        if override is None:\n            return base\n        if cls.is_true(base) or base is None:\n            return override\n        if cls.is_true(override):\n            return base if intersect else override\n\n        # intersection or union of keys while preserving ordering:\n        if intersect:\n            merge_keys = [k for k in base if k in override] + [k for k in override if k in base]\n        else:\n            merge_keys = list(base) + [k for k in override if k not in base]\n\n        merged: dict[int | str, Any] = {}\n        for k in merge_keys:\n            merged_item = cls.merge(base.get(k), override.get(k), intersect=intersect)\n            if merged_item is not None:\n                merged[k] = merged_item\n\n        return merged", "metadata": {"license": "MIT", "len_tokens": 339}}
{"id": "pydantic:pydantic/experimental/arguments_schema.py", "language": "python", "code": "\"\"\"Experimental module exposing a function to generate a core schema that validates callable arguments.\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Callable\nfrom typing import Any, Literal\n\nfrom pydantic_core import CoreSchema\n\nfrom pydantic import ConfigDict\nfrom pydantic._internal import _config, _generate_schema, _namespace_utils\n\n\ndef generate_arguments_schema(\n    func: Callable[..., Any],\n    schema_type: Literal['arguments', 'arguments-v3'] = 'arguments-v3',\n    parameters_callback: Callable[[int, str, Any], Literal['skip'] | None] | None = None,\n    config: ConfigDict | None = None,\n) -> CoreSchema:\n    \"\"\"Generate the schema for the arguments of a function.\n\n    Args:\n        func: The function to generate the schema for.\n        schema_type: The type of schema to generate.\n        parameters_callback: A callable that will be invoked for each parameter. The callback\n            should take three required arguments: the index, the name and the type annotation\n            (or [`Parameter.empty`][inspect.Parameter.empty] if not annotated) of the parameter.\n            The callback can optionally return `'skip'`, so that the parameter gets excluded\n            from the resulting schema.\n        config: The configuration to use.\n\n    Returns:\n        The generated schema.\n    \"\"\"\n    generate_schema = _generate_schema.GenerateSchema(\n        _config.ConfigWrapper(config),\n        ns_resolver=_namespace_utils.NsResolver(namespaces_tuple=_namespace_utils.ns_for_function(func)),\n    )\n\n    if schema_type == 'arguments':\n        schema = generate_schema._arguments_schema(func, parameters_callback)  # pyright: ignore[reportArgumentType]\n    else:\n        schema = generate_schema._arguments_v3_schema(func, parameters_callback)  # pyright: ignore[reportArgumentType]\n    return generate_schema.clean_schema(schema)\n", "metadata": {"license": "MIT", "len_tokens": 383}}
{"id": "pydantic:pydantic/experimental/arguments_schema.py", "language": "python", "code": "def generate_arguments_schema(\n    func: Callable[..., Any],\n    schema_type: Literal['arguments', 'arguments-v3'] = 'arguments-v3',\n    parameters_callback: Callable[[int, str, Any], Literal['skip'] | None] | None = None,\n    config: ConfigDict | None = None,\n) -> CoreSchema:\n    \"\"\"Generate the schema for the arguments of a function.\n\n    Args:\n        func: The function to generate the schema for.\n        schema_type: The type of schema to generate.\n        parameters_callback: A callable that will be invoked for each parameter. The callback\n            should take three required arguments: the index, the name and the type annotation\n            (or [`Parameter.empty`][inspect.Parameter.empty] if not annotated) of the parameter.\n            The callback can optionally return `'skip'`, so that the parameter gets excluded\n            from the resulting schema.\n        config: The configuration to use.\n\n    Returns:\n        The generated schema.\n    \"\"\"\n    generate_schema = _generate_schema.GenerateSchema(\n        _config.ConfigWrapper(config),\n        ns_resolver=_namespace_utils.NsResolver(namespaces_tuple=_namespace_utils.ns_for_function(func)),\n    )\n\n    if schema_type == 'arguments':\n        schema = generate_schema._arguments_schema(func, parameters_callback)  # pyright: ignore[reportArgumentType]\n    else:\n        schema = generate_schema._arguments_v3_schema(func, parameters_callback)  # pyright: ignore[reportArgumentType]\n    return generate_schema.clean_schema(schema)", "metadata": {"license": "MIT", "len_tokens": 311}}
{"id": "pydantic:pydantic/plugin/_loader.py", "language": "python", "code": "from __future__ import annotations\n\nimport importlib.metadata as importlib_metadata\nimport os\nimport warnings\nfrom collections.abc import Iterable\nfrom typing import TYPE_CHECKING, Final\n\nif TYPE_CHECKING:\n    from . import PydanticPluginProtocol\n\n\nPYDANTIC_ENTRY_POINT_GROUP: Final[str] = 'pydantic'\n\n# cache of plugins\n_plugins: dict[str, PydanticPluginProtocol] | None = None\n# return no plugins while loading plugins to avoid recursion and errors while import plugins\n# this means that if plugins use pydantic\n_loading_plugins: bool = False\n\n\ndef get_plugins() -> Iterable[PydanticPluginProtocol]:\n    \"\"\"Load plugins for Pydantic.\n\n    Inspired by: https://github.com/pytest-dev/pluggy/blob/1.3.0/src/pluggy/_manager.py#L376-L402\n    \"\"\"\n    disabled_plugins = os.getenv('PYDANTIC_DISABLE_PLUGINS')\n    global _plugins, _loading_plugins\n    if _loading_plugins:\n        # this happens when plugins themselves use pydantic, we return no plugins\n        return ()\n    elif disabled_plugins in ('__all__', '1', 'true'):\n        return ()\n    elif _plugins is None:\n        _plugins = {}\n        # set _loading_plugins so any plugins that use pydantic don't themselves use plugins\n        _loading_plugins = True\n        try:\n            for dist in importlib_metadata.distributions():\n                for entry_point in dist.entry_points:\n                    if entry_point.group != PYDANTIC_ENTRY_POINT_GROUP:\n                        continue\n                    if entry_point.value in _plugins:\n                        continue\n                    if disabled_plugins is not None and entry_point.name in disabled_plugins.split(','):\n                        continue\n                    try:\n                        _plugins[entry_point.value] = entry_point.load()\n                    except (ImportError, AttributeError) as e:\n                        warnings.warn(\n                            f'{e.__class__.__name__} while loading the `{entry_point.name}` Pydantic plugin, '\n                            f'this plugin will not be installed.\\n\\n{e!r}',\n                            stacklevel=2,\n                        )\n        finally:\n            _loading_plugins = False\n\n    return _plugins.values()\n", "metadata": {"license": "MIT", "len_tokens": 458}}
{"id": "pydantic:pydantic/plugin/_loader.py", "language": "python", "code": "def get_plugins() -> Iterable[PydanticPluginProtocol]:\n    \"\"\"Load plugins for Pydantic.\n\n    Inspired by: https://github.com/pytest-dev/pluggy/blob/1.3.0/src/pluggy/_manager.py#L376-L402\n    \"\"\"\n    disabled_plugins = os.getenv('PYDANTIC_DISABLE_PLUGINS')\n    global _plugins, _loading_plugins\n    if _loading_plugins:\n        # this happens when plugins themselves use pydantic, we return no plugins\n        return ()\n    elif disabled_plugins in ('__all__', '1', 'true'):\n        return ()\n    elif _plugins is None:\n        _plugins = {}\n        # set _loading_plugins so any plugins that use pydantic don't themselves use plugins\n        _loading_plugins = True\n        try:\n            for dist in importlib_metadata.distributions():\n                for entry_point in dist.entry_points:\n                    if entry_point.group != PYDANTIC_ENTRY_POINT_GROUP:\n                        continue\n                    if entry_point.value in _plugins:\n                        continue\n                    if disabled_plugins is not None and entry_point.name in disabled_plugins.split(','):\n                        continue\n                    try:\n                        _plugins[entry_point.value] = entry_point.load()\n                    except (ImportError, AttributeError) as e:\n                        warnings.warn(\n                            f'{e.__class__.__name__} while loading the `{entry_point.name}` Pydantic plugin, '\n                            f'this plugin will not be installed.\\n\\n{e!r}',\n                            stacklevel=2,\n                        )\n        finally:\n            _loading_plugins = False\n\n    return _plugins.values()", "metadata": {"license": "MIT", "len_tokens": 333}}
{"id": "pydantic:pydantic/plugin/__init__.py", "language": "python", "code": "class PydanticPluginProtocol(Protocol):\n    \"\"\"Protocol defining the interface for Pydantic plugins.\"\"\"\n\n    def new_schema_validator(\n        self,\n        schema: CoreSchema,\n        schema_type: Any,\n        schema_type_path: SchemaTypePath,\n        schema_kind: SchemaKind,\n        config: CoreConfig | None,\n        plugin_settings: dict[str, object],\n    ) -> tuple[\n        ValidatePythonHandlerProtocol | None, ValidateJsonHandlerProtocol | None, ValidateStringsHandlerProtocol | None\n    ]:\n        \"\"\"This method is called for each plugin every time a new [`SchemaValidator`][pydantic_core.SchemaValidator]\n        is created.\n\n        It should return an event handler for each of the three validation methods, or `None` if the plugin does not\n        implement that method.\n\n        Args:\n            schema: The schema to validate against.\n            schema_type: The original type which the schema was created from, e.g. the model class.\n            schema_type_path: Path defining where `schema_type` was defined, or where `TypeAdapter` was called.\n            schema_kind: The kind of schema to validate against.\n            config: The config to use for validation.\n            plugin_settings: Any plugin settings.\n\n        Returns:\n            A tuple of optional event handlers for each of the three validation methods -\n                `validate_python`, `validate_json`, `validate_strings`.\n        \"\"\"\n        raise NotImplementedError('Pydantic plugins should implement `new_schema_validator`.')", "metadata": {"license": "MIT", "len_tokens": 302}}
{"id": "pydantic:pydantic/plugin/__init__.py", "language": "python", "code": "class ValidatePythonHandlerProtocol(BaseValidateHandlerProtocol, Protocol):\n    \"\"\"Event handler for `SchemaValidator.validate_python`.\"\"\"\n\n    def on_enter(\n        self,\n        input: Any,\n        *,\n        strict: bool | None = None,\n        extra: ExtraValues | None = None,\n        from_attributes: bool | None = None,\n        context: Any | None = None,\n        self_instance: Any | None = None,\n        by_alias: bool | None = None,\n        by_name: bool | None = None,\n    ) -> None:\n        \"\"\"Callback to be notified of validation start, and create an instance of the event handler.\n\n        Args:\n            input: The input to be validated.\n            strict: Whether to validate the object in strict mode.\n            extra: Whether to ignore, allow, or forbid extra data during model validation.\n            from_attributes: Whether to validate objects as inputs by extracting attributes.\n            context: The context to use for validation, this is passed to functional validators.\n            self_instance: An instance of a model to set attributes on from validation, this is used when running\n                validation from the `__init__` method of a model.\n            by_alias: Whether to use the field's alias to match the input data to an attribute.\n            by_name: Whether to use the field's name to match the input data to an attribute.\n        \"\"\"", "metadata": {"license": "MIT", "len_tokens": 281}}
{"id": "pydantic:pydantic/plugin/__init__.py", "language": "python", "code": "class ValidateJsonHandlerProtocol(BaseValidateHandlerProtocol, Protocol):\n    \"\"\"Event handler for `SchemaValidator.validate_json`.\"\"\"\n\n    def on_enter(\n        self,\n        input: str | bytes | bytearray,\n        *,\n        strict: bool | None = None,\n        extra: ExtraValues | None = None,\n        context: Any | None = None,\n        self_instance: Any | None = None,\n        by_alias: bool | None = None,\n        by_name: bool | None = None,\n    ) -> None:\n        \"\"\"Callback to be notified of validation start, and create an instance of the event handler.\n\n        Args:\n            input: The JSON data to be validated.\n            strict: Whether to validate the object in strict mode.\n            extra: Whether to ignore, allow, or forbid extra data during model validation.\n            context: The context to use for validation, this is passed to functional validators.\n            self_instance: An instance of a model to set attributes on from validation, this is used when running\n                validation from the `__init__` method of a model.\n            by_alias: Whether to use the field's alias to match the input data to an attribute.\n            by_name: Whether to use the field's name to match the input data to an attribute.\n        \"\"\"", "metadata": {"license": "MIT", "len_tokens": 263}}
{"id": "pydantic:pydantic/plugin/__init__.py", "language": "python", "code": "class ValidateStringsHandlerProtocol(BaseValidateHandlerProtocol, Protocol):\n    \"\"\"Event handler for `SchemaValidator.validate_strings`.\"\"\"\n\n    def on_enter(\n        self,\n        input: StringInput,\n        *,\n        strict: bool | None = None,\n        extra: ExtraValues | None = None,\n        context: Any | None = None,\n        by_alias: bool | None = None,\n        by_name: bool | None = None,\n    ) -> None:\n        \"\"\"Callback to be notified of validation start, and create an instance of the event handler.\n\n        Args:\n            input: The string data to be validated.\n            strict: Whether to validate the object in strict mode.\n            extra: Whether to ignore, allow, or forbid extra data during model validation.\n            context: The context to use for validation, this is passed to functional validators.\n            by_alias: Whether to use the field's alias to match the input data to an attribute.\n            by_name: Whether to use the field's name to match the input data to an attribute.\n        \"\"\"", "metadata": {"license": "MIT", "len_tokens": 213}}
{"id": "pydantic:pydantic/plugin/__init__.py", "language": "python", "code": "def new_schema_validator(\n        self,\n        schema: CoreSchema,\n        schema_type: Any,\n        schema_type_path: SchemaTypePath,\n        schema_kind: SchemaKind,\n        config: CoreConfig | None,\n        plugin_settings: dict[str, object],\n    ) -> tuple[\n        ValidatePythonHandlerProtocol | None, ValidateJsonHandlerProtocol | None, ValidateStringsHandlerProtocol | None\n    ]:\n        \"\"\"This method is called for each plugin every time a new [`SchemaValidator`][pydantic_core.SchemaValidator]\n        is created.\n\n        It should return an event handler for each of the three validation methods, or `None` if the plugin does not\n        implement that method.\n\n        Args:\n            schema: The schema to validate against.\n            schema_type: The original type which the schema was created from, e.g. the model class.\n            schema_type_path: Path defining where `schema_type` was defined, or where `TypeAdapter` was called.\n            schema_kind: The kind of schema to validate against.\n            config: The config to use for validation.\n            plugin_settings: Any plugin settings.\n\n        Returns:\n            A tuple of optional event handlers for each of the three validation methods -\n                `validate_python`, `validate_json`, `validate_strings`.\n        \"\"\"\n        raise NotImplementedError('Pydantic plugins should implement `new_schema_validator`.')", "metadata": {"license": "MIT", "len_tokens": 280}}
{"id": "pydantic:pydantic/plugin/__init__.py", "language": "python", "code": "def on_enter(\n        self,\n        input: Any,\n        *,\n        strict: bool | None = None,\n        extra: ExtraValues | None = None,\n        from_attributes: bool | None = None,\n        context: Any | None = None,\n        self_instance: Any | None = None,\n        by_alias: bool | None = None,\n        by_name: bool | None = None,\n    ) -> None:\n        \"\"\"Callback to be notified of validation start, and create an instance of the event handler.\n\n        Args:\n            input: The input to be validated.\n            strict: Whether to validate the object in strict mode.\n            extra: Whether to ignore, allow, or forbid extra data during model validation.\n            from_attributes: Whether to validate objects as inputs by extracting attributes.\n            context: The context to use for validation, this is passed to functional validators.\n            self_instance: An instance of a model to set attributes on from validation, this is used when running\n                validation from the `__init__` method of a model.\n            by_alias: Whether to use the field's alias to match the input data to an attribute.\n            by_name: Whether to use the field's name to match the input data to an attribute.\n        \"\"\"", "metadata": {"license": "MIT", "len_tokens": 256}}
{"id": "pydantic:pydantic/plugin/__init__.py", "language": "python", "code": "def on_enter(\n        self,\n        input: str | bytes | bytearray,\n        *,\n        strict: bool | None = None,\n        extra: ExtraValues | None = None,\n        context: Any | None = None,\n        self_instance: Any | None = None,\n        by_alias: bool | None = None,\n        by_name: bool | None = None,\n    ) -> None:\n        \"\"\"Callback to be notified of validation start, and create an instance of the event handler.\n\n        Args:\n            input: The JSON data to be validated.\n            strict: Whether to validate the object in strict mode.\n            extra: Whether to ignore, allow, or forbid extra data during model validation.\n            context: The context to use for validation, this is passed to functional validators.\n            self_instance: An instance of a model to set attributes on from validation, this is used when running\n                validation from the `__init__` method of a model.\n            by_alias: Whether to use the field's alias to match the input data to an attribute.\n            by_name: Whether to use the field's name to match the input data to an attribute.\n        \"\"\"", "metadata": {"license": "MIT", "len_tokens": 238}}
{"id": "pydantic:pydantic/plugin/_schema_validator.py", "language": "python", "code": "def create_schema_validator(\n    schema: CoreSchema,\n    schema_type: Any,\n    schema_type_module: str,\n    schema_type_name: str,\n    schema_kind: SchemaKind,\n    config: CoreConfig | None = None,\n    plugin_settings: dict[str, Any] | None = None,\n) -> SchemaValidator | PluggableSchemaValidator:\n    \"\"\"Create a `SchemaValidator` or `PluggableSchemaValidator` if plugins are installed.\n\n    Returns:\n        If plugins are installed then return `PluggableSchemaValidator`, otherwise return `SchemaValidator`.\n    \"\"\"\n    from . import SchemaTypePath\n    from ._loader import get_plugins\n\n    plugins = get_plugins()\n    if plugins:\n        return PluggableSchemaValidator(\n            schema,\n            schema_type,\n            SchemaTypePath(schema_type_module, schema_type_name),\n            schema_kind,\n            config,\n            plugins,\n            plugin_settings or {},\n        )\n    else:\n        return SchemaValidator(schema, config)", "metadata": {"license": "MIT", "len_tokens": 201}}
{"id": "pydantic:pydantic/plugin/_schema_validator.py", "language": "python", "code": "class PluggableSchemaValidator:\n    \"\"\"Pluggable schema validator.\"\"\"\n\n    __slots__ = '_schema_validator', 'validate_json', 'validate_python', 'validate_strings'\n\n    def __init__(\n        self,\n        schema: CoreSchema,\n        schema_type: Any,\n        schema_type_path: SchemaTypePath,\n        schema_kind: SchemaKind,\n        config: CoreConfig | None,\n        plugins: Iterable[PydanticPluginProtocol],\n        plugin_settings: dict[str, Any],\n    ) -> None:\n        self._schema_validator = SchemaValidator(schema, config)\n\n        python_event_handlers: list[BaseValidateHandlerProtocol] = []\n        json_event_handlers: list[BaseValidateHandlerProtocol] = []\n        strings_event_handlers: list[BaseValidateHandlerProtocol] = []\n        for plugin in plugins:\n            try:\n                p, j, s = plugin.new_schema_validator(\n                    schema, schema_type, schema_type_path, schema_kind, config, plugin_settings\n                )\n            except TypeError as e:  # pragma: no cover\n                raise TypeError(f'Error using plugin `{plugin.__module__}:{plugin.__class__.__name__}`: {e}') from e\n            if p is not None:\n                python_event_handlers.append(p)\n            if j is not None:\n                json_event_handlers.append(j)\n            if s is not None:\n                strings_event_handlers.append(s)\n\n        self.validate_python = build_wrapper(self._schema_validator.validate_python, python_event_handlers)\n        self.validate_json = build_wrapper(self._schema_validator.validate_json, json_event_handlers)\n        self.validate_strings = build_wrapper(self._schema_validator.validate_strings, strings_event_handlers)\n\n    def __getattr__(self, name: str) -> Any:\n        return getattr(self._schema_validator, name)", "metadata": {"license": "MIT", "len_tokens": 363}}
{"id": "pydantic:pydantic/plugin/_schema_validator.py", "language": "python", "code": "def build_wrapper(func: Callable[P, R], event_handlers: list[BaseValidateHandlerProtocol]) -> Callable[P, R]:\n    if not event_handlers:\n        return func\n    else:\n        on_enters = tuple(h.on_enter for h in event_handlers if filter_handlers(h, 'on_enter'))\n        on_successes = tuple(h.on_success for h in event_handlers if filter_handlers(h, 'on_success'))\n        on_errors = tuple(h.on_error for h in event_handlers if filter_handlers(h, 'on_error'))\n        on_exceptions = tuple(h.on_exception for h in event_handlers if filter_handlers(h, 'on_exception'))\n\n        @functools.wraps(func)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:\n            for on_enter_handler in on_enters:\n                on_enter_handler(*args, **kwargs)\n\n            try:\n                result = func(*args, **kwargs)\n            except ValidationError as error:\n                for on_error_handler in on_errors:\n                    on_error_handler(error)\n                raise\n            except Exception as exception:\n                for on_exception_handler in on_exceptions:\n                    on_exception_handler(exception)\n                raise\n            else:\n                for on_success_handler in on_successes:\n                    on_success_handler(result)\n                return result\n\n        return wrapper", "metadata": {"license": "MIT", "len_tokens": 267}}
{"id": "pydantic:pydantic/plugin/_schema_validator.py", "language": "python", "code": "def __init__(\n        self,\n        schema: CoreSchema,\n        schema_type: Any,\n        schema_type_path: SchemaTypePath,\n        schema_kind: SchemaKind,\n        config: CoreConfig | None,\n        plugins: Iterable[PydanticPluginProtocol],\n        plugin_settings: dict[str, Any],\n    ) -> None:\n        self._schema_validator = SchemaValidator(schema, config)\n\n        python_event_handlers: list[BaseValidateHandlerProtocol] = []\n        json_event_handlers: list[BaseValidateHandlerProtocol] = []\n        strings_event_handlers: list[BaseValidateHandlerProtocol] = []\n        for plugin in plugins:\n            try:\n                p, j, s = plugin.new_schema_validator(\n                    schema, schema_type, schema_type_path, schema_kind, config, plugin_settings\n                )\n            except TypeError as e:  # pragma: no cover\n                raise TypeError(f'Error using plugin `{plugin.__module__}:{plugin.__class__.__name__}`: {e}') from e\n            if p is not None:\n                python_event_handlers.append(p)\n            if j is not None:\n                json_event_handlers.append(j)\n            if s is not None:\n                strings_event_handlers.append(s)\n\n        self.validate_python = build_wrapper(self._schema_validator.validate_python, python_event_handlers)\n        self.validate_json = build_wrapper(self._schema_validator.validate_json, json_event_handlers)\n        self.validate_strings = build_wrapper(self._schema_validator.validate_strings, strings_event_handlers)", "metadata": {"license": "MIT", "len_tokens": 301}}
{"id": "pydantic:pydantic/deprecated/config.py", "language": "python", "code": "from __future__ import annotations as _annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Any, Literal\n\nfrom typing_extensions import deprecated\n\nfrom .._internal import _config\nfrom ..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\n__all__ = 'BaseConfig', 'Extra'\n\n\nclass _ConfigMetaclass(type):\n    def __getattr__(self, item: str) -> Any:\n        try:\n            obj = _config.config_defaults[item]\n            warnings.warn(_config.DEPRECATION_MESSAGE, DeprecationWarning)\n            return obj\n        except KeyError as exc:\n            raise AttributeError(f\"type object '{self.__name__}' has no attribute {exc}\") from exc\n\n\n@deprecated('BaseConfig is deprecated. Use the `pydantic.ConfigDict` instead.', category=PydanticDeprecatedSince20)\nclass BaseConfig(metaclass=_ConfigMetaclass):\n    \"\"\"This class is only retained for backwards compatibility.\n\n    !!! Warning \"Deprecated\"\n        BaseConfig is deprecated. Use the [`pydantic.ConfigDict`][pydantic.ConfigDict] instead.\n    \"\"\"\n\n    def __getattr__(self, item: str) -> Any:\n        try:\n            obj = super().__getattribute__(item)\n            warnings.warn(_config.DEPRECATION_MESSAGE, DeprecationWarning)\n            return obj\n        except AttributeError as exc:\n            try:\n                return getattr(type(self), item)\n            except AttributeError:\n                # re-raising changes the displayed text to reflect that `self` is not a type\n                raise AttributeError(str(exc)) from exc\n\n    def __init_subclass__(cls, **kwargs: Any) -> None:\n        warnings.warn(_config.DEPRECATION_MESSAGE, DeprecationWarning)\n        return super().__init_subclass__(**kwargs)\n\n\nclass _ExtraMeta(type):\n    def __getattribute__(self, __name: str) -> Any:\n        # The @deprecated decorator accesses other attributes, so we only emit a warning for the expected ones\n        if __name in {'allow', 'ignore', 'forbid'}:\n            warnings.warn(\n                \"`pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`)\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        return super().__getattribute__(__name)\n\n\n@deprecated(\n    \"Extra is deprecated. Use literal values instead (e.g. `extra='allow'`)\", category=PydanticDeprecatedSince20\n)\nclass Extra(metaclass=_ExtraMeta):\n    allow: Literal['allow'] = 'allow'\n    ignore: Literal['ignore'] = 'ignore'\n    forbid: Literal['forbid'] = 'forbid'\n", "metadata": {"license": "MIT", "len_tokens": 616}}
{"id": "pydantic:pydantic/deprecated/decorator.py", "language": "python", "code": "def validate_arguments(func: Optional['AnyCallableT'] = None, *, config: 'ConfigType' = None) -> Any:\n    \"\"\"Decorator to validate the arguments passed to a function.\"\"\"\n    warnings.warn(\n        'The `validate_arguments` method is deprecated; use `validate_call` instead.',\n        PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n\n    def validate(_func: 'AnyCallable') -> 'AnyCallable':\n        vd = ValidatedFunction(_func, config)\n\n        @wraps(_func)\n        def wrapper_function(*args: Any, **kwargs: Any) -> Any:\n            return vd.call(*args, **kwargs)\n\n        wrapper_function.vd = vd  # type: ignore\n        wrapper_function.validate = vd.init_model_instance  # type: ignore\n        wrapper_function.raw_function = vd.raw_function  # type: ignore\n        wrapper_function.model = vd.model  # type: ignore\n        return wrapper_function\n\n    if func:\n        return validate(func)\n    else:\n        return validate", "metadata": {"license": "MIT", "len_tokens": 213}}
{"id": "pydantic:pydantic/deprecated/decorator.py", "language": "python", "code": "def __init__(self, function: 'AnyCallable', config: 'ConfigType'):\n        from inspect import Parameter, signature\n\n        parameters: Mapping[str, Parameter] = signature(function).parameters\n\n        if parameters.keys() & {ALT_V_ARGS, ALT_V_KWARGS, V_POSITIONAL_ONLY_NAME, V_DUPLICATE_KWARGS}:\n            raise PydanticUserError(\n                f'\"{ALT_V_ARGS}\", \"{ALT_V_KWARGS}\", \"{V_POSITIONAL_ONLY_NAME}\" and \"{V_DUPLICATE_KWARGS}\" '\n                f'are not permitted as argument names when using the \"{validate_arguments.__name__}\" decorator',\n                code=None,\n            )\n\n        self.raw_function = function\n        self.arg_mapping: dict[int, str] = {}\n        self.positional_only_args: set[str] = set()\n        self.v_args_name = 'args'\n        self.v_kwargs_name = 'kwargs'\n\n        type_hints = _typing_extra.get_type_hints(function, include_extras=True)\n        takes_args = False\n        takes_kwargs = False\n        fields: dict[str, tuple[Any, Any]] = {}\n        for i, (name, p) in enumerate(parameters.items()):\n            if p.annotation is p.empty:\n                annotation = Any\n            else:\n                annotation = type_hints[name]\n\n            default = ... if p.default is p.empty else p.default\n            if p.kind == Parameter.POSITIONAL_ONLY:\n                self.arg_mapping[i] = name\n                fields[name] = annotation, default\n                fields[V_POSITIONAL_ONLY_NAME] = list[str], None\n                self.positional_only_args.add(name)\n            elif p.kind == Parameter.POSITIONAL_OR_KEYWORD:\n                self.arg_mapping[i] = name\n                fields[name] = annotation, default\n                fields[V_DUPLICATE_KWARGS] = list[str], None\n            elif p.kind == Parameter.KEYWORD_ONLY:\n                fields[name] = annotation, default\n            elif p.kind == Parameter.VAR_POSITIONAL:\n                self.v_args_name = name\n                fields[name] = tuple[annotation, ...], None\n                takes_args = True\n            else:\n                assert p.kind == Parameter.VAR_KEYWORD, p.kind\n                self.v_kwargs_name = name\n                fields[name] = dict[str, annotation], None\n                takes_kwargs = True\n\n        # these checks avoid a clash between \"args\" and a field with that name\n        if not takes_args and self.v_args_name in fields:\n            self.v_args_name = ALT_V_ARGS\n\n        # same with \"kwargs\"\n        if not takes_kwargs and self.v_kwargs_name in fields:\n            self.v_kwargs_name = ALT_V_KWARGS\n\n        if not takes_args:\n            # we add the field so validation below can raise the correct exception\n            fields[self.v_args_name] = list[Any], None\n\n        if not takes_kwargs:\n            # same with kwargs\n            fields[self.v_kwargs_name] = dict[Any, Any], None\n\n        self.create_model(fields, takes_args, takes_kwargs, config)", "metadata": {"license": "MIT", "len_tokens": 633}}
{"id": "pydantic:pydantic/deprecated/decorator.py", "language": "python", "code": "def build_values(self, args: tuple[Any, ...], kwargs: dict[str, Any]) -> dict[str, Any]:\n        values: dict[str, Any] = {}\n        if args:\n            arg_iter = enumerate(args)\n            while True:\n                try:\n                    i, a = next(arg_iter)\n                except StopIteration:\n                    break\n                arg_name = self.arg_mapping.get(i)\n                if arg_name is not None:\n                    values[arg_name] = a\n                else:\n                    values[self.v_args_name] = [a] + [a for _, a in arg_iter]\n                    break\n\n        var_kwargs: dict[str, Any] = {}\n        wrong_positional_args = []\n        duplicate_kwargs = []\n        fields_alias = [\n            field.alias\n            for name, field in self.model.__pydantic_fields__.items()\n            if name not in (self.v_args_name, self.v_kwargs_name)\n        ]\n        non_var_fields = set(self.model.__pydantic_fields__) - {self.v_args_name, self.v_kwargs_name}\n        for k, v in kwargs.items():\n            if k in non_var_fields or k in fields_alias:\n                if k in self.positional_only_args:\n                    wrong_positional_args.append(k)\n                if k in values:\n                    duplicate_kwargs.append(k)\n                values[k] = v\n            else:\n                var_kwargs[k] = v\n\n        if var_kwargs:\n            values[self.v_kwargs_name] = var_kwargs\n        if wrong_positional_args:\n            values[V_POSITIONAL_ONLY_NAME] = wrong_positional_args\n        if duplicate_kwargs:\n            values[V_DUPLICATE_KWARGS] = duplicate_kwargs\n        return values", "metadata": {"license": "MIT", "len_tokens": 344}}
{"id": "pydantic:pydantic/deprecated/decorator.py", "language": "python", "code": "def execute(self, m: BaseModel) -> Any:\n        d = {\n            k: v\n            for k, v in m.__dict__.items()\n            if k in m.__pydantic_fields_set__ or m.__pydantic_fields__[k].default_factory\n        }\n        var_kwargs = d.pop(self.v_kwargs_name, {})\n\n        if self.v_args_name in d:\n            args_: list[Any] = []\n            in_kwargs = False\n            kwargs = {}\n            for name, value in d.items():\n                if in_kwargs:\n                    kwargs[name] = value\n                elif name == self.v_args_name:\n                    args_ += value\n                    in_kwargs = True\n                else:\n                    args_.append(value)\n            return self.raw_function(*args_, **kwargs, **var_kwargs)\n        elif self.positional_only_args:\n            args_ = []\n            kwargs = {}\n            for name, value in d.items():\n                if name in self.positional_only_args:\n                    args_.append(value)\n                else:\n                    kwargs[name] = value\n            return self.raw_function(*args_, **kwargs, **var_kwargs)\n        else:\n            return self.raw_function(**d, **var_kwargs)", "metadata": {"license": "MIT", "len_tokens": 248}}
{"id": "pydantic:pydantic/deprecated/decorator.py", "language": "python", "code": "def create_model(self, fields: dict[str, Any], takes_args: bool, takes_kwargs: bool, config: 'ConfigType') -> None:\n        pos_args = len(self.arg_mapping)\n\n        config_wrapper = _config.ConfigWrapper(config)\n\n        if config_wrapper.alias_generator:\n            raise PydanticUserError(\n                'Setting the \"alias_generator\" property on custom Config for '\n                '@validate_arguments is not yet supported, please remove.',\n                code=None,\n            )\n        if config_wrapper.extra is None:\n            config_wrapper.config_dict['extra'] = 'forbid'\n\n        class DecoratorBaseModel(BaseModel):\n            @field_validator(self.v_args_name, check_fields=False)\n            @classmethod\n            def check_args(cls, v: Optional[list[Any]]) -> Optional[list[Any]]:\n                if takes_args or v is None:\n                    return v\n\n                raise TypeError(f'{pos_args} positional arguments expected but {pos_args + len(v)} given')\n\n            @field_validator(self.v_kwargs_name, check_fields=False)\n            @classmethod\n            def check_kwargs(cls, v: Optional[dict[str, Any]]) -> Optional[dict[str, Any]]:\n                if takes_kwargs or v is None:\n                    return v\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v.keys()))\n                raise TypeError(f'unexpected keyword argument{plural}: {keys}')\n\n            @field_validator(V_POSITIONAL_ONLY_NAME, check_fields=False)\n            @classmethod\n            def check_positional_only(cls, v: Optional[list[str]]) -> None:\n                if v is None:\n                    return\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v))\n                raise TypeError(f'positional-only argument{plural} passed as keyword argument{plural}: {keys}')\n\n            @field_validator(V_DUPLICATE_KWARGS, check_fields=False)\n            @classmethod\n            def check_duplicate_kwargs(cls, v: Optional[list[str]]) -> None:\n                if v is None:\n                    return\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v))\n                raise TypeError(f'multiple values for argument{plural}: {keys}')\n\n            model_config = config_wrapper.config_dict\n\n        self.model = create_model(to_pascal(self.raw_function.__name__), __base__=DecoratorBaseModel, **fields)", "metadata": {"license": "MIT", "len_tokens": 509}}
{"id": "pydantic:pydantic/deprecated/decorator.py", "language": "python", "code": "class DecoratorBaseModel(BaseModel):\n            @field_validator(self.v_args_name, check_fields=False)\n            @classmethod\n            def check_args(cls, v: Optional[list[Any]]) -> Optional[list[Any]]:\n                if takes_args or v is None:\n                    return v\n\n                raise TypeError(f'{pos_args} positional arguments expected but {pos_args + len(v)} given')\n\n            @field_validator(self.v_kwargs_name, check_fields=False)\n            @classmethod\n            def check_kwargs(cls, v: Optional[dict[str, Any]]) -> Optional[dict[str, Any]]:\n                if takes_kwargs or v is None:\n                    return v\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v.keys()))\n                raise TypeError(f'unexpected keyword argument{plural}: {keys}')\n\n            @field_validator(V_POSITIONAL_ONLY_NAME, check_fields=False)\n            @classmethod\n            def check_positional_only(cls, v: Optional[list[str]]) -> None:\n                if v is None:\n                    return\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v))\n                raise TypeError(f'positional-only argument{plural} passed as keyword argument{plural}: {keys}')\n\n            @field_validator(V_DUPLICATE_KWARGS, check_fields=False)\n            @classmethod\n            def check_duplicate_kwargs(cls, v: Optional[list[str]]) -> None:\n                if v is None:\n                    return\n\n                plural = '' if len(v) == 1 else 's'\n                keys = ', '.join(map(repr, v))\n                raise TypeError(f'multiple values for argument{plural}: {keys}')\n\n            model_config = config_wrapper.config_dict", "metadata": {"license": "MIT", "len_tokens": 363}}
{"id": "pydantic:pydantic/deprecated/tools.py", "language": "python", "code": "from __future__ import annotations\n\nimport json\nimport warnings\nfrom typing import TYPE_CHECKING, Any, Callable, TypeVar, Union\n\nfrom typing_extensions import deprecated\n\nfrom ..json_schema import DEFAULT_REF_TEMPLATE, GenerateJsonSchema\nfrom ..type_adapter import TypeAdapter\nfrom ..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\n__all__ = 'parse_obj_as', 'schema_of', 'schema_json_of'\n\nNameFactory = Union[str, Callable[[type[Any]], str]]\n\n\nT = TypeVar('T')\n\n\n@deprecated(\n    '`parse_obj_as` is deprecated. Use `pydantic.TypeAdapter.validate_python` instead.',\n    category=None,\n)\ndef parse_obj_as(type_: type[T], obj: Any, type_name: NameFactory | None = None) -> T:\n    warnings.warn(\n        '`parse_obj_as` is deprecated. Use `pydantic.TypeAdapter.validate_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    if type_name is not None:  # pragma: no cover\n        warnings.warn(\n            'The type_name parameter is deprecated. parse_obj_as no longer creates temporary models',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    return TypeAdapter(type_).validate_python(obj)\n\n\n@deprecated(\n    '`schema_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead.',\n    category=None,\n)\ndef schema_of(\n    type_: Any,\n    *,\n    title: NameFactory | None = None,\n    by_alias: bool = True,\n    ref_template: str = DEFAULT_REF_TEMPLATE,\n    schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n) -> dict[str, Any]:\n    \"\"\"Generate a JSON schema (as dict) for the passed model or dynamically generated one.\"\"\"\n    warnings.warn(\n        '`schema_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    res = TypeAdapter(type_).json_schema(\n        by_alias=by_alias,\n        schema_generator=schema_generator,\n        ref_template=ref_template,\n    )\n    if title is not None:\n        if isinstance(title, str):\n            res['title'] = title\n        else:\n            warnings.warn(\n                'Passing a callable for the `title` parameter is deprecated and no longer supported',\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            res['title'] = title(type_)\n    return res\n\n\n@deprecated(\n    '`schema_json_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead.',\n    category=None,\n)\ndef schema_json_of(\n    type_: Any,\n    *,\n    title: NameFactory | None = None,\n    by_alias: bool = True,\n    ref_template: str = DEFAULT_REF_TEMPLATE,\n    schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n    **dumps_kwargs: Any,\n) -> str:\n    \"\"\"Generate a JSON schema (as JSON) for the passed model or dynamically generated one.\"\"\"\n    warnings.warn(\n        '`schema_json_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    return json.dumps(\n        schema_of(type_, title=title, by_alias=by_alias, ref_template=ref_template, schema_generator=schema_generator),\n        **dumps_kwargs,\n    )\n", "metadata": {"license": "MIT", "len_tokens": 775}}
{"id": "pydantic:pydantic/deprecated/tools.py", "language": "python", "code": "def schema_of(\n    type_: Any,\n    *,\n    title: NameFactory | None = None,\n    by_alias: bool = True,\n    ref_template: str = DEFAULT_REF_TEMPLATE,\n    schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n) -> dict[str, Any]:\n    \"\"\"Generate a JSON schema (as dict) for the passed model or dynamically generated one.\"\"\"\n    warnings.warn(\n        '`schema_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    res = TypeAdapter(type_).json_schema(\n        by_alias=by_alias,\n        schema_generator=schema_generator,\n        ref_template=ref_template,\n    )\n    if title is not None:\n        if isinstance(title, str):\n            res['title'] = title\n        else:\n            warnings.warn(\n                'Passing a callable for the `title` parameter is deprecated and no longer supported',\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            res['title'] = title(type_)\n    return res", "metadata": {"license": "MIT", "len_tokens": 228}}
{"id": "pydantic:pydantic/deprecated/copy_internals.py", "language": "python", "code": "def _iter(\n    self: BaseModel,\n    to_dict: bool = False,\n    by_alias: bool = False,\n    include: AbstractSetIntStr | MappingIntStrAny | None = None,\n    exclude: AbstractSetIntStr | MappingIntStrAny | None = None,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n) -> TupleGenerator:\n    # Merge field set excludes with explicit exclude parameter with explicit overriding field set options.\n    # The extra \"is not None\" guards are not logically necessary but optimizes performance for the simple case.\n    if exclude is not None:\n        exclude = _utils.ValueItems.merge(\n            {k: v.exclude for k, v in self.__pydantic_fields__.items() if v.exclude is not None}, exclude\n        )\n\n    if include is not None:\n        include = _utils.ValueItems.merge(dict.fromkeys(self.__pydantic_fields__, True), include, intersect=True)\n\n    allowed_keys = _calculate_keys(self, include=include, exclude=exclude, exclude_unset=exclude_unset)  # type: ignore\n    if allowed_keys is None and not (to_dict or by_alias or exclude_unset or exclude_defaults or exclude_none):\n        # huge boost for plain _iter()\n        yield from self.__dict__.items()\n        if self.__pydantic_extra__:\n            yield from self.__pydantic_extra__.items()\n        return\n\n    value_exclude = _utils.ValueItems(self, exclude) if exclude is not None else None\n    value_include = _utils.ValueItems(self, include) if include is not None else None\n\n    if self.__pydantic_extra__ is None:\n        items = self.__dict__.items()\n    else:\n        items = list(self.__dict__.items()) + list(self.__pydantic_extra__.items())\n\n    for field_key, v in items:\n        if (allowed_keys is not None and field_key not in allowed_keys) or (exclude_none and v is None):\n            continue\n\n        if exclude_defaults:\n            try:\n                field = self.__pydantic_fields__[field_key]\n            except KeyError:\n                pass\n            else:\n                if not field.is_required() and field.default == v:\n                    continue\n\n        if by_alias and field_key in self.__pydantic_fields__:\n            dict_key = self.__pydantic_fields__[field_key].alias or field_key\n        else:\n            dict_key = field_key\n\n        if to_dict or value_include or value_exclude:\n            v = _get_value(\n                type(self),\n                v,\n                to_dict=to_dict,\n                by_alias=by_alias,\n                include=value_include and value_include.for_element(field_key),\n                exclude=value_exclude and value_exclude.for_element(field_key),\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n        yield dict_key, v", "metadata": {"license": "MIT", "len_tokens": 621}}
{"id": "pydantic:pydantic/deprecated/copy_internals.py", "language": "python", "code": "def _get_value(\n    cls: type[BaseModel],\n    v: Any,\n    to_dict: bool,\n    by_alias: bool,\n    include: AbstractSetIntStr | MappingIntStrAny | None,\n    exclude: AbstractSetIntStr | MappingIntStrAny | None,\n    exclude_unset: bool,\n    exclude_defaults: bool,\n    exclude_none: bool,\n) -> Any:\n    from .. import BaseModel\n\n    if isinstance(v, BaseModel):\n        if to_dict:\n            return v.model_dump(\n                by_alias=by_alias,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                include=include,  # type: ignore\n                exclude=exclude,  # type: ignore\n                exclude_none=exclude_none,\n            )\n        else:\n            return v.copy(include=include, exclude=exclude)\n\n    value_exclude = _utils.ValueItems(v, exclude) if exclude else None\n    value_include = _utils.ValueItems(v, include) if include else None\n\n    if isinstance(v, dict):\n        return {\n            k_: _get_value(\n                cls,\n                v_,\n                to_dict=to_dict,\n                by_alias=by_alias,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                include=value_include and value_include.for_element(k_),\n                exclude=value_exclude and value_exclude.for_element(k_),\n                exclude_none=exclude_none,\n            )\n            for k_, v_ in v.items()\n            if (not value_exclude or not value_exclude.is_excluded(k_))\n            and (not value_include or value_include.is_included(k_))\n        }\n\n    elif _utils.sequence_like(v):\n        seq_args = (\n            _get_value(\n                cls,\n                v_,\n                to_dict=to_dict,\n                by_alias=by_alias,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                include=value_include and value_include.for_element(i),\n                exclude=value_exclude and value_exclude.for_element(i),\n                exclude_none=exclude_none,\n            )\n            for i, v_ in enumerate(v)\n            if (not value_exclude or not value_exclude.is_excluded(i))\n            and (not value_include or value_include.is_included(i))\n        )\n\n        return v.__class__(*seq_args) if _typing_extra.is_namedtuple(v.__class__) else v.__class__(seq_args)\n\n    elif isinstance(v, Enum) and getattr(cls.model_config, 'use_enum_values', False):\n        return v.value\n\n    else:\n        return v", "metadata": {"license": "MIT", "len_tokens": 535}}
{"id": "pydantic:pydantic/deprecated/class_validators.py", "language": "python", "code": "def validator(\n    __field: str,\n    *fields: str,\n    pre: bool = False,\n    each_item: bool = False,\n    always: bool = False,\n    check_fields: bool | None = None,\n    allow_reuse: bool = False,\n) -> Callable[[_V1ValidatorType], _V1ValidatorType]:\n    \"\"\"Decorate methods on the class indicating that they should be used to validate fields.\n\n    Args:\n        __field (str): The first field the validator should be called on; this is separate\n            from `fields` to ensure an error is raised if you don't pass at least one.\n        *fields (str): Additional field(s) the validator should be called on.\n        pre (bool, optional): Whether this validator should be called before the standard\n            validators (else after). Defaults to False.\n        each_item (bool, optional): For complex objects (sets, lists etc.) whether to validate\n            individual elements rather than the whole object. Defaults to False.\n        always (bool, optional): Whether this method and other validators should be called even if\n            the value is missing. Defaults to False.\n        check_fields (bool | None, optional): Whether to check that the fields actually exist on the model.\n            Defaults to None.\n        allow_reuse (bool, optional): Whether to track and raise an error if another validator refers to\n            the decorated function. Defaults to False.\n\n    Returns:\n        Callable: A decorator that can be used to decorate a\n            function to be used as a validator.\n    \"\"\"\n    warn(\n        'Pydantic V1 style `@validator` validators are deprecated.'\n        ' You should migrate to Pydantic V2 style `@field_validator` validators,'\n        ' see the migration guide for more details',\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    if allow_reuse is True:  # pragma: no cover\n        warn(_ALLOW_REUSE_WARNING_MESSAGE, DeprecationWarning, stacklevel=2)\n    fields = __field, *fields\n    if isinstance(fields[0], FunctionType):\n        raise PydanticUserError(\n            '`@validator` should be used with fields and keyword arguments, not bare. '\n            \"E.g. usage should be `@validator('<field_name>', ...)`\",\n            code='decorator-missing-arguments',\n        )\n    elif not all(isinstance(field, str) for field in fields):\n        raise PydanticUserError(\n            '`@validator` fields should be passed as separate string args. '\n            \"E.g. usage should be `@validator('<field_name_1>', '<field_name_2>', ...)`\",\n            code='decorator-invalid-fields',\n        )\n\n    mode: Literal['before', 'after'] = 'before' if pre is True else 'after'\n\n    def dec(f: Any) -> _decorators.PydanticDescriptorProxy[Any]:\n        if _decorators.is_instance_method_from_sig(f):\n            raise PydanticUserError(\n                '`@validator` cannot be applied to instance methods', code='validator-instance-method'\n            )\n        # auto apply the @classmethod decorator\n        f = _decorators.ensure_classmethod_based_on_signature(f)\n        wrap = _decorators_v1.make_generic_v1_field_validator\n        validator_wrapper_info = _decorators.ValidatorDecoratorInfo(\n            fields=fields,\n            mode=mode,\n            each_item=each_item,\n            always=always,\n            check_fields=check_fields,\n        )\n        return _decorators.PydanticDescriptorProxy(f, validator_wrapper_info, shim=wrap)\n\n    return dec", "metadata": {"license": "MIT", "len_tokens": 754}}
{"id": "pydantic:pydantic/deprecated/class_validators.py", "language": "python", "code": "def root_validator(\n    *__args,\n    pre: bool = False,\n    skip_on_failure: bool = False,\n    allow_reuse: bool = False,\n) -> Any:\n    \"\"\"Decorate methods on a model indicating that they should be used to validate (and perhaps\n    modify) data either before or after standard model parsing/validation is performed.\n\n    Args:\n        pre (bool, optional): Whether this validator should be called before the standard\n            validators (else after). Defaults to False.\n        skip_on_failure (bool, optional): Whether to stop validation and return as soon as a\n            failure is encountered. Defaults to False.\n        allow_reuse (bool, optional): Whether to track and raise an error if another validator\n            refers to the decorated function. Defaults to False.\n\n    Returns:\n        Any: A decorator that can be used to decorate a function to be used as a root_validator.\n    \"\"\"\n    warn(\n        'Pydantic V1 style `@root_validator` validators are deprecated.'\n        ' You should migrate to Pydantic V2 style `@model_validator` validators,'\n        ' see the migration guide for more details',\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    if __args:\n        # Ensure a nice error is raised if someone attempts to use the bare decorator\n        return root_validator()(*__args)  # type: ignore\n\n    if allow_reuse is True:  # pragma: no cover\n        warn(_ALLOW_REUSE_WARNING_MESSAGE, DeprecationWarning, stacklevel=2)\n    mode: Literal['before', 'after'] = 'before' if pre is True else 'after'\n    if pre is False and skip_on_failure is not True:\n        raise PydanticUserError(\n            'If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`.'\n            ' Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.',\n            code='root-validator-pre-skip',\n        )\n\n    wrap = partial(_decorators_v1.make_v1_generic_root_validator, pre=pre)\n\n    def dec(f: Callable[..., Any] | classmethod[Any, Any, Any] | staticmethod[Any, Any]) -> Any:\n        if _decorators.is_instance_method_from_sig(f):\n            raise TypeError('`@root_validator` cannot be applied to instance methods')\n        # auto apply the @classmethod decorator\n        res = _decorators.ensure_classmethod_based_on_signature(f)\n        dec_info = _decorators.RootValidatorDecoratorInfo(mode=mode)\n        return _decorators.PydanticDescriptorProxy(res, dec_info, shim=wrap)\n\n    return dec", "metadata": {"license": "MIT", "len_tokens": 564}}
{"id": "pydantic:pydantic/deprecated/parse.py", "language": "python", "code": "from __future__ import annotations\n\nimport json\nimport pickle\nimport warnings\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Callable\n\nfrom typing_extensions import deprecated\n\nfrom ..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\n\nclass Protocol(str, Enum):\n    json = 'json'\n    pickle = 'pickle'\n\n\n@deprecated('`load_str_bytes` is deprecated.', category=None)\ndef load_str_bytes(\n    b: str | bytes,\n    *,\n    content_type: str | None = None,\n    encoding: str = 'utf8',\n    proto: Protocol | None = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n) -> Any:\n    warnings.warn('`load_str_bytes` is deprecated.', category=PydanticDeprecatedSince20, stacklevel=2)\n    if proto is None and content_type:\n        if content_type.endswith(('json', 'javascript')):\n            pass\n        elif allow_pickle and content_type.endswith('pickle'):\n            proto = Protocol.pickle\n        else:\n            raise TypeError(f'Unknown content-type: {content_type}')\n\n    proto = proto or Protocol.json\n\n    if proto == Protocol.json:\n        if isinstance(b, bytes):\n            b = b.decode(encoding)\n        return json_loads(b)  # type: ignore\n    elif proto == Protocol.pickle:\n        if not allow_pickle:\n            raise RuntimeError('Trying to decode with pickle with allow_pickle=False')\n        bb = b if isinstance(b, bytes) else b.encode()  # type: ignore\n        return pickle.loads(bb)\n    else:\n        raise TypeError(f'Unknown protocol: {proto}')\n\n\n@deprecated('`load_file` is deprecated.', category=None)\ndef load_file(\n    path: str | Path,\n    *,\n    content_type: str | None = None,\n    encoding: str = 'utf8',\n    proto: Protocol | None = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n) -> Any:\n    warnings.warn('`load_file` is deprecated.', category=PydanticDeprecatedSince20, stacklevel=2)\n    path = Path(path)\n    b = path.read_bytes()\n    if content_type is None:\n        if path.suffix in ('.js', '.json'):\n            proto = Protocol.json\n        elif path.suffix == '.pkl':\n            proto = Protocol.pickle\n\n    return load_str_bytes(\n        b, proto=proto, content_type=content_type, encoding=encoding, allow_pickle=allow_pickle, json_loads=json_loads\n    )\n", "metadata": {"license": "MIT", "len_tokens": 607}}
{"id": "pydantic:pydantic/deprecated/parse.py", "language": "python", "code": "def load_str_bytes(\n    b: str | bytes,\n    *,\n    content_type: str | None = None,\n    encoding: str = 'utf8',\n    proto: Protocol | None = None,\n    allow_pickle: bool = False,\n    json_loads: Callable[[str], Any] = json.loads,\n) -> Any:\n    warnings.warn('`load_str_bytes` is deprecated.', category=PydanticDeprecatedSince20, stacklevel=2)\n    if proto is None and content_type:\n        if content_type.endswith(('json', 'javascript')):\n            pass\n        elif allow_pickle and content_type.endswith('pickle'):\n            proto = Protocol.pickle\n        else:\n            raise TypeError(f'Unknown content-type: {content_type}')\n\n    proto = proto or Protocol.json\n\n    if proto == Protocol.json:\n        if isinstance(b, bytes):\n            b = b.decode(encoding)\n        return json_loads(b)  # type: ignore\n    elif proto == Protocol.pickle:\n        if not allow_pickle:\n            raise RuntimeError('Trying to decode with pickle with allow_pickle=False')\n        bb = b if isinstance(b, bytes) else b.encode()  # type: ignore\n        return pickle.loads(bb)\n    else:\n        raise TypeError(f'Unknown protocol: {proto}')", "metadata": {"license": "MIT", "len_tokens": 265}}
{"id": "pydantic:pydantic/deprecated/json.py", "language": "python", "code": "def pydantic_encoder(obj: Any) -> Any:\n    warnings.warn(\n        '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n        category=PydanticDeprecatedSince20,\n        stacklevel=2,\n    )\n    from dataclasses import asdict, is_dataclass\n\n    BaseModel = import_cached_base_model()\n\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    elif is_dataclass(obj):\n        return asdict(obj)  # type: ignore\n\n    # Check the class type and its superclasses for a matching encoder\n    for base in obj.__class__.__mro__[:-1]:\n        try:\n            encoder = ENCODERS_BY_TYPE[base]\n        except KeyError:\n            continue\n        return encoder(obj)\n    else:  # We have exited the for loop without finding a suitable encoder\n        raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")", "metadata": {"license": "MIT", "len_tokens": 206}}
{"id": "pydantic:.github/actions/people/people.py", "language": "python", "code": "def get_graphql_response(\n    *,\n    settings: Settings,\n    query: str,\n    after: str | None = None,\n) -> dict[str, Any]:\n    \"\"\"Make a GraphQL request to GitHub API.\n\n    Args:\n        settings: Configuration settings including API token\n        query: GraphQL query string\n        after: Cursor for pagination, if any\n\n    Returns:\n        Response data from GitHub API in JSON format\n\n    Raises:\n        RuntimeError: If the API request fails or returns errors\n    \"\"\"\n    headers = {'Authorization': f'token {settings.input_token.get_secret_value()}'}\n    variables = {'after': after}\n    response = requests.post(\n        github_graphql_url,\n        headers=headers,\n        timeout=settings.request_timeout,\n        json={'query': query, 'variables': variables, 'operationName': 'Q'},\n    )\n    if response.status_code != 200:\n        logging.error(f'Response was not 200, after: {after}')\n        logging.error(response.text)\n        raise RuntimeError(response.text)\n    data = response.json()\n    if 'errors' in data:\n        logging.error(f'Errors in response, after: {after}')\n        logging.error(data['errors'])\n        logging.error(response.text)\n        raise RuntimeError(response.text)\n    return data", "metadata": {"license": "MIT", "len_tokens": 268}}
{"id": "pydantic:.github/actions/people/people.py", "language": "python", "code": "def get_issues_experts(settings: Settings) -> tuple[Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze issues to identify expert contributors.\n\n    Args:\n        settings: Configuration settings\n\n    Returns:\n        A tuple containing:\n            - Counter of all commentors\n            - Counter of commentors from the last month\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    issue_nodes: list[IssuesNode] = []\n    issue_edges = get_graphql_issue_edges(settings=settings)\n\n    while issue_edges:\n        issue_nodes.extend(edge.node for edge in issue_edges)\n        last_edge = issue_edges[-1]\n        issue_edges = get_graphql_issue_edges(settings=settings, after=last_edge.cursor)\n\n    commentors = Counter()\n    last_month_commentors = Counter()\n    authors: dict[str, Author] = {}\n\n    now = datetime.now(tz=timezone.utc)\n    one_month_ago = now - timedelta(days=30)\n\n    for issue in issue_nodes:\n        issue_author_name = None\n        if issue.author:\n            authors[issue.author.login] = issue.author\n            issue_author_name = issue.author.login\n        issue_commentors = set()\n        for comment in issue.comments.nodes:\n            if comment.author:\n                authors[comment.author.login] = comment.author\n                if comment.author.login != issue_author_name:\n                    issue_commentors.add(comment.author.login)\n        for author_name in issue_commentors:\n            commentors[author_name] += 1\n            if issue.createdAt > one_month_ago:\n                last_month_commentors[author_name] += 1\n\n    return commentors, last_month_commentors, authors", "metadata": {"license": "MIT", "len_tokens": 337}}
{"id": "pydantic:.github/actions/people/people.py", "language": "python", "code": "def get_discussions_experts(settings: Settings) -> tuple[Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze discussions to identify expert contributors.\n\n    Args:\n        settings: Configuration settings\n\n    Returns:\n        A tuple containing:\n            - Counter of all commentors\n            - Counter of commentors from the last month\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    discussion_nodes: list[DiscussionsNode] = []\n    discussion_edges = get_graphql_question_discussion_edges(settings=settings)\n\n    while discussion_edges:\n        discussion_nodes.extend(discussion_edge.node for discussion_edge in discussion_edges)\n        last_edge = discussion_edges[-1]\n        discussion_edges = get_graphql_question_discussion_edges(settings=settings, after=last_edge.cursor)\n\n    commentors = Counter()\n    last_month_commentors = Counter()\n    authors: dict[str, Author] = {}\n\n    now = datetime.now(tz=timezone.utc)\n    one_month_ago = now - timedelta(days=30)\n\n    for discussion in discussion_nodes:\n        discussion_author_name = None\n        if discussion.author:\n            authors[discussion.author.login] = discussion.author\n            discussion_author_name = discussion.author.login\n        discussion_commentors = set()\n        for comment in discussion.comments.nodes:\n            if comment.author:\n                authors[comment.author.login] = comment.author\n                if comment.author.login != discussion_author_name:\n                    discussion_commentors.add(comment.author.login)\n            for reply in comment.replies.nodes:\n                if reply.author:\n                    authors[reply.author.login] = reply.author\n                    if reply.author.login != discussion_author_name:\n                        discussion_commentors.add(reply.author.login)\n        for author_name in discussion_commentors:\n            commentors[author_name] += 1\n            if discussion.createdAt > one_month_ago:\n                last_month_commentors[author_name] += 1\n    return commentors, last_month_commentors, authors", "metadata": {"license": "MIT", "len_tokens": 390}}
{"id": "pydantic:.github/actions/people/people.py", "language": "python", "code": "def get_experts(settings: Settings) -> tuple[Counter, Counter, dict[str, Author]]:\n    \"\"\"Get combined expert contributors from discussions.\n\n    Args:\n        settings: Configuration settings\n\n    Returns:\n        A tuple containing:\n            - Counter of all commentors\n            - Counter of commentors from the last month\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    # Migrated to only use GitHub Discussions\n    # (\n    #     issues_commentors,\n    #     issues_last_month_commentors,\n    #     issues_authors,\n    # ) = get_issues_experts(settings=settings)\n    (\n        discussions_commentors,\n        discussions_last_month_commentors,\n        discussions_authors,\n    ) = get_discussions_experts(settings=settings)\n    # commentors = issues_commentors + discussions_commentors\n    commentors = discussions_commentors\n    # last_month_commentors = (\n    #     issues_last_month_commentors + discussions_last_month_commentors\n    # )\n    last_month_commentors = discussions_last_month_commentors\n    # authors = {**issues_authors, **discussions_authors}\n    authors = {**discussions_authors}\n    return commentors, last_month_commentors, authors", "metadata": {"license": "MIT", "len_tokens": 251}}
{"id": "pydantic:.github/actions/people/people.py", "language": "python", "code": "def get_contributors(settings: Settings) -> tuple[Counter, Counter, Counter, dict[str, Author]]:\n    \"\"\"Analyze pull requests to identify contributors, commentors, and reviewers.\n\n    Args:\n        settings: Configuration settings\n\n    Returns:\n        A tuple containing:\n            - Counter of contributors (merged PRs)\n            - Counter of commentors\n            - Counter of reviewers\n            - Dictionary mapping usernames to Author objects\n    \"\"\"\n    pr_nodes: list[PullRequestNode] = []\n    pr_edges = get_graphql_pr_edges(settings=settings)\n\n    while pr_edges:\n        pr_nodes.extend(edge.node for edge in pr_edges)\n        last_edge = pr_edges[-1]\n        pr_edges = get_graphql_pr_edges(settings=settings, after=last_edge.cursor)\n\n    contributors = Counter()\n    commentors = Counter()\n    reviewers = Counter()\n    authors: dict[str, Author] = {}\n\n    for pr in pr_nodes:\n        author_name = None\n        if pr.author:\n            authors[pr.author.login] = pr.author\n            author_name = pr.author.login\n        pr_commentors: set[str] = set()\n        pr_reviewers: set[str] = set()\n        for comment in pr.comments.nodes:\n            if comment.author:\n                authors[comment.author.login] = comment.author\n                if comment.author.login == author_name:\n                    continue\n                pr_commentors.add(comment.author.login)\n        for author_name in pr_commentors:\n            commentors[author_name] += 1\n        for review in pr.reviews.nodes:\n            if review.author:\n                authors[review.author.login] = review.author\n                pr_reviewers.add(review.author.login)\n        for reviewer in pr_reviewers:\n            reviewers[reviewer] += 1\n        if pr.state == 'MERGED' and pr.author:\n            contributors[pr.author.login] += 1\n    return contributors, commentors, reviewers, authors", "metadata": {"license": "MIT", "len_tokens": 392}}
{"id": "pydantic:.github/actions/people/people.py", "language": "python", "code": "def get_top_users(\n    *,\n    counter: Counter,\n    min_count: int,\n    authors: dict[str, Author],\n    skip_users: Container[str],\n) -> list[dict[str, Any]]:\n    \"\"\"Get top users based on their contribution counts.\n\n    Args:\n        counter: Counter with user contribution counts\n        min_count: Minimum count to be included in results\n        authors: Dictionary mapping usernames to Author objects\n        skip_users: Container of usernames to exclude from results\n\n    Returns:\n        List of dictionaries containing:\n            - login: Username\n            - count: Number of contributions\n            - avatarUrl: URL to user's avatar\n            - url: URL to user's GitHub profile\n    \"\"\"\n    users: list[dict[str, Any]] = []\n    for commentor, count in counter.most_common(50):\n        if commentor in skip_users:\n            continue\n        if count >= min_count:\n            author = authors[commentor]\n            users.append(\n                {\n                    'login': commentor,\n                    'count': count,\n                    'avatarUrl': author.avatarUrl,\n                    'url': author.url,\n                }\n            )\n    return users", "metadata": {"license": "MIT", "len_tokens": 239}}
{"id": "pydantic:pydantic-core/.github/check_version.py", "language": "python", "code": "#!/usr/bin/env python3\n\"\"\"\nCheck the version in Cargo.toml matches the version from `GITHUB_REF` environment variable.\n\"\"\"\n\nimport os\nimport re\nimport sys\nfrom pathlib import Path\n\n\ndef main() -> int:\n    cargo_path = Path('Cargo.toml')\n    if not cargo_path.is_file():\n        print(f' path \"{cargo_path}\" does not exist')\n        return 1\n\n    version_ref = os.getenv('GITHUB_REF')\n    if version_ref:\n        version = re.sub('^refs/tags/v*', '', version_ref.lower())\n    else:\n        print(' \"GITHUB_REF\" env variables not found')\n        return 1\n\n    # convert from python pre-release version to rust pre-release version\n    # this is the reverse of what's done in lib.rs::_rust_notify\n    version = version.replace('a', '-alpha').replace('b', '-beta')\n\n    version_regex = re.compile(r\"\"\"^version ?= ?([\"'])(.+)\\1\"\"\", re.M)\n    cargo_content = cargo_path.read_text()\n    match = version_regex.search(cargo_content)\n    if not match:\n        print(f' {version_regex!r} not found in {cargo_path}')\n        return 1\n\n    cargo_version = match.group(2)\n    if cargo_version == version:\n        print(f' GITHUB_REF version matches {cargo_path} version \"{cargo_version}\"')\n        return 0\n    else:\n        print(f' GITHUB_REF version \"{version}\" does not match {cargo_path} version \"{cargo_version}\"')\n        return 1\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n", "metadata": {"license": "MIT", "len_tokens": 351}}
{"id": "pydantic:pydantic-core/.github/check_version.py", "language": "python", "code": "def main() -> int:\n    cargo_path = Path('Cargo.toml')\n    if not cargo_path.is_file():\n        print(f' path \"{cargo_path}\" does not exist')\n        return 1\n\n    version_ref = os.getenv('GITHUB_REF')\n    if version_ref:\n        version = re.sub('^refs/tags/v*', '', version_ref.lower())\n    else:\n        print(' \"GITHUB_REF\" env variables not found')\n        return 1\n\n    # convert from python pre-release version to rust pre-release version\n    # this is the reverse of what's done in lib.rs::_rust_notify\n    version = version.replace('a', '-alpha').replace('b', '-beta')\n\n    version_regex = re.compile(r\"\"\"^version ?= ?([\"'])(.+)\\1\"\"\", re.M)\n    cargo_content = cargo_path.read_text()\n    match = version_regex.search(cargo_content)\n    if not match:\n        print(f' {version_regex!r} not found in {cargo_path}')\n        return 1\n\n    cargo_version = match.group(2)\n    if cargo_version == version:\n        print(f' GITHUB_REF version matches {cargo_path} version \"{cargo_version}\"')\n        return 0\n    else:\n        print(f' GITHUB_REF version \"{version}\" does not match {cargo_path} version \"{cargo_version}\"')\n        return 1", "metadata": {"license": "MIT", "len_tokens": 295}}
{"id": "pydantic:pydantic-core/python/pydantic_core/__init__.py", "language": "python", "code": "class ErrorDetails(_TypedDict):\n    type: str\n    \"\"\"\n    The type of error that occurred, this is an identifier designed for\n    programmatic use that will change rarely or never.\n\n    `type` is unique for each error message, and can hence be used as an identifier to build custom error messages.\n    \"\"\"\n    loc: tuple[int | str, ...]\n    \"\"\"Tuple of strings and ints identifying where in the schema the error occurred.\"\"\"\n    msg: str\n    \"\"\"A human readable error message.\"\"\"\n    input: _Any\n    \"\"\"The input data at this `loc` that caused the error.\"\"\"\n    ctx: _NotRequired[dict[str, _Any]]\n    \"\"\"\n    Values which are required to render the error message, and could hence be useful in rendering custom error messages.\n    Also useful for passing custom error data forward.\n    \"\"\"\n    url: _NotRequired[str]\n    \"\"\"\n    The documentation URL giving information about the error. No URL is available if\n    a [`PydanticCustomError`][pydantic_core.PydanticCustomError] is used.\n    \"\"\"", "metadata": {"license": "MIT", "len_tokens": 227}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "class SerializationInfo(Protocol[ContextT]):\n    \"\"\"Extra data used during serialization.\"\"\"\n\n    @property\n    def include(self) -> IncExCall:\n        \"\"\"The `include` argument set during serialization.\"\"\"\n        ...\n\n    @property\n    def exclude(self) -> IncExCall:\n        \"\"\"The `exclude` argument set during serialization.\"\"\"\n        ...\n\n    @property\n    def context(self) -> ContextT:\n        \"\"\"The current serialization context.\"\"\"\n        ...\n\n    @property\n    def mode(self) -> Literal['python', 'json'] | str:\n        \"\"\"The serialization mode set during serialization.\"\"\"\n        ...\n\n    @property\n    def by_alias(self) -> bool:\n        \"\"\"The `by_alias` argument set during serialization.\"\"\"\n        ...\n\n    @property\n    def exclude_unset(self) -> bool:\n        \"\"\"The `exclude_unset` argument set during serialization.\"\"\"\n        ...\n\n    @property\n    def exclude_defaults(self) -> bool:\n        \"\"\"The `exclude_defaults` argument set during serialization.\"\"\"\n        ...\n\n    @property\n    def exclude_none(self) -> bool:\n        \"\"\"The `exclude_none` argument set during serialization.\"\"\"\n        ...\n\n    @property\n    def exclude_computed_fields(self) -> bool:\n        \"\"\"The `exclude_computed_fields` argument set during serialization.\"\"\"\n        ...\n\n    @property\n    def serialize_as_any(self) -> bool:\n        \"\"\"The `serialize_as_any` argument set during serialization.\"\"\"\n        ...\n\n    @property\n    def round_trip(self) -> bool:\n        \"\"\"The `round_trip` argument set during serialization.\"\"\"\n        ...\n\n    def mode_is_json(self) -> bool: ...\n\n    def __str__(self) -> str: ...\n\n    def __repr__(self) -> str: ...", "metadata": {"license": "MIT", "len_tokens": 354}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def plain_serializer_function_ser_schema(\n    function: SerializerFunction,\n    *,\n    is_field_serializer: bool | None = None,\n    info_arg: bool | None = None,\n    return_schema: CoreSchema | None = None,\n    when_used: WhenUsed = 'always',\n) -> PlainSerializerFunctionSerSchema:\n    \"\"\"\n    Returns a schema for serialization with a function, can be either a \"general\" or \"field\" function.\n\n    Args:\n        function: The function to use for serialization\n        is_field_serializer: Whether the serializer is for a field, e.g. takes `model` as the first argument,\n            and `info` includes `field_name`\n        info_arg: Whether the function takes an `info` argument\n        return_schema: Schema to use for serializing return value\n        when_used: When the function should be called\n    \"\"\"\n    if when_used == 'always':\n        # just to avoid extra elements in schema, and to use the actual default defined in rust\n        when_used = None  # type: ignore\n    return _dict_not_none(\n        type='function-plain',\n        function=function,\n        is_field_serializer=is_field_serializer,\n        info_arg=info_arg,\n        return_schema=return_schema,\n        when_used=when_used,\n    )", "metadata": {"license": "MIT", "len_tokens": 266}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def wrap_serializer_function_ser_schema(\n    function: WrapSerializerFunction,\n    *,\n    is_field_serializer: bool | None = None,\n    info_arg: bool | None = None,\n    schema: CoreSchema | None = None,\n    return_schema: CoreSchema | None = None,\n    when_used: WhenUsed = 'always',\n) -> WrapSerializerFunctionSerSchema:\n    \"\"\"\n    Returns a schema for serialization with a wrap function, can be either a \"general\" or \"field\" function.\n\n    Args:\n        function: The function to use for serialization\n        is_field_serializer: Whether the serializer is for a field, e.g. takes `model` as the first argument,\n            and `info` includes `field_name`\n        info_arg: Whether the function takes an `info` argument\n        schema: The schema to use for the inner serialization\n        return_schema: Schema to use for serializing return value\n        when_used: When the function should be called\n    \"\"\"\n    if when_used == 'always':\n        # just to avoid extra elements in schema, and to use the actual default defined in rust\n        when_used = None  # type: ignore\n    return _dict_not_none(\n        type='function-wrap',\n        function=function,\n        is_field_serializer=is_field_serializer,\n        info_arg=info_arg,\n        schema=schema,\n        return_schema=return_schema,\n        when_used=when_used,\n    )", "metadata": {"license": "MIT", "len_tokens": 294}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def bool_schema(\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> BoolSchema:\n    \"\"\"\n    Returns a schema that matches a bool value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.bool_schema()\n    v = SchemaValidator(schema)\n    assert v.validate_python('True') is True\n    ```\n\n    Args:\n        strict: Whether the value should be a bool or a value that can be converted to a bool\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(type='bool', strict=strict, ref=ref, metadata=metadata, serialization=serialization)", "metadata": {"license": "MIT", "len_tokens": 211}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def int_schema(\n    *,\n    multiple_of: int | None = None,\n    le: int | None = None,\n    ge: int | None = None,\n    lt: int | None = None,\n    gt: int | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> IntSchema:\n    \"\"\"\n    Returns a schema that matches a int value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.int_schema(multiple_of=2, le=6, ge=2)\n    v = SchemaValidator(schema)\n    assert v.validate_python('4') == 4\n    ```\n\n    Args:\n        multiple_of: The value must be a multiple of this number\n        le: The value must be less than or equal to this number\n        ge: The value must be greater than or equal to this number\n        lt: The value must be strictly less than this number\n        gt: The value must be strictly greater than this number\n        strict: Whether the value should be a int or a value that can be converted to a int\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='int',\n        multiple_of=multiple_of,\n        le=le,\n        ge=ge,\n        lt=lt,\n        gt=gt,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 378}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def float_schema(\n    *,\n    allow_inf_nan: bool | None = None,\n    multiple_of: float | None = None,\n    le: float | None = None,\n    ge: float | None = None,\n    lt: float | None = None,\n    gt: float | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> FloatSchema:\n    \"\"\"\n    Returns a schema that matches a float value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.float_schema(le=0.8, ge=0.2)\n    v = SchemaValidator(schema)\n    assert v.validate_python('0.5') == 0.5\n    ```\n\n    Args:\n        allow_inf_nan: Whether to allow inf and nan values\n        multiple_of: The value must be a multiple of this number\n        le: The value must be less than or equal to this number\n        ge: The value must be greater than or equal to this number\n        lt: The value must be strictly less than this number\n        gt: The value must be strictly greater than this number\n        strict: Whether the value should be a float or a value that can be converted to a float\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='float',\n        allow_inf_nan=allow_inf_nan,\n        multiple_of=multiple_of,\n        le=le,\n        ge=ge,\n        lt=lt,\n        gt=gt,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 413}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def decimal_schema(\n    *,\n    allow_inf_nan: bool | None = None,\n    multiple_of: Decimal | None = None,\n    le: Decimal | None = None,\n    ge: Decimal | None = None,\n    lt: Decimal | None = None,\n    gt: Decimal | None = None,\n    max_digits: int | None = None,\n    decimal_places: int | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> DecimalSchema:\n    \"\"\"\n    Returns a schema that matches a decimal value, e.g.:\n\n    ```py\n    from decimal import Decimal\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.decimal_schema(le=0.8, ge=0.2)\n    v = SchemaValidator(schema)\n    assert v.validate_python('0.5') == Decimal('0.5')\n    ```\n\n    Args:\n        allow_inf_nan: Whether to allow inf and nan values\n        multiple_of: The value must be a multiple of this number\n        le: The value must be less than or equal to this number\n        ge: The value must be greater than or equal to this number\n        lt: The value must be strictly less than this number\n        gt: The value must be strictly greater than this number\n        max_digits: The maximum number of decimal digits allowed\n        decimal_places: The maximum number of decimal places allowed\n        strict: Whether the value should be a float or a value that can be converted to a float\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='decimal',\n        gt=gt,\n        ge=ge,\n        lt=lt,\n        le=le,\n        max_digits=max_digits,\n        decimal_places=decimal_places,\n        multiple_of=multiple_of,\n        allow_inf_nan=allow_inf_nan,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 478}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def complex_schema(\n    *,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> ComplexSchema:\n    \"\"\"\n    Returns a schema that matches a complex value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.complex_schema()\n    v = SchemaValidator(schema)\n    assert v.validate_python('1+2j') == complex(1, 2)\n    assert v.validate_python(complex(1, 2)) == complex(1, 2)\n    ```\n\n    Args:\n        strict: Whether the value should be a complex object instance or a value that can be converted to a complex object\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='complex',\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 254}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def str_schema(\n    *,\n    pattern: str | Pattern[str] | None = None,\n    max_length: int | None = None,\n    min_length: int | None = None,\n    strip_whitespace: bool | None = None,\n    to_lower: bool | None = None,\n    to_upper: bool | None = None,\n    regex_engine: Literal['rust-regex', 'python-re'] | None = None,\n    strict: bool | None = None,\n    coerce_numbers_to_str: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> StringSchema:\n    \"\"\"\n    Returns a schema that matches a string value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.str_schema(max_length=10, min_length=2)\n    v = SchemaValidator(schema)\n    assert v.validate_python('hello') == 'hello'\n    ```\n\n    Args:\n        pattern: A regex pattern that the value must match\n        max_length: The value must be at most this length\n        min_length: The value must be at least this length\n        strip_whitespace: Whether to strip whitespace from the value\n        to_lower: Whether to convert the value to lowercase\n        to_upper: Whether to convert the value to uppercase\n        regex_engine: The regex engine to use for pattern validation. Default is 'rust-regex'.\n            - `rust-regex` uses the [`regex`](https://docs.rs/regex) Rust\n              crate, which is non-backtracking and therefore more DDoS\n              resistant, but does not support all regex features.\n            - `python-re` use the [`re`](https://docs.python.org/3/library/re.html) module,\n              which supports all regex features, but may be slower.\n        strict: Whether the value should be a string or a value that can be converted to a string\n        coerce_numbers_to_str: Whether to enable coercion of any `Number` type to `str` (not applicable in `strict` mode).\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='str',\n        pattern=pattern,\n        max_length=max_length,\n        min_length=min_length,\n        strip_whitespace=strip_whitespace,\n        to_lower=to_lower,\n        to_upper=to_upper,\n        regex_engine=regex_engine,\n        strict=strict,\n        coerce_numbers_to_str=coerce_numbers_to_str,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 596}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def bytes_schema(\n    *,\n    max_length: int | None = None,\n    min_length: int | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> BytesSchema:\n    \"\"\"\n    Returns a schema that matches a bytes value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.bytes_schema(max_length=10, min_length=2)\n    v = SchemaValidator(schema)\n    assert v.validate_python(b'hello') == b'hello'\n    ```\n\n    Args:\n        max_length: The value must be at most this length\n        min_length: The value must be at least this length\n        strict: Whether the value should be a bytes or a value that can be converted to a bytes\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='bytes',\n        max_length=max_length,\n        min_length=min_length,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 291}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def date_schema(\n    *,\n    strict: bool | None = None,\n    le: date | None = None,\n    ge: date | None = None,\n    lt: date | None = None,\n    gt: date | None = None,\n    now_op: Literal['past', 'future'] | None = None,\n    now_utc_offset: int | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> DateSchema:\n    \"\"\"\n    Returns a schema that matches a date value, e.g.:\n\n    ```py\n    from datetime import date\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.date_schema(le=date(2020, 1, 1), ge=date(2019, 1, 1))\n    v = SchemaValidator(schema)\n    assert v.validate_python(date(2019, 6, 1)) == date(2019, 6, 1)\n    ```\n\n    Args:\n        strict: Whether the value should be a date or a value that can be converted to a date\n        le: The value must be less than or equal to this date\n        ge: The value must be greater than or equal to this date\n        lt: The value must be strictly less than this date\n        gt: The value must be strictly greater than this date\n        now_op: The value must be in the past or future relative to the current date\n        now_utc_offset: The value must be in the past or future relative to the current date with this utc offset\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='date',\n        strict=strict,\n        le=le,\n        ge=ge,\n        lt=lt,\n        gt=gt,\n        now_op=now_op,\n        now_utc_offset=now_utc_offset,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 469}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def time_schema(\n    *,\n    strict: bool | None = None,\n    le: time | None = None,\n    ge: time | None = None,\n    lt: time | None = None,\n    gt: time | None = None,\n    tz_constraint: Literal['aware', 'naive'] | int | None = None,\n    microseconds_precision: Literal['truncate', 'error'] = 'truncate',\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> TimeSchema:\n    \"\"\"\n    Returns a schema that matches a time value, e.g.:\n\n    ```py\n    from datetime import time\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.time_schema(le=time(12, 0, 0), ge=time(6, 0, 0))\n    v = SchemaValidator(schema)\n    assert v.validate_python(time(9, 0, 0)) == time(9, 0, 0)\n    ```\n\n    Args:\n        strict: Whether the value should be a time or a value that can be converted to a time\n        le: The value must be less than or equal to this time\n        ge: The value must be greater than or equal to this time\n        lt: The value must be strictly less than this time\n        gt: The value must be strictly greater than this time\n        tz_constraint: The value must be timezone aware or naive, or an int to indicate required tz offset\n        microseconds_precision: The behavior when seconds have more than 6 digits or microseconds is too large\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='time',\n        strict=strict,\n        le=le,\n        ge=ge,\n        lt=lt,\n        gt=gt,\n        tz_constraint=tz_constraint,\n        microseconds_precision=microseconds_precision,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 470}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def datetime_schema(\n    *,\n    strict: bool | None = None,\n    le: datetime | None = None,\n    ge: datetime | None = None,\n    lt: datetime | None = None,\n    gt: datetime | None = None,\n    now_op: Literal['past', 'future'] | None = None,\n    tz_constraint: Literal['aware', 'naive'] | int | None = None,\n    now_utc_offset: int | None = None,\n    microseconds_precision: Literal['truncate', 'error'] = 'truncate',\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> DatetimeSchema:\n    \"\"\"\n    Returns a schema that matches a datetime value, e.g.:\n\n    ```py\n    from datetime import datetime\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.datetime_schema()\n    v = SchemaValidator(schema)\n    now = datetime.now()\n    assert v.validate_python(str(now)) == now\n    ```\n\n    Args:\n        strict: Whether the value should be a datetime or a value that can be converted to a datetime\n        le: The value must be less than or equal to this datetime\n        ge: The value must be greater than or equal to this datetime\n        lt: The value must be strictly less than this datetime\n        gt: The value must be strictly greater than this datetime\n        now_op: The value must be in the past or future relative to the current datetime\n        tz_constraint: The value must be timezone aware or naive, or an int to indicate required tz offset\n            TODO: use of a tzinfo where offset changes based on the datetime is not yet supported\n        now_utc_offset: The value must be in the past or future relative to the current datetime with this utc offset\n        microseconds_precision: The behavior when seconds have more than 6 digits or microseconds is too large\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='datetime',\n        strict=strict,\n        le=le,\n        ge=ge,\n        lt=lt,\n        gt=gt,\n        now_op=now_op,\n        tz_constraint=tz_constraint,\n        now_utc_offset=now_utc_offset,\n        microseconds_precision=microseconds_precision,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 551}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def timedelta_schema(\n    *,\n    strict: bool | None = None,\n    le: timedelta | None = None,\n    ge: timedelta | None = None,\n    lt: timedelta | None = None,\n    gt: timedelta | None = None,\n    microseconds_precision: Literal['truncate', 'error'] = 'truncate',\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> TimedeltaSchema:\n    \"\"\"\n    Returns a schema that matches a timedelta value, e.g.:\n\n    ```py\n    from datetime import timedelta\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.timedelta_schema(le=timedelta(days=1), ge=timedelta(days=0))\n    v = SchemaValidator(schema)\n    assert v.validate_python(timedelta(hours=12)) == timedelta(hours=12)\n    ```\n\n    Args:\n        strict: Whether the value should be a timedelta or a value that can be converted to a timedelta\n        le: The value must be less than or equal to this timedelta\n        ge: The value must be greater than or equal to this timedelta\n        lt: The value must be strictly less than this timedelta\n        gt: The value must be strictly greater than this timedelta\n        microseconds_precision: The behavior when seconds have more than 6 digits or microseconds is too large\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='timedelta',\n        strict=strict,\n        le=le,\n        ge=ge,\n        lt=lt,\n        gt=gt,\n        microseconds_precision=microseconds_precision,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 413}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def literal_schema(\n    expected: list[Any],\n    *,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> LiteralSchema:\n    \"\"\"\n    Returns a schema that matches a literal value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.literal_schema(['hello', 'world'])\n    v = SchemaValidator(schema)\n    assert v.validate_python('hello') == 'hello'\n    ```\n\n    Args:\n        expected: The value must be one of these values\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(type='literal', expected=expected, ref=ref, metadata=metadata, serialization=serialization)", "metadata": {"license": "MIT", "len_tokens": 207}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def enum_schema(\n    cls: Any,\n    members: list[Any],\n    *,\n    sub_type: Literal['str', 'int', 'float'] | None = None,\n    missing: Callable[[Any], Any] | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> EnumSchema:\n    \"\"\"\n    Returns a schema that matches an enum value, e.g.:\n\n    ```py\n    from enum import Enum\n    from pydantic_core import SchemaValidator, core_schema\n\n    class Color(Enum):\n        RED = 1\n        GREEN = 2\n        BLUE = 3\n\n    schema = core_schema.enum_schema(Color, list(Color.__members__.values()))\n    v = SchemaValidator(schema)\n    assert v.validate_python(2) is Color.GREEN\n    ```\n\n    Args:\n        cls: The enum class\n        members: The members of the enum, generally `list(MyEnum.__members__.values())`\n        sub_type: The type of the enum, either 'str' or 'int' or None for plain enums\n        missing: A function to use when the value is not found in the enum, from `_missing_`\n        strict: Whether to use strict mode, defaults to False\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='enum',\n        cls=cls,\n        members=members,\n        sub_type=sub_type,\n        missing=missing,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 389}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def is_instance_schema(\n    cls: Any,\n    *,\n    cls_repr: str | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> IsInstanceSchema:\n    \"\"\"\n    Returns a schema that checks if a value is an instance of a class, equivalent to python's `isinstance` method, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    class A:\n        pass\n\n    schema = core_schema.is_instance_schema(cls=A)\n    v = SchemaValidator(schema)\n    v.validate_python(A())\n    ```\n\n    Args:\n        cls: The value must be an instance of this class\n        cls_repr: If provided this string is used in the validator name instead of `repr(cls)`\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='is-instance', cls=cls, cls_repr=cls_repr, ref=ref, metadata=metadata, serialization=serialization\n    )", "metadata": {"license": "MIT", "len_tokens": 265}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def is_subclass_schema(\n    cls: type[Any],\n    *,\n    cls_repr: str | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> IsInstanceSchema:\n    \"\"\"\n    Returns a schema that checks if a value is a subtype of a class, equivalent to python's `issubclass` method, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    schema = core_schema.is_subclass_schema(cls=A)\n    v = SchemaValidator(schema)\n    v.validate_python(B)\n    ```\n\n    Args:\n        cls: The value must be a subclass of this class\n        cls_repr: If provided this string is used in the validator name instead of `repr(cls)`\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='is-subclass', cls=cls, cls_repr=cls_repr, ref=ref, metadata=metadata, serialization=serialization\n    )", "metadata": {"license": "MIT", "len_tokens": 278}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def list_schema(\n    items_schema: CoreSchema | None = None,\n    *,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    fail_fast: bool | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: IncExSeqOrElseSerSchema | None = None,\n) -> ListSchema:\n    \"\"\"\n    Returns a schema that matches a list value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.list_schema(core_schema.int_schema(), min_length=0, max_length=10)\n    v = SchemaValidator(schema)\n    assert v.validate_python(['4']) == [4]\n    ```\n\n    Args:\n        items_schema: The value must be a list of items that match this schema\n        min_length: The value must be a list with at least this many items\n        max_length: The value must be a list with at most this many items\n        fail_fast: Stop validation on the first error\n        strict: The value must be a list with exactly this many items\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='list',\n        items_schema=items_schema,\n        min_length=min_length,\n        max_length=max_length,\n        fail_fast=fail_fast,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 364}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def tuple_positional_schema(\n    items_schema: list[CoreSchema],\n    *,\n    extras_schema: CoreSchema | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: IncExSeqOrElseSerSchema | None = None,\n) -> TupleSchema:\n    \"\"\"\n    Returns a schema that matches a tuple of schemas, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.tuple_positional_schema(\n        [core_schema.int_schema(), core_schema.str_schema()]\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python((1, 'hello')) == (1, 'hello')\n    ```\n\n    Args:\n        items_schema: The value must be a tuple with items that match these schemas\n        extras_schema: The value must be a tuple with items that match this schema\n            This was inspired by JSON schema's `prefixItems` and `items` fields.\n            In python's `typing.Tuple`, you can't specify a type for \"extra\" items -- they must all be the same type\n            if the length is variable. So this field won't be set from a `typing.Tuple` annotation on a pydantic model.\n        strict: The value must be a tuple with exactly this many items\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    if extras_schema is not None:\n        variadic_item_index = len(items_schema)\n        items_schema = items_schema + [extras_schema]\n    else:\n        variadic_item_index = None\n    return tuple_schema(\n        items_schema=items_schema,\n        variadic_item_index=variadic_item_index,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 424}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def tuple_variable_schema(\n    items_schema: CoreSchema | None = None,\n    *,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: IncExSeqOrElseSerSchema | None = None,\n) -> TupleSchema:\n    \"\"\"\n    Returns a schema that matches a tuple of a given schema, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.tuple_variable_schema(\n        items_schema=core_schema.int_schema(), min_length=0, max_length=10\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python(('1', 2, 3)) == (1, 2, 3)\n    ```\n\n    Args:\n        items_schema: The value must be a tuple with items that match this schema\n        min_length: The value must be a tuple with at least this many items\n        max_length: The value must be a tuple with at most this many items\n        strict: The value must be a tuple with exactly this many items\n        ref: Optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return tuple_schema(\n        items_schema=[items_schema or any_schema()],\n        variadic_item_index=0,\n        min_length=min_length,\n        max_length=max_length,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 365}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def tuple_schema(\n    items_schema: list[CoreSchema],\n    *,\n    variadic_item_index: int | None = None,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    fail_fast: bool | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: IncExSeqOrElseSerSchema | None = None,\n) -> TupleSchema:\n    \"\"\"\n    Returns a schema that matches a tuple of schemas, with an optional variadic item, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.tuple_schema(\n        [core_schema.int_schema(), core_schema.str_schema(), core_schema.float_schema()],\n        variadic_item_index=1,\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python((1, 'hello', 'world', 1.5)) == (1, 'hello', 'world', 1.5)\n    ```\n\n    Args:\n        items_schema: The value must be a tuple with items that match these schemas\n        variadic_item_index: The index of the schema in `items_schema` to be treated as variadic (following PEP 646)\n        min_length: The value must be a tuple with at least this many items\n        max_length: The value must be a tuple with at most this many items\n        fail_fast: Stop validation on the first error\n        strict: The value must be a tuple with exactly this many items\n        ref: Optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='tuple',\n        items_schema=items_schema,\n        variadic_item_index=variadic_item_index,\n        min_length=min_length,\n        max_length=max_length,\n        fail_fast=fail_fast,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 458}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def set_schema(\n    items_schema: CoreSchema | None = None,\n    *,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    fail_fast: bool | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> SetSchema:\n    \"\"\"\n    Returns a schema that matches a set of a given schema, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.set_schema(\n        items_schema=core_schema.int_schema(), min_length=0, max_length=10\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python({1, '2', 3}) == {1, 2, 3}\n    ```\n\n    Args:\n        items_schema: The value must be a set with items that match this schema\n        min_length: The value must be a set with at least this many items\n        max_length: The value must be a set with at most this many items\n        fail_fast: Stop validation on the first error\n        strict: The value must be a set with exactly this many items\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='set',\n        items_schema=items_schema,\n        min_length=min_length,\n        max_length=max_length,\n        fail_fast=fail_fast,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 381}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def frozenset_schema(\n    items_schema: CoreSchema | None = None,\n    *,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    fail_fast: bool | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> FrozenSetSchema:\n    \"\"\"\n    Returns a schema that matches a frozenset of a given schema, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.frozenset_schema(\n        items_schema=core_schema.int_schema(), min_length=0, max_length=10\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python(frozenset(range(3))) == frozenset({0, 1, 2})\n    ```\n\n    Args:\n        items_schema: The value must be a frozenset with items that match this schema\n        min_length: The value must be a frozenset with at least this many items\n        max_length: The value must be a frozenset with at most this many items\n        fail_fast: Stop validation on the first error\n        strict: The value must be a frozenset with exactly this many items\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='frozenset',\n        items_schema=items_schema,\n        min_length=min_length,\n        max_length=max_length,\n        fail_fast=fail_fast,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 401}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def generator_schema(\n    items_schema: CoreSchema | None = None,\n    *,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: IncExSeqOrElseSerSchema | None = None,\n) -> GeneratorSchema:\n    \"\"\"\n    Returns a schema that matches a generator value, e.g.:\n\n    ```py\n    from typing import Iterator\n    from pydantic_core import SchemaValidator, core_schema\n\n    def gen() -> Iterator[int]:\n        yield 1\n\n    schema = core_schema.generator_schema(items_schema=core_schema.int_schema())\n    v = SchemaValidator(schema)\n    v.validate_python(gen())\n    ```\n\n    Unlike other types, validated generators do not raise ValidationErrors eagerly,\n    but instead will raise a ValidationError when a violating value is actually read from the generator.\n    This is to ensure that \"validated\" generators retain the benefit of lazy evaluation.\n\n    Args:\n        items_schema: The value must be a generator with items that match this schema\n        min_length: The value must be a generator that yields at least this many items\n        max_length: The value must be a generator that yields at most this many items\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='generator',\n        items_schema=items_schema,\n        min_length=min_length,\n        max_length=max_length,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 365}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def dict_schema(\n    keys_schema: CoreSchema | None = None,\n    values_schema: CoreSchema | None = None,\n    *,\n    min_length: int | None = None,\n    max_length: int | None = None,\n    fail_fast: bool | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> DictSchema:\n    \"\"\"\n    Returns a schema that matches a dict value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.dict_schema(\n        keys_schema=core_schema.str_schema(), values_schema=core_schema.int_schema()\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python({'a': '1', 'b': 2}) == {'a': 1, 'b': 2}\n    ```\n\n    Args:\n        keys_schema: The value must be a dict with keys that match this schema\n        values_schema: The value must be a dict with values that match this schema\n        min_length: The value must be a dict with at least this many items\n        max_length: The value must be a dict with at most this many items\n        fail_fast: Stop validation on the first error\n        strict: Whether the keys and values should be validated with strict mode\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='dict',\n        keys_schema=keys_schema,\n        values_schema=values_schema,\n        min_length=min_length,\n        max_length=max_length,\n        fail_fast=fail_fast,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 417}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def no_info_before_validator_function(\n    function: NoInfoValidatorFunction,\n    schema: CoreSchema,\n    *,\n    ref: str | None = None,\n    json_schema_input_schema: CoreSchema | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> BeforeValidatorFunctionSchema:\n    \"\"\"\n    Returns a schema that calls a validator function before validating, no `info` argument is provided, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    def fn(v: bytes) -> str:\n        return v.decode() + 'world'\n\n    func_schema = core_schema.no_info_before_validator_function(\n        function=fn, schema=core_schema.str_schema()\n    )\n    schema = core_schema.typed_dict_schema({'a': core_schema.typed_dict_field(func_schema)})\n\n    v = SchemaValidator(schema)\n    assert v.validate_python({'a': b'hello '}) == {'a': 'hello world'}\n    ```\n\n    Args:\n        function: The validator function to call\n        schema: The schema to validate the output of the validator function\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='function-before',\n        function={'type': 'no-info', 'function': function},\n        schema=schema,\n        ref=ref,\n        json_schema_input_schema=json_schema_input_schema,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 371}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def with_info_before_validator_function(\n    function: WithInfoValidatorFunction,\n    schema: CoreSchema,\n    *,\n    field_name: str | None = None,\n    ref: str | None = None,\n    json_schema_input_schema: CoreSchema | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> BeforeValidatorFunctionSchema:\n    \"\"\"\n    Returns a schema that calls a validator function before validation, the function is called with\n    an `info` argument, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    def fn(v: bytes, info: core_schema.ValidationInfo) -> str:\n        assert info.data is not None\n        assert info.field_name is not None\n        return v.decode() + 'world'\n\n    func_schema = core_schema.with_info_before_validator_function(\n        function=fn, schema=core_schema.str_schema()\n    )\n    schema = core_schema.typed_dict_schema({'a': core_schema.typed_dict_field(func_schema)})\n\n    v = SchemaValidator(schema)\n    assert v.validate_python({'a': b'hello '}) == {'a': 'hello world'}\n    ```\n\n    Args:\n        function: The validator function to call\n        field_name: The name of the field this validator is applied to, if any (deprecated)\n        schema: The schema to validate the output of the validator function\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    if field_name is not None:\n        warnings.warn(\n            'The `field_name` argument on `with_info_before_validator_function` is deprecated, it will be passed to the function through `ValidationState` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    return _dict_not_none(\n        type='function-before',\n        function=_dict_not_none(type='with-info', function=function, field_name=field_name),\n        schema=schema,\n        ref=ref,\n        json_schema_input_schema=json_schema_input_schema,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 495}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def no_info_after_validator_function(\n    function: NoInfoValidatorFunction,\n    schema: CoreSchema,\n    *,\n    ref: str | None = None,\n    json_schema_input_schema: CoreSchema | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> AfterValidatorFunctionSchema:\n    \"\"\"\n    Returns a schema that calls a validator function after validating, no `info` argument is provided, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    def fn(v: str) -> str:\n        return v + 'world'\n\n    func_schema = core_schema.no_info_after_validator_function(fn, core_schema.str_schema())\n    schema = core_schema.typed_dict_schema({'a': core_schema.typed_dict_field(func_schema)})\n\n    v = SchemaValidator(schema)\n    assert v.validate_python({'a': b'hello '}) == {'a': 'hello world'}\n    ```\n\n    Args:\n        function: The validator function to call after the schema is validated\n        schema: The schema to validate before the validator function\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='function-after',\n        function={'type': 'no-info', 'function': function},\n        schema=schema,\n        ref=ref,\n        json_schema_input_schema=json_schema_input_schema,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 364}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def with_info_after_validator_function(\n    function: WithInfoValidatorFunction,\n    schema: CoreSchema,\n    *,\n    field_name: str | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> AfterValidatorFunctionSchema:\n    \"\"\"\n    Returns a schema that calls a validator function after validation, the function is called with\n    an `info` argument, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    def fn(v: str, info: core_schema.ValidationInfo) -> str:\n        assert info.data is not None\n        assert info.field_name is not None\n        return v + 'world'\n\n    func_schema = core_schema.with_info_after_validator_function(\n        function=fn, schema=core_schema.str_schema()\n    )\n    schema = core_schema.typed_dict_schema({'a': core_schema.typed_dict_field(func_schema)})\n\n    v = SchemaValidator(schema)\n    assert v.validate_python({'a': b'hello '}) == {'a': 'hello world'}\n    ```\n\n    Args:\n        function: The validator function to call after the schema is validated\n        schema: The schema to validate before the validator function\n        field_name: The name of the field this validator is applied to, if any (deprecated)\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    if field_name is not None:\n        warnings.warn(\n            'The `field_name` argument on `with_info_after_validator_function` is deprecated, it will be passed to the function through `ValidationState` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    return _dict_not_none(\n        type='function-after',\n        function=_dict_not_none(type='with-info', function=function, field_name=field_name),\n        schema=schema,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 452}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def no_info_wrap_validator_function(\n    function: NoInfoWrapValidatorFunction,\n    schema: CoreSchema,\n    *,\n    ref: str | None = None,\n    json_schema_input_schema: CoreSchema | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> WrapValidatorFunctionSchema:\n    \"\"\"\n    Returns a schema which calls a function with a `validator` callable argument which can\n    optionally be used to call inner validation with the function logic, this is much like the\n    \"onion\" implementation of middleware in many popular web frameworks, no `info` argument is passed, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    def fn(\n        v: str,\n        validator: core_schema.ValidatorFunctionWrapHandler,\n    ) -> str:\n        return validator(input_value=v) + 'world'\n\n    schema = core_schema.no_info_wrap_validator_function(\n        function=fn, schema=core_schema.str_schema()\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python('hello ') == 'hello world'\n    ```\n\n    Args:\n        function: The validator function to call\n        schema: The schema to validate the output of the validator function\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='function-wrap',\n        function={'type': 'no-info', 'function': function},\n        schema=schema,\n        json_schema_input_schema=json_schema_input_schema,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 396}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def with_info_wrap_validator_function(\n    function: WithInfoWrapValidatorFunction,\n    schema: CoreSchema,\n    *,\n    field_name: str | None = None,\n    json_schema_input_schema: CoreSchema | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> WrapValidatorFunctionSchema:\n    \"\"\"\n    Returns a schema which calls a function with a `validator` callable argument which can\n    optionally be used to call inner validation with the function logic, this is much like the\n    \"onion\" implementation of middleware in many popular web frameworks, an `info` argument is also passed, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    def fn(\n        v: str,\n        validator: core_schema.ValidatorFunctionWrapHandler,\n        info: core_schema.ValidationInfo,\n    ) -> str:\n        return validator(input_value=v) + 'world'\n\n    schema = core_schema.with_info_wrap_validator_function(\n        function=fn, schema=core_schema.str_schema()\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python('hello ') == 'hello world'\n    ```\n\n    Args:\n        function: The validator function to call\n        schema: The schema to validate the output of the validator function\n        field_name: The name of the field this validator is applied to, if any (deprecated)\n        json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    if field_name is not None:\n        warnings.warn(\n            'The `field_name` argument on `with_info_wrap_validator_function` is deprecated, it will be passed to the function through `ValidationState` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    return _dict_not_none(\n        type='function-wrap',\n        function=_dict_not_none(type='with-info', function=function, field_name=field_name),\n        schema=schema,\n        json_schema_input_schema=json_schema_input_schema,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 500}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def no_info_plain_validator_function(\n    function: NoInfoValidatorFunction,\n    *,\n    ref: str | None = None,\n    json_schema_input_schema: CoreSchema | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> PlainValidatorFunctionSchema:\n    \"\"\"\n    Returns a schema that uses the provided function for validation, no `info` argument is passed, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    def fn(v: str) -> str:\n        assert 'hello' in v\n        return v + 'world'\n\n    schema = core_schema.no_info_plain_validator_function(function=fn)\n    v = SchemaValidator(schema)\n    assert v.validate_python('hello ') == 'hello world'\n    ```\n\n    Args:\n        function: The validator function to call\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='function-plain',\n        function={'type': 'no-info', 'function': function},\n        ref=ref,\n        json_schema_input_schema=json_schema_input_schema,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 311}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def with_info_plain_validator_function(\n    function: WithInfoValidatorFunction,\n    *,\n    field_name: str | None = None,\n    ref: str | None = None,\n    json_schema_input_schema: CoreSchema | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> PlainValidatorFunctionSchema:\n    \"\"\"\n    Returns a schema that uses the provided function for validation, an `info` argument is passed, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    def fn(v: str, info: core_schema.ValidationInfo) -> str:\n        assert 'hello' in v\n        return v + 'world'\n\n    schema = core_schema.with_info_plain_validator_function(function=fn)\n    v = SchemaValidator(schema)\n    assert v.validate_python('hello ') == 'hello world'\n    ```\n\n    Args:\n        function: The validator function to call\n        field_name: The name of the field this validator is applied to, if any (deprecated)\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    if field_name is not None:\n        warnings.warn(\n            'The `field_name` argument on `with_info_plain_validator_function` is deprecated, it will be passed to the function through `ValidationState` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    return _dict_not_none(\n        type='function-plain',\n        function=_dict_not_none(type='with-info', function=function, field_name=field_name),\n        ref=ref,\n        json_schema_input_schema=json_schema_input_schema,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 413}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def with_default_schema(\n    schema: CoreSchema,\n    *,\n    default: Any = PydanticUndefined,\n    default_factory: Union[Callable[[], Any], Callable[[dict[str, Any]], Any], None] = None,\n    default_factory_takes_data: bool | None = None,\n    on_error: Literal['raise', 'omit', 'default'] | None = None,\n    validate_default: bool | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> WithDefaultSchema:\n    \"\"\"\n    Returns a schema that adds a default value to the given schema, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.with_default_schema(core_schema.str_schema(), default='hello')\n    wrapper_schema = core_schema.typed_dict_schema(\n        {'a': core_schema.typed_dict_field(schema)}\n    )\n    v = SchemaValidator(wrapper_schema)\n    assert v.validate_python({}) == v.validate_python({'a': 'hello'})\n    ```\n\n    Args:\n        schema: The schema to add a default value to\n        default: The default value to use\n        default_factory: A callable that returns the default value to use\n        default_factory_takes_data: Whether the default factory takes a validated data argument\n        on_error: What to do if the schema validation fails. One of 'raise', 'omit', 'default'\n        validate_default: Whether the default value should be validated\n        strict: Whether the underlying schema should be validated with strict mode\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    s = _dict_not_none(\n        type='default',\n        schema=schema,\n        default_factory=default_factory,\n        default_factory_takes_data=default_factory_takes_data,\n        on_error=on_error,\n        validate_default=validate_default,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )\n    if default is not PydanticUndefined:\n        s['default'] = default\n    return s", "metadata": {"license": "MIT", "len_tokens": 493}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def nullable_schema(\n    schema: CoreSchema,\n    *,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> NullableSchema:\n    \"\"\"\n    Returns a schema that matches a nullable value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.nullable_schema(core_schema.str_schema())\n    v = SchemaValidator(schema)\n    assert v.validate_python(None) is None\n    ```\n\n    Args:\n        schema: The schema to wrap\n        strict: Whether the underlying schema should be validated with strict mode\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='nullable', schema=schema, strict=strict, ref=ref, metadata=metadata, serialization=serialization\n    )", "metadata": {"license": "MIT", "len_tokens": 232}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def union_schema(\n    choices: list[CoreSchema | tuple[CoreSchema, str]],\n    *,\n    auto_collapse: bool | None = None,\n    custom_error_type: str | None = None,\n    custom_error_message: str | None = None,\n    custom_error_context: dict[str, str | int] | None = None,\n    mode: Literal['smart', 'left_to_right'] | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> UnionSchema:\n    \"\"\"\n    Returns a schema that matches a union value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.union_schema([core_schema.str_schema(), core_schema.int_schema()])\n    v = SchemaValidator(schema)\n    assert v.validate_python('hello') == 'hello'\n    assert v.validate_python(1) == 1\n    ```\n\n    Args:\n        choices: The schemas to match. If a tuple, the second item is used as the label for the case.\n        auto_collapse: whether to automatically collapse unions with one element to the inner validator, default true\n        custom_error_type: The custom error type to use if the validation fails\n        custom_error_message: The custom error message to use if the validation fails\n        custom_error_context: The custom error context to use if the validation fails\n        mode: How to select which choice to return\n            * `smart` (default) will try to return the choice which is the closest match to the input value\n            * `left_to_right` will return the first choice in `choices` which succeeds validation\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='union',\n        choices=choices,\n        auto_collapse=auto_collapse,\n        custom_error_type=custom_error_type,\n        custom_error_message=custom_error_message,\n        custom_error_context=custom_error_context,\n        mode=mode,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 485}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def chain_schema(\n    steps: list[CoreSchema],\n    *,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> ChainSchema:\n    \"\"\"\n    Returns a schema that chains the provided validation schemas, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    def fn(v: str, info: core_schema.ValidationInfo) -> str:\n        assert 'hello' in v\n        return v + ' world'\n\n    fn_schema = core_schema.with_info_plain_validator_function(function=fn)\n    schema = core_schema.chain_schema(\n        [fn_schema, fn_schema, fn_schema, core_schema.str_schema()]\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python('hello') == 'hello world world world'\n    ```\n\n    Args:\n        steps: The schemas to chain\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(type='chain', steps=steps, ref=ref, metadata=metadata, serialization=serialization)", "metadata": {"license": "MIT", "len_tokens": 269}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def lax_or_strict_schema(\n    lax_schema: CoreSchema,\n    strict_schema: CoreSchema,\n    *,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> LaxOrStrictSchema:\n    \"\"\"\n    Returns a schema that uses the lax or strict schema, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    def fn(v: str, info: core_schema.ValidationInfo) -> str:\n        assert 'hello' in v\n        return v + ' world'\n\n    lax_schema = core_schema.int_schema(strict=False)\n    strict_schema = core_schema.int_schema(strict=True)\n\n    schema = core_schema.lax_or_strict_schema(\n        lax_schema=lax_schema, strict_schema=strict_schema, strict=True\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python(123) == 123\n\n    schema = core_schema.lax_or_strict_schema(\n        lax_schema=lax_schema, strict_schema=strict_schema, strict=False\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python('123') == 123\n    ```\n\n    Args:\n        lax_schema: The lax schema to use\n        strict_schema: The strict schema to use\n        strict: Whether the strict schema should be used\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='lax-or-strict',\n        lax_schema=lax_schema,\n        strict_schema=strict_schema,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 397}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def json_or_python_schema(\n    json_schema: CoreSchema,\n    python_schema: CoreSchema,\n    *,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> JsonOrPythonSchema:\n    \"\"\"\n    Returns a schema that uses the Json or Python schema depending on the input:\n\n    ```py\n    from pydantic_core import SchemaValidator, ValidationError, core_schema\n\n    v = SchemaValidator(\n        core_schema.json_or_python_schema(\n            json_schema=core_schema.int_schema(),\n            python_schema=core_schema.int_schema(strict=True),\n        )\n    )\n\n    assert v.validate_json('\"123\"') == 123\n\n    try:\n        v.validate_python('123')\n    except ValidationError:\n        pass\n    else:\n        raise AssertionError('Validation should have failed')\n    ```\n\n    Args:\n        json_schema: The schema to use for Json inputs\n        python_schema: The schema to use for Python inputs\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='json-or-python',\n        json_schema=json_schema,\n        python_schema=python_schema,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 305}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def typed_dict_field(\n    schema: CoreSchema,\n    *,\n    required: bool | None = None,\n    validation_alias: str | list[str | int] | list[list[str | int]] | None = None,\n    serialization_alias: str | None = None,\n    serialization_exclude: bool | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization_exclude_if: Callable[[Any], bool] | None = None,\n) -> TypedDictField:\n    \"\"\"\n    Returns a schema that matches a typed dict field, e.g.:\n\n    ```py\n    from pydantic_core import core_schema\n\n    field = core_schema.typed_dict_field(schema=core_schema.int_schema(), required=True)\n    ```\n\n    Args:\n        schema: The schema to use for the field\n        required: Whether the field is required, otherwise uses the value from `total` on the typed dict\n        validation_alias: The alias(es) to use to find the field in the validation data\n        serialization_alias: The alias to use as a key when serializing\n        serialization_exclude: Whether to exclude the field when serializing\n        serialization_exclude_if: A callable that determines whether to exclude the field when serializing based on its value.\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n    \"\"\"\n    return _dict_not_none(\n        type='typed-dict-field',\n        schema=schema,\n        required=required,\n        validation_alias=validation_alias,\n        serialization_alias=serialization_alias,\n        serialization_exclude=serialization_exclude,\n        serialization_exclude_if=serialization_exclude_if,\n        metadata=metadata,\n    )", "metadata": {"license": "MIT", "len_tokens": 351}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def typed_dict_schema(\n    fields: dict[str, TypedDictField],\n    *,\n    cls: type[Any] | None = None,\n    cls_name: str | None = None,\n    computed_fields: list[ComputedField] | None = None,\n    strict: bool | None = None,\n    extras_schema: CoreSchema | None = None,\n    extra_behavior: ExtraBehavior | None = None,\n    total: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n    config: CoreConfig | None = None,\n) -> TypedDictSchema:\n    \"\"\"\n    Returns a schema that matches a typed dict, e.g.:\n\n    ```py\n    from typing_extensions import TypedDict\n\n    from pydantic_core import SchemaValidator, core_schema\n\n    class MyTypedDict(TypedDict):\n        a: str\n\n    wrapper_schema = core_schema.typed_dict_schema(\n        {'a': core_schema.typed_dict_field(core_schema.str_schema())}, cls=MyTypedDict\n    )\n    v = SchemaValidator(wrapper_schema)\n    assert v.validate_python({'a': 'hello'}) == {'a': 'hello'}\n    ```\n\n    Args:\n        fields: The fields to use for the typed dict\n        cls: The class to use for the typed dict\n        cls_name: The name to use in error locations. Falls back to `cls.__name__`, or the validator name if no class\n            is provided.\n        computed_fields: Computed fields to use when serializing the model, only applies when directly inside a model\n        strict: Whether the typed dict is strict\n        extras_schema: The extra validator to use for the typed dict\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        extra_behavior: The extra behavior to use for the typed dict\n        total: Whether the typed dict is total, otherwise uses `typed_dict_total` from config\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='typed-dict',\n        fields=fields,\n        cls=cls,\n        cls_name=cls_name,\n        computed_fields=computed_fields,\n        strict=strict,\n        extras_schema=extras_schema,\n        extra_behavior=extra_behavior,\n        total=total,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n        config=config,\n    )", "metadata": {"license": "MIT", "len_tokens": 532}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def model_field(\n    schema: CoreSchema,\n    *,\n    validation_alias: str | list[str | int] | list[list[str | int]] | None = None,\n    serialization_alias: str | None = None,\n    serialization_exclude: bool | None = None,\n    serialization_exclude_if: Callable[[Any], bool] | None = None,\n    frozen: bool | None = None,\n    metadata: dict[str, Any] | None = None,\n) -> ModelField:\n    \"\"\"\n    Returns a schema for a model field, e.g.:\n\n    ```py\n    from pydantic_core import core_schema\n\n    field = core_schema.model_field(schema=core_schema.int_schema())\n    ```\n\n    Args:\n        schema: The schema to use for the field\n        validation_alias: The alias(es) to use to find the field in the validation data\n        serialization_alias: The alias to use as a key when serializing\n        serialization_exclude: Whether to exclude the field when serializing\n        serialization_exclude_if: A Callable that determines whether to exclude a field during serialization based on its value.\n        frozen: Whether the field is frozen\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n    \"\"\"\n    return _dict_not_none(\n        type='model-field',\n        schema=schema,\n        validation_alias=validation_alias,\n        serialization_alias=serialization_alias,\n        serialization_exclude=serialization_exclude,\n        serialization_exclude_if=serialization_exclude_if,\n        frozen=frozen,\n        metadata=metadata,\n    )", "metadata": {"license": "MIT", "len_tokens": 326}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def model_fields_schema(\n    fields: dict[str, ModelField],\n    *,\n    model_name: str | None = None,\n    computed_fields: list[ComputedField] | None = None,\n    strict: bool | None = None,\n    extras_schema: CoreSchema | None = None,\n    extras_keys_schema: CoreSchema | None = None,\n    extra_behavior: ExtraBehavior | None = None,\n    from_attributes: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> ModelFieldsSchema:\n    \"\"\"\n    Returns a schema that matches the fields of a Pydantic model, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    wrapper_schema = core_schema.model_fields_schema(\n        {'a': core_schema.model_field(core_schema.str_schema())}\n    )\n    v = SchemaValidator(wrapper_schema)\n    print(v.validate_python({'a': 'hello'}))\n    #> ({'a': 'hello'}, None, {'a'})\n    ```\n\n    Args:\n        fields: The fields of the model\n        model_name: The name of the model, used for error messages, defaults to \"Model\"\n        computed_fields: Computed fields to use when serializing the model, only applies when directly inside a model\n        strict: Whether the model is strict\n        extras_schema: The schema to use when validating extra input data\n        extras_keys_schema: The schema to use when validating the keys of extra input data\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        extra_behavior: The extra behavior to use for the model fields\n        from_attributes: Whether the model fields should be populated from attributes\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='model-fields',\n        fields=fields,\n        model_name=model_name,\n        computed_fields=computed_fields,\n        strict=strict,\n        extras_schema=extras_schema,\n        extras_keys_schema=extras_keys_schema,\n        extra_behavior=extra_behavior,\n        from_attributes=from_attributes,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 489}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def model_schema(\n    cls: type[Any],\n    schema: CoreSchema,\n    *,\n    generic_origin: type[Any] | None = None,\n    custom_init: bool | None = None,\n    root_model: bool | None = None,\n    post_init: str | None = None,\n    revalidate_instances: Literal['always', 'never', 'subclass-instances'] | None = None,\n    strict: bool | None = None,\n    frozen: bool | None = None,\n    extra_behavior: ExtraBehavior | None = None,\n    config: CoreConfig | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> ModelSchema:\n    \"\"\"\n    A model schema generally contains a typed-dict schema.\n    It will run the typed dict validator, then create a new class\n    and set the dict and fields set returned from the typed dict validator\n    to `__dict__` and `__pydantic_fields_set__` respectively.\n\n    Example:\n\n    ```py\n    from pydantic_core import CoreConfig, SchemaValidator, core_schema\n\n    class MyModel:\n        __slots__ = (\n            '__dict__',\n            '__pydantic_fields_set__',\n            '__pydantic_extra__',\n            '__pydantic_private__',\n        )\n\n    schema = core_schema.model_schema(\n        cls=MyModel,\n        config=CoreConfig(str_max_length=5),\n        schema=core_schema.model_fields_schema(\n            fields={'a': core_schema.model_field(core_schema.str_schema())},\n        ),\n    )\n    v = SchemaValidator(schema)\n    assert v.isinstance_python({'a': 'hello'}) is True\n    assert v.isinstance_python({'a': 'too long'}) is False\n    ```\n\n    Args:\n        cls: The class to use for the model\n        schema: The schema to use for the model\n        generic_origin: The origin type used for this model, if it's a parametrized generic. Ex,\n            if this model schema represents `SomeModel[int]`, generic_origin is `SomeModel`\n        custom_init: Whether the model has a custom init method\n        root_model: Whether the model is a `RootModel`\n        post_init: The call after init to use for the model\n        revalidate_instances: whether instances of models and dataclasses (including subclass instances)\n            should re-validate defaults to config.revalidate_instances, else 'never'\n        strict: Whether the model is strict\n        frozen: Whether the model is frozen\n        extra_behavior: The extra behavior to use for the model, used in serialization\n        config: The config to use for the model\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='model',\n        cls=cls,\n        generic_origin=generic_origin,\n        schema=schema,\n        custom_init=custom_init,\n        root_model=root_model,\n        post_init=post_init,\n        revalidate_instances=revalidate_instances,\n        strict=strict,\n        frozen=frozen,\n        extra_behavior=extra_behavior,\n        config=config,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 711}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def dataclass_field(\n    name: str,\n    schema: CoreSchema,\n    *,\n    kw_only: bool | None = None,\n    init: bool | None = None,\n    init_only: bool | None = None,\n    validation_alias: str | list[str | int] | list[list[str | int]] | None = None,\n    serialization_alias: str | None = None,\n    serialization_exclude: bool | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization_exclude_if: Callable[[Any], bool] | None = None,\n    frozen: bool | None = None,\n) -> DataclassField:\n    \"\"\"\n    Returns a schema for a dataclass field, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    field = core_schema.dataclass_field(\n        name='a', schema=core_schema.str_schema(), kw_only=False\n    )\n    schema = core_schema.dataclass_args_schema('Foobar', [field])\n    v = SchemaValidator(schema)\n    assert v.validate_python({'a': 'hello'}) == ({'a': 'hello'}, None)\n    ```\n\n    Args:\n        name: The name to use for the argument parameter\n        schema: The schema to use for the argument parameter\n        kw_only: Whether the field can be set with a positional argument as well as a keyword argument\n        init: Whether the field should be validated during initialization\n        init_only: Whether the field should be omitted  from `__dict__` and passed to `__post_init__`\n        validation_alias: The alias(es) to use to find the field in the validation data\n        serialization_alias: The alias to use as a key when serializing\n        serialization_exclude: Whether to exclude the field when serializing\n        serialization_exclude_if: A callable that determines whether to exclude the field when serializing based on its value.\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        frozen: Whether the field is frozen\n    \"\"\"\n    return _dict_not_none(\n        type='dataclass-field',\n        name=name,\n        schema=schema,\n        kw_only=kw_only,\n        init=init,\n        init_only=init_only,\n        validation_alias=validation_alias,\n        serialization_alias=serialization_alias,\n        serialization_exclude=serialization_exclude,\n        serialization_exclude_if=serialization_exclude_if,\n        metadata=metadata,\n        frozen=frozen,\n    )", "metadata": {"license": "MIT", "len_tokens": 520}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def dataclass_args_schema(\n    dataclass_name: str,\n    fields: list[DataclassField],\n    *,\n    computed_fields: list[ComputedField] | None = None,\n    collect_init_only: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n    extra_behavior: ExtraBehavior | None = None,\n) -> DataclassArgsSchema:\n    \"\"\"\n    Returns a schema for validating dataclass arguments, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    field_a = core_schema.dataclass_field(\n        name='a', schema=core_schema.str_schema(), kw_only=False\n    )\n    field_b = core_schema.dataclass_field(\n        name='b', schema=core_schema.bool_schema(), kw_only=False\n    )\n    schema = core_schema.dataclass_args_schema('Foobar', [field_a, field_b])\n    v = SchemaValidator(schema)\n    assert v.validate_python({'a': 'hello', 'b': True}) == ({'a': 'hello', 'b': True}, None)\n    ```\n\n    Args:\n        dataclass_name: The name of the dataclass being validated\n        fields: The fields to use for the dataclass\n        computed_fields: Computed fields to use when serializing the dataclass\n        collect_init_only: Whether to collect init only fields into a dict to pass to `__post_init__`\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n        extra_behavior: How to handle extra fields\n    \"\"\"\n    return _dict_not_none(\n        type='dataclass-args',\n        dataclass_name=dataclass_name,\n        fields=fields,\n        computed_fields=computed_fields,\n        collect_init_only=collect_init_only,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n        extra_behavior=extra_behavior,\n    )", "metadata": {"license": "MIT", "len_tokens": 445}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def dataclass_schema(\n    cls: type[Any],\n    schema: CoreSchema,\n    fields: list[str],\n    *,\n    generic_origin: type[Any] | None = None,\n    cls_name: str | None = None,\n    post_init: bool | None = None,\n    revalidate_instances: Literal['always', 'never', 'subclass-instances'] | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n    frozen: bool | None = None,\n    slots: bool | None = None,\n    config: CoreConfig | None = None,\n) -> DataclassSchema:\n    \"\"\"\n    Returns a schema for a dataclass. As with `ModelSchema`, this schema can only be used as a field within\n    another schema, not as the root type.\n\n    Args:\n        cls: The dataclass type, used to perform subclass checks\n        schema: The schema to use for the dataclass fields\n        fields: Fields of the dataclass, this is used in serialization and in validation during re-validation\n            and while validating assignment\n        generic_origin: The origin type used for this dataclass, if it's a parametrized generic. Ex,\n            if this model schema represents `SomeDataclass[int]`, generic_origin is `SomeDataclass`\n        cls_name: The name to use in error locs, etc; this is useful for generics (default: `cls.__name__`)\n        post_init: Whether to call `__post_init__` after validation\n        revalidate_instances: whether instances of models and dataclasses (including subclass instances)\n            should re-validate defaults to config.revalidate_instances, else 'never'\n        strict: Whether to require an exact instance of `cls`\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n        frozen: Whether the dataclass is frozen\n        slots: Whether `slots=True` on the dataclass, means each field is assigned independently, rather than\n            simply setting `__dict__`, default false\n    \"\"\"\n    return _dict_not_none(\n        type='dataclass',\n        cls=cls,\n        generic_origin=generic_origin,\n        fields=fields,\n        cls_name=cls_name,\n        schema=schema,\n        post_init=post_init,\n        revalidate_instances=revalidate_instances,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n        frozen=frozen,\n        slots=slots,\n        config=config,\n    )", "metadata": {"license": "MIT", "len_tokens": 572}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def arguments_parameter(\n    name: str,\n    schema: CoreSchema,\n    *,\n    mode: Literal['positional_only', 'positional_or_keyword', 'keyword_only'] | None = None,\n    alias: str | list[str | int] | list[list[str | int]] | None = None,\n) -> ArgumentsParameter:\n    \"\"\"\n    Returns a schema that matches an argument parameter, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    param = core_schema.arguments_parameter(\n        name='a', schema=core_schema.str_schema(), mode='positional_only'\n    )\n    schema = core_schema.arguments_schema([param])\n    v = SchemaValidator(schema)\n    assert v.validate_python(('hello',)) == (('hello',), {})\n    ```\n\n    Args:\n        name: The name to use for the argument parameter\n        schema: The schema to use for the argument parameter\n        mode: The mode to use for the argument parameter\n        alias: The alias to use for the argument parameter\n    \"\"\"\n    return _dict_not_none(name=name, schema=schema, mode=mode, alias=alias)", "metadata": {"license": "MIT", "len_tokens": 238}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def arguments_schema(\n    arguments: list[ArgumentsParameter],\n    *,\n    validate_by_name: bool | None = None,\n    validate_by_alias: bool | None = None,\n    var_args_schema: CoreSchema | None = None,\n    var_kwargs_mode: VarKwargsMode | None = None,\n    var_kwargs_schema: CoreSchema | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> ArgumentsSchema:\n    \"\"\"\n    Returns a schema that matches an arguments schema, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    param_a = core_schema.arguments_parameter(\n        name='a', schema=core_schema.str_schema(), mode='positional_only'\n    )\n    param_b = core_schema.arguments_parameter(\n        name='b', schema=core_schema.bool_schema(), mode='positional_only'\n    )\n    schema = core_schema.arguments_schema([param_a, param_b])\n    v = SchemaValidator(schema)\n    assert v.validate_python(('hello', True)) == (('hello', True), {})\n    ```\n\n    Args:\n        arguments: The arguments to use for the arguments schema\n        validate_by_name: Whether to populate by the parameter names, defaults to `False`.\n        validate_by_alias: Whether to populate by the parameter aliases, defaults to `True`.\n        var_args_schema: The variable args schema to use for the arguments schema\n        var_kwargs_mode: The validation mode to use for variadic keyword arguments. If `'uniform'`, every value of the\n            keyword arguments will be validated against the `var_kwargs_schema` schema. If `'unpacked-typed-dict'`,\n            the `var_kwargs_schema` argument must be a [`typed_dict_schema`][pydantic_core.core_schema.typed_dict_schema]\n        var_kwargs_schema: The variable kwargs schema to use for the arguments schema\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='arguments',\n        arguments_schema=arguments,\n        validate_by_name=validate_by_name,\n        validate_by_alias=validate_by_alias,\n        var_args_schema=var_args_schema,\n        var_kwargs_mode=var_kwargs_mode,\n        var_kwargs_schema=var_kwargs_schema,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 535}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def arguments_v3_parameter(\n    name: str,\n    schema: CoreSchema,\n    *,\n    mode: Literal[\n        'positional_only',\n        'positional_or_keyword',\n        'keyword_only',\n        'var_args',\n        'var_kwargs_uniform',\n        'var_kwargs_unpacked_typed_dict',\n    ]\n    | None = None,\n    alias: str | list[str | int] | list[list[str | int]] | None = None,\n) -> ArgumentsV3Parameter:\n    \"\"\"\n    Returns a schema that matches an argument parameter, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    param = core_schema.arguments_v3_parameter(\n        name='a', schema=core_schema.str_schema(), mode='positional_only'\n    )\n    schema = core_schema.arguments_v3_schema([param])\n    v = SchemaValidator(schema)\n    assert v.validate_python({'a': 'hello'}) == (('hello',), {})\n    ```\n\n    Args:\n        name: The name to use for the argument parameter\n        schema: The schema to use for the argument parameter\n        mode: The mode to use for the argument parameter\n        alias: The alias to use for the argument parameter\n    \"\"\"\n    return _dict_not_none(name=name, schema=schema, mode=mode, alias=alias)", "metadata": {"license": "MIT", "len_tokens": 276}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def arguments_v3_schema(\n    arguments: list[ArgumentsV3Parameter],\n    *,\n    validate_by_name: bool | None = None,\n    validate_by_alias: bool | None = None,\n    extra_behavior: Literal['forbid', 'ignore'] | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> ArgumentsV3Schema:\n    \"\"\"\n    Returns a schema that matches an arguments schema, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    param_a = core_schema.arguments_v3_parameter(\n        name='a', schema=core_schema.str_schema(), mode='positional_only'\n    )\n    param_b = core_schema.arguments_v3_parameter(\n        name='kwargs', schema=core_schema.bool_schema(), mode='var_kwargs_uniform'\n    )\n    schema = core_schema.arguments_v3_schema([param_a, param_b])\n    v = SchemaValidator(schema)\n    assert v.validate_python({'a': 'hi', 'kwargs': {'b': True}}) == (('hi',), {'b': True})\n    ```\n\n    This schema is currently not used by other Pydantic components. In V3, it will most likely\n    become the default arguments schema for the `'call'` schema.\n\n    Args:\n        arguments: The arguments to use for the arguments schema.\n        validate_by_name: Whether to populate by the parameter names, defaults to `False`.\n        validate_by_alias: Whether to populate by the parameter aliases, defaults to `True`.\n        extra_behavior: The extra behavior to use.\n        ref: optional unique identifier of the schema, used to reference the schema in other places.\n        metadata: Any other information you want to include with the schema, not used by pydantic-core.\n        serialization: Custom serialization schema.\n    \"\"\"\n    return _dict_not_none(\n        type='arguments-v3',\n        arguments_schema=arguments,\n        validate_by_name=validate_by_name,\n        validate_by_alias=validate_by_alias,\n        extra_behavior=extra_behavior,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 457}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def call_schema(\n    arguments: CoreSchema,\n    function: Callable[..., Any],\n    *,\n    function_name: str | None = None,\n    return_schema: CoreSchema | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> CallSchema:\n    \"\"\"\n    Returns a schema that matches an arguments schema, then calls a function, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    param_a = core_schema.arguments_parameter(\n        name='a', schema=core_schema.str_schema(), mode='positional_only'\n    )\n    param_b = core_schema.arguments_parameter(\n        name='b', schema=core_schema.bool_schema(), mode='positional_only'\n    )\n    args_schema = core_schema.arguments_schema([param_a, param_b])\n\n    schema = core_schema.call_schema(\n        arguments=args_schema,\n        function=lambda a, b: a + str(not b),\n        return_schema=core_schema.str_schema(),\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python((('hello', True))) == 'helloFalse'\n    ```\n\n    Args:\n        arguments: The arguments to use for the arguments schema\n        function: The function to use for the call schema\n        function_name: The function name to use for the call schema, if not provided `function.__name__` is used\n        return_schema: The return schema to use for the call schema\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='call',\n        arguments_schema=arguments,\n        function=function,\n        function_name=function_name,\n        return_schema=return_schema,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 419}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def custom_error_schema(\n    schema: CoreSchema,\n    custom_error_type: str,\n    *,\n    custom_error_message: str | None = None,\n    custom_error_context: dict[str, Any] | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> CustomErrorSchema:\n    \"\"\"\n    Returns a schema that matches a custom error value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.custom_error_schema(\n        schema=core_schema.int_schema(),\n        custom_error_type='MyError',\n        custom_error_message='Error msg',\n    )\n    v = SchemaValidator(schema)\n    v.validate_python(1)\n    ```\n\n    Args:\n        schema: The schema to use for the custom error schema\n        custom_error_type: The custom error type to use for the custom error schema\n        custom_error_message: The custom error message to use for the custom error schema\n        custom_error_context: The custom error context to use for the custom error schema\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='custom-error',\n        schema=schema,\n        custom_error_type=custom_error_type,\n        custom_error_message=custom_error_message,\n        custom_error_context=custom_error_context,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 348}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def json_schema(\n    schema: CoreSchema | None = None,\n    *,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> JsonSchema:\n    \"\"\"\n    Returns a schema that matches a JSON value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    dict_schema = core_schema.model_fields_schema(\n        {\n            'field_a': core_schema.model_field(core_schema.str_schema()),\n            'field_b': core_schema.model_field(core_schema.bool_schema()),\n        },\n    )\n\n    class MyModel:\n        __slots__ = (\n            '__dict__',\n            '__pydantic_fields_set__',\n            '__pydantic_extra__',\n            '__pydantic_private__',\n        )\n        field_a: str\n        field_b: bool\n\n    json_schema = core_schema.json_schema(schema=dict_schema)\n    schema = core_schema.model_schema(cls=MyModel, schema=json_schema)\n    v = SchemaValidator(schema)\n    m = v.validate_python('{\"field_a\": \"hello\", \"field_b\": true}')\n    assert isinstance(m, MyModel)\n    ```\n\n    Args:\n        schema: The schema to use for the JSON schema\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(type='json', schema=schema, ref=ref, metadata=metadata, serialization=serialization)", "metadata": {"license": "MIT", "len_tokens": 341}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def url_schema(\n    *,\n    max_length: int | None = None,\n    allowed_schemes: list[str] | None = None,\n    host_required: bool | None = None,\n    default_host: str | None = None,\n    default_port: int | None = None,\n    default_path: str | None = None,\n    preserve_empty_path: bool | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> UrlSchema:\n    \"\"\"\n    Returns a schema that matches a URL value, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.url_schema()\n    v = SchemaValidator(schema)\n    print(v.validate_python('https://example.com'))\n    #> https://example.com/\n    ```\n\n    Args:\n        max_length: The maximum length of the URL\n        allowed_schemes: The allowed URL schemes\n        host_required: Whether the URL must have a host\n        default_host: The default host to use if the URL does not have a host\n        default_port: The default port to use if the URL does not have a port\n        default_path: The default path to use if the URL does not have a path\n        preserve_empty_path: Whether to preserve an empty path or convert it to '/', default False\n        strict: Whether to use strict URL parsing\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='url',\n        max_length=max_length,\n        allowed_schemes=allowed_schemes,\n        host_required=host_required,\n        default_host=default_host,\n        default_port=default_port,\n        default_path=default_path,\n        preserve_empty_path=preserve_empty_path,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 455}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def multi_host_url_schema(\n    *,\n    max_length: int | None = None,\n    allowed_schemes: list[str] | None = None,\n    host_required: bool | None = None,\n    default_host: str | None = None,\n    default_port: int | None = None,\n    default_path: str | None = None,\n    preserve_empty_path: bool | None = None,\n    strict: bool | None = None,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> MultiHostUrlSchema:\n    \"\"\"\n    Returns a schema that matches a URL value with possibly multiple hosts, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema = core_schema.multi_host_url_schema()\n    v = SchemaValidator(schema)\n    print(v.validate_python('redis://localhost,0.0.0.0,127.0.0.1'))\n    #> redis://localhost,0.0.0.0,127.0.0.1\n    ```\n\n    Args:\n        max_length: The maximum length of the URL\n        allowed_schemes: The allowed URL schemes\n        host_required: Whether the URL must have a host\n        default_host: The default host to use if the URL does not have a host\n        default_port: The default port to use if the URL does not have a port\n        default_path: The default path to use if the URL does not have a path\n        preserve_empty_path: Whether to preserve an empty path or convert it to '/', default False\n        strict: Whether to use strict URL parsing\n        ref: optional unique identifier of the schema, used to reference the schema in other places\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='multi-host-url',\n        max_length=max_length,\n        allowed_schemes=allowed_schemes,\n        host_required=host_required,\n        default_host=default_host,\n        default_port=default_port,\n        default_path=default_path,\n        preserve_empty_path=preserve_empty_path,\n        strict=strict,\n        ref=ref,\n        metadata=metadata,\n        serialization=serialization,\n    )", "metadata": {"license": "MIT", "len_tokens": 497}}
{"id": "pydantic:pydantic-core/python/pydantic_core/core_schema.py", "language": "python", "code": "def definition_reference_schema(\n    schema_ref: str,\n    ref: str | None = None,\n    metadata: dict[str, Any] | None = None,\n    serialization: SerSchema | None = None,\n) -> DefinitionReferenceSchema:\n    \"\"\"\n    Returns a schema that points to a schema stored in \"definitions\", this is useful for nested recursive\n    models and also when you want to define validators separately from the main schema, e.g.:\n\n    ```py\n    from pydantic_core import SchemaValidator, core_schema\n\n    schema_definition = core_schema.definition_reference_schema('list-schema')\n    schema = core_schema.definitions_schema(\n        schema=schema_definition,\n        definitions=[\n            core_schema.list_schema(items_schema=schema_definition, ref='list-schema'),\n        ],\n    )\n    v = SchemaValidator(schema)\n    assert v.validate_python([()]) == [[]]\n    ```\n\n    Args:\n        schema_ref: The schema ref to use for the definition reference schema\n        metadata: Any other information you want to include with the schema, not used by pydantic-core\n        serialization: Custom serialization schema\n    \"\"\"\n    return _dict_not_none(\n        type='definition-ref', schema_ref=schema_ref, ref=ref, metadata=metadata, serialization=serialization\n    )", "metadata": {"license": "MIT", "len_tokens": 262}}
{"id": "pydantic:docs/plugins/conversion_table.py", "language": "python", "code": "class Row:\n    field_type: type[Any] | str\n    input_type: type[Any] | str\n    python_input: bool = False\n    json_input: bool = False\n    strict: bool = False\n    condition: str | None = None\n    valid_examples: list[Any] | None = None\n    invalid_examples: list[Any] | None = None\n    core_schemas: list[type[CoreSchema]] | None = None\n\n    @property\n    def field_type_str(self) -> str:\n        return f'{self.field_type.__name__}' if hasattr(self.field_type, '__name__') else f'{self.field_type}'\n\n    @property\n    def input_type_str(self) -> str:\n        return f'{self.input_type.__name__}' if hasattr(self.input_type, '__name__') else f'{self.input_type}'\n\n    @property\n    def input_source_str(self) -> str:\n        if self.python_input:\n            if self.json_input:\n                return 'Python & JSON'\n            else:\n                return 'Python'\n        elif self.json_input:\n            return 'JSON'\n        else:\n            return ''", "metadata": {"license": "MIT", "len_tokens": 239}}
{"id": "pydantic:docs/plugins/conversion_table.py", "language": "python", "code": "class ConversionTable:\n    rows: list[Row]\n\n    col_names = [\n        'Field Type',\n        'Input',\n        'Strict',\n        'Input Source',\n        'Conditions',\n    ]\n    open_nowrap_span = '<span style=\"white-space: nowrap;\">'\n    close_nowrap_span = '</span>'\n\n    def col_values(self, row: Row) -> list[str]:\n        o = self.open_nowrap_span\n        c = self.close_nowrap_span\n\n        return [\n            f'{o}`{row.field_type_str}`{c}',\n            f'{o}`{row.input_type_str}`{c}',\n            '' if row.strict else '',\n            f'{o}{row.input_source_str}{c}',\n            row.condition if row.condition else '',\n        ]\n\n    @staticmethod\n    def row_as_markdown(cols: list[str]) -> str:\n        return f'| {\" | \".join(cols)} |'\n\n    def as_markdown(self) -> str:\n        lines = [self.row_as_markdown(self.col_names), self.row_as_markdown(['-'] * len(self.col_names))] + [\n            self.row_as_markdown(self.col_values(row)) for row in self.rows\n        ]\n        return '\\n'.join(lines)\n\n    @staticmethod\n    def row_sort_key(row: Row) -> Any:\n        field_type = row.field_type_str or ' '\n        input_type = row.input_type_str or ' '\n        input_source = row.input_source_str\n\n        # Include the .isupper() to make it so that leading-lowercase items come first\n        return field_type[0].isupper(), field_type, input_type[0].isupper(), input_type, input_source\n\n    def sorted(self) -> ConversionTable:\n        return ConversionTable(sorted(self.rows, key=self.row_sort_key))\n\n    def filtered(self, predicate: Callable[[Row], bool]) -> ConversionTable:\n        return ConversionTable([row for row in self.rows if predicate(row)])", "metadata": {"license": "MIT", "len_tokens": 408}}
{"id": "pydantic:docs/plugins/algolia.py", "language": "python", "code": "def algolia_upload() -> None:\n    from algoliasearch.search.client import SearchClientSync\n\n    algolia_write_api_key = os.environ['ALGOLIA_WRITE_API_KEY']\n\n    client = SearchClientSync(ALGOLIA_APP_ID, algolia_write_api_key)\n    filtered_records: list[AlgoliaRecord] = []\n\n    algolia_records_path = Path.cwd() / 'site' / ALGOLIA_RECORDS_FILE\n\n    with algolia_records_path.open('rb') as f:\n        all_records = records_ta.validate_json(f.read())\n\n    for record in all_records:\n        content = record['content']\n        if len(content) > MAX_CONTENT_LENGTH:\n            print(\n                f\"Record with title '{record['title']}' has more than {MAX_CONTENT_LENGTH} characters, {len(content)}.\"\n            )\n            print(content)\n        else:\n            filtered_records.append(record)\n\n    print(f'Uploading {len(filtered_records)} out of {len(all_records)} records to Algolia...')\n\n    client.clear_objects(index_name=ALGOLIA_INDEX_NAME)\n    client.set_settings(\n        index_name=ALGOLIA_INDEX_NAME,\n        index_settings={\n            'searchableAttributes': ['title', 'content'],\n            'attributesToSnippet': ['content:40'],\n            'customRanking': [\n                'desc(rank)',\n            ],\n        },\n    )\n\n    client.batch(\n        index_name=ALGOLIA_INDEX_NAME,\n        batch_write_params={'requests': [{'action': 'addObject', 'body': record} for record in filtered_records]},\n    )", "metadata": {"license": "MIT", "len_tokens": 320}}
{"id": "pydantic:docs/plugins/main.py", "language": "python", "code": "def add_mkdocs_run_deps(site_url: str) -> None:\n    # set the pydantic, pydantic-core, pydantic-extra-types versions to configure for running examples in the browser\n    pyproject_toml = (PROJECT_ROOT / 'pyproject.toml').read_text()\n    m = re.search(r'pydantic-core==(.+?)[\"\\']', pyproject_toml)\n    if not m:\n        logger.info(\n            \"Could not find pydantic-core version in pyproject.toml, this is expected if you're using a git ref\"\n        )\n        return\n\n    pydantic_core_version = m.group(1)\n\n    version_py = (PROJECT_ROOT / 'pydantic' / 'version.py').read_text()\n    pydantic_version_str: str = re.search(r'^VERSION ?= ([\"\\'])(.+)\\1', version_py, flags=re.M).group(2)  # pyright: ignore[reportOptionalMemberAccess]\n    if os.getenv('CI') and Version(pydantic_version_str).local == 'dev':\n        build_package(\n            PROJECT_ROOT,\n            DOCS_DIR,\n            distributions=['wheel'],\n        )\n        wheel_file = next(DOCS_DIR.glob('*.whl'))\n        pydantic_dep = f'{site_url.removesuffix(\"/\").removesuffix(\"/latest\")}/dev/{wheel_file.name}'\n    else:\n        pydantic_dep = f'pydantic=={pydantic_version_str}'\n\n    uv_lock = (PROJECT_ROOT / 'uv.lock').read_text()\n    pydantic_extra_types_version: str = re.search(r'name = \"pydantic-extra-types\"\\nversion = \"(.+?)\"', uv_lock).group(1)  # pyright: ignore[reportOptionalMemberAccess]\n\n    mkdocs_run_deps = json.dumps(\n        [\n            pydantic_dep,\n            'email-validator>=2.0.0',\n            f'pydantic-core=={pydantic_core_version}',\n            f'pydantic-extra-types=={pydantic_extra_types_version}',\n        ]\n    )\n    logger.info('Setting mkdocs_run_deps=%s', mkdocs_run_deps)\n\n    html = f\"\"\"\\\n    <script>\n    window.mkdocs_run_deps = {mkdocs_run_deps}\n    </script>\n\"\"\"\n    path = DOCS_DIR / 'theme/mkdocs_run_deps.html'\n    path.write_text(html)", "metadata": {"license": "MIT", "len_tokens": 513}}
{"id": "pydantic:docs/plugins/main.py", "language": "python", "code": "def upgrade_python(markdown: str) -> str:\n    \"\"\"\n    Apply pyupgrade to all Python code blocks, unless explicitly skipped, create a tab for each version.\n    \"\"\"\n\n    def add_tabs(match: re.Match[str]) -> str:\n        prefix = match.group(1)\n        if 'upgrade=\"skip\"' in prefix:\n            return match.group(0)\n\n        if m := re.search(r'requires=\"3.(\\d+)\"', prefix):\n            min_minor_version = int(m.group(1))\n        else:\n            min_minor_version = MIN_MINOR_VERSION\n\n        py_code = match.group(2)\n        numbers = match.group(3)\n        # import devtools\n        # devtools.debug(numbers)\n        output = []\n        last_code = py_code\n        for minor_version in range(min_minor_version, MAX_MINOR_VERSION + 1):\n            if minor_version == min_minor_version:\n                tab_code = py_code\n            else:\n                tab_code = _upgrade_code(py_code, minor_version)\n                if tab_code == last_code:\n                    continue\n                last_code = tab_code\n\n            content = indent(f'{prefix}\\n{tab_code}```{numbers}', ' ' * 4)\n            output.append(f'=== \"Python 3.{minor_version} and above\"\\n\\n{content}')\n\n        if len(output) == 1:\n            return match.group(0)\n        else:\n            return '\\n\\n'.join(output)\n\n    # Note: we should move away from this regex approach. It does not handle edge cases (indented code blocks inside\n    # other blocks, etc) and can lead to bugs in the rendering of annotations. Edit with care and make sure the rendered\n    # documentation does not break:\n    return re.sub(r'(``` *py.*?)\\n(.+?)^```(\\s+(?:^\\d+\\. (?:[^\\n][\\n]?)+\\n?)*)', add_tabs, markdown, flags=re.M | re.S)", "metadata": {"license": "MIT", "len_tokens": 410}}
{"id": "pydantic:docs/plugins/main.py", "language": "python", "code": "def insert_json_output(markdown: str) -> str:\n    \"\"\"\n    Find `output=\"json\"` code fence tags and replace with a separate JSON section\n    \"\"\"\n\n    def replace_json(m: re.Match[str]) -> str:\n        start, attrs, code = m.groups()\n\n        def replace_last_print(m2: re.Match[str]) -> str:\n            ind, json_text = m2.groups()\n            json_text = indent(json.dumps(json.loads(json_text), indent=2), ind)\n            # no trailing fence as that's not part of code\n            return f'\\n{ind}```\\n\\n{ind}JSON output:\\n\\n{ind}```json\\n{json_text}\\n'\n\n        code = re.sub(r'\\n( *)\"\"\"(.*?)\\1\"\"\"\\n$', replace_last_print, code, flags=re.S)\n        return f'{start}{attrs}{code}{start}\\n'\n\n    return re.sub(r'(^ *```)([^\\n]*?output=\"json\"[^\\n]*?\\n)(.+?)\\1', replace_json, markdown, flags=re.M | re.S)", "metadata": {"license": "MIT", "len_tokens": 229}}
{"id": "pydantic:docs/plugins/main.py", "language": "python", "code": "def render_index(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'index.md':\n        return None\n\n    if version := os.getenv('PYDANTIC_VERSION'):\n        url = f'https://github.com/pydantic/pydantic/releases/tag/{version}'\n        version_str = f'Documentation for version: [{version}]({url})'\n    elif (version_ref := os.getenv('GITHUB_REF')) and version_ref.startswith('refs/tags/'):\n        version = re.sub('^refs/tags/', '', version_ref.lower())\n        url = f'https://github.com/pydantic/pydantic/releases/tag/{version}'\n        version_str = f'Documentation for version: [{version}]({url})'\n    elif sha := os.getenv('GITHUB_SHA'):\n        url = f'https://github.com/pydantic/pydantic/commit/{sha}'\n        sha = sha[:7]\n        version_str = f'Documentation for development version: [{sha}]({url})'\n    else:\n        version_str = 'Documentation for development version'\n    logger.info('Setting version prefix: %r', version_str)\n    markdown = re.sub(r'{{ *version *}}', version_str, markdown)\n\n    elements = [tile_template.format(**org) for org in get_orgs_data()]\n\n    orgs_grid = f'<div id=\"grid-container\"><div id=\"company-grid\" class=\"grid\">{\"\".join(elements)}</div></div>'\n    return re.sub(r'{{ *organisations *}}', orgs_grid, markdown)", "metadata": {"license": "MIT", "len_tokens": 329}}
{"id": "pydantic:docs/plugins/main.py", "language": "python", "code": "def build_schema_mappings(markdown: str, page: Page) -> str | None:\n    if page.file.src_uri != 'usage/schema.md':\n        return None\n\n    col_names = [\n        'Python type',\n        'JSON Schema Type',\n        'Additional JSON Schema',\n        'Defined in',\n        'Notes',\n    ]\n    table_text = _generate_table_heading(col_names)\n\n    with (THIS_DIR / 'schema_mappings.toml').open('rb') as f:\n        table = tomli.load(f)\n\n    for t in table.values():\n        py_type = t['py_type']\n        json_type = t['json_type']\n        additional = t['additional']\n        defined_in = t['defined_in']\n        notes = t['notes']\n        if additional and not isinstance(additional, str):\n            additional = json.dumps(additional)\n        cols = [f'`{py_type}`', f'`{json_type}`', f'`{additional}`' if additional else '', defined_in, notes]\n        table_text += _generate_table_row(cols)\n\n    return re.sub(r'{{ *schema_mappings_table *}}', table_text, markdown)", "metadata": {"license": "MIT", "len_tokens": 240}}
{"id": "pydantic:docs/plugins/main.py", "language": "python", "code": "def add_tabs(match: re.Match[str]) -> str:\n        prefix = match.group(1)\n        if 'upgrade=\"skip\"' in prefix:\n            return match.group(0)\n\n        if m := re.search(r'requires=\"3.(\\d+)\"', prefix):\n            min_minor_version = int(m.group(1))\n        else:\n            min_minor_version = MIN_MINOR_VERSION\n\n        py_code = match.group(2)\n        numbers = match.group(3)\n        # import devtools\n        # devtools.debug(numbers)\n        output = []\n        last_code = py_code\n        for minor_version in range(min_minor_version, MAX_MINOR_VERSION + 1):\n            if minor_version == min_minor_version:\n                tab_code = py_code\n            else:\n                tab_code = _upgrade_code(py_code, minor_version)\n                if tab_code == last_code:\n                    continue\n                last_code = tab_code\n\n            content = indent(f'{prefix}\\n{tab_code}```{numbers}', ' ' * 4)\n            output.append(f'=== \"Python 3.{minor_version} and above\"\\n\\n{content}')\n\n        if len(output) == 1:\n            return match.group(0)\n        else:\n            return '\\n\\n'.join(output)", "metadata": {"license": "MIT", "len_tokens": 260}}
{"id": "pydantic:docs/plugins/using_update.py", "language": "python", "code": "from pathlib import Path\nfrom time import sleep\n\nimport requests\nimport tomli\n\nTHIS_DIR = Path(__file__).parent\n\nsession = requests.Session()\n\n\ndef update_lib(lib, *, retry=0):\n    repo = lib['repo']\n    url = f'https://api.github.com/repos/{repo}'\n    resp = session.get(url)\n    if resp.status_code == 403 and retry < 3:\n        print(f'retrying {repo} {retry}')\n        sleep(5)\n        return update_lib(lib, retry=retry + 1)\n\n    resp.raise_for_status()\n    data = resp.json()\n    stars = data['watchers_count']\n    print(f'{repo}: {stars}')\n    lib['stars'] = stars\n\n\nwith (THIS_DIR / 'using.toml').open('rb') as f:\n    table = tomli.load(f)\n\nlibs = table['libs']\nfor lib in libs:\n    update_lib(lib)\n\nlibs.sort(key=lambda lib: lib['stars'], reverse=True)\n\nwith (THIS_DIR / 'using.toml').open('w') as f:\n    for lib in libs:\n        f.write('[[libs]]\\nrepo = \"{repo}\"\\nstars = {stars}\\n'.format(**lib))\n", "metadata": {"license": "MIT", "len_tokens": 253}}
{"id": "fastapi:fastapi/params.py", "language": "python", "code": "class Path(Param):  # type: ignore[misc]\n    in_ = ParamTypes.path\n\n    def __init__(\n        self,\n        default: Any = ...,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        assert default is ..., \"Path parameters cannot have a default value\"\n        self.in_ = self.in_\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 693}}
{"id": "fastapi:fastapi/params.py", "language": "python", "code": "class Query(Param):  # type: ignore[misc]\n    in_ = ParamTypes.query\n\n    def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 671}}
{"id": "fastapi:fastapi/params.py", "language": "python", "code": "class Header(Param):  # type: ignore[misc]\n    in_ = ParamTypes.header\n\n    def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        convert_underscores: bool = True,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        self.convert_underscores = convert_underscores\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 693}}
{"id": "fastapi:fastapi/params.py", "language": "python", "code": "class Cookie(Param):  # type: ignore[misc]\n    in_ = ParamTypes.cookie\n\n    def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 671}}
{"id": "fastapi:fastapi/params.py", "language": "python", "code": "class Form(Body):  # type: ignore[misc]\n    def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        media_type: str = \"application/x-www-form-urlencoded\",\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            media_type=media_type,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 683}}
{"id": "fastapi:fastapi/params.py", "language": "python", "code": "class File(Form):  # type: ignore[misc]\n    def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        media_type: str = \"multipart/form-data\",\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            media_type=media_type,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 680}}
{"id": "fastapi:fastapi/params.py", "language": "python", "code": "def __init__(\n        self,\n        default: Any = ...,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        assert default is ..., \"Path parameters cannot have a default value\"\n        self.in_ = self.in_\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 671}}
{"id": "fastapi:fastapi/params.py", "language": "python", "code": "def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 649}}
{"id": "fastapi:fastapi/params.py", "language": "python", "code": "def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        convert_underscores: bool = True,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        self.convert_underscores = convert_underscores\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 671}}
{"id": "fastapi:fastapi/params.py", "language": "python", "code": "def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 649}}
{"id": "fastapi:fastapi/params.py", "language": "python", "code": "def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        media_type: str = \"application/x-www-form-urlencoded\",\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            media_type=media_type,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 669}}
{"id": "fastapi:fastapi/params.py", "language": "python", "code": "def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        media_type: str = \"multipart/form-data\",\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            media_type=media_type,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 667}}
{"id": "fastapi:fastapi/responses.py", "language": "python", "code": "from typing import Any\n\nfrom starlette.responses import FileResponse as FileResponse  # noqa\nfrom starlette.responses import HTMLResponse as HTMLResponse  # noqa\nfrom starlette.responses import JSONResponse as JSONResponse  # noqa\nfrom starlette.responses import PlainTextResponse as PlainTextResponse  # noqa\nfrom starlette.responses import RedirectResponse as RedirectResponse  # noqa\nfrom starlette.responses import Response as Response  # noqa\nfrom starlette.responses import StreamingResponse as StreamingResponse  # noqa\n\ntry:\n    import ujson\nexcept ImportError:  # pragma: nocover\n    ujson = None  # type: ignore\n\n\ntry:\n    import orjson\nexcept ImportError:  # pragma: nocover\n    orjson = None  # type: ignore\n\n\nclass UJSONResponse(JSONResponse):\n    \"\"\"\n    JSON response using the high-performance ujson library to serialize data to JSON.\n\n    Read more about it in the\n    [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/).\n    \"\"\"\n\n    def render(self, content: Any) -> bytes:\n        assert ujson is not None, \"ujson must be installed to use UJSONResponse\"\n        return ujson.dumps(content, ensure_ascii=False).encode(\"utf-8\")\n\n\nclass ORJSONResponse(JSONResponse):\n    \"\"\"\n    JSON response using the high-performance orjson library to serialize data to JSON.\n\n    Read more about it in the\n    [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/).\n    \"\"\"\n\n    def render(self, content: Any) -> bytes:\n        assert orjson is not None, \"orjson must be installed to use ORJSONResponse\"\n        return orjson.dumps(\n            content, option=orjson.OPT_NON_STR_KEYS | orjson.OPT_SERIALIZE_NUMPY\n        )\n", "metadata": {"license": "MIT", "len_tokens": 404}}
{"id": "fastapi:fastapi/exception_handlers.py", "language": "python", "code": "from fastapi.encoders import jsonable_encoder\nfrom fastapi.exceptions import RequestValidationError, WebSocketRequestValidationError\nfrom fastapi.utils import is_body_allowed_for_status_code\nfrom fastapi.websockets import WebSocket\nfrom starlette.exceptions import HTTPException\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse, Response\nfrom starlette.status import WS_1008_POLICY_VIOLATION\n\n\nasync def http_exception_handler(request: Request, exc: HTTPException) -> Response:\n    headers = getattr(exc, \"headers\", None)\n    if not is_body_allowed_for_status_code(exc.status_code):\n        return Response(status_code=exc.status_code, headers=headers)\n    return JSONResponse(\n        {\"detail\": exc.detail}, status_code=exc.status_code, headers=headers\n    )\n\n\nasync def request_validation_exception_handler(\n    request: Request, exc: RequestValidationError\n) -> JSONResponse:\n    return JSONResponse(\n        status_code=422,\n        content={\"detail\": jsonable_encoder(exc.errors())},\n    )\n\n\nasync def websocket_request_validation_exception_handler(\n    websocket: WebSocket, exc: WebSocketRequestValidationError\n) -> None:\n    await websocket.close(\n        code=WS_1008_POLICY_VIOLATION, reason=jsonable_encoder(exc.errors())\n    )\n", "metadata": {"license": "MIT", "len_tokens": 266}}
{"id": "fastapi:fastapi/applications.py", "language": "python", "code": "def build_middleware_stack(self) -> ASGIApp:\n        # Duplicate/override from Starlette to add AsyncExitStackMiddleware\n        # inside of ExceptionMiddleware, inside of custom user middlewares\n        debug = self.debug\n        error_handler = None\n        exception_handlers: dict[Any, ExceptionHandler] = {}\n\n        for key, value in self.exception_handlers.items():\n            if key in (500, Exception):\n                error_handler = value\n            else:\n                exception_handlers[key] = value\n\n        middleware = (\n            [Middleware(ServerErrorMiddleware, handler=error_handler, debug=debug)]\n            + self.user_middleware\n            + [\n                Middleware(\n                    ExceptionMiddleware, handlers=exception_handlers, debug=debug\n                ),\n                # Add FastAPI-specific AsyncExitStackMiddleware for closing files.\n                # Before this was also used for closing dependencies with yield but\n                # those now have their own AsyncExitStack, to properly support\n                # streaming responses while keeping compatibility with the previous\n                # versions (as of writing 0.117.1) that allowed doing\n                # except HTTPException inside a dependency with yield.\n                # This needs to happen after user middlewares because those create a\n                # new contextvars context copy by using a new AnyIO task group.\n                # This AsyncExitStack preserves the context for contextvars, not\n                # strictly necessary for closing files but it was one of the original\n                # intentions.\n                # If the AsyncExitStack lived outside of the custom middlewares and\n                # contextvars were set, for example in a dependency with 'yield'\n                # in that internal contextvars context, the values would not be\n                # available in the outer context of the AsyncExitStack.\n                # By placing the middleware and the AsyncExitStack here, inside all\n                # user middlewares, the same context is used.\n                # This is currently not needed, only for closing files, but used to be\n                # important when dependencies with yield were closed here.\n                Middleware(AsyncExitStackMiddleware),\n            ]\n        )\n\n        app = self.router\n        for cls, args, kwargs in reversed(middleware):\n            app = cls(app, *args, **kwargs)\n        return app", "metadata": {"license": "MIT", "len_tokens": 460}}
{"id": "fastapi:fastapi/applications.py", "language": "python", "code": "def openapi(self) -> Dict[str, Any]:\n        \"\"\"\n        Generate the OpenAPI schema of the application. This is called by FastAPI\n        internally.\n\n        The first time it is called it stores the result in the attribute\n        `app.openapi_schema`, and next times it is called, it just returns that same\n        result. To avoid the cost of generating the schema every time.\n\n        If you need to modify the generated OpenAPI schema, you could modify it.\n\n        Read more in the\n        [FastAPI docs for OpenAPI](https://fastapi.tiangolo.com/how-to/extending-openapi/).\n        \"\"\"\n        if not self.openapi_schema:\n            self.openapi_schema = get_openapi(\n                title=self.title,\n                version=self.version,\n                openapi_version=self.openapi_version,\n                summary=self.summary,\n                description=self.description,\n                terms_of_service=self.terms_of_service,\n                contact=self.contact,\n                license_info=self.license_info,\n                routes=self.routes,\n                webhooks=self.webhooks.routes,\n                tags=self.openapi_tags,\n                servers=self.servers,\n                separate_input_output_schemas=self.separate_input_output_schemas,\n                external_docs=self.openapi_external_docs,\n            )\n        return self.openapi_schema", "metadata": {"license": "MIT", "len_tokens": 252}}
{"id": "fastapi:fastapi/applications.py", "language": "python", "code": "def setup(self) -> None:\n        if self.openapi_url:\n            urls = (server_data.get(\"url\") for server_data in self.servers)\n            server_urls = {url for url in urls if url}\n\n            async def openapi(req: Request) -> JSONResponse:\n                root_path = req.scope.get(\"root_path\", \"\").rstrip(\"/\")\n                if root_path not in server_urls:\n                    if root_path and self.root_path_in_servers:\n                        self.servers.insert(0, {\"url\": root_path})\n                        server_urls.add(root_path)\n                return JSONResponse(self.openapi())\n\n            self.add_route(self.openapi_url, openapi, include_in_schema=False)\n        if self.openapi_url and self.docs_url:\n\n            async def swagger_ui_html(req: Request) -> HTMLResponse:\n                root_path = req.scope.get(\"root_path\", \"\").rstrip(\"/\")\n                openapi_url = root_path + self.openapi_url\n                oauth2_redirect_url = self.swagger_ui_oauth2_redirect_url\n                if oauth2_redirect_url:\n                    oauth2_redirect_url = root_path + oauth2_redirect_url\n                return get_swagger_ui_html(\n                    openapi_url=openapi_url,\n                    title=f\"{self.title} - Swagger UI\",\n                    oauth2_redirect_url=oauth2_redirect_url,\n                    init_oauth=self.swagger_ui_init_oauth,\n                    swagger_ui_parameters=self.swagger_ui_parameters,\n                )\n\n            self.add_route(self.docs_url, swagger_ui_html, include_in_schema=False)\n\n            if self.swagger_ui_oauth2_redirect_url:\n\n                async def swagger_ui_redirect(req: Request) -> HTMLResponse:\n                    return get_swagger_ui_oauth2_redirect_html()\n\n                self.add_route(\n                    self.swagger_ui_oauth2_redirect_url,\n                    swagger_ui_redirect,\n                    include_in_schema=False,\n                )\n        if self.openapi_url and self.redoc_url:\n\n            async def redoc_html(req: Request) -> HTMLResponse:\n                root_path = req.scope.get(\"root_path\", \"\").rstrip(\"/\")\n                openapi_url = root_path + self.openapi_url\n                return get_redoc_html(\n                    openapi_url=openapi_url, title=f\"{self.title} - ReDoc\"\n                )\n\n            self.add_route(self.redoc_url, redoc_html, include_in_schema=False)", "metadata": {"license": "MIT", "len_tokens": 459}}
{"id": "fastapi:fastapi/applications.py", "language": "python", "code": "def add_api_route(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[List[str]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Union[Type[Response], DefaultPlaceholder] = Default(\n            JSONResponse\n        ),\n        name: Optional[str] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> None:\n        self.router.add_api_route(\n            path,\n            endpoint=endpoint,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=methods,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )", "metadata": {"license": "MIT", "len_tokens": 482}}
{"id": "fastapi:fastapi/applications.py", "language": "python", "code": "def api_route(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[List[str]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.router.add_api_route(\n                path,\n                func,\n                response_model=response_model,\n                status_code=status_code,\n                tags=tags,\n                dependencies=dependencies,\n                summary=summary,\n                description=description,\n                response_description=response_description,\n                responses=responses,\n                deprecated=deprecated,\n                methods=methods,\n                operation_id=operation_id,\n                response_model_include=response_model_include,\n                response_model_exclude=response_model_exclude,\n                response_model_by_alias=response_model_by_alias,\n                response_model_exclude_unset=response_model_exclude_unset,\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                include_in_schema=include_in_schema,\n                response_class=response_class,\n                name=name,\n                openapi_extra=openapi_extra,\n                generate_unique_id_function=generate_unique_id_function,\n            )\n            return func\n\n        return decorator", "metadata": {"license": "MIT", "len_tokens": 492}}
{"id": "fastapi:fastapi/applications.py", "language": "python", "code": "def websocket(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                WebSocket path.\n                \"\"\"\n            ),\n        ],\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A name for the WebSocket. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        *,\n        dependencies: Annotated[\n            Optional[Sequence[Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be used for this\n                WebSocket.\n\n                Read more about it in the\n                [FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).\n                \"\"\"\n            ),\n        ] = None,\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Decorate a WebSocket function.\n\n        Read more about it in the\n        [FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).\n\n        **Example**\n\n        ```python\n        from fastapi import FastAPI, WebSocket\n\n        app = FastAPI()\n\n        @app.websocket(\"/ws\")\n        async def websocket_endpoint(websocket: WebSocket):\n            await websocket.accept()\n            while True:\n                data = await websocket.receive_text()\n                await websocket.send_text(f\"Message text was: {data}\")\n        ```\n        \"\"\"\n\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_api_websocket_route(\n                path,\n                func,\n                name=name,\n                dependencies=dependencies,\n            )\n            return func\n\n        return decorator", "metadata": {"license": "MIT", "len_tokens": 328}}
{"id": "fastapi:fastapi/applications.py", "language": "python", "code": "def middleware(\n        self,\n        middleware_type: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The type of middleware. Currently only supports `http`.\n                \"\"\"\n            ),\n        ],\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a middleware to the application.\n\n        Read more about it in the\n        [FastAPI docs for Middleware](https://fastapi.tiangolo.com/tutorial/middleware/).\n\n        ## Example\n\n        ```python\n        import time\n        from typing import Awaitable, Callable\n\n        from fastapi import FastAPI, Request, Response\n\n        app = FastAPI()\n\n\n        @app.middleware(\"http\")\n        async def add_process_time_header(\n            request: Request, call_next: Callable[[Request], Awaitable[Response]]\n        ) -> Response:\n            start_time = time.time()\n            response = await call_next(request)\n            process_time = time.time() - start_time\n            response.headers[\"X-Process-Time\"] = str(process_time)\n            return response\n        ```\n        \"\"\"\n\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_middleware(BaseHTTPMiddleware, dispatch=func)\n            return func\n\n        return decorator", "metadata": {"license": "MIT", "len_tokens": 249}}
{"id": "fastapi:fastapi/applications.py", "language": "python", "code": "def exception_handler(\n        self,\n        exc_class_or_status_code: Annotated[\n            Union[int, Type[Exception]],\n            Doc(\n                \"\"\"\n                The Exception class this would handle, or a status code.\n                \"\"\"\n            ),\n        ],\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add an exception handler to the app.\n\n        Read more about it in the\n        [FastAPI docs for Handling Errors](https://fastapi.tiangolo.com/tutorial/handling-errors/).\n\n        ## Example\n\n        ```python\n        from fastapi import FastAPI, Request\n        from fastapi.responses import JSONResponse\n\n\n        class UnicornException(Exception):\n            def __init__(self, name: str):\n                self.name = name\n\n\n        app = FastAPI()\n\n\n        @app.exception_handler(UnicornException)\n        async def unicorn_exception_handler(request: Request, exc: UnicornException):\n            return JSONResponse(\n                status_code=418,\n                content={\"message\": f\"Oops! {exc.name} did something. There goes a rainbow...\"},\n            )\n        ```\n        \"\"\"\n\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_exception_handler(exc_class_or_status_code, func)\n            return func\n\n        return decorator", "metadata": {"license": "MIT", "len_tokens": 257}}
{"id": "fastapi:fastapi/concurrency.py", "language": "python", "code": "from contextlib import asynccontextmanager as asynccontextmanager\nfrom typing import AsyncGenerator, ContextManager, TypeVar\n\nimport anyio.to_thread\nfrom anyio import CapacityLimiter\nfrom starlette.concurrency import iterate_in_threadpool as iterate_in_threadpool  # noqa\nfrom starlette.concurrency import run_in_threadpool as run_in_threadpool  # noqa\nfrom starlette.concurrency import (  # noqa\n    run_until_first_complete as run_until_first_complete,\n)\n\n_T = TypeVar(\"_T\")\n\n\n@asynccontextmanager\nasync def contextmanager_in_threadpool(\n    cm: ContextManager[_T],\n) -> AsyncGenerator[_T, None]:\n    # blocking __exit__ from running waiting on a free thread\n    # can create race conditions/deadlocks if the context manager itself\n    # has its own internal pool (e.g. a database connection pool)\n    # to avoid this we let __exit__ run without a capacity limit\n    # since we're creating a new limiter for each call, any non-zero limit\n    # works (1 is arbitrary)\n    exit_limiter = CapacityLimiter(1)\n    try:\n        yield await run_in_threadpool(cm.__enter__)\n    except Exception as e:\n        ok = bool(\n            await anyio.to_thread.run_sync(\n                cm.__exit__, type(e), e, e.__traceback__, limiter=exit_limiter\n            )\n        )\n        if not ok:\n            raise e\n    else:\n        await anyio.to_thread.run_sync(\n            cm.__exit__, None, None, None, limiter=exit_limiter\n        )\n", "metadata": {"license": "MIT", "len_tokens": 330}}
{"id": "fastapi:fastapi/concurrency.py", "language": "python", "code": "async def contextmanager_in_threadpool(\n    cm: ContextManager[_T],\n) -> AsyncGenerator[_T, None]:\n    # blocking __exit__ from running waiting on a free thread\n    # can create race conditions/deadlocks if the context manager itself\n    # has its own internal pool (e.g. a database connection pool)\n    # to avoid this we let __exit__ run without a capacity limit\n    # since we're creating a new limiter for each call, any non-zero limit\n    # works (1 is arbitrary)\n    exit_limiter = CapacityLimiter(1)\n    try:\n        yield await run_in_threadpool(cm.__enter__)\n    except Exception as e:\n        ok = bool(\n            await anyio.to_thread.run_sync(\n                cm.__exit__, type(e), e, e.__traceback__, limiter=exit_limiter\n            )\n        )\n        if not ok:\n            raise e\n    else:\n        await anyio.to_thread.run_sync(\n            cm.__exit__, None, None, None, limiter=exit_limiter\n        )", "metadata": {"license": "MIT", "len_tokens": 220}}
{"id": "fastapi:fastapi/background.py", "language": "python", "code": "from typing import Any, Callable\n\nfrom annotated_doc import Doc\nfrom starlette.background import BackgroundTasks as StarletteBackgroundTasks\nfrom typing_extensions import Annotated, ParamSpec\n\nP = ParamSpec(\"P\")\n\n\nclass BackgroundTasks(StarletteBackgroundTasks):\n    \"\"\"\n    A collection of background tasks that will be called after a response has been\n    sent to the client.\n\n    Read more about it in the\n    [FastAPI docs for Background Tasks](https://fastapi.tiangolo.com/tutorial/background-tasks/).\n\n    ## Example\n\n    ```python\n    from fastapi import BackgroundTasks, FastAPI\n\n    app = FastAPI()\n\n\n    def write_notification(email: str, message=\"\"):\n        with open(\"log.txt\", mode=\"w\") as email_file:\n            content = f\"notification for {email}: {message}\"\n            email_file.write(content)\n\n\n    @app.post(\"/send-notification/{email}\")\n    async def send_notification(email: str, background_tasks: BackgroundTasks):\n        background_tasks.add_task(write_notification, email, message=\"some notification\")\n        return {\"message\": \"Notification sent in the background\"}\n    ```\n    \"\"\"\n\n    def add_task(\n        self,\n        func: Annotated[\n            Callable[P, Any],\n            Doc(\n                \"\"\"\n                The function to call after the response is sent.\n\n                It can be a regular `def` function or an `async def` function.\n                \"\"\"\n            ),\n        ],\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -> None:\n        \"\"\"\n        Add a function to be called in the background after the response is sent.\n\n        Read more about it in the\n        [FastAPI docs for Background Tasks](https://fastapi.tiangolo.com/tutorial/background-tasks/).\n        \"\"\"\n        return super().add_task(func, *args, **kwargs)\n", "metadata": {"license": "MIT", "len_tokens": 379}}
{"id": "fastapi:fastapi/background.py", "language": "python", "code": "class BackgroundTasks(StarletteBackgroundTasks):\n    \"\"\"\n    A collection of background tasks that will be called after a response has been\n    sent to the client.\n\n    Read more about it in the\n    [FastAPI docs for Background Tasks](https://fastapi.tiangolo.com/tutorial/background-tasks/).\n\n    ## Example\n\n    ```python\n    from fastapi import BackgroundTasks, FastAPI\n\n    app = FastAPI()\n\n\n    def write_notification(email: str, message=\"\"):\n        with open(\"log.txt\", mode=\"w\") as email_file:\n            content = f\"notification for {email}: {message}\"\n            email_file.write(content)\n\n\n    @app.post(\"/send-notification/{email}\")\n    async def send_notification(email: str, background_tasks: BackgroundTasks):\n        background_tasks.add_task(write_notification, email, message=\"some notification\")\n        return {\"message\": \"Notification sent in the background\"}\n    ```\n    \"\"\"\n\n    def add_task(\n        self,\n        func: Annotated[\n            Callable[P, Any],\n            Doc(\n                \"\"\"\n                The function to call after the response is sent.\n\n                It can be a regular `def` function or an `async def` function.\n                \"\"\"\n            ),\n        ],\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -> None:\n        \"\"\"\n        Add a function to be called in the background after the response is sent.\n\n        Read more about it in the\n        [FastAPI docs for Background Tasks](https://fastapi.tiangolo.com/tutorial/background-tasks/).\n        \"\"\"\n        return super().add_task(func, *args, **kwargs)", "metadata": {"license": "MIT", "len_tokens": 336}}
{"id": "fastapi:fastapi/__init__.py", "language": "python", "code": "\"\"\"FastAPI framework, high performance, easy to learn, fast to code, ready for production\"\"\"\n\n__version__ = \"0.124.4\"\n\nfrom starlette import status as status\n\nfrom .applications import FastAPI as FastAPI\nfrom .background import BackgroundTasks as BackgroundTasks\nfrom .datastructures import UploadFile as UploadFile\nfrom .exceptions import HTTPException as HTTPException\nfrom .exceptions import WebSocketException as WebSocketException\nfrom .param_functions import Body as Body\nfrom .param_functions import Cookie as Cookie\nfrom .param_functions import Depends as Depends\nfrom .param_functions import File as File\nfrom .param_functions import Form as Form\nfrom .param_functions import Header as Header\nfrom .param_functions import Path as Path\nfrom .param_functions import Query as Query\nfrom .param_functions import Security as Security\nfrom .requests import Request as Request\nfrom .responses import Response as Response\nfrom .routing import APIRouter as APIRouter\nfrom .websockets import WebSocket as WebSocket\nfrom .websockets import WebSocketDisconnect as WebSocketDisconnect\n", "metadata": {"license": "MIT", "len_tokens": 228}}
{"id": "fastapi:fastapi/utils.py", "language": "python", "code": "def create_model_field(\n    name: str,\n    type_: Any,\n    class_validators: Optional[Dict[str, Validator]] = None,\n    default: Optional[Any] = Undefined,\n    required: Union[bool, UndefinedType] = Undefined,\n    model_config: Union[Type[BaseConfig], None] = None,\n    field_info: Optional[FieldInfo] = None,\n    alias: Optional[str] = None,\n    mode: Literal[\"validation\", \"serialization\"] = \"validation\",\n    version: Literal[\"1\", \"auto\"] = \"auto\",\n) -> ModelField:\n    class_validators = class_validators or {}\n\n    v1_model_config = may_v1.BaseConfig\n    v1_field_info = field_info or may_v1.FieldInfo()\n    v1_kwargs = {\n        \"name\": name,\n        \"field_info\": v1_field_info,\n        \"type_\": type_,\n        \"class_validators\": class_validators,\n        \"default\": default,\n        \"required\": required,\n        \"model_config\": v1_model_config,\n        \"alias\": alias,\n    }\n\n    if (\n        annotation_is_pydantic_v1(type_)\n        or isinstance(field_info, may_v1.FieldInfo)\n        or version == \"1\"\n    ):\n        from fastapi._compat import v1\n\n        try:\n            return v1.ModelField(**v1_kwargs)  # type: ignore[no-any-return]\n        except RuntimeError:\n            raise fastapi.exceptions.FastAPIError(\n                _invalid_args_message.format(type_=type_)\n            ) from None\n    elif PYDANTIC_V2:\n        from ._compat import v2\n\n        field_info = field_info or FieldInfo(\n            annotation=type_, default=default, alias=alias\n        )\n        kwargs = {\"mode\": mode, \"name\": name, \"field_info\": field_info}\n        try:\n            return v2.ModelField(**kwargs)  # type: ignore[return-value,arg-type]\n        except PydanticSchemaGenerationError:\n            raise fastapi.exceptions.FastAPIError(\n                _invalid_args_message.format(type_=type_)\n            ) from None\n    # Pydantic v2 is not installed, but it's not a Pydantic v1 ModelField, it could be\n    # a Pydantic v1 type, like a constrained int\n    from fastapi._compat import v1\n\n    try:\n        return v1.ModelField(**v1_kwargs)  # type: ignore[no-any-return]\n    except RuntimeError:\n        raise fastapi.exceptions.FastAPIError(\n            _invalid_args_message.format(type_=type_)\n        ) from None", "metadata": {"license": "MIT", "len_tokens": 548}}
{"id": "fastapi:fastapi/utils.py", "language": "python", "code": "def create_cloned_field(\n    field: ModelField,\n    *,\n    cloned_types: Optional[MutableMapping[Type[BaseModel], Type[BaseModel]]] = None,\n) -> ModelField:\n    if PYDANTIC_V2:\n        from ._compat import v2\n\n        if isinstance(field, v2.ModelField):\n            return field\n\n    from fastapi._compat import v1\n\n    # cloned_types caches already cloned types to support recursive models and improve\n    # performance by avoiding unnecessary cloning\n    if cloned_types is None:\n        cloned_types = _CLONED_TYPES_CACHE\n\n    original_type = field.type_\n    if is_dataclass(original_type) and hasattr(original_type, \"__pydantic_model__\"):\n        original_type = original_type.__pydantic_model__\n    use_type = original_type\n    if lenient_issubclass(original_type, v1.BaseModel):\n        original_type = cast(Type[v1.BaseModel], original_type)\n        use_type = cloned_types.get(original_type)\n        if use_type is None:\n            use_type = v1.create_model(original_type.__name__, __base__=original_type)\n            cloned_types[original_type] = use_type\n            for f in original_type.__fields__.values():\n                use_type.__fields__[f.name] = create_cloned_field(\n                    f,\n                    cloned_types=cloned_types,\n                )\n    new_field = create_model_field(name=field.name, type_=use_type, version=\"1\")\n    new_field.has_alias = field.has_alias  # type: ignore[attr-defined]\n    new_field.alias = field.alias  # type: ignore[misc]\n    new_field.class_validators = field.class_validators  # type: ignore[attr-defined]\n    new_field.default = field.default  # type: ignore[misc]\n    new_field.default_factory = field.default_factory  # type: ignore[attr-defined]\n    new_field.required = field.required  # type: ignore[misc]\n    new_field.model_config = field.model_config  # type: ignore[attr-defined]\n    new_field.field_info = field.field_info\n    new_field.allow_none = field.allow_none  # type: ignore[attr-defined]\n    new_field.validate_always = field.validate_always  # type: ignore[attr-defined]\n    if field.sub_fields:  # type: ignore[attr-defined]\n        new_field.sub_fields = [  # type: ignore[attr-defined]\n            create_cloned_field(sub_field, cloned_types=cloned_types)\n            for sub_field in field.sub_fields  # type: ignore[attr-defined]\n        ]\n    if field.key_field:  # type: ignore[attr-defined]\n        new_field.key_field = create_cloned_field(  # type: ignore[attr-defined]\n            field.key_field,  # type: ignore[attr-defined]\n            cloned_types=cloned_types,\n        )\n    new_field.validators = field.validators  # type: ignore[attr-defined]\n    new_field.pre_validators = field.pre_validators  # type: ignore[attr-defined]\n    new_field.post_validators = field.post_validators  # type: ignore[attr-defined]\n    new_field.parse_json = field.parse_json  # type: ignore[attr-defined]\n    new_field.shape = field.shape  # type: ignore[attr-defined]\n    new_field.populate_validators()  # type: ignore[attr-defined]\n    return new_field", "metadata": {"license": "MIT", "len_tokens": 691}}
{"id": "fastapi:fastapi/routing.py", "language": "python", "code": "def request_response(\n    func: Callable[[Request], Union[Awaitable[Response], Response]],\n) -> ASGIApp:\n    \"\"\"\n    Takes a function or coroutine `func(request) -> response`,\n    and returns an ASGI application.\n    \"\"\"\n    f: Callable[[Request], Awaitable[Response]] = (\n        func if is_async_callable(func) else functools.partial(run_in_threadpool, func)  # type:ignore\n    )\n\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        request = Request(scope, receive, send)\n\n        async def app(scope: Scope, receive: Receive, send: Send) -> None:\n            # Starts customization\n            response_awaited = False\n            async with AsyncExitStack() as request_stack:\n                scope[\"fastapi_inner_astack\"] = request_stack\n                async with AsyncExitStack() as function_stack:\n                    scope[\"fastapi_function_astack\"] = function_stack\n                    response = await f(request)\n                await response(scope, receive, send)\n                # Continues customization\n                response_awaited = True\n            if not response_awaited:\n                raise FastAPIError(\n                    \"Response not awaited. There's a high chance that the \"\n                    \"application code is raising an exception and a dependency with yield \"\n                    \"has a block with a bare except, or a block with except Exception, \"\n                    \"and is not raising the exception again. Read more about it in the \"\n                    \"docs: https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-with-yield/#dependencies-with-yield-and-except\"\n                )\n\n        # Same as in Starlette\n        await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n\n    return app", "metadata": {"license": "MIT", "len_tokens": 366}}
{"id": "fastapi:fastapi/routing.py", "language": "python", "code": "def _prepare_response_content(\n    res: Any,\n    *,\n    exclude_unset: bool,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n) -> Any:\n    if isinstance(res, BaseModel):\n        read_with_orm_mode = getattr(_get_model_config(res), \"read_with_orm_mode\", None)\n        if read_with_orm_mode:\n            # Let from_orm extract the data from this model instead of converting\n            # it now to a dict.\n            # Otherwise, there's no way to extract lazy data that requires attribute\n            # access instead of dict iteration, e.g. lazy relationships.\n            return res\n        return _model_dump(\n            res,\n            by_alias=True,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n        )\n    elif isinstance(res, list):\n        return [\n            _prepare_response_content(\n                item,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n            for item in res\n        ]\n    elif isinstance(res, dict):\n        return {\n            k: _prepare_response_content(\n                v,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n            for k, v in res.items()\n        }\n    elif dataclasses.is_dataclass(res):\n        assert not isinstance(res, type)\n        return dataclasses.asdict(res)\n    return res", "metadata": {"license": "MIT", "len_tokens": 312}}
{"id": "fastapi:fastapi/routing.py", "language": "python", "code": "async def serialize_response(\n    *,\n    field: Optional[ModelField] = None,\n    response_content: Any,\n    include: Optional[IncEx] = None,\n    exclude: Optional[IncEx] = None,\n    by_alias: bool = True,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n    is_coroutine: bool = True,\n    endpoint_ctx: Optional[EndpointContext] = None,\n) -> Any:\n    if field:\n        errors = []\n        if not hasattr(field, \"serialize\"):\n            # pydantic v1\n            response_content = _prepare_response_content(\n                response_content,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n        if is_coroutine:\n            value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n        else:\n            value, errors_ = await run_in_threadpool(\n                field.validate, response_content, {}, loc=(\"response\",)\n            )\n        if isinstance(errors_, list):\n            errors.extend(errors_)\n        elif errors_:\n            errors.append(errors_)\n        if errors:\n            ctx = endpoint_ctx or EndpointContext()\n            raise ResponseValidationError(\n                errors=_normalize_errors(errors),\n                body=response_content,\n                endpoint_ctx=ctx,\n            )\n\n        if hasattr(field, \"serialize\"):\n            return field.serialize(\n                value,\n                include=include,\n                exclude=exclude,\n                by_alias=by_alias,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n\n        return jsonable_encoder(\n            value,\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n        )\n    else:\n        return jsonable_encoder(response_content)", "metadata": {"license": "MIT", "len_tokens": 401}}
{"id": "fastapi:fastapi/routing.py", "language": "python", "code": "def get_websocket_app(\n    dependant: Dependant,\n    dependency_overrides_provider: Optional[Any] = None,\n    embed_body_fields: bool = False,\n) -> Callable[[WebSocket], Coroutine[Any, Any, Any]]:\n    async def app(websocket: WebSocket) -> None:\n        endpoint_ctx = (\n            _extract_endpoint_context(dependant.call)\n            if dependant.call\n            else EndpointContext()\n        )\n        if dependant.path:\n            # For mounted sub-apps, include the mount path prefix\n            mount_path = websocket.scope.get(\"root_path\", \"\").rstrip(\"/\")\n            endpoint_ctx[\"path\"] = f\"WS {mount_path}{dependant.path}\"\n        async_exit_stack = websocket.scope.get(\"fastapi_inner_astack\")\n        assert isinstance(async_exit_stack, AsyncExitStack), (\n            \"fastapi_inner_astack not found in request scope\"\n        )\n        solved_result = await solve_dependencies(\n            request=websocket,\n            dependant=dependant,\n            dependency_overrides_provider=dependency_overrides_provider,\n            async_exit_stack=async_exit_stack,\n            embed_body_fields=embed_body_fields,\n        )\n        if solved_result.errors:\n            raise WebSocketRequestValidationError(\n                _normalize_errors(solved_result.errors),\n                endpoint_ctx=endpoint_ctx,\n            )\n        assert dependant.call is not None, \"dependant.call must be a function\"\n        await dependant.call(**solved_result.values)\n\n    return app", "metadata": {"license": "MIT", "len_tokens": 293}}
{"id": "fastapi:fastapi/routing.py", "language": "python", "code": "class APIWebSocketRoute(routing.WebSocketRoute):\n    def __init__(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        name: Optional[str] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        dependency_overrides_provider: Optional[Any] = None,\n    ) -> None:\n        self.path = path\n        self.endpoint = endpoint\n        self.name = get_name(endpoint) if name is None else name\n        self.dependencies = list(dependencies or [])\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n        self.dependant = get_dependant(\n            path=self.path_format, call=self.endpoint, scope=\"function\"\n        )\n        for depends in self.dependencies[::-1]:\n            self.dependant.dependencies.insert(\n                0,\n                get_parameterless_sub_dependant(depends=depends, path=self.path_format),\n            )\n        self._flat_dependant = get_flat_dependant(self.dependant)\n        self._embed_body_fields = _should_embed_body_fields(\n            self._flat_dependant.body_params\n        )\n        self.app = websocket_session(\n            get_websocket_app(\n                dependant=self.dependant,\n                dependency_overrides_provider=dependency_overrides_provider,\n                embed_body_fields=self._embed_body_fields,\n            )\n        )\n\n    def matches(self, scope: Scope) -> Tuple[Match, Scope]:\n        match, child_scope = super().matches(scope)\n        if match != Match.NONE:\n            child_scope[\"route\"] = self\n        return match, child_scope", "metadata": {"license": "MIT", "len_tokens": 327}}
{"id": "fastapi:fastapi/routing.py", "language": "python", "code": "async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        request = Request(scope, receive, send)\n\n        async def app(scope: Scope, receive: Receive, send: Send) -> None:\n            # Starts customization\n            response_awaited = False\n            async with AsyncExitStack() as request_stack:\n                scope[\"fastapi_inner_astack\"] = request_stack\n                async with AsyncExitStack() as function_stack:\n                    scope[\"fastapi_function_astack\"] = function_stack\n                    response = await f(request)\n                await response(scope, receive, send)\n                # Continues customization\n                response_awaited = True\n            if not response_awaited:\n                raise FastAPIError(\n                    \"Response not awaited. There's a high chance that the \"\n                    \"application code is raising an exception and a dependency with yield \"\n                    \"has a block with a bare except, or a block with except Exception, \"\n                    \"and is not raising the exception again. Read more about it in the \"\n                    \"docs: https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-with-yield/#dependencies-with-yield-and-except\"\n                )\n\n        # Same as in Starlette\n        await wrap_app_handling_exceptions(app, request)(scope, receive, send)", "metadata": {"license": "MIT", "len_tokens": 271}}
{"id": "fastapi:fastapi/routing.py", "language": "python", "code": "async def app(websocket: WebSocket) -> None:\n        endpoint_ctx = (\n            _extract_endpoint_context(dependant.call)\n            if dependant.call\n            else EndpointContext()\n        )\n        if dependant.path:\n            # For mounted sub-apps, include the mount path prefix\n            mount_path = websocket.scope.get(\"root_path\", \"\").rstrip(\"/\")\n            endpoint_ctx[\"path\"] = f\"WS {mount_path}{dependant.path}\"\n        async_exit_stack = websocket.scope.get(\"fastapi_inner_astack\")\n        assert isinstance(async_exit_stack, AsyncExitStack), (\n            \"fastapi_inner_astack not found in request scope\"\n        )\n        solved_result = await solve_dependencies(\n            request=websocket,\n            dependant=dependant,\n            dependency_overrides_provider=dependency_overrides_provider,\n            async_exit_stack=async_exit_stack,\n            embed_body_fields=embed_body_fields,\n        )\n        if solved_result.errors:\n            raise WebSocketRequestValidationError(\n                _normalize_errors(solved_result.errors),\n                endpoint_ctx=endpoint_ctx,\n            )\n        assert dependant.call is not None, \"dependant.call must be a function\"\n        await dependant.call(**solved_result.values)", "metadata": {"license": "MIT", "len_tokens": 242}}
{"id": "fastapi:fastapi/routing.py", "language": "python", "code": "def __init__(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        name: Optional[str] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        dependency_overrides_provider: Optional[Any] = None,\n    ) -> None:\n        self.path = path\n        self.endpoint = endpoint\n        self.name = get_name(endpoint) if name is None else name\n        self.dependencies = list(dependencies or [])\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n        self.dependant = get_dependant(\n            path=self.path_format, call=self.endpoint, scope=\"function\"\n        )\n        for depends in self.dependencies[::-1]:\n            self.dependant.dependencies.insert(\n                0,\n                get_parameterless_sub_dependant(depends=depends, path=self.path_format),\n            )\n        self._flat_dependant = get_flat_dependant(self.dependant)\n        self._embed_body_fields = _should_embed_body_fields(\n            self._flat_dependant.body_params\n        )\n        self.app = websocket_session(\n            get_websocket_app(\n                dependant=self.dependant,\n                dependency_overrides_provider=dependency_overrides_provider,\n                embed_body_fields=self._embed_body_fields,\n            )\n        )", "metadata": {"license": "MIT", "len_tokens": 266}}
{"id": "fastapi:fastapi/routing.py", "language": "python", "code": "def add_api_route(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[Union[Set[str], List[str]]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Union[Type[Response], DefaultPlaceholder] = Default(\n            JSONResponse\n        ),\n        name: Optional[str] = None,\n        route_class_override: Optional[Type[APIRoute]] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Union[\n            Callable[[APIRoute], str], DefaultPlaceholder\n        ] = Default(generate_unique_id),\n    ) -> None:\n        route_class = route_class_override or self.route_class\n        responses = responses or {}\n        combined_responses = {**self.responses, **responses}\n        current_response_class = get_value_or_default(\n            response_class, self.default_response_class\n        )\n        current_tags = self.tags.copy()\n        if tags:\n            current_tags.extend(tags)\n        current_dependencies = self.dependencies.copy()\n        if dependencies:\n            current_dependencies.extend(dependencies)\n        current_callbacks = self.callbacks.copy()\n        if callbacks:\n            current_callbacks.extend(callbacks)\n        current_generate_unique_id = get_value_or_default(\n            generate_unique_id_function, self.generate_unique_id_function\n        )\n        route = route_class(\n            self.prefix + path,\n            endpoint=endpoint,\n            response_model=response_model,\n            status_code=status_code,\n            tags=current_tags,\n            dependencies=current_dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=combined_responses,\n            deprecated=deprecated or self.deprecated,\n            methods=methods,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema and self.include_in_schema,\n            response_class=current_response_class,\n            name=name,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n            callbacks=current_callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=current_generate_unique_id,\n        )\n        self.routes.append(route)", "metadata": {"license": "MIT", "len_tokens": 690}}
{"id": "fastapi:fastapi/routing.py", "language": "python", "code": "def api_route(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[List[str]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_api_route(\n                path,\n                func,\n                response_model=response_model,\n                status_code=status_code,\n                tags=tags,\n                dependencies=dependencies,\n                summary=summary,\n                description=description,\n                response_description=response_description,\n                responses=responses,\n                deprecated=deprecated,\n                methods=methods,\n                operation_id=operation_id,\n                response_model_include=response_model_include,\n                response_model_exclude=response_model_exclude,\n                response_model_by_alias=response_model_by_alias,\n                response_model_exclude_unset=response_model_exclude_unset,\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                include_in_schema=include_in_schema,\n                response_class=response_class,\n                name=name,\n                callbacks=callbacks,\n                openapi_extra=openapi_extra,\n                generate_unique_id_function=generate_unique_id_function,\n            )\n            return func\n\n        return decorator", "metadata": {"license": "MIT", "len_tokens": 509}}
{"id": "fastapi:fastapi/routing.py", "language": "python", "code": "def websocket(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                WebSocket path.\n                \"\"\"\n            ),\n        ],\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A name for the WebSocket. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        *,\n        dependencies: Annotated[\n            Optional[Sequence[params.Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be used for this\n                WebSocket.\n\n                Read more about it in the\n                [FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).\n                \"\"\"\n            ),\n        ] = None,\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Decorate a WebSocket function.\n\n        Read more about it in the\n        [FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).\n\n        **Example**\n\n        ## Example\n\n        ```python\n        from fastapi import APIRouter, FastAPI, WebSocket\n\n        app = FastAPI()\n        router = APIRouter()\n\n        @router.websocket(\"/ws\")\n        async def websocket_endpoint(websocket: WebSocket):\n            await websocket.accept()\n            while True:\n                data = await websocket.receive_text()\n                await websocket.send_text(f\"Message text was: {data}\")\n\n        app.include_router(router)\n        ```\n        \"\"\"\n\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_api_websocket_route(\n                path, func, name=name, dependencies=dependencies\n            )\n            return func\n\n        return decorator", "metadata": {"license": "MIT", "len_tokens": 348}}
{"id": "fastapi:fastapi/routing.py", "language": "python", "code": "async def app(scope: Scope, receive: Receive, send: Send) -> None:\n            # Starts customization\n            response_awaited = False\n            async with AsyncExitStack() as request_stack:\n                scope[\"fastapi_inner_astack\"] = request_stack\n                async with AsyncExitStack() as function_stack:\n                    scope[\"fastapi_function_astack\"] = function_stack\n                    response = await f(request)\n                await response(scope, receive, send)\n                # Continues customization\n                response_awaited = True\n            if not response_awaited:\n                raise FastAPIError(\n                    \"Response not awaited. There's a high chance that the \"\n                    \"application code is raising an exception and a dependency with yield \"\n                    \"has a block with a bare except, or a block with except Exception, \"\n                    \"and is not raising the exception again. Read more about it in the \"\n                    \"docs: https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-with-yield/#dependencies-with-yield-and-except\"\n                )", "metadata": {"license": "MIT", "len_tokens": 216}}
{"id": "fastapi:fastapi/temp_pydantic_v1_params.py", "language": "python", "code": "class Path(Param):  # type: ignore[misc]\n    in_ = ParamTypes.path\n\n    def __init__(\n        self,\n        default: Any = ...,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        assert default is ..., \"Path parameters cannot have a default value\"\n        self.in_ = self.in_\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 693}}
{"id": "fastapi:fastapi/temp_pydantic_v1_params.py", "language": "python", "code": "class Query(Param):  # type: ignore[misc]\n    in_ = ParamTypes.query\n\n    def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 671}}
{"id": "fastapi:fastapi/temp_pydantic_v1_params.py", "language": "python", "code": "class Header(Param):  # type: ignore[misc]\n    in_ = ParamTypes.header\n\n    def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        convert_underscores: bool = True,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        self.convert_underscores = convert_underscores\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 693}}
{"id": "fastapi:fastapi/temp_pydantic_v1_params.py", "language": "python", "code": "class Cookie(Param):  # type: ignore[misc]\n    in_ = ParamTypes.cookie\n\n    def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 671}}
{"id": "fastapi:fastapi/temp_pydantic_v1_params.py", "language": "python", "code": "class Form(Body):  # type: ignore[misc]\n    def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        media_type: str = \"application/x-www-form-urlencoded\",\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            media_type=media_type,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 683}}
{"id": "fastapi:fastapi/temp_pydantic_v1_params.py", "language": "python", "code": "class File(Form):  # type: ignore[misc]\n    def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        media_type: str = \"multipart/form-data\",\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            media_type=media_type,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 680}}
{"id": "fastapi:fastapi/temp_pydantic_v1_params.py", "language": "python", "code": "def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        if example is not _Unset:\n            warnings.warn(\n                \"`example` has been deprecated, please use `examples` instead\",\n                category=DeprecationWarning,\n                stacklevel=4,\n            )\n        self.example = example\n        self.include_in_schema = include_in_schema\n        self.openapi_examples = openapi_examples\n        kwargs = dict(\n            default=default,\n            default_factory=default_factory,\n            alias=alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            discriminator=discriminator,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            **extra,\n        )\n        if examples is not None:\n            kwargs[\"examples\"] = examples\n        if regex is not None:\n            warnings.warn(\n                \"`regex` has been deprecated, please use `pattern` instead\",\n                category=DeprecationWarning,\n                stacklevel=4,\n            )\n        current_json_schema_extra = json_schema_extra or extra\n        if PYDANTIC_VERSION_MINOR_TUPLE < (2, 7):\n            self.deprecated = deprecated\n        else:\n            kwargs[\"deprecated\"] = deprecated\n        kwargs[\"regex\"] = pattern or regex\n        kwargs.update(**current_json_schema_extra)\n        use_kwargs = {k: v for k, v in kwargs.items() if v is not _Unset}\n\n        super().__init__(**use_kwargs)", "metadata": {"license": "MIT", "len_tokens": 792}}
{"id": "fastapi:fastapi/temp_pydantic_v1_params.py", "language": "python", "code": "def __init__(\n        self,\n        default: Any = ...,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        assert default is ..., \"Path parameters cannot have a default value\"\n        self.in_ = self.in_\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 671}}
{"id": "fastapi:fastapi/temp_pydantic_v1_params.py", "language": "python", "code": "def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 649}}
{"id": "fastapi:fastapi/temp_pydantic_v1_params.py", "language": "python", "code": "def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        convert_underscores: bool = True,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        self.convert_underscores = convert_underscores\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 671}}
{"id": "fastapi:fastapi/temp_pydantic_v1_params.py", "language": "python", "code": "def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 649}}
{"id": "fastapi:fastapi/temp_pydantic_v1_params.py", "language": "python", "code": "def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        media_type: str = \"application/x-www-form-urlencoded\",\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            media_type=media_type,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 669}}
{"id": "fastapi:fastapi/temp_pydantic_v1_params.py", "language": "python", "code": "def __init__(\n        self,\n        default: Any = Undefined,\n        *,\n        default_factory: Union[Callable[[], Any], None] = _Unset,\n        annotation: Optional[Any] = None,\n        media_type: str = \"multipart/form-data\",\n        alias: Optional[str] = None,\n        alias_priority: Union[int, None] = _Unset,\n        # TODO: update when deprecating Pydantic v1, import these types\n        # validation_alias: str | AliasPath | AliasChoices | None\n        validation_alias: Union[str, None] = None,\n        serialization_alias: Union[str, None] = None,\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        gt: Optional[float] = None,\n        ge: Optional[float] = None,\n        lt: Optional[float] = None,\n        le: Optional[float] = None,\n        min_length: Optional[int] = None,\n        max_length: Optional[int] = None,\n        pattern: Optional[str] = None,\n        regex: Annotated[\n            Optional[str],\n            deprecated(\n                \"Deprecated in FastAPI 0.100.0 and Pydantic v2, use `pattern` instead.\"\n            ),\n        ] = None,\n        discriminator: Union[str, None] = None,\n        strict: Union[bool, None] = _Unset,\n        multiple_of: Union[float, None] = _Unset,\n        allow_inf_nan: Union[bool, None] = _Unset,\n        max_digits: Union[int, None] = _Unset,\n        decimal_places: Union[int, None] = _Unset,\n        examples: Optional[List[Any]] = None,\n        example: Annotated[\n            Optional[Any],\n            deprecated(\n                \"Deprecated in OpenAPI 3.1.0 that now uses JSON Schema 2020-12, \"\n                \"although still supported. Use examples instead.\"\n            ),\n        ] = _Unset,\n        openapi_examples: Optional[Dict[str, Example]] = None,\n        deprecated: Union[deprecated, str, bool, None] = None,\n        include_in_schema: bool = True,\n        json_schema_extra: Union[Dict[str, Any], None] = None,\n        **extra: Any,\n    ):\n        super().__init__(\n            default=default,\n            default_factory=default_factory,\n            annotation=annotation,\n            media_type=media_type,\n            alias=alias,\n            alias_priority=alias_priority,\n            validation_alias=validation_alias,\n            serialization_alias=serialization_alias,\n            title=title,\n            description=description,\n            gt=gt,\n            ge=ge,\n            lt=lt,\n            le=le,\n            min_length=min_length,\n            max_length=max_length,\n            pattern=pattern,\n            regex=regex,\n            discriminator=discriminator,\n            strict=strict,\n            multiple_of=multiple_of,\n            allow_inf_nan=allow_inf_nan,\n            max_digits=max_digits,\n            decimal_places=decimal_places,\n            deprecated=deprecated,\n            example=example,\n            examples=examples,\n            openapi_examples=openapi_examples,\n            include_in_schema=include_in_schema,\n            json_schema_extra=json_schema_extra,\n            **extra,\n        )", "metadata": {"license": "MIT", "len_tokens": 667}}
{"id": "fastapi:fastapi/exceptions.py", "language": "python", "code": "class HTTPException(StarletteHTTPException):\n    \"\"\"\n    An HTTP exception you can raise in your own code to show errors to the client.\n\n    This is for client errors, invalid authentication, invalid data, etc. Not for server\n    errors in your code.\n\n    Read more about it in the\n    [FastAPI docs for Handling Errors](https://fastapi.tiangolo.com/tutorial/handling-errors/).\n\n    ## Example\n\n    ```python\n    from fastapi import FastAPI, HTTPException\n\n    app = FastAPI()\n\n    items = {\"foo\": \"The Foo Wrestlers\"}\n\n\n    @app.get(\"/items/{item_id}\")\n    async def read_item(item_id: str):\n        if item_id not in items:\n            raise HTTPException(status_code=404, detail=\"Item not found\")\n        return {\"item\": items[item_id]}\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        status_code: Annotated[\n            int,\n            Doc(\n                \"\"\"\n                HTTP status code to send to the client.\n                \"\"\"\n            ),\n        ],\n        detail: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                Any data to be sent to the client in the `detail` key of the JSON\n                response.\n                \"\"\"\n            ),\n        ] = None,\n        headers: Annotated[\n            Optional[Dict[str, str]],\n            Doc(\n                \"\"\"\n                Any headers to send to the client in the response.\n                \"\"\"\n            ),\n        ] = None,\n    ) -> None:\n        super().__init__(status_code=status_code, detail=detail, headers=headers)", "metadata": {"license": "MIT", "len_tokens": 325}}
{"id": "fastapi:fastapi/exceptions.py", "language": "python", "code": "class WebSocketException(StarletteWebSocketException):\n    \"\"\"\n    A WebSocket exception you can raise in your own code to show errors to the client.\n\n    This is for client errors, invalid authentication, invalid data, etc. Not for server\n    errors in your code.\n\n    Read more about it in the\n    [FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).\n\n    ## Example\n\n    ```python\n    from typing import Annotated\n\n    from fastapi import (\n        Cookie,\n        FastAPI,\n        WebSocket,\n        WebSocketException,\n        status,\n    )\n\n    app = FastAPI()\n\n    @app.websocket(\"/items/{item_id}/ws\")\n    async def websocket_endpoint(\n        *,\n        websocket: WebSocket,\n        session: Annotated[str | None, Cookie()] = None,\n        item_id: str,\n    ):\n        if session is None:\n            raise WebSocketException(code=status.WS_1008_POLICY_VIOLATION)\n        await websocket.accept()\n        while True:\n            data = await websocket.receive_text()\n            await websocket.send_text(f\"Session cookie is: {session}\")\n            await websocket.send_text(f\"Message text was: {data}, for item ID: {item_id}\")\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        code: Annotated[\n            int,\n            Doc(\n                \"\"\"\n                A closing code from the\n                [valid codes defined in the specification](https://datatracker.ietf.org/doc/html/rfc6455#section-7.4.1).\n                \"\"\"\n            ),\n        ],\n        reason: Annotated[\n            Union[str, None],\n            Doc(\n                \"\"\"\n                The reason to close the WebSocket connection.\n\n                It is UTF-8-encoded data. The interpretation of the reason is up to the\n                application, it is not specified by the WebSocket specification.\n\n                It could contain text that could be human-readable or interpretable\n                by the client code, etc.\n                \"\"\"\n            ),\n        ] = None,\n    ) -> None:\n        super().__init__(code=code, reason=reason)", "metadata": {"license": "MIT", "len_tokens": 442}}
{"id": "fastapi:fastapi/exceptions.py", "language": "python", "code": "class ValidationException(Exception):\n    def __init__(\n        self,\n        errors: Sequence[Any],\n        *,\n        endpoint_ctx: Optional[EndpointContext] = None,\n    ) -> None:\n        self._errors = errors\n        self.endpoint_ctx = endpoint_ctx\n\n        ctx = endpoint_ctx or {}\n        self.endpoint_function = ctx.get(\"function\")\n        self.endpoint_path = ctx.get(\"path\")\n        self.endpoint_file = ctx.get(\"file\")\n        self.endpoint_line = ctx.get(\"line\")\n\n    def errors(self) -> Sequence[Any]:\n        return self._errors\n\n    def _format_endpoint_context(self) -> str:\n        if not (self.endpoint_file and self.endpoint_line and self.endpoint_function):\n            if self.endpoint_path:\n                return f\"\\n  Endpoint: {self.endpoint_path}\"\n            return \"\"\n\n        context = f'\\n  File \"{self.endpoint_file}\", line {self.endpoint_line}, in {self.endpoint_function}'\n        if self.endpoint_path:\n            context += f\"\\n    {self.endpoint_path}\"\n        return context\n\n    def __str__(self) -> str:\n        message = f\"{len(self._errors)} validation error{'s' if len(self._errors) != 1 else ''}:\\n\"\n        for err in self._errors:\n            message += f\"  {err}\\n\"\n        message += self._format_endpoint_context()\n        return message.rstrip()", "metadata": {"license": "MIT", "len_tokens": 284}}
{"id": "fastapi:fastapi/param_functions.py", "language": "python", "code": "def Depends(  # noqa: N802\n    dependency: Annotated[\n        Optional[Callable[..., Any]],\n        Doc(\n            \"\"\"\n            A \"dependable\" callable (like a function).\n\n            Don't call it directly, FastAPI will call it for you, just pass the object\n            directly.\n            \"\"\"\n        ),\n    ] = None,\n    *,\n    use_cache: Annotated[\n        bool,\n        Doc(\n            \"\"\"\n            By default, after a dependency is called the first time in a request, if\n            the dependency is declared again for the rest of the request (for example\n            if the dependency is needed by several dependencies), the value will be\n            re-used for the rest of the request.\n\n            Set `use_cache` to `False` to disable this behavior and ensure the\n            dependency is called again (if declared more than once) in the same request.\n            \"\"\"\n        ),\n    ] = True,\n    scope: Annotated[\n        Union[Literal[\"function\", \"request\"], None],\n        Doc(\n            \"\"\"\n            Mainly for dependencies with `yield`, define when the dependency function\n            should start (the code before `yield`) and when it should end (the code\n            after `yield`).\n\n            * `\"function\"`: start the dependency before the *path operation function*\n                that handles the request, end the dependency after the *path operation\n                function* ends, but **before** the response is sent back to the client.\n                So, the dependency function will be executed **around** the *path operation\n                **function***.\n            * `\"request\"`: start the dependency before the *path operation function*\n                that handles the request (similar to when using `\"function\"`), but end\n                **after** the response is sent back to the client. So, the dependency\n                function will be executed **around** the **request** and response cycle.\n            \"\"\"\n        ),\n    ] = None,\n) -> Any:\n    \"\"\"\n    Declare a FastAPI dependency.\n\n    It takes a single \"dependable\" callable (like a function).\n\n    Don't call it directly, FastAPI will call it for you.\n\n    Read more about it in the\n    [FastAPI docs for Dependencies](https://fastapi.tiangolo.com/tutorial/dependencies/).\n\n    **Example**\n\n    ```python\n    from typing import Annotated\n\n    from fastapi import Depends, FastAPI\n\n    app = FastAPI()\n\n\n    async def common_parameters(q: str | None = None, skip: int = 0, limit: int = 100):\n        return {\"q\": q, \"skip\": skip, \"limit\": limit}\n\n\n    @app.get(\"/items/\")\n    async def read_items(commons: Annotated[dict, Depends(common_parameters)]):\n        return commons\n    ```\n    \"\"\"\n    return params.Depends(dependency=dependency, use_cache=use_cache, scope=scope)", "metadata": {"license": "MIT", "len_tokens": 599}}
{"id": "fastapi:fastapi/param_functions.py", "language": "python", "code": "def Security(  # noqa: N802\n    dependency: Annotated[\n        Optional[Callable[..., Any]],\n        Doc(\n            \"\"\"\n            A \"dependable\" callable (like a function).\n\n            Don't call it directly, FastAPI will call it for you, just pass the object\n            directly.\n            \"\"\"\n        ),\n    ] = None,\n    *,\n    scopes: Annotated[\n        Optional[Sequence[str]],\n        Doc(\n            \"\"\"\n            OAuth2 scopes required for the *path operation* that uses this Security\n            dependency.\n\n            The term \"scope\" comes from the OAuth2 specification, it seems to be\n            intentionally vague and interpretable. It normally refers to permissions,\n            in cases to roles.\n\n            These scopes are integrated with OpenAPI (and the API docs at `/docs`).\n            So they are visible in the OpenAPI specification.\n            )\n            \"\"\"\n        ),\n    ] = None,\n    use_cache: Annotated[\n        bool,\n        Doc(\n            \"\"\"\n            By default, after a dependency is called the first time in a request, if\n            the dependency is declared again for the rest of the request (for example\n            if the dependency is needed by several dependencies), the value will be\n            re-used for the rest of the request.\n\n            Set `use_cache` to `False` to disable this behavior and ensure the\n            dependency is called again (if declared more than once) in the same request.\n            \"\"\"\n        ),\n    ] = True,\n) -> Any:\n    \"\"\"\n    Declare a FastAPI Security dependency.\n\n    The only difference with a regular dependency is that it can declare OAuth2\n    scopes that will be integrated with OpenAPI and the automatic UI docs (by default\n    at `/docs`).\n\n    It takes a single \"dependable\" callable (like a function).\n\n    Don't call it directly, FastAPI will call it for you.\n\n    Read more about it in the\n    [FastAPI docs for Security](https://fastapi.tiangolo.com/tutorial/security/) and\n    in the\n    [FastAPI docs for OAuth2 scopes](https://fastapi.tiangolo.com/advanced/security/oauth2-scopes/).\n\n    **Example**\n\n    ```python\n    from typing import Annotated\n\n    from fastapi import Security, FastAPI\n\n    from .db import User\n    from .security import get_current_active_user\n\n    app = FastAPI()\n\n    @app.get(\"/users/me/items/\")\n    async def read_own_items(\n        current_user: Annotated[User, Security(get_current_active_user, scopes=[\"items\"])]\n    ):\n        return [{\"item_id\": \"Foo\", \"owner\": current_user.username}]\n    ```\n    \"\"\"\n    return params.Security(dependency=dependency, scopes=scopes, use_cache=use_cache)", "metadata": {"license": "MIT", "len_tokens": 573}}
{"id": "fastapi:scripts/contributors.py", "language": "python", "code": "def get_contributors(pr_nodes: list[PullRequestNode]) -> ContributorsResults:\n    contributors = Counter[str]()\n    translation_reviewers = Counter[str]()\n    translators = Counter[str]()\n    authors: dict[str, Author] = {}\n\n    for pr in pr_nodes:\n        if pr.author:\n            authors[pr.author.login] = pr.author\n        is_lang = False\n        for label in pr.labels.nodes:\n            if label.name == \"lang-all\":\n                is_lang = True\n                break\n        for review in pr.reviews.nodes:\n            if review.author:\n                authors[review.author.login] = review.author\n                if is_lang:\n                    translation_reviewers[review.author.login] += 1\n        if pr.state == \"MERGED\" and pr.author:\n            if is_lang:\n                translators[pr.author.login] += 1\n            else:\n                contributors[pr.author.login] += 1\n    return ContributorsResults(\n        contributors=contributors,\n        translation_reviewers=translation_reviewers,\n        translators=translators,\n        authors=authors,\n    )", "metadata": {"license": "MIT", "len_tokens": 221}}
{"id": "fastapi:scripts/contributors.py", "language": "python", "code": "def main() -> None:\n    logging.basicConfig(level=logging.INFO)\n    settings = Settings()\n    logging.info(f\"Using config: {settings.model_dump_json()}\")\n    g = Github(settings.github_token.get_secret_value())\n    repo = g.get_repo(settings.github_repository)\n\n    pr_nodes = get_pr_nodes(settings=settings)\n    contributors_results = get_contributors(pr_nodes=pr_nodes)\n    authors = contributors_results.authors\n\n    top_contributors = get_users_to_write(\n        counter=contributors_results.contributors,\n        authors=authors,\n    )\n\n    top_translators = get_users_to_write(\n        counter=contributors_results.translators,\n        authors=authors,\n    )\n    top_translations_reviewers = get_users_to_write(\n        counter=contributors_results.translation_reviewers,\n        authors=authors,\n    )\n\n    # For local development\n    # contributors_path = Path(\"../docs/en/data/contributors.yml\")\n    contributors_path = Path(\"./docs/en/data/contributors.yml\")\n    # translators_path = Path(\"../docs/en/data/translators.yml\")\n    translators_path = Path(\"./docs/en/data/translators.yml\")\n    # translation_reviewers_path = Path(\"../docs/en/data/translation_reviewers.yml\")\n    translation_reviewers_path = Path(\"./docs/en/data/translation_reviewers.yml\")\n\n    updated = [\n        update_content(content_path=contributors_path, new_content=top_contributors),\n        update_content(content_path=translators_path, new_content=top_translators),\n        update_content(\n            content_path=translation_reviewers_path,\n            new_content=top_translations_reviewers,\n        ),\n    ]\n\n    if not any(updated):\n        logging.info(\"The data hasn't changed, finishing.\")\n        return\n\n    logging.info(\"Setting up GitHub Actions git user\")\n    subprocess.run([\"git\", \"config\", \"user.name\", \"github-actions\"], check=True)\n    subprocess.run(\n        [\"git\", \"config\", \"user.email\", \"github-actions@github.com\"], check=True\n    )\n    branch_name = f\"fastapi-people-contributors-{secrets.token_hex(4)}\"\n    logging.info(f\"Creating a new branch {branch_name}\")\n    subprocess.run([\"git\", \"checkout\", \"-b\", branch_name], check=True)\n    logging.info(\"Adding updated file\")\n    subprocess.run(\n        [\n            \"git\",\n            \"add\",\n            str(contributors_path),\n            str(translators_path),\n            str(translation_reviewers_path),\n        ],\n        check=True,\n    )\n    logging.info(\"Committing updated file\")\n    message = \" Update FastAPI People - Contributors and Translators\"\n    subprocess.run([\"git\", \"commit\", \"-m\", message], check=True)\n    logging.info(\"Pushing branch\")\n    subprocess.run([\"git\", \"push\", \"origin\", branch_name], check=True)\n    logging.info(\"Creating PR\")\n    pr = repo.create_pull(title=message, body=message, base=\"master\", head=branch_name)\n    logging.info(f\"Created PR: {pr.number}\")\n    logging.info(\"Finished\")", "metadata": {"license": "MIT", "len_tokens": 627}}
{"id": "fastapi:scripts/translate.py", "language": "python", "code": "def translate_page(\n    *,\n    language: Annotated[str, typer.Option(envvar=\"LANGUAGE\")],\n    en_path: Annotated[Path, typer.Option(envvar=\"EN_PATH\")],\n) -> None:\n    assert language != \"en\", (\n        \"`en` is the source language, choose another language as translation target\"\n    )\n    langs = get_langs()\n    language_name = langs[language]\n    lang_path = Path(f\"docs/{language}\")\n    lang_path.mkdir(exist_ok=True)\n    lang_prompt_path = lang_path / \"llm-prompt.md\"\n    assert lang_prompt_path.exists(), f\"Prompt file not found: {lang_prompt_path}\"\n    lang_prompt_content = lang_prompt_path.read_text(encoding=\"utf-8\")\n\n    en_docs_path = Path(\"docs/en/docs\")\n    assert str(en_path).startswith(str(en_docs_path)), (\n        f\"Path must be inside {en_docs_path}\"\n    )\n    out_path = generate_lang_path(lang=language, path=en_path)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    original_content = en_path.read_text(encoding=\"utf-8\")\n    old_translation: str | None = None\n    if out_path.exists():\n        print(f\"Found existing translation: {out_path}\")\n        old_translation = out_path.read_text(encoding=\"utf-8\")\n    print(f\"Translating {en_path} to {language} ({language_name})\")\n    agent = Agent(\"openai:gpt-5\")\n\n    prompt_segments = [\n        general_prompt,\n        lang_prompt_content,\n    ]\n    if old_translation:\n        prompt_segments.extend(\n            [\n                \"There is an existing previous translation for the original English content, that may be outdated.\",\n                \"Update the translation only where necessary:\",\n                \"- If the original English content has added parts, also add these parts to the translation.\",\n                \"- If the original English content has removed parts, also remove them from the translation, unless you were instructed earlier to not do that in specific cases.\",\n                \"- If parts of the original English content have changed, also change those parts in the translation.\",\n                \"- If the previous translation violates current instructions, update it.\",\n                \"- Otherwise, preserve the original translation LINE-BY-LINE, AS-IS.\",\n                \"Do not:\",\n                \"- rephrase or rewrite correct lines just to improve the style.\",\n                \"- add or remove line breaks, unless the original English content changed.\",\n                \"- change formatting or whitespace unless absolutely required.\",\n                \"Only change what must be changed. The goal is to minimize diffs for easier human review.\",\n                \"UNLESS you were instructed earlier to behave different, there MUST NOT be whole sentences or partial sentences in the updated translation, which are not in the original English content, and there MUST NOT be whole sentences or partial sentences in the original English content, which are not in the updated translation. Remember: the updated translation shall be IN SYNC with the original English content.\",\n                \"Previous translation:\",\n                f\"%%%\\n{old_translation}%%%\",\n            ]\n        )\n    prompt_segments.extend(\n        [\n            f\"Translate to {language} ({language_name}).\",\n            \"Original content:\",\n            f\"%%%\\n{original_content}%%%\",\n        ]\n    )\n    prompt = \"\\n\\n\".join(prompt_segments)\n    print(f\"Running agent for {out_path}\")\n    result = agent.run_sync(prompt)\n    out_content = f\"{result.output.strip()}\\n\"\n    print(f\"Saving translation to {out_path}\")\n    out_path.write_text(out_content, encoding=\"utf-8\", newline=\"\\n\")", "metadata": {"license": "MIT", "len_tokens": 743}}
{"id": "fastapi:scripts/translate.py", "language": "python", "code": "def translate_lang(language: Annotated[str, typer.Option(envvar=\"LANGUAGE\")]) -> None:\n    paths_to_process = list(iter_en_paths_to_translate())\n    print(\"Original paths:\")\n    for p in paths_to_process:\n        print(f\"  - {p}\")\n    print(f\"Total original paths: {len(paths_to_process)}\")\n    missing_paths: list[Path] = []\n    skipped_paths: list[Path] = []\n    for p in paths_to_process:\n        lang_path = generate_lang_path(lang=language, path=p)\n        if lang_path.exists():\n            skipped_paths.append(p)\n            continue\n        missing_paths.append(p)\n    print(\"Paths to skip:\")\n    for p in skipped_paths:\n        print(f\"  - {p}\")\n    print(f\"Total paths to skip: {len(skipped_paths)}\")\n    print(\"Paths to process:\")\n    for p in missing_paths:\n        print(f\"  - {p}\")\n    print(f\"Total paths to process: {len(missing_paths)}\")\n    for p in missing_paths:\n        print(f\"Translating: {p}\")\n        translate_page(language=\"es\", en_path=p)\n        print(f\"Done translating: {p}\")", "metadata": {"license": "MIT", "len_tokens": 251}}
{"id": "fastapi:scripts/translate.py", "language": "python", "code": "def make_pr(\n    *,\n    language: Annotated[str | None, typer.Option(envvar=\"LANGUAGE\")] = None,\n    github_token: Annotated[str, typer.Option(envvar=\"GITHUB_TOKEN\")],\n    github_repository: Annotated[str, typer.Option(envvar=\"GITHUB_REPOSITORY\")],\n) -> None:\n    print(\"Setting up GitHub Actions git user\")\n    repo = git.Repo(Path(__file__).absolute().parent.parent)\n    if not repo.is_dirty(untracked_files=True):\n        print(\"Repository is clean, no changes to commit\")\n        return\n    subprocess.run([\"git\", \"config\", \"user.name\", \"github-actions\"], check=True)\n    subprocess.run(\n        [\"git\", \"config\", \"user.email\", \"github-actions@github.com\"], check=True\n    )\n    branch_name = \"translate\"\n    if language:\n        branch_name += f\"-{language}\"\n    branch_name += f\"-{secrets.token_hex(4)}\"\n    print(f\"Creating a new branch {branch_name}\")\n    subprocess.run([\"git\", \"checkout\", \"-b\", branch_name], check=True)\n    print(\"Adding updated files\")\n    git_path = Path(\"docs\")\n    subprocess.run([\"git\", \"add\", str(git_path)], check=True)\n    print(\"Committing updated file\")\n    message = \" Update translations\"\n    if language:\n        message += f\" for {language}\"\n    subprocess.run([\"git\", \"commit\", \"-m\", message], check=True)\n    print(\"Pushing branch\")\n    subprocess.run([\"git\", \"push\", \"origin\", branch_name], check=True)\n    print(\"Creating PR\")\n    g = Github(github_token)\n    gh_repo = g.get_repo(github_repository)\n    pr = gh_repo.create_pull(\n        title=message, body=message, base=\"master\", head=branch_name\n    )\n    print(f\"Created PR: {pr.number}\")\n    print(\"Finished\")", "metadata": {"license": "MIT", "len_tokens": 405}}
{"id": "fastapi:scripts/label_approved.py", "language": "python", "code": "import logging\nfrom typing import Literal\n\nfrom github import Github\nfrom github.PullRequestReview import PullRequestReview\nfrom pydantic import BaseModel, SecretStr\nfrom pydantic_settings import BaseSettings\n\n\nclass LabelSettings(BaseModel):\n    await_label: str | None = None\n    number: int\n\n\ndefault_config = {\"approved-2\": LabelSettings(await_label=\"awaiting-review\", number=2)}\n\n\nclass Settings(BaseSettings):\n    github_repository: str\n    token: SecretStr\n    debug: bool | None = False\n    config: dict[str, LabelSettings] | Literal[\"\"] = default_config\n\n\nsettings = Settings()\nif settings.debug:\n    logging.basicConfig(level=logging.DEBUG)\nelse:\n    logging.basicConfig(level=logging.INFO)\nlogging.debug(f\"Using config: {settings.model_dump_json()}\")\ng = Github(settings.token.get_secret_value())\nrepo = g.get_repo(settings.github_repository)\nfor pr in repo.get_pulls(state=\"open\"):\n    logging.info(f\"Checking PR: #{pr.number}\")\n    pr_labels = list(pr.get_labels())\n    pr_label_by_name = {label.name: label for label in pr_labels}\n    reviews = list(pr.get_reviews())\n    review_by_user: dict[str, PullRequestReview] = {}\n    for review in reviews:\n        if review.user.login in review_by_user:\n            stored_review = review_by_user[review.user.login]\n            if review.submitted_at >= stored_review.submitted_at:\n                review_by_user[review.user.login] = review\n        else:\n            review_by_user[review.user.login] = review\n    approved_reviews = [\n        review for review in review_by_user.values() if review.state == \"APPROVED\"\n    ]\n    config = settings.config or default_config\n    for approved_label, conf in config.items():\n        logging.debug(f\"Processing config: {conf.model_dump_json()}\")\n        if conf.await_label is None or (conf.await_label in pr_label_by_name):\n            logging.debug(f\"Processable PR: {pr.number}\")\n            if len(approved_reviews) >= conf.number:\n                logging.info(f\"Adding label to PR: {pr.number}\")\n                pr.add_to_labels(approved_label)\n                if conf.await_label:\n                    logging.info(f\"Removing label from PR: {pr.number}\")\n                    pr.remove_from_labels(conf.await_label)\nlogging.info(\"Finished\")\n", "metadata": {"license": "MIT", "len_tokens": 485}}
{"id": "fastapi:scripts/notify_translations.py", "language": "python", "code": "def get_graphql_response(\n    *,\n    settings: Settings,\n    query: str,\n    after: Union[str, None] = None,\n    category_id: Union[str, None] = None,\n    discussion_number: Union[int, None] = None,\n    discussion_id: Union[str, None] = None,\n    comment_id: Union[str, None] = None,\n    body: Union[str, None] = None,\n) -> Dict[str, Any]:\n    headers = {\"Authorization\": f\"token {settings.github_token.get_secret_value()}\"}\n    variables = {\n        \"after\": after,\n        \"category_id\": category_id,\n        \"discussion_number\": discussion_number,\n        \"discussion_id\": discussion_id,\n        \"comment_id\": comment_id,\n        \"body\": body,\n    }\n    response = httpx.post(\n        github_graphql_url,\n        headers=headers,\n        timeout=settings.httpx_timeout,\n        json={\"query\": query, \"variables\": variables, \"operationName\": \"Q\"},\n    )\n    if response.status_code != 200:\n        logging.error(\n            f\"Response was not 200, after: {after}, category_id: {category_id}\"\n        )\n        logging.error(response.text)\n        raise RuntimeError(response.text)\n    data = response.json()\n    if \"errors\" in data:\n        logging.error(f\"Errors in response, after: {after}, category_id: {category_id}\")\n        logging.error(data[\"errors\"])\n        logging.error(response.text)\n        raise RuntimeError(response.text)\n    return cast(Dict[str, Any], data)", "metadata": {"license": "MIT", "len_tokens": 324}}
{"id": "fastapi:scripts/sponsors.py", "language": "python", "code": "def main() -> None:\n    logging.basicConfig(level=logging.INFO)\n    settings = Settings()\n    logging.info(f\"Using config: {settings.model_dump_json()}\")\n    g = Github(settings.pr_token.get_secret_value())\n    repo = g.get_repo(settings.github_repository)\n\n    tiers = get_individual_sponsors(settings=settings)\n    keys = list(tiers.keys())\n    keys.sort(reverse=True)\n    sponsors = []\n    for key in keys:\n        sponsor_group = []\n        for login, sponsor in tiers[key].items():\n            sponsor_group.append(\n                {\"login\": login, \"avatarUrl\": sponsor.avatarUrl, \"url\": sponsor.url}\n            )\n        sponsors.append(sponsor_group)\n    github_sponsors = {\n        \"sponsors\": sponsors,\n    }\n\n    # For local development\n    # github_sponsors_path = Path(\"../docs/en/data/github_sponsors.yml\")\n    github_sponsors_path = Path(\"./docs/en/data/github_sponsors.yml\")\n    updated = update_content(\n        content_path=github_sponsors_path, new_content=github_sponsors\n    )\n\n    if not updated:\n        logging.info(\"The data hasn't changed, finishing.\")\n        return\n\n    logging.info(\"Setting up GitHub Actions git user\")\n    subprocess.run([\"git\", \"config\", \"user.name\", \"github-actions\"], check=True)\n    subprocess.run(\n        [\"git\", \"config\", \"user.email\", \"github-actions@github.com\"], check=True\n    )\n    branch_name = f\"fastapi-people-sponsors-{secrets.token_hex(4)}\"\n    logging.info(f\"Creating a new branch {branch_name}\")\n    subprocess.run([\"git\", \"checkout\", \"-b\", branch_name], check=True)\n    logging.info(\"Adding updated file\")\n    subprocess.run(\n        [\n            \"git\",\n            \"add\",\n            str(github_sponsors_path),\n        ],\n        check=True,\n    )\n    logging.info(\"Committing updated file\")\n    message = \" Update FastAPI People - Sponsors\"\n    subprocess.run([\"git\", \"commit\", \"-m\", message], check=True)\n    logging.info(\"Pushing branch\")\n    subprocess.run([\"git\", \"push\", \"origin\", branch_name], check=True)\n    logging.info(\"Creating PR\")\n    pr = repo.create_pull(title=message, body=message, base=\"master\", head=branch_name)\n    logging.info(f\"Created PR: {pr.number}\")\n    logging.info(\"Finished\")", "metadata": {"license": "MIT", "len_tokens": 499}}
{"id": "fastapi:scripts/docs.py", "language": "python", "code": "def new_lang(lang: str = typer.Argument(..., callback=lang_callback)):\n    \"\"\"\n    Generate a new docs translation directory for the language LANG.\n    \"\"\"\n    new_path: Path = Path(\"docs\") / lang\n    if new_path.exists():\n        typer.echo(f\"The language was already created: {lang}\")\n        raise typer.Abort()\n    new_path.mkdir()\n    new_config_path: Path = Path(new_path) / mkdocs_name\n    new_config_path.write_text(\"INHERIT: ../en/mkdocs.yml\\n\", encoding=\"utf-8\")\n    new_config_docs_path: Path = new_path / \"docs\"\n    new_config_docs_path.mkdir()\n    en_index_path: Path = en_docs_path / \"docs\" / \"index.md\"\n    new_index_path: Path = new_config_docs_path / \"index.md\"\n    en_index_content = en_index_path.read_text(encoding=\"utf-8\")\n    new_index_content = f\"{missing_translation_snippet}\\n\\n{en_index_content}\"\n    new_index_path.write_text(new_index_content, encoding=\"utf-8\")\n    typer.secho(f\"Successfully initialized: {new_path}\", color=typer.colors.GREEN)\n    update_languages()", "metadata": {"license": "MIT", "len_tokens": 248}}
{"id": "fastapi:scripts/docs.py", "language": "python", "code": "def build_lang(\n    lang: str = typer.Argument(\n        ..., callback=lang_callback, autocompletion=complete_existing_lang\n    ),\n) -> None:\n    \"\"\"\n    Build the docs for a language.\n    \"\"\"\n    lang_path: Path = Path(\"docs\") / lang\n    if not lang_path.is_dir():\n        typer.echo(f\"The language translation doesn't seem to exist yet: {lang}\")\n        raise typer.Abort()\n    typer.echo(f\"Building docs for: {lang}\")\n    build_site_dist_path = build_site_path / lang\n    if lang == \"en\":\n        dist_path = site_path\n        # Don't remove en dist_path as it might already contain other languages.\n        # When running build_all(), that function already removes site_path.\n        # All this is only relevant locally, on GitHub Actions all this is done through\n        # artifacts and multiple workflows, so it doesn't matter if directories are\n        # removed or not.\n    else:\n        dist_path = site_path / lang\n        shutil.rmtree(dist_path, ignore_errors=True)\n    current_dir = os.getcwd()\n    os.chdir(lang_path)\n    shutil.rmtree(build_site_dist_path, ignore_errors=True)\n    subprocess.run([\"mkdocs\", \"build\", \"--site-dir\", build_site_dist_path], check=True)\n    shutil.copytree(build_site_dist_path, dist_path, dirs_exist_ok=True)\n    os.chdir(current_dir)\n    typer.secho(f\"Successfully built docs for: {lang}\", color=typer.colors.GREEN)", "metadata": {"license": "MIT", "len_tokens": 310}}
{"id": "fastapi:scripts/docs.py", "language": "python", "code": "def generate_readme_content() -> str:\n    en_index = en_docs_path / \"docs\" / \"index.md\"\n    content = en_index.read_text(\"utf-8\")\n    content = remove_header_permalinks(content)  # remove permalinks from headers\n    match_pre = re.search(r\"</style>\\n\\n\", content)\n    match_start = re.search(r\"<!-- sponsors -->\", content)\n    match_end = re.search(r\"<!-- /sponsors -->\", content)\n    sponsors_data_path = en_docs_path / \"data\" / \"sponsors.yml\"\n    sponsors = mkdocs.utils.yaml_load(sponsors_data_path.read_text(encoding=\"utf-8\"))\n    if not (match_start and match_end):\n        raise RuntimeError(\"Couldn't auto-generate sponsors section\")\n    if not match_pre:\n        raise RuntimeError(\"Couldn't find pre section (<style>) in index.md\")\n    frontmatter_end = match_pre.end()\n    pre_end = match_start.end()\n    post_start = match_end.start()\n    template = Template(index_sponsors_template)\n    message = template.render(sponsors=sponsors)\n    pre_content = content[frontmatter_end:pre_end]\n    post_content = content[post_start:]\n    new_content = pre_content + message + post_content\n    # Remove content between <!-- only-mkdocs --> and <!-- /only-mkdocs -->\n    new_content = re.sub(\n        r\"<!-- only-mkdocs -->.*?<!-- /only-mkdocs -->\",\n        \"\",\n        new_content,\n        flags=re.DOTALL,\n    )\n    return new_content", "metadata": {"license": "MIT", "len_tokens": 326}}
{"id": "fastapi:scripts/docs.py", "language": "python", "code": "def live(\n    lang: str = typer.Argument(\n        None, callback=lang_callback, autocompletion=complete_existing_lang\n    ),\n    dirty: bool = False,\n) -> None:\n    \"\"\"\n    Serve with livereload a docs site for a specific language.\n\n    This only shows the actual translated files, not the placeholders created with\n    build-all.\n\n    Takes an optional LANG argument with the name of the language to serve, by default\n    en.\n    \"\"\"\n    # Enable line numbers during local development to make it easier to highlight\n    if lang is None:\n        lang = \"en\"\n    lang_path: Path = docs_path / lang\n    # Enable line numbers during local development to make it easier to highlight\n    args = [\"mkdocs\", \"serve\", \"--dev-addr\", \"127.0.0.1:8008\"]\n    if dirty:\n        args.append(\"--dirty\")\n    subprocess.run(\n        args, env={**os.environ, \"LINENUMS\": \"true\"}, cwd=lang_path, check=True\n    )", "metadata": {"license": "MIT", "len_tokens": 218}}
{"id": "fastapi:scripts/docs.py", "language": "python", "code": "def get_updated_config_content() -> Dict[str, Any]:\n    config = get_en_config()\n    languages = [{\"en\": \"/\"}]\n    new_alternate: List[Dict[str, str]] = []\n    # Language names sourced from https://quickref.me/iso-639-1\n    # Contributors may wish to update or change these, e.g. to fix capitalization.\n    language_names_path = Path(__file__).parent / \"../docs/language_names.yml\"\n    local_language_names: Dict[str, str] = mkdocs.utils.yaml_load(\n        language_names_path.read_text(encoding=\"utf-8\")\n    )\n    for lang_path in get_lang_paths():\n        if lang_path.name in {\"en\", \"em\"} or not lang_path.is_dir():\n            continue\n        code = lang_path.name\n        languages.append({code: f\"/{code}/\"})\n    for lang_dict in languages:\n        code = list(lang_dict.keys())[0]\n        url = lang_dict[code]\n        if code not in local_language_names:\n            print(\n                f\"Missing language name for: {code}, \"\n                \"update it in docs/language_names.yml\"\n            )\n            raise typer.Abort()\n        use_name = f\"{code} - {local_language_names[code]}\"\n        new_alternate.append({\"link\": url, \"name\": use_name})\n    new_alternate.append({\"link\": \"/em/\", \"name\": \"\"})\n    config[\"extra\"][\"alternate\"] = new_alternate\n    return config", "metadata": {"license": "MIT", "len_tokens": 310}}
{"id": "fastapi:scripts/docs.py", "language": "python", "code": "def generate_docs_src_versions_for_file(file_path: Path) -> None:\n    target_versions = [\"py39\", \"py310\"]\n    base_content = file_path.read_text(encoding=\"utf-8\")\n    previous_content = {base_content}\n    for target_version in target_versions:\n        version_result = subprocess.run(\n            [\n                find_ruff_bin(),\n                \"check\",\n                \"--target-version\",\n                target_version,\n                \"--fix\",\n                \"--unsafe-fixes\",\n                \"-\",\n            ],\n            input=base_content.encode(\"utf-8\"),\n            capture_output=True,\n        )\n        content_target = version_result.stdout.decode(\"utf-8\")\n        format_result = subprocess.run(\n            [find_ruff_bin(), \"format\", \"-\"],\n            input=content_target.encode(\"utf-8\"),\n            capture_output=True,\n        )\n        content_format = format_result.stdout.decode(\"utf-8\")\n        if content_format in previous_content:\n            continue\n        previous_content.add(content_format)\n        version_file = file_path.with_name(\n            file_path.name.replace(\".py\", f\"_{target_version}.py\")\n        )\n        logging.info(f\"Writing to {version_file}\")\n        version_file.write_text(content_format, encoding=\"utf-8\")", "metadata": {"license": "MIT", "len_tokens": 252}}
{"id": "fastapi:scripts/docs.py", "language": "python", "code": "def add_permalinks_page(path: Path, update_existing: bool = False):\n    \"\"\"\n    Add or update header permalinks in specific page of En docs.\n    \"\"\"\n\n    if not path.is_relative_to(en_docs_path / \"docs\"):\n        raise RuntimeError(f\"Path must be inside {en_docs_path}\")\n    rel_path = path.relative_to(en_docs_path / \"docs\")\n\n    # Skip excluded sections\n    if str(rel_path).startswith(non_translated_sections):\n        return\n\n    visible_text_extractor = VisibleTextExtractor()\n    updated_lines = []\n    in_code_block3 = False\n    in_code_block4 = False\n    permalinks = set()\n\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        lines = f.readlines()\n\n    for line in lines:\n        # Handle codeblocks start and end\n        if not (in_code_block3 or in_code_block4):\n            if code_block4_pattern.match(line):\n                in_code_block4 = True\n            elif code_block3_pattern.match(line):\n                in_code_block3 = True\n        else:\n            if in_code_block4 and code_block4_pattern.match(line):\n                in_code_block4 = False\n            elif in_code_block3 and code_block3_pattern.match(line):\n                in_code_block3 = False\n\n        # Process Headers only outside codeblocks\n        if not (in_code_block3 or in_code_block4):\n            match = header_pattern.match(line)\n            if match:\n                hashes, title, _permalink = match.groups()\n                if (not _permalink) or update_existing:\n                    slug = slugify(visible_text_extractor.extract_visible_text(title))\n                    if slug in permalinks:\n                        # If the slug is already used, append a number to make it unique\n                        count = 1\n                        original_slug = slug\n                        while slug in permalinks:\n                            slug = f\"{original_slug}_{count}\"\n                            count += 1\n                    permalinks.add(slug)\n\n                    line = f\"{hashes} {title} {{ #{slug} }}\\n\"\n\n        updated_lines.append(line)\n\n    with path.open(\"w\", encoding=\"utf-8\") as f:\n        f.writelines(updated_lines)", "metadata": {"license": "MIT", "len_tokens": 462}}
{"id": "fastapi:scripts/mkdocs_hooks.py", "language": "python", "code": "def generate_renamed_section_items(\n    items: List[Union[Page, Section, Link]], *, config: MkDocsConfig\n) -> List[Union[Page, Section, Link]]:\n    new_items: List[Union[Page, Section, Link]] = []\n    for item in items:\n        if isinstance(item, Section):\n            new_title = item.title\n            new_children = generate_renamed_section_items(item.children, config=config)\n            first_child = new_children[0]\n            if isinstance(first_child, Page):\n                if first_child.file.src_path.endswith(\"index.md\"):\n                    # Read the source so that the title is parsed and available\n                    first_child.read_source(config=config)\n                    new_title = first_child.title or new_title\n            # Creating a new section makes it render it collapsed by default\n            # no idea why, so, let's just modify the existing one\n            # new_section = Section(title=new_title, children=new_children)\n            item.title = new_title.split(\"{ #\")[0]\n            item.children = new_children\n            new_items.append(item)\n        else:\n            new_items.append(item)\n    return new_items", "metadata": {"license": "MIT", "len_tokens": 235}}
{"id": "fastapi:scripts/mkdocs_hooks.py", "language": "python", "code": "def on_page_markdown(\n    markdown: str, *, page: Page, config: MkDocsConfig, files: Files\n) -> str:\n    # Set metadata[\"social\"][\"cards_layout_options\"][\"title\"] to clean title (without\n    # permalink)\n    title = page.title\n    clean_title = title.split(\"{ #\")[0]\n    if clean_title:\n        page.meta.setdefault(\"social\", {})\n        page.meta[\"social\"].setdefault(\"cards_layout_options\", {})\n        page.meta[\"social\"][\"cards_layout_options\"][\"title\"] = clean_title\n\n    if isinstance(page.file, EnFile):\n        for excluded_section in non_translated_sections:\n            if page.file.src_path.startswith(excluded_section):\n                return markdown\n        missing_translation_content = get_missing_translation_content(config.docs_dir)\n        header = \"\"\n        body = markdown\n        if markdown.startswith(\"#\"):\n            header, _, body = markdown.partition(\"\\n\\n\")\n        return f\"{header}\\n\\n{missing_translation_content}\\n\\n{body}\"\n    return markdown", "metadata": {"license": "MIT", "len_tokens": 208}}
{"id": "fastapi:scripts/people.py", "language": "python", "code": "def get_graphql_response(\n    *,\n    settings: Settings,\n    query: str,\n    after: Union[str, None] = None,\n    category_id: Union[str, None] = None,\n) -> dict[str, Any]:\n    headers = {\"Authorization\": f\"token {settings.github_token.get_secret_value()}\"}\n    variables = {\"after\": after, \"category_id\": category_id}\n    response = httpx.post(\n        github_graphql_url,\n        headers=headers,\n        timeout=settings.httpx_timeout,\n        json={\"query\": query, \"variables\": variables, \"operationName\": \"Q\"},\n    )\n    if response.status_code != 200:\n        logging.error(\n            f\"Response was not 200, after: {after}, category_id: {category_id}\"\n        )\n        logging.error(response.text)\n        raise RuntimeError(response.text)\n    data = response.json()\n    if \"errors\" in data:\n        logging.error(f\"Errors in response, after: {after}, category_id: {category_id}\")\n        logging.error(data[\"errors\"])\n        logging.error(response.text)\n        raise RuntimeError(response.text)\n    return data", "metadata": {"license": "MIT", "len_tokens": 234}}
{"id": "fastapi:scripts/people.py", "language": "python", "code": "def get_discussions_experts(\n    discussion_nodes: list[DiscussionsNode],\n) -> DiscussionExpertsResults:\n    commenters = Counter[str]()\n    last_month_commenters = Counter[str]()\n    three_months_commenters = Counter[str]()\n    six_months_commenters = Counter[str]()\n    one_year_commenters = Counter[str]()\n    authors: dict[str, Author] = {}\n\n    now = datetime.now(tz=timezone.utc)\n    one_month_ago = now - timedelta(days=30)\n    three_months_ago = now - timedelta(days=90)\n    six_months_ago = now - timedelta(days=180)\n    one_year_ago = now - timedelta(days=365)\n\n    for discussion in discussion_nodes:\n        discussion_author_name = None\n        if discussion.author:\n            authors[discussion.author.login] = discussion.author\n            discussion_author_name = discussion.author.login\n        discussion_commentors: dict[str, datetime] = {}\n        for comment in discussion.comments.nodes:\n            if comment.author:\n                authors[comment.author.login] = comment.author\n                if comment.author.login != discussion_author_name:\n                    author_time = discussion_commentors.get(\n                        comment.author.login, comment.createdAt\n                    )\n                    discussion_commentors[comment.author.login] = max(\n                        author_time, comment.createdAt\n                    )\n            for reply in comment.replies.nodes:\n                if reply.author:\n                    authors[reply.author.login] = reply.author\n                    if reply.author.login != discussion_author_name:\n                        author_time = discussion_commentors.get(\n                            reply.author.login, reply.createdAt\n                        )\n                        discussion_commentors[reply.author.login] = max(\n                            author_time, reply.createdAt\n                        )\n        for author_name, author_time in discussion_commentors.items():\n            commenters[author_name] += 1\n            if author_time > one_month_ago:\n                last_month_commenters[author_name] += 1\n            if author_time > three_months_ago:\n                three_months_commenters[author_name] += 1\n            if author_time > six_months_ago:\n                six_months_commenters[author_name] += 1\n            if author_time > one_year_ago:\n                one_year_commenters[author_name] += 1\n    discussion_experts_results = DiscussionExpertsResults(\n        authors=authors,\n        commenters=commenters,\n        last_month_commenters=last_month_commenters,\n        three_months_commenters=three_months_commenters,\n        six_months_commenters=six_months_commenters,\n        one_year_commenters=one_year_commenters,\n    )\n    return discussion_experts_results", "metadata": {"license": "MIT", "len_tokens": 542}}
{"id": "fastapi:scripts/people.py", "language": "python", "code": "def main() -> None:\n    logging.basicConfig(level=logging.INFO)\n    settings = Settings()\n    logging.info(f\"Using config: {settings.model_dump_json()}\")\n    g = Github(settings.github_token.get_secret_value())\n    repo = g.get_repo(settings.github_repository)\n\n    discussion_nodes = get_discussion_nodes(settings=settings)\n    experts_results = get_discussions_experts(discussion_nodes=discussion_nodes)\n\n    authors = experts_results.authors\n    maintainers_logins = {\"tiangolo\"}\n    maintainers = []\n    for login in maintainers_logins:\n        user = authors[login]\n        maintainers.append(\n            {\n                \"login\": login,\n                \"answers\": experts_results.commenters[login],\n                \"avatarUrl\": user.avatarUrl,\n                \"url\": user.url,\n            }\n        )\n\n    experts = get_users_to_write(\n        counter=experts_results.commenters,\n        authors=authors,\n    )\n    last_month_experts = get_users_to_write(\n        counter=experts_results.last_month_commenters,\n        authors=authors,\n    )\n    three_months_experts = get_users_to_write(\n        counter=experts_results.three_months_commenters,\n        authors=authors,\n    )\n    six_months_experts = get_users_to_write(\n        counter=experts_results.six_months_commenters,\n        authors=authors,\n    )\n    one_year_experts = get_users_to_write(\n        counter=experts_results.one_year_commenters,\n        authors=authors,\n    )\n\n    people = {\n        \"maintainers\": maintainers,\n        \"experts\": experts,\n        \"last_month_experts\": last_month_experts,\n        \"three_months_experts\": three_months_experts,\n        \"six_months_experts\": six_months_experts,\n        \"one_year_experts\": one_year_experts,\n    }\n\n    # For local development\n    # people_path = Path(\"../docs/en/data/people.yml\")\n    people_path = Path(\"./docs/en/data/people.yml\")\n\n    updated = update_content(content_path=people_path, new_content=people)\n\n    if not updated:\n        logging.info(\"The data hasn't changed, finishing.\")\n        return\n\n    logging.info(\"Setting up GitHub Actions git user\")\n    subprocess.run([\"git\", \"config\", \"user.name\", \"github-actions\"], check=True)\n    subprocess.run(\n        [\"git\", \"config\", \"user.email\", \"github-actions@github.com\"], check=True\n    )\n    branch_name = f\"fastapi-people-experts-{secrets.token_hex(4)}\"\n    logging.info(f\"Creating a new branch {branch_name}\")\n    subprocess.run([\"git\", \"checkout\", \"-b\", branch_name], check=True)\n    logging.info(\"Adding updated file\")\n    subprocess.run([\"git\", \"add\", str(people_path)], check=True)\n    logging.info(\"Committing updated file\")\n    message = \" Update FastAPI People - Experts\"\n    subprocess.run([\"git\", \"commit\", \"-m\", message], check=True)\n    logging.info(\"Pushing branch\")\n    subprocess.run([\"git\", \"push\", \"origin\", branch_name], check=True)\n    logging.info(\"Creating PR\")\n    pr = repo.create_pull(title=message, body=message, base=\"master\", head=branch_name)\n    logging.info(f\"Created PR: {pr.number}\")\n    logging.info(\"Finished\")", "metadata": {"license": "MIT", "len_tokens": 703}}
{"id": "fastapi:scripts/topic_repos.py", "language": "python", "code": "import logging\nimport secrets\nimport subprocess\nfrom pathlib import Path\n\nimport yaml\nfrom github import Github\nfrom pydantic import BaseModel, SecretStr\nfrom pydantic_settings import BaseSettings\n\n\nclass Settings(BaseSettings):\n    github_repository: str\n    github_token: SecretStr\n\n\nclass Repo(BaseModel):\n    name: str\n    html_url: str\n    stars: int\n    owner_login: str\n    owner_html_url: str\n\n\ndef main() -> None:\n    logging.basicConfig(level=logging.INFO)\n    settings = Settings()\n\n    logging.info(f\"Using config: {settings.model_dump_json()}\")\n    g = Github(settings.github_token.get_secret_value(), per_page=100)\n    r = g.get_repo(settings.github_repository)\n    repos = g.search_repositories(query=\"topic:fastapi\")\n    repos_list = list(repos)\n    final_repos: list[Repo] = []\n    for repo in repos_list[:100]:\n        if repo.full_name == settings.github_repository:\n            continue\n        final_repos.append(\n            Repo(\n                name=repo.name,\n                html_url=repo.html_url,\n                stars=repo.stargazers_count,\n                owner_login=repo.owner.login,\n                owner_html_url=repo.owner.html_url,\n            )\n        )\n    data = [repo.model_dump() for repo in final_repos]\n\n    # Local development\n    # repos_path = Path(\"../docs/en/data/topic_repos.yml\")\n    repos_path = Path(\"./docs/en/data/topic_repos.yml\")\n    repos_old_content = repos_path.read_text(encoding=\"utf-8\")\n    new_repos_content = yaml.dump(data, sort_keys=False, width=200, allow_unicode=True)\n    if repos_old_content == new_repos_content:\n        logging.info(\"The data hasn't changed. Finishing.\")\n        return\n    repos_path.write_text(new_repos_content, encoding=\"utf-8\")\n    logging.info(\"Setting up GitHub Actions git user\")\n    subprocess.run([\"git\", \"config\", \"user.name\", \"github-actions\"], check=True)\n    subprocess.run(\n        [\"git\", \"config\", \"user.email\", \"github-actions@github.com\"], check=True\n    )\n    branch_name = f\"fastapi-topic-repos-{secrets.token_hex(4)}\"\n    logging.info(f\"Creating a new branch {branch_name}\")\n    subprocess.run([\"git\", \"checkout\", \"-b\", branch_name], check=True)\n    logging.info(\"Adding updated file\")\n    subprocess.run([\"git\", \"add\", str(repos_path)], check=True)\n    logging.info(\"Committing updated file\")\n    message = \" Update FastAPI GitHub topic repositories\"\n    subprocess.run([\"git\", \"commit\", \"-m\", message], check=True)\n    logging.info(\"Pushing branch\")\n    subprocess.run([\"git\", \"push\", \"origin\", branch_name], check=True)\n    logging.info(\"Creating PR\")\n    pr = r.create_pull(title=message, body=message, base=\"master\", head=branch_name)\n    logging.info(f\"Created PR: {pr.number}\")\n    logging.info(\"Finished\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"license": "MIT", "len_tokens": 652}}
{"id": "fastapi:scripts/topic_repos.py", "language": "python", "code": "def main() -> None:\n    logging.basicConfig(level=logging.INFO)\n    settings = Settings()\n\n    logging.info(f\"Using config: {settings.model_dump_json()}\")\n    g = Github(settings.github_token.get_secret_value(), per_page=100)\n    r = g.get_repo(settings.github_repository)\n    repos = g.search_repositories(query=\"topic:fastapi\")\n    repos_list = list(repos)\n    final_repos: list[Repo] = []\n    for repo in repos_list[:100]:\n        if repo.full_name == settings.github_repository:\n            continue\n        final_repos.append(\n            Repo(\n                name=repo.name,\n                html_url=repo.html_url,\n                stars=repo.stargazers_count,\n                owner_login=repo.owner.login,\n                owner_html_url=repo.owner.html_url,\n            )\n        )\n    data = [repo.model_dump() for repo in final_repos]\n\n    # Local development\n    # repos_path = Path(\"../docs/en/data/topic_repos.yml\")\n    repos_path = Path(\"./docs/en/data/topic_repos.yml\")\n    repos_old_content = repos_path.read_text(encoding=\"utf-8\")\n    new_repos_content = yaml.dump(data, sort_keys=False, width=200, allow_unicode=True)\n    if repos_old_content == new_repos_content:\n        logging.info(\"The data hasn't changed. Finishing.\")\n        return\n    repos_path.write_text(new_repos_content, encoding=\"utf-8\")\n    logging.info(\"Setting up GitHub Actions git user\")\n    subprocess.run([\"git\", \"config\", \"user.name\", \"github-actions\"], check=True)\n    subprocess.run(\n        [\"git\", \"config\", \"user.email\", \"github-actions@github.com\"], check=True\n    )\n    branch_name = f\"fastapi-topic-repos-{secrets.token_hex(4)}\"\n    logging.info(f\"Creating a new branch {branch_name}\")\n    subprocess.run([\"git\", \"checkout\", \"-b\", branch_name], check=True)\n    logging.info(\"Adding updated file\")\n    subprocess.run([\"git\", \"add\", str(repos_path)], check=True)\n    logging.info(\"Committing updated file\")\n    message = \" Update FastAPI GitHub topic repositories\"\n    subprocess.run([\"git\", \"commit\", \"-m\", message], check=True)\n    logging.info(\"Pushing branch\")\n    subprocess.run([\"git\", \"push\", \"origin\", branch_name], check=True)\n    logging.info(\"Creating PR\")\n    pr = r.create_pull(title=message, body=message, base=\"master\", head=branch_name)\n    logging.info(f\"Created PR: {pr.number}\")\n    logging.info(\"Finished\")", "metadata": {"license": "MIT", "len_tokens": 547}}
{"id": "fastapi:docs_src/request_files/tutorial003.py", "language": "python", "code": "from typing import List\n\nfrom fastapi import FastAPI, File, UploadFile\nfrom fastapi.responses import HTMLResponse\n\napp = FastAPI()\n\n\n@app.post(\"/files/\")\nasync def create_files(\n    files: List[bytes] = File(description=\"Multiple files as bytes\"),\n):\n    return {\"file_sizes\": [len(file) for file in files]}\n\n\n@app.post(\"/uploadfiles/\")\nasync def create_upload_files(\n    files: List[UploadFile] = File(description=\"Multiple files as UploadFile\"),\n):\n    return {\"filenames\": [file.filename for file in files]}\n\n\n@app.get(\"/\")\nasync def main():\n    content = \"\"\"\n<body>\n<form action=\"/files/\" enctype=\"multipart/form-data\" method=\"post\">\n<input name=\"files\" type=\"file\" multiple>\n<input type=\"submit\">\n</form>\n<form action=\"/uploadfiles/\" enctype=\"multipart/form-data\" method=\"post\">\n<input name=\"files\" type=\"file\" multiple>\n<input type=\"submit\">\n</form>\n</body>\n    \"\"\"\n    return HTMLResponse(content=content)\n", "metadata": {"license": "MIT", "len_tokens": 214}}
{"id": "fastapi:docs_src/request_files/tutorial003_an_py39.py", "language": "python", "code": "from typing import Annotated\n\nfrom fastapi import FastAPI, File, UploadFile\nfrom fastapi.responses import HTMLResponse\n\napp = FastAPI()\n\n\n@app.post(\"/files/\")\nasync def create_files(\n    files: Annotated[list[bytes], File(description=\"Multiple files as bytes\")],\n):\n    return {\"file_sizes\": [len(file) for file in files]}\n\n\n@app.post(\"/uploadfiles/\")\nasync def create_upload_files(\n    files: Annotated[\n        list[UploadFile], File(description=\"Multiple files as UploadFile\")\n    ],\n):\n    return {\"filenames\": [file.filename for file in files]}\n\n\n@app.get(\"/\")\nasync def main():\n    content = \"\"\"\n<body>\n<form action=\"/files/\" enctype=\"multipart/form-data\" method=\"post\">\n<input name=\"files\" type=\"file\" multiple>\n<input type=\"submit\">\n</form>\n<form action=\"/uploadfiles/\" enctype=\"multipart/form-data\" method=\"post\">\n<input name=\"files\" type=\"file\" multiple>\n<input type=\"submit\">\n</form>\n</body>\n    \"\"\"\n    return HTMLResponse(content=content)\n", "metadata": {"license": "MIT", "len_tokens": 222}}
{"id": "fastapi:docs_src/request_files/tutorial002_an.py", "language": "python", "code": "from typing import List\n\nfrom fastapi import FastAPI, File, UploadFile\nfrom fastapi.responses import HTMLResponse\nfrom typing_extensions import Annotated\n\napp = FastAPI()\n\n\n@app.post(\"/files/\")\nasync def create_files(files: Annotated[List[bytes], File()]):\n    return {\"file_sizes\": [len(file) for file in files]}\n\n\n@app.post(\"/uploadfiles/\")\nasync def create_upload_files(files: List[UploadFile]):\n    return {\"filenames\": [file.filename for file in files]}\n\n\n@app.get(\"/\")\nasync def main():\n    content = \"\"\"\n<body>\n<form action=\"/files/\" enctype=\"multipart/form-data\" method=\"post\">\n<input name=\"files\" type=\"file\" multiple>\n<input type=\"submit\">\n</form>\n<form action=\"/uploadfiles/\" enctype=\"multipart/form-data\" method=\"post\">\n<input name=\"files\" type=\"file\" multiple>\n<input type=\"submit\">\n</form>\n</body>\n    \"\"\"\n    return HTMLResponse(content=content)\n", "metadata": {"license": "MIT", "len_tokens": 201}}
{"id": "fastapi:docs_src/request_files/tutorial003_py39.py", "language": "python", "code": "from fastapi import FastAPI, File, UploadFile\nfrom fastapi.responses import HTMLResponse\n\napp = FastAPI()\n\n\n@app.post(\"/files/\")\nasync def create_files(\n    files: list[bytes] = File(description=\"Multiple files as bytes\"),\n):\n    return {\"file_sizes\": [len(file) for file in files]}\n\n\n@app.post(\"/uploadfiles/\")\nasync def create_upload_files(\n    files: list[UploadFile] = File(description=\"Multiple files as UploadFile\"),\n):\n    return {\"filenames\": [file.filename for file in files]}\n\n\n@app.get(\"/\")\nasync def main():\n    content = \"\"\"\n<body>\n<form action=\"/files/\" enctype=\"multipart/form-data\" method=\"post\">\n<input name=\"files\" type=\"file\" multiple>\n<input type=\"submit\">\n</form>\n<form action=\"/uploadfiles/\" enctype=\"multipart/form-data\" method=\"post\">\n<input name=\"files\" type=\"file\" multiple>\n<input type=\"submit\">\n</form>\n</body>\n    \"\"\"\n    return HTMLResponse(content=content)\n", "metadata": {"license": "MIT", "len_tokens": 209}}
{"id": "fastapi:docs_src/request_files/tutorial003_an.py", "language": "python", "code": "from typing import List\n\nfrom fastapi import FastAPI, File, UploadFile\nfrom fastapi.responses import HTMLResponse\nfrom typing_extensions import Annotated\n\napp = FastAPI()\n\n\n@app.post(\"/files/\")\nasync def create_files(\n    files: Annotated[List[bytes], File(description=\"Multiple files as bytes\")],\n):\n    return {\"file_sizes\": [len(file) for file in files]}\n\n\n@app.post(\"/uploadfiles/\")\nasync def create_upload_files(\n    files: Annotated[\n        List[UploadFile], File(description=\"Multiple files as UploadFile\")\n    ],\n):\n    return {\"filenames\": [file.filename for file in files]}\n\n\n@app.get(\"/\")\nasync def main():\n    content = \"\"\"\n<body>\n<form action=\"/files/\" enctype=\"multipart/form-data\" method=\"post\">\n<input name=\"files\" type=\"file\" multiple>\n<input type=\"submit\">\n</form>\n<form action=\"/uploadfiles/\" enctype=\"multipart/form-data\" method=\"post\">\n<input name=\"files\" type=\"file\" multiple>\n<input type=\"submit\">\n</form>\n</body>\n    \"\"\"\n    return HTMLResponse(content=content)\n", "metadata": {"license": "MIT", "len_tokens": 228}}
{"id": "fastapi:docs_src/openapi_callbacks/tutorial001_py310.py", "language": "python", "code": "from fastapi import APIRouter, FastAPI\nfrom pydantic import BaseModel, HttpUrl\n\napp = FastAPI()\n\n\nclass Invoice(BaseModel):\n    id: str\n    title: str | None = None\n    customer: str\n    total: float\n\n\nclass InvoiceEvent(BaseModel):\n    description: str\n    paid: bool\n\n\nclass InvoiceEventReceived(BaseModel):\n    ok: bool\n\n\ninvoices_callback_router = APIRouter()\n\n\n@invoices_callback_router.post(\n    \"{$callback_url}/invoices/{$request.body.id}\", response_model=InvoiceEventReceived\n)\ndef invoice_notification(body: InvoiceEvent):\n    pass\n\n\n@app.post(\"/invoices/\", callbacks=invoices_callback_router.routes)\ndef create_invoice(invoice: Invoice, callback_url: HttpUrl | None = None):\n    \"\"\"\n    Create an invoice.\n\n    This will (let's imagine) let the API user (some external developer) create an\n    invoice.\n\n    And this path operation will:\n\n    * Send the invoice to the client.\n    * Collect the money from the client.\n    * Send a notification back to the API user (the external developer), as a callback.\n        * At this point is that the API will somehow send a POST request to the\n            external API with the notification of the invoice event\n            (e.g. \"payment successful\").\n    \"\"\"\n    # Send the invoice, collect the money, send the notification (the callback)\n    return {\"msg\": \"Invoice received\"}\n", "metadata": {"license": "MIT", "len_tokens": 302}}
{"id": "fastapi:docs_src/openapi_callbacks/tutorial001.py", "language": "python", "code": "from typing import Union\n\nfrom fastapi import APIRouter, FastAPI\nfrom pydantic import BaseModel, HttpUrl\n\napp = FastAPI()\n\n\nclass Invoice(BaseModel):\n    id: str\n    title: Union[str, None] = None\n    customer: str\n    total: float\n\n\nclass InvoiceEvent(BaseModel):\n    description: str\n    paid: bool\n\n\nclass InvoiceEventReceived(BaseModel):\n    ok: bool\n\n\ninvoices_callback_router = APIRouter()\n\n\n@invoices_callback_router.post(\n    \"{$callback_url}/invoices/{$request.body.id}\", response_model=InvoiceEventReceived\n)\ndef invoice_notification(body: InvoiceEvent):\n    pass\n\n\n@app.post(\"/invoices/\", callbacks=invoices_callback_router.routes)\ndef create_invoice(invoice: Invoice, callback_url: Union[HttpUrl, None] = None):\n    \"\"\"\n    Create an invoice.\n\n    This will (let's imagine) let the API user (some external developer) create an\n    invoice.\n\n    And this path operation will:\n\n    * Send the invoice to the client.\n    * Collect the money from the client.\n    * Send a notification back to the API user (the external developer), as a callback.\n        * At this point is that the API will somehow send a POST request to the\n            external API with the notification of the invoice event\n            (e.g. \"payment successful\").\n    \"\"\"\n    # Send the invoice, collect the money, send the notification (the callback)\n    return {\"msg\": \"Invoice received\"}\n", "metadata": {"license": "MIT", "len_tokens": 312}}
{"id": "fastapi:docs_src/response_model/tutorial006.py", "language": "python", "code": "from typing import Union\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    name: str\n    description: Union[str, None] = None\n    price: float\n    tax: float = 10.5\n\n\nitems = {\n    \"foo\": {\"name\": \"Foo\", \"price\": 50.2},\n    \"bar\": {\"name\": \"Bar\", \"description\": \"The Bar fighters\", \"price\": 62, \"tax\": 20.2},\n    \"baz\": {\n        \"name\": \"Baz\",\n        \"description\": \"There goes my baz\",\n        \"price\": 50.2,\n        \"tax\": 10.5,\n    },\n}\n\n\n@app.get(\n    \"/items/{item_id}/name\",\n    response_model=Item,\n    response_model_include=[\"name\", \"description\"],\n)\nasync def read_item_name(item_id: str):\n    return items[item_id]\n\n\n@app.get(\"/items/{item_id}/public\", response_model=Item, response_model_exclude=[\"tax\"])\nasync def read_item_public_data(item_id: str):\n    return items[item_id]\n", "metadata": {"license": "MIT", "len_tokens": 241}}
{"id": "fastapi:docs_src/response_model/tutorial006_py310.py", "language": "python", "code": "from fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    name: str\n    description: str | None = None\n    price: float\n    tax: float = 10.5\n\n\nitems = {\n    \"foo\": {\"name\": \"Foo\", \"price\": 50.2},\n    \"bar\": {\"name\": \"Bar\", \"description\": \"The Bar fighters\", \"price\": 62, \"tax\": 20.2},\n    \"baz\": {\n        \"name\": \"Baz\",\n        \"description\": \"There goes my baz\",\n        \"price\": 50.2,\n        \"tax\": 10.5,\n    },\n}\n\n\n@app.get(\n    \"/items/{item_id}/name\",\n    response_model=Item,\n    response_model_include=[\"name\", \"description\"],\n)\nasync def read_item_name(item_id: str):\n    return items[item_id]\n\n\n@app.get(\"/items/{item_id}/public\", response_model=Item, response_model_exclude=[\"tax\"])\nasync def read_item_public_data(item_id: str):\n    return items[item_id]\n", "metadata": {"license": "MIT", "len_tokens": 234}}
{"id": "fastapi:docs_src/response_model/tutorial005_py310.py", "language": "python", "code": "from fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    name: str\n    description: str | None = None\n    price: float\n    tax: float = 10.5\n\n\nitems = {\n    \"foo\": {\"name\": \"Foo\", \"price\": 50.2},\n    \"bar\": {\"name\": \"Bar\", \"description\": \"The Bar fighters\", \"price\": 62, \"tax\": 20.2},\n    \"baz\": {\n        \"name\": \"Baz\",\n        \"description\": \"There goes my baz\",\n        \"price\": 50.2,\n        \"tax\": 10.5,\n    },\n}\n\n\n@app.get(\n    \"/items/{item_id}/name\",\n    response_model=Item,\n    response_model_include={\"name\", \"description\"},\n)\nasync def read_item_name(item_id: str):\n    return items[item_id]\n\n\n@app.get(\"/items/{item_id}/public\", response_model=Item, response_model_exclude={\"tax\"})\nasync def read_item_public_data(item_id: str):\n    return items[item_id]\n", "metadata": {"license": "MIT", "len_tokens": 234}}
{"id": "fastapi:docs_src/response_model/tutorial005.py", "language": "python", "code": "from typing import Union\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    name: str\n    description: Union[str, None] = None\n    price: float\n    tax: float = 10.5\n\n\nitems = {\n    \"foo\": {\"name\": \"Foo\", \"price\": 50.2},\n    \"bar\": {\"name\": \"Bar\", \"description\": \"The Bar fighters\", \"price\": 62, \"tax\": 20.2},\n    \"baz\": {\n        \"name\": \"Baz\",\n        \"description\": \"There goes my baz\",\n        \"price\": 50.2,\n        \"tax\": 10.5,\n    },\n}\n\n\n@app.get(\n    \"/items/{item_id}/name\",\n    response_model=Item,\n    response_model_include={\"name\", \"description\"},\n)\nasync def read_item_name(item_id: str):\n    return items[item_id]\n\n\n@app.get(\"/items/{item_id}/public\", response_model=Item, response_model_exclude={\"tax\"})\nasync def read_item_public_data(item_id: str):\n    return items[item_id]\n", "metadata": {"license": "MIT", "len_tokens": 241}}
{"id": "fastapi:docs_src/security/tutorial005_py39.py", "language": "python", "code": "async def get_current_user(\n    security_scopes: SecurityScopes, token: str = Depends(oauth2_scheme)\n):\n    if security_scopes.scopes:\n        authenticate_value = f'Bearer scope=\"{security_scopes.scope_str}\"'\n    else:\n        authenticate_value = \"Bearer\"\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": authenticate_value},\n    )\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username: str = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n        scope: str = payload.get(\"scope\", \"\")\n        token_scopes = scope.split(\" \")\n        token_data = TokenData(scopes=token_scopes, username=username)\n    except (InvalidTokenError, ValidationError):\n        raise credentials_exception\n    user = get_user(fake_users_db, username=token_data.username)\n    if user is None:\n        raise credentials_exception\n    for scope in security_scopes.scopes:\n        if scope not in token_data.scopes:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Not enough permissions\",\n                headers={\"WWW-Authenticate\": authenticate_value},\n            )\n    return user", "metadata": {"license": "MIT", "len_tokens": 269}}
{"id": "fastapi:docs_src/security/tutorial003.py", "language": "python", "code": "from typing import Union\n\nfrom fastapi import Depends, FastAPI, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom pydantic import BaseModel\n\nfake_users_db = {\n    \"johndoe\": {\n        \"username\": \"johndoe\",\n        \"full_name\": \"John Doe\",\n        \"email\": \"johndoe@example.com\",\n        \"hashed_password\": \"fakehashedsecret\",\n        \"disabled\": False,\n    },\n    \"alice\": {\n        \"username\": \"alice\",\n        \"full_name\": \"Alice Wonderson\",\n        \"email\": \"alice@example.com\",\n        \"hashed_password\": \"fakehashedsecret2\",\n        \"disabled\": True,\n    },\n}\n\napp = FastAPI()\n\n\ndef fake_hash_password(password: str):\n    return \"fakehashed\" + password\n\n\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n\nclass User(BaseModel):\n    username: str\n    email: Union[str, None] = None\n    full_name: Union[str, None] = None\n    disabled: Union[bool, None] = None\n\n\nclass UserInDB(User):\n    hashed_password: str\n\n\ndef get_user(db, username: str):\n    if username in db:\n        user_dict = db[username]\n        return UserInDB(**user_dict)\n\n\ndef fake_decode_token(token):\n    # This doesn't provide any security at all\n    # Check the next version\n    user = get_user(fake_users_db, token)\n    return user\n\n\nasync def get_current_user(token: str = Depends(oauth2_scheme)):\n    user = fake_decode_token(token)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Not authenticated\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    return user\n\n\nasync def get_current_active_user(current_user: User = Depends(get_current_user)):\n    if current_user.disabled:\n        raise HTTPException(status_code=400, detail=\"Inactive user\")\n    return current_user\n\n\n@app.post(\"/token\")\nasync def login(form_data: OAuth2PasswordRequestForm = Depends()):\n    user_dict = fake_users_db.get(form_data.username)\n    if not user_dict:\n        raise HTTPException(status_code=400, detail=\"Incorrect username or password\")\n    user = UserInDB(**user_dict)\n    hashed_password = fake_hash_password(form_data.password)\n    if not hashed_password == user.hashed_password:\n        raise HTTPException(status_code=400, detail=\"Incorrect username or password\")\n\n    return {\"access_token\": user.username, \"token_type\": \"bearer\"}\n\n\n@app.get(\"/users/me\")\nasync def read_users_me(current_user: User = Depends(get_current_active_user)):\n    return current_user\n", "metadata": {"license": "MIT", "len_tokens": 571}}
{"id": "fastapi:docs_src/security/tutorial007.py", "language": "python", "code": "import secrets\n\nfrom fastapi import Depends, FastAPI, HTTPException, status\nfrom fastapi.security import HTTPBasic, HTTPBasicCredentials\n\napp = FastAPI()\n\nsecurity = HTTPBasic()\n\n\ndef get_current_username(credentials: HTTPBasicCredentials = Depends(security)):\n    current_username_bytes = credentials.username.encode(\"utf8\")\n    correct_username_bytes = b\"stanleyjobson\"\n    is_correct_username = secrets.compare_digest(\n        current_username_bytes, correct_username_bytes\n    )\n    current_password_bytes = credentials.password.encode(\"utf8\")\n    correct_password_bytes = b\"swordfish\"\n    is_correct_password = secrets.compare_digest(\n        current_password_bytes, correct_password_bytes\n    )\n    if not (is_correct_username and is_correct_password):\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect username or password\",\n            headers={\"WWW-Authenticate\": \"Basic\"},\n        )\n    return credentials.username\n\n\n@app.get(\"/users/me\")\ndef read_current_user(username: str = Depends(get_current_username)):\n    return {\"username\": username}\n", "metadata": {"license": "MIT", "len_tokens": 217}}
{"id": "fastapi:docs_src/security/tutorial003_an_py39.py", "language": "python", "code": "from typing import Annotated, Union\n\nfrom fastapi import Depends, FastAPI, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom pydantic import BaseModel\n\nfake_users_db = {\n    \"johndoe\": {\n        \"username\": \"johndoe\",\n        \"full_name\": \"John Doe\",\n        \"email\": \"johndoe@example.com\",\n        \"hashed_password\": \"fakehashedsecret\",\n        \"disabled\": False,\n    },\n    \"alice\": {\n        \"username\": \"alice\",\n        \"full_name\": \"Alice Wonderson\",\n        \"email\": \"alice@example.com\",\n        \"hashed_password\": \"fakehashedsecret2\",\n        \"disabled\": True,\n    },\n}\n\napp = FastAPI()\n\n\ndef fake_hash_password(password: str):\n    return \"fakehashed\" + password\n\n\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n\nclass User(BaseModel):\n    username: str\n    email: Union[str, None] = None\n    full_name: Union[str, None] = None\n    disabled: Union[bool, None] = None\n\n\nclass UserInDB(User):\n    hashed_password: str\n\n\ndef get_user(db, username: str):\n    if username in db:\n        user_dict = db[username]\n        return UserInDB(**user_dict)\n\n\ndef fake_decode_token(token):\n    # This doesn't provide any security at all\n    # Check the next version\n    user = get_user(fake_users_db, token)\n    return user\n\n\nasync def get_current_user(token: Annotated[str, Depends(oauth2_scheme)]):\n    user = fake_decode_token(token)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Not authenticated\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    return user\n\n\nasync def get_current_active_user(\n    current_user: Annotated[User, Depends(get_current_user)],\n):\n    if current_user.disabled:\n        raise HTTPException(status_code=400, detail=\"Inactive user\")\n    return current_user\n\n\n@app.post(\"/token\")\nasync def login(form_data: Annotated[OAuth2PasswordRequestForm, Depends()]):\n    user_dict = fake_users_db.get(form_data.username)\n    if not user_dict:\n        raise HTTPException(status_code=400, detail=\"Incorrect username or password\")\n    user = UserInDB(**user_dict)\n    hashed_password = fake_hash_password(form_data.password)\n    if not hashed_password == user.hashed_password:\n        raise HTTPException(status_code=400, detail=\"Incorrect username or password\")\n\n    return {\"access_token\": user.username, \"token_type\": \"bearer\"}\n\n\n@app.get(\"/users/me\")\nasync def read_users_me(\n    current_user: Annotated[User, Depends(get_current_active_user)],\n):\n    return current_user\n", "metadata": {"license": "MIT", "len_tokens": 593}}
{"id": "fastapi:docs_src/security/tutorial005_py310.py", "language": "python", "code": "async def get_current_user(\n    security_scopes: SecurityScopes, token: str = Depends(oauth2_scheme)\n):\n    if security_scopes.scopes:\n        authenticate_value = f'Bearer scope=\"{security_scopes.scope_str}\"'\n    else:\n        authenticate_value = \"Bearer\"\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": authenticate_value},\n    )\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username: str = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n        scope: str = payload.get(\"scope\", \"\")\n        token_scopes = scope.split(\" \")\n        token_data = TokenData(scopes=token_scopes, username=username)\n    except (InvalidTokenError, ValidationError):\n        raise credentials_exception\n    user = get_user(fake_users_db, username=token_data.username)\n    if user is None:\n        raise credentials_exception\n    for scope in security_scopes.scopes:\n        if scope not in token_data.scopes:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Not enough permissions\",\n                headers={\"WWW-Authenticate\": authenticate_value},\n            )\n    return user", "metadata": {"license": "MIT", "len_tokens": 269}}
{"id": "fastapi:docs_src/security/tutorial005_an_py39.py", "language": "python", "code": "async def get_current_user(\n    security_scopes: SecurityScopes, token: Annotated[str, Depends(oauth2_scheme)]\n):\n    if security_scopes.scopes:\n        authenticate_value = f'Bearer scope=\"{security_scopes.scope_str}\"'\n    else:\n        authenticate_value = \"Bearer\"\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": authenticate_value},\n    )\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n        scope: str = payload.get(\"scope\", \"\")\n        token_scopes = scope.split(\" \")\n        token_data = TokenData(scopes=token_scopes, username=username)\n    except (InvalidTokenError, ValidationError):\n        raise credentials_exception\n    user = get_user(fake_users_db, username=token_data.username)\n    if user is None:\n        raise credentials_exception\n    for scope in security_scopes.scopes:\n        if scope not in token_data.scopes:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Not enough permissions\",\n                headers={\"WWW-Authenticate\": authenticate_value},\n            )\n    return user", "metadata": {"license": "MIT", "len_tokens": 269}}
{"id": "fastapi:docs_src/security/tutorial003_an_py310.py", "language": "python", "code": "from typing import Annotated\n\nfrom fastapi import Depends, FastAPI, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom pydantic import BaseModel\n\nfake_users_db = {\n    \"johndoe\": {\n        \"username\": \"johndoe\",\n        \"full_name\": \"John Doe\",\n        \"email\": \"johndoe@example.com\",\n        \"hashed_password\": \"fakehashedsecret\",\n        \"disabled\": False,\n    },\n    \"alice\": {\n        \"username\": \"alice\",\n        \"full_name\": \"Alice Wonderson\",\n        \"email\": \"alice@example.com\",\n        \"hashed_password\": \"fakehashedsecret2\",\n        \"disabled\": True,\n    },\n}\n\napp = FastAPI()\n\n\ndef fake_hash_password(password: str):\n    return \"fakehashed\" + password\n\n\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n\nclass User(BaseModel):\n    username: str\n    email: str | None = None\n    full_name: str | None = None\n    disabled: bool | None = None\n\n\nclass UserInDB(User):\n    hashed_password: str\n\n\ndef get_user(db, username: str):\n    if username in db:\n        user_dict = db[username]\n        return UserInDB(**user_dict)\n\n\ndef fake_decode_token(token):\n    # This doesn't provide any security at all\n    # Check the next version\n    user = get_user(fake_users_db, token)\n    return user\n\n\nasync def get_current_user(token: Annotated[str, Depends(oauth2_scheme)]):\n    user = fake_decode_token(token)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Not authenticated\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    return user\n\n\nasync def get_current_active_user(\n    current_user: Annotated[User, Depends(get_current_user)],\n):\n    if current_user.disabled:\n        raise HTTPException(status_code=400, detail=\"Inactive user\")\n    return current_user\n\n\n@app.post(\"/token\")\nasync def login(form_data: Annotated[OAuth2PasswordRequestForm, Depends()]):\n    user_dict = fake_users_db.get(form_data.username)\n    if not user_dict:\n        raise HTTPException(status_code=400, detail=\"Incorrect username or password\")\n    user = UserInDB(**user_dict)\n    hashed_password = fake_hash_password(form_data.password)\n    if not hashed_password == user.hashed_password:\n        raise HTTPException(status_code=400, detail=\"Incorrect username or password\")\n\n    return {\"access_token\": user.username, \"token_type\": \"bearer\"}\n\n\n@app.get(\"/users/me\")\nasync def read_users_me(\n    current_user: Annotated[User, Depends(get_current_active_user)],\n):\n    return current_user\n", "metadata": {"license": "MIT", "len_tokens": 584}}
{"id": "fastapi:docs_src/security/tutorial003_py310.py", "language": "python", "code": "from fastapi import Depends, FastAPI, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom pydantic import BaseModel\n\nfake_users_db = {\n    \"johndoe\": {\n        \"username\": \"johndoe\",\n        \"full_name\": \"John Doe\",\n        \"email\": \"johndoe@example.com\",\n        \"hashed_password\": \"fakehashedsecret\",\n        \"disabled\": False,\n    },\n    \"alice\": {\n        \"username\": \"alice\",\n        \"full_name\": \"Alice Wonderson\",\n        \"email\": \"alice@example.com\",\n        \"hashed_password\": \"fakehashedsecret2\",\n        \"disabled\": True,\n    },\n}\n\napp = FastAPI()\n\n\ndef fake_hash_password(password: str):\n    return \"fakehashed\" + password\n\n\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n\nclass User(BaseModel):\n    username: str\n    email: str | None = None\n    full_name: str | None = None\n    disabled: bool | None = None\n\n\nclass UserInDB(User):\n    hashed_password: str\n\n\ndef get_user(db, username: str):\n    if username in db:\n        user_dict = db[username]\n        return UserInDB(**user_dict)\n\n\ndef fake_decode_token(token):\n    # This doesn't provide any security at all\n    # Check the next version\n    user = get_user(fake_users_db, token)\n    return user\n\n\nasync def get_current_user(token: str = Depends(oauth2_scheme)):\n    user = fake_decode_token(token)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Not authenticated\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    return user\n\n\nasync def get_current_active_user(current_user: User = Depends(get_current_user)):\n    if current_user.disabled:\n        raise HTTPException(status_code=400, detail=\"Inactive user\")\n    return current_user\n\n\n@app.post(\"/token\")\nasync def login(form_data: OAuth2PasswordRequestForm = Depends()):\n    user_dict = fake_users_db.get(form_data.username)\n    if not user_dict:\n        raise HTTPException(status_code=400, detail=\"Incorrect username or password\")\n    user = UserInDB(**user_dict)\n    hashed_password = fake_hash_password(form_data.password)\n    if not hashed_password == user.hashed_password:\n        raise HTTPException(status_code=400, detail=\"Incorrect username or password\")\n\n    return {\"access_token\": user.username, \"token_type\": \"bearer\"}\n\n\n@app.get(\"/users/me\")\nasync def read_users_me(current_user: User = Depends(get_current_active_user)):\n    return current_user\n", "metadata": {"license": "MIT", "len_tokens": 559}}
{"id": "fastapi:docs_src/security/tutorial007_an.py", "language": "python", "code": "import secrets\n\nfrom fastapi import Depends, FastAPI, HTTPException, status\nfrom fastapi.security import HTTPBasic, HTTPBasicCredentials\nfrom typing_extensions import Annotated\n\napp = FastAPI()\n\nsecurity = HTTPBasic()\n\n\ndef get_current_username(\n    credentials: Annotated[HTTPBasicCredentials, Depends(security)],\n):\n    current_username_bytes = credentials.username.encode(\"utf8\")\n    correct_username_bytes = b\"stanleyjobson\"\n    is_correct_username = secrets.compare_digest(\n        current_username_bytes, correct_username_bytes\n    )\n    current_password_bytes = credentials.password.encode(\"utf8\")\n    correct_password_bytes = b\"swordfish\"\n    is_correct_password = secrets.compare_digest(\n        current_password_bytes, correct_password_bytes\n    )\n    if not (is_correct_username and is_correct_password):\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect username or password\",\n            headers={\"WWW-Authenticate\": \"Basic\"},\n        )\n    return credentials.username\n\n\n@app.get(\"/users/me\")\ndef read_current_user(username: Annotated[str, Depends(get_current_username)]):\n    return {\"username\": username}\n", "metadata": {"license": "MIT", "len_tokens": 233}}
{"id": "fastapi:docs_src/security/tutorial005_an.py", "language": "python", "code": "async def get_current_user(\n    security_scopes: SecurityScopes, token: Annotated[str, Depends(oauth2_scheme)]\n):\n    if security_scopes.scopes:\n        authenticate_value = f'Bearer scope=\"{security_scopes.scope_str}\"'\n    else:\n        authenticate_value = \"Bearer\"\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": authenticate_value},\n    )\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n        scope: str = payload.get(\"scope\", \"\")\n        token_scopes = scope.split(\" \")\n        token_data = TokenData(scopes=token_scopes, username=username)\n    except (InvalidTokenError, ValidationError):\n        raise credentials_exception\n    user = get_user(fake_users_db, username=token_data.username)\n    if user is None:\n        raise credentials_exception\n    for scope in security_scopes.scopes:\n        if scope not in token_data.scopes:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Not enough permissions\",\n                headers={\"WWW-Authenticate\": authenticate_value},\n            )\n    return user", "metadata": {"license": "MIT", "len_tokens": 269}}
{"id": "fastapi:docs_src/security/tutorial005.py", "language": "python", "code": "async def get_current_user(\n    security_scopes: SecurityScopes, token: str = Depends(oauth2_scheme)\n):\n    if security_scopes.scopes:\n        authenticate_value = f'Bearer scope=\"{security_scopes.scope_str}\"'\n    else:\n        authenticate_value = \"Bearer\"\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": authenticate_value},\n    )\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username: str = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n        scope: str = payload.get(\"scope\", \"\")\n        token_scopes = scope.split(\" \")\n        token_data = TokenData(scopes=token_scopes, username=username)\n    except (InvalidTokenError, ValidationError):\n        raise credentials_exception\n    user = get_user(fake_users_db, username=token_data.username)\n    if user is None:\n        raise credentials_exception\n    for scope in security_scopes.scopes:\n        if scope not in token_data.scopes:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Not enough permissions\",\n                headers={\"WWW-Authenticate\": authenticate_value},\n            )\n    return user", "metadata": {"license": "MIT", "len_tokens": 269}}
{"id": "fastapi:docs_src/security/tutorial005_an_py310.py", "language": "python", "code": "async def get_current_user(\n    security_scopes: SecurityScopes, token: Annotated[str, Depends(oauth2_scheme)]\n):\n    if security_scopes.scopes:\n        authenticate_value = f'Bearer scope=\"{security_scopes.scope_str}\"'\n    else:\n        authenticate_value = \"Bearer\"\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": authenticate_value},\n    )\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n        scope: str = payload.get(\"scope\", \"\")\n        token_scopes = scope.split(\" \")\n        token_data = TokenData(scopes=token_scopes, username=username)\n    except (InvalidTokenError, ValidationError):\n        raise credentials_exception\n    user = get_user(fake_users_db, username=token_data.username)\n    if user is None:\n        raise credentials_exception\n    for scope in security_scopes.scopes:\n        if scope not in token_data.scopes:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Not enough permissions\",\n                headers={\"WWW-Authenticate\": authenticate_value},\n            )\n    return user", "metadata": {"license": "MIT", "len_tokens": 269}}
{"id": "fastapi:docs_src/security/tutorial003_an.py", "language": "python", "code": "from typing import Union\n\nfrom fastapi import Depends, FastAPI, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom pydantic import BaseModel\nfrom typing_extensions import Annotated\n\nfake_users_db = {\n    \"johndoe\": {\n        \"username\": \"johndoe\",\n        \"full_name\": \"John Doe\",\n        \"email\": \"johndoe@example.com\",\n        \"hashed_password\": \"fakehashedsecret\",\n        \"disabled\": False,\n    },\n    \"alice\": {\n        \"username\": \"alice\",\n        \"full_name\": \"Alice Wonderson\",\n        \"email\": \"alice@example.com\",\n        \"hashed_password\": \"fakehashedsecret2\",\n        \"disabled\": True,\n    },\n}\n\napp = FastAPI()\n\n\ndef fake_hash_password(password: str):\n    return \"fakehashed\" + password\n\n\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n\nclass User(BaseModel):\n    username: str\n    email: Union[str, None] = None\n    full_name: Union[str, None] = None\n    disabled: Union[bool, None] = None\n\n\nclass UserInDB(User):\n    hashed_password: str\n\n\ndef get_user(db, username: str):\n    if username in db:\n        user_dict = db[username]\n        return UserInDB(**user_dict)\n\n\ndef fake_decode_token(token):\n    # This doesn't provide any security at all\n    # Check the next version\n    user = get_user(fake_users_db, token)\n    return user\n\n\nasync def get_current_user(token: Annotated[str, Depends(oauth2_scheme)]):\n    user = fake_decode_token(token)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Not authenticated\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    return user\n\n\nasync def get_current_active_user(\n    current_user: Annotated[User, Depends(get_current_user)],\n):\n    if current_user.disabled:\n        raise HTTPException(status_code=400, detail=\"Inactive user\")\n    return current_user\n\n\n@app.post(\"/token\")\nasync def login(form_data: Annotated[OAuth2PasswordRequestForm, Depends()]):\n    user_dict = fake_users_db.get(form_data.username)\n    if not user_dict:\n        raise HTTPException(status_code=400, detail=\"Incorrect username or password\")\n    user = UserInDB(**user_dict)\n    hashed_password = fake_hash_password(form_data.password)\n    if not hashed_password == user.hashed_password:\n        raise HTTPException(status_code=400, detail=\"Incorrect username or password\")\n\n    return {\"access_token\": user.username, \"token_type\": \"bearer\"}\n\n\n@app.get(\"/users/me\")\nasync def read_users_me(\n    current_user: Annotated[User, Depends(get_current_active_user)],\n):\n    return current_user\n", "metadata": {"license": "MIT", "len_tokens": 597}}
{"id": "fastapi:docs_src/security/tutorial007_an_py39.py", "language": "python", "code": "import secrets\nfrom typing import Annotated\n\nfrom fastapi import Depends, FastAPI, HTTPException, status\nfrom fastapi.security import HTTPBasic, HTTPBasicCredentials\n\napp = FastAPI()\n\nsecurity = HTTPBasic()\n\n\ndef get_current_username(\n    credentials: Annotated[HTTPBasicCredentials, Depends(security)],\n):\n    current_username_bytes = credentials.username.encode(\"utf8\")\n    correct_username_bytes = b\"stanleyjobson\"\n    is_correct_username = secrets.compare_digest(\n        current_username_bytes, correct_username_bytes\n    )\n    current_password_bytes = credentials.password.encode(\"utf8\")\n    correct_password_bytes = b\"swordfish\"\n    is_correct_password = secrets.compare_digest(\n        current_password_bytes, correct_password_bytes\n    )\n    if not (is_correct_username and is_correct_password):\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect username or password\",\n            headers={\"WWW-Authenticate\": \"Basic\"},\n        )\n    return credentials.username\n\n\n@app.get(\"/users/me\")\ndef read_current_user(username: Annotated[str, Depends(get_current_username)]):\n    return {\"username\": username}\n", "metadata": {"license": "MIT", "len_tokens": 232}}
{"id": "fastapi:docs_src/sql_databases/tutorial002_an_py310.py", "language": "python", "code": "from typing import Annotated\n\nfrom fastapi import Depends, FastAPI, HTTPException, Query\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass HeroBase(SQLModel):\n    name: str = Field(index=True)\n    age: int | None = Field(default=None, index=True)\n\n\nclass Hero(HeroBase, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    secret_name: str\n\n\nclass HeroPublic(HeroBase):\n    id: int\n\n\nclass HeroCreate(HeroBase):\n    secret_name: str\n\n\nclass HeroUpdate(HeroBase):\n    name: str | None = None\n    age: int | None = None\n    secret_name: str | None = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nconnect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_url, connect_args=connect_args)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\nSessionDep = Annotated[Session, Depends(get_session)]\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\ndef on_startup():\n    create_db_and_tables()\n\n\n@app.post(\"/heroes/\", response_model=HeroPublic)\ndef create_hero(hero: HeroCreate, session: SessionDep):\n    db_hero = Hero.model_validate(hero)\n    session.add(db_hero)\n    session.commit()\n    session.refresh(db_hero)\n    return db_hero\n\n\n@app.get(\"/heroes/\", response_model=list[HeroPublic])\ndef read_heroes(\n    session: SessionDep,\n    offset: int = 0,\n    limit: Annotated[int, Query(le=100)] = 100,\n):\n    heroes = session.exec(select(Hero).offset(offset).limit(limit)).all()\n    return heroes\n\n\n@app.get(\"/heroes/{hero_id}\", response_model=HeroPublic)\ndef read_hero(hero_id: int, session: SessionDep):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    return hero\n\n\n@app.patch(\"/heroes/{hero_id}\", response_model=HeroPublic)\ndef update_hero(hero_id: int, hero: HeroUpdate, session: SessionDep):\n    hero_db = session.get(Hero, hero_id)\n    if not hero_db:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    hero_data = hero.model_dump(exclude_unset=True)\n    hero_db.sqlmodel_update(hero_data)\n    session.add(hero_db)\n    session.commit()\n    session.refresh(hero_db)\n    return hero_db\n\n\n@app.delete(\"/heroes/{hero_id}\")\ndef delete_hero(hero_id: int, session: SessionDep):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    session.delete(hero)\n    session.commit()\n    return {\"ok\": True}\n", "metadata": {"license": "MIT", "len_tokens": 632}}
{"id": "fastapi:docs_src/sql_databases/tutorial002.py", "language": "python", "code": "from typing import List, Union\n\nfrom fastapi import Depends, FastAPI, HTTPException, Query\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass HeroBase(SQLModel):\n    name: str = Field(index=True)\n    age: Union[int, None] = Field(default=None, index=True)\n\n\nclass Hero(HeroBase, table=True):\n    id: Union[int, None] = Field(default=None, primary_key=True)\n    secret_name: str\n\n\nclass HeroPublic(HeroBase):\n    id: int\n\n\nclass HeroCreate(HeroBase):\n    secret_name: str\n\n\nclass HeroUpdate(HeroBase):\n    name: Union[str, None] = None\n    age: Union[int, None] = None\n    secret_name: Union[str, None] = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nconnect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_url, connect_args=connect_args)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\ndef on_startup():\n    create_db_and_tables()\n\n\n@app.post(\"/heroes/\", response_model=HeroPublic)\ndef create_hero(hero: HeroCreate, session: Session = Depends(get_session)):\n    db_hero = Hero.model_validate(hero)\n    session.add(db_hero)\n    session.commit()\n    session.refresh(db_hero)\n    return db_hero\n\n\n@app.get(\"/heroes/\", response_model=List[HeroPublic])\ndef read_heroes(\n    session: Session = Depends(get_session),\n    offset: int = 0,\n    limit: int = Query(default=100, le=100),\n):\n    heroes = session.exec(select(Hero).offset(offset).limit(limit)).all()\n    return heroes\n\n\n@app.get(\"/heroes/{hero_id}\", response_model=HeroPublic)\ndef read_hero(hero_id: int, session: Session = Depends(get_session)):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    return hero\n\n\n@app.patch(\"/heroes/{hero_id}\", response_model=HeroPublic)\ndef update_hero(\n    hero_id: int, hero: HeroUpdate, session: Session = Depends(get_session)\n):\n    hero_db = session.get(Hero, hero_id)\n    if not hero_db:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    hero_data = hero.model_dump(exclude_unset=True)\n    hero_db.sqlmodel_update(hero_data)\n    session.add(hero_db)\n    session.commit()\n    session.refresh(hero_db)\n    return hero_db\n\n\n@app.delete(\"/heroes/{hero_id}\")\ndef delete_hero(hero_id: int, session: Session = Depends(get_session)):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    session.delete(hero)\n    session.commit()\n    return {\"ok\": True}\n", "metadata": {"license": "MIT", "len_tokens": 648}}
{"id": "fastapi:docs_src/sql_databases/tutorial001_an_py39.py", "language": "python", "code": "from typing import Annotated, Union\n\nfrom fastapi import Depends, FastAPI, HTTPException, Query\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, table=True):\n    id: Union[int, None] = Field(default=None, primary_key=True)\n    name: str = Field(index=True)\n    age: Union[int, None] = Field(default=None, index=True)\n    secret_name: str\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nconnect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_url, connect_args=connect_args)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\nSessionDep = Annotated[Session, Depends(get_session)]\n\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\ndef on_startup():\n    create_db_and_tables()\n\n\n@app.post(\"/heroes/\")\ndef create_hero(hero: Hero, session: SessionDep) -> Hero:\n    session.add(hero)\n    session.commit()\n    session.refresh(hero)\n    return hero\n\n\n@app.get(\"/heroes/\")\ndef read_heroes(\n    session: SessionDep,\n    offset: int = 0,\n    limit: Annotated[int, Query(le=100)] = 100,\n) -> list[Hero]:\n    heroes = session.exec(select(Hero).offset(offset).limit(limit)).all()\n    return heroes\n\n\n@app.get(\"/heroes/{hero_id}\")\ndef read_hero(hero_id: int, session: SessionDep) -> Hero:\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    return hero\n\n\n@app.delete(\"/heroes/{hero_id}\")\ndef delete_hero(hero_id: int, session: SessionDep):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    session.delete(hero)\n    session.commit()\n    return {\"ok\": True}\n", "metadata": {"license": "MIT", "len_tokens": 437}}
{"id": "fastapi:docs_src/sql_databases/tutorial001_py310.py", "language": "python", "code": "from fastapi import Depends, FastAPI, HTTPException, Query\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    name: str = Field(index=True)\n    age: int | None = Field(default=None, index=True)\n    secret_name: str\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nconnect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_url, connect_args=connect_args)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\ndef on_startup():\n    create_db_and_tables()\n\n\n@app.post(\"/heroes/\")\ndef create_hero(hero: Hero, session: Session = Depends(get_session)) -> Hero:\n    session.add(hero)\n    session.commit()\n    session.refresh(hero)\n    return hero\n\n\n@app.get(\"/heroes/\")\ndef read_heroes(\n    session: Session = Depends(get_session),\n    offset: int = 0,\n    limit: int = Query(default=100, le=100),\n) -> list[Hero]:\n    heroes = session.exec(select(Hero).offset(offset).limit(limit)).all()\n    return heroes\n\n\n@app.get(\"/heroes/{hero_id}\")\ndef read_hero(hero_id: int, session: Session = Depends(get_session)) -> Hero:\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    return hero\n\n\n@app.delete(\"/heroes/{hero_id}\")\ndef delete_hero(hero_id: int, session: Session = Depends(get_session)):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    session.delete(hero)\n    session.commit()\n    return {\"ok\": True}\n", "metadata": {"license": "MIT", "len_tokens": 423}}
{"id": "fastapi:docs_src/sql_databases/tutorial001_an_py310.py", "language": "python", "code": "from typing import Annotated\n\nfrom fastapi import Depends, FastAPI, HTTPException, Query\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    name: str = Field(index=True)\n    age: int | None = Field(default=None, index=True)\n    secret_name: str\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nconnect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_url, connect_args=connect_args)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\nSessionDep = Annotated[Session, Depends(get_session)]\n\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\ndef on_startup():\n    create_db_and_tables()\n\n\n@app.post(\"/heroes/\")\ndef create_hero(hero: Hero, session: SessionDep) -> Hero:\n    session.add(hero)\n    session.commit()\n    session.refresh(hero)\n    return hero\n\n\n@app.get(\"/heroes/\")\ndef read_heroes(\n    session: SessionDep,\n    offset: int = 0,\n    limit: Annotated[int, Query(le=100)] = 100,\n) -> list[Hero]:\n    heroes = session.exec(select(Hero).offset(offset).limit(limit)).all()\n    return heroes\n\n\n@app.get(\"/heroes/{hero_id}\")\ndef read_hero(hero_id: int, session: SessionDep) -> Hero:\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    return hero\n\n\n@app.delete(\"/heroes/{hero_id}\")\ndef delete_hero(hero_id: int, session: SessionDep):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    session.delete(hero)\n    session.commit()\n    return {\"ok\": True}\n", "metadata": {"license": "MIT", "len_tokens": 431}}
{"id": "fastapi:docs_src/sql_databases/tutorial002_py39.py", "language": "python", "code": "from typing import Union\n\nfrom fastapi import Depends, FastAPI, HTTPException, Query\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass HeroBase(SQLModel):\n    name: str = Field(index=True)\n    age: Union[int, None] = Field(default=None, index=True)\n\n\nclass Hero(HeroBase, table=True):\n    id: Union[int, None] = Field(default=None, primary_key=True)\n    secret_name: str\n\n\nclass HeroPublic(HeroBase):\n    id: int\n\n\nclass HeroCreate(HeroBase):\n    secret_name: str\n\n\nclass HeroUpdate(HeroBase):\n    name: Union[str, None] = None\n    age: Union[int, None] = None\n    secret_name: Union[str, None] = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nconnect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_url, connect_args=connect_args)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\ndef on_startup():\n    create_db_and_tables()\n\n\n@app.post(\"/heroes/\", response_model=HeroPublic)\ndef create_hero(hero: HeroCreate, session: Session = Depends(get_session)):\n    db_hero = Hero.model_validate(hero)\n    session.add(db_hero)\n    session.commit()\n    session.refresh(db_hero)\n    return db_hero\n\n\n@app.get(\"/heroes/\", response_model=list[HeroPublic])\ndef read_heroes(\n    session: Session = Depends(get_session),\n    offset: int = 0,\n    limit: int = Query(default=100, le=100),\n):\n    heroes = session.exec(select(Hero).offset(offset).limit(limit)).all()\n    return heroes\n\n\n@app.get(\"/heroes/{hero_id}\", response_model=HeroPublic)\ndef read_hero(hero_id: int, session: Session = Depends(get_session)):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    return hero\n\n\n@app.patch(\"/heroes/{hero_id}\", response_model=HeroPublic)\ndef update_hero(\n    hero_id: int, hero: HeroUpdate, session: Session = Depends(get_session)\n):\n    hero_db = session.get(Hero, hero_id)\n    if not hero_db:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    hero_data = hero.model_dump(exclude_unset=True)\n    hero_db.sqlmodel_update(hero_data)\n    session.add(hero_db)\n    session.commit()\n    session.refresh(hero_db)\n    return hero_db\n\n\n@app.delete(\"/heroes/{hero_id}\")\ndef delete_hero(hero_id: int, session: Session = Depends(get_session)):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    session.delete(hero)\n    session.commit()\n    return {\"ok\": True}\n", "metadata": {"license": "MIT", "len_tokens": 645}}
{"id": "fastapi:docs_src/sql_databases/tutorial002_an.py", "language": "python", "code": "from typing import List, Union\n\nfrom fastapi import Depends, FastAPI, HTTPException, Query\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\nfrom typing_extensions import Annotated\n\n\nclass HeroBase(SQLModel):\n    name: str = Field(index=True)\n    age: Union[int, None] = Field(default=None, index=True)\n\n\nclass Hero(HeroBase, table=True):\n    id: Union[int, None] = Field(default=None, primary_key=True)\n    secret_name: str\n\n\nclass HeroPublic(HeroBase):\n    id: int\n\n\nclass HeroCreate(HeroBase):\n    secret_name: str\n\n\nclass HeroUpdate(HeroBase):\n    name: Union[str, None] = None\n    age: Union[int, None] = None\n    secret_name: Union[str, None] = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nconnect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_url, connect_args=connect_args)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\nSessionDep = Annotated[Session, Depends(get_session)]\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\ndef on_startup():\n    create_db_and_tables()\n\n\n@app.post(\"/heroes/\", response_model=HeroPublic)\ndef create_hero(hero: HeroCreate, session: SessionDep):\n    db_hero = Hero.model_validate(hero)\n    session.add(db_hero)\n    session.commit()\n    session.refresh(db_hero)\n    return db_hero\n\n\n@app.get(\"/heroes/\", response_model=List[HeroPublic])\ndef read_heroes(\n    session: SessionDep,\n    offset: int = 0,\n    limit: Annotated[int, Query(le=100)] = 100,\n):\n    heroes = session.exec(select(Hero).offset(offset).limit(limit)).all()\n    return heroes\n\n\n@app.get(\"/heroes/{hero_id}\", response_model=HeroPublic)\ndef read_hero(hero_id: int, session: SessionDep):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    return hero\n\n\n@app.patch(\"/heroes/{hero_id}\", response_model=HeroPublic)\ndef update_hero(hero_id: int, hero: HeroUpdate, session: SessionDep):\n    hero_db = session.get(Hero, hero_id)\n    if not hero_db:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    hero_data = hero.model_dump(exclude_unset=True)\n    hero_db.sqlmodel_update(hero_data)\n    session.add(hero_db)\n    session.commit()\n    session.refresh(hero_db)\n    return hero_db\n\n\n@app.delete(\"/heroes/{hero_id}\")\ndef delete_hero(hero_id: int, session: SessionDep):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    session.delete(hero)\n    session.commit()\n    return {\"ok\": True}\n", "metadata": {"license": "MIT", "len_tokens": 651}}
{"id": "fastapi:docs_src/sql_databases/tutorial002_py310.py", "language": "python", "code": "from fastapi import Depends, FastAPI, HTTPException, Query\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass HeroBase(SQLModel):\n    name: str = Field(index=True)\n    age: int | None = Field(default=None, index=True)\n\n\nclass Hero(HeroBase, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    secret_name: str\n\n\nclass HeroPublic(HeroBase):\n    id: int\n\n\nclass HeroCreate(HeroBase):\n    secret_name: str\n\n\nclass HeroUpdate(HeroBase):\n    name: str | None = None\n    age: int | None = None\n    secret_name: str | None = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nconnect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_url, connect_args=connect_args)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\ndef on_startup():\n    create_db_and_tables()\n\n\n@app.post(\"/heroes/\", response_model=HeroPublic)\ndef create_hero(hero: HeroCreate, session: Session = Depends(get_session)):\n    db_hero = Hero.model_validate(hero)\n    session.add(db_hero)\n    session.commit()\n    session.refresh(db_hero)\n    return db_hero\n\n\n@app.get(\"/heroes/\", response_model=list[HeroPublic])\ndef read_heroes(\n    session: Session = Depends(get_session),\n    offset: int = 0,\n    limit: int = Query(default=100, le=100),\n):\n    heroes = session.exec(select(Hero).offset(offset).limit(limit)).all()\n    return heroes\n\n\n@app.get(\"/heroes/{hero_id}\", response_model=HeroPublic)\ndef read_hero(hero_id: int, session: Session = Depends(get_session)):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    return hero\n\n\n@app.patch(\"/heroes/{hero_id}\", response_model=HeroPublic)\ndef update_hero(\n    hero_id: int, hero: HeroUpdate, session: Session = Depends(get_session)\n):\n    hero_db = session.get(Hero, hero_id)\n    if not hero_db:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    hero_data = hero.model_dump(exclude_unset=True)\n    hero_db.sqlmodel_update(hero_data)\n    session.add(hero_db)\n    session.commit()\n    session.refresh(hero_db)\n    return hero_db\n\n\n@app.delete(\"/heroes/{hero_id}\")\ndef delete_hero(hero_id: int, session: Session = Depends(get_session)):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    session.delete(hero)\n    session.commit()\n    return {\"ok\": True}\n", "metadata": {"license": "MIT", "len_tokens": 630}}
{"id": "fastapi:docs_src/sql_databases/tutorial002_an_py39.py", "language": "python", "code": "from typing import Annotated, Union\n\nfrom fastapi import Depends, FastAPI, HTTPException, Query\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass HeroBase(SQLModel):\n    name: str = Field(index=True)\n    age: Union[int, None] = Field(default=None, index=True)\n\n\nclass Hero(HeroBase, table=True):\n    id: Union[int, None] = Field(default=None, primary_key=True)\n    secret_name: str\n\n\nclass HeroPublic(HeroBase):\n    id: int\n\n\nclass HeroCreate(HeroBase):\n    secret_name: str\n\n\nclass HeroUpdate(HeroBase):\n    name: Union[str, None] = None\n    age: Union[int, None] = None\n    secret_name: Union[str, None] = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nconnect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_url, connect_args=connect_args)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\nSessionDep = Annotated[Session, Depends(get_session)]\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\ndef on_startup():\n    create_db_and_tables()\n\n\n@app.post(\"/heroes/\", response_model=HeroPublic)\ndef create_hero(hero: HeroCreate, session: SessionDep):\n    db_hero = Hero.model_validate(hero)\n    session.add(db_hero)\n    session.commit()\n    session.refresh(db_hero)\n    return db_hero\n\n\n@app.get(\"/heroes/\", response_model=list[HeroPublic])\ndef read_heroes(\n    session: SessionDep,\n    offset: int = 0,\n    limit: Annotated[int, Query(le=100)] = 100,\n):\n    heroes = session.exec(select(Hero).offset(offset).limit(limit)).all()\n    return heroes\n\n\n@app.get(\"/heroes/{hero_id}\", response_model=HeroPublic)\ndef read_hero(hero_id: int, session: SessionDep):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    return hero\n\n\n@app.patch(\"/heroes/{hero_id}\", response_model=HeroPublic)\ndef update_hero(hero_id: int, hero: HeroUpdate, session: SessionDep):\n    hero_db = session.get(Hero, hero_id)\n    if not hero_db:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    hero_data = hero.model_dump(exclude_unset=True)\n    hero_db.sqlmodel_update(hero_data)\n    session.add(hero_db)\n    session.commit()\n    session.refresh(hero_db)\n    return hero_db\n\n\n@app.delete(\"/heroes/{hero_id}\")\ndef delete_hero(hero_id: int, session: SessionDep):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    session.delete(hero)\n    session.commit()\n    return {\"ok\": True}\n", "metadata": {"license": "MIT", "len_tokens": 644}}
{"id": "fastapi:docs_src/sql_databases/tutorial001_py39.py", "language": "python", "code": "from typing import Union\n\nfrom fastapi import Depends, FastAPI, HTTPException, Query\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, table=True):\n    id: Union[int, None] = Field(default=None, primary_key=True)\n    name: str = Field(index=True)\n    age: Union[int, None] = Field(default=None, index=True)\n    secret_name: str\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nconnect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_url, connect_args=connect_args)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\ndef on_startup():\n    create_db_and_tables()\n\n\n@app.post(\"/heroes/\")\ndef create_hero(hero: Hero, session: Session = Depends(get_session)) -> Hero:\n    session.add(hero)\n    session.commit()\n    session.refresh(hero)\n    return hero\n\n\n@app.get(\"/heroes/\")\ndef read_heroes(\n    session: Session = Depends(get_session),\n    offset: int = 0,\n    limit: int = Query(default=100, le=100),\n) -> list[Hero]:\n    heroes = session.exec(select(Hero).offset(offset).limit(limit)).all()\n    return heroes\n\n\n@app.get(\"/heroes/{hero_id}\")\ndef read_hero(hero_id: int, session: Session = Depends(get_session)) -> Hero:\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    return hero\n\n\n@app.delete(\"/heroes/{hero_id}\")\ndef delete_hero(hero_id: int, session: Session = Depends(get_session)):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    session.delete(hero)\n    session.commit()\n    return {\"ok\": True}\n", "metadata": {"license": "MIT", "len_tokens": 432}}
{"id": "fastapi:docs_src/sql_databases/tutorial001_an.py", "language": "python", "code": "from typing import List, Union\n\nfrom fastapi import Depends, FastAPI, HTTPException, Query\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\nfrom typing_extensions import Annotated\n\n\nclass Hero(SQLModel, table=True):\n    id: Union[int, None] = Field(default=None, primary_key=True)\n    name: str = Field(index=True)\n    age: Union[int, None] = Field(default=None, index=True)\n    secret_name: str\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nconnect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_url, connect_args=connect_args)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\nSessionDep = Annotated[Session, Depends(get_session)]\n\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\ndef on_startup():\n    create_db_and_tables()\n\n\n@app.post(\"/heroes/\")\ndef create_hero(hero: Hero, session: SessionDep) -> Hero:\n    session.add(hero)\n    session.commit()\n    session.refresh(hero)\n    return hero\n\n\n@app.get(\"/heroes/\")\ndef read_heroes(\n    session: SessionDep,\n    offset: int = 0,\n    limit: Annotated[int, Query(le=100)] = 100,\n) -> List[Hero]:\n    heroes = session.exec(select(Hero).offset(offset).limit(limit)).all()\n    return heroes\n\n\n@app.get(\"/heroes/{hero_id}\")\ndef read_hero(hero_id: int, session: SessionDep) -> Hero:\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    return hero\n\n\n@app.delete(\"/heroes/{hero_id}\")\ndef delete_hero(hero_id: int, session: SessionDep):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    session.delete(hero)\n    session.commit()\n    return {\"ok\": True}\n", "metadata": {"license": "MIT", "len_tokens": 443}}
{"id": "fastapi:docs_src/sql_databases/tutorial001.py", "language": "python", "code": "from typing import List, Union\n\nfrom fastapi import Depends, FastAPI, HTTPException, Query\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, table=True):\n    id: Union[int, None] = Field(default=None, primary_key=True)\n    name: str = Field(index=True)\n    age: Union[int, None] = Field(default=None, index=True)\n    secret_name: str\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nconnect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_url, connect_args=connect_args)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\ndef on_startup():\n    create_db_and_tables()\n\n\n@app.post(\"/heroes/\")\ndef create_hero(hero: Hero, session: Session = Depends(get_session)) -> Hero:\n    session.add(hero)\n    session.commit()\n    session.refresh(hero)\n    return hero\n\n\n@app.get(\"/heroes/\")\ndef read_heroes(\n    session: Session = Depends(get_session),\n    offset: int = 0,\n    limit: int = Query(default=100, le=100),\n) -> List[Hero]:\n    heroes = session.exec(select(Hero).offset(offset).limit(limit)).all()\n    return heroes\n\n\n@app.get(\"/heroes/{hero_id}\")\ndef read_hero(hero_id: int, session: Session = Depends(get_session)) -> Hero:\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    return hero\n\n\n@app.delete(\"/heroes/{hero_id}\")\ndef delete_hero(hero_id: int, session: Session = Depends(get_session)):\n    hero = session.get(Hero, hero_id)\n    if not hero:\n        raise HTTPException(status_code=404, detail=\"Hero not found\")\n    session.delete(hero)\n    session.commit()\n    return {\"ok\": True}\n", "metadata": {"license": "MIT", "len_tokens": 434}}
{"id": "fastapi:docs_src/query_params_str_validations/tutorial015_an_py39.py", "language": "python", "code": "import random\nfrom typing import Annotated, Union\n\nfrom fastapi import FastAPI\nfrom pydantic import AfterValidator\n\napp = FastAPI()\n\ndata = {\n    \"isbn-9781529046137\": \"The Hitchhiker's Guide to the Galaxy\",\n    \"imdb-tt0371724\": \"The Hitchhiker's Guide to the Galaxy\",\n    \"isbn-9781439512982\": \"Isaac Asimov: The Complete Stories, Vol. 2\",\n}\n\n\ndef check_valid_id(id: str):\n    if not id.startswith((\"isbn-\", \"imdb-\")):\n        raise ValueError('Invalid ID format, it must start with \"isbn-\" or \"imdb-\"')\n    return id\n\n\n@app.get(\"/items/\")\nasync def read_items(\n    id: Annotated[Union[str, None], AfterValidator(check_valid_id)] = None,\n):\n    if id:\n        item = data.get(id)\n    else:\n        id, item = random.choice(list(data.items()))\n    return {\"id\": id, \"name\": item}\n", "metadata": {"license": "MIT", "len_tokens": 219}}
{"id": "fastapi:docs_src/query_params_str_validations/tutorial015_an.py", "language": "python", "code": "import random\nfrom typing import Union\n\nfrom fastapi import FastAPI\nfrom pydantic import AfterValidator\nfrom typing_extensions import Annotated\n\napp = FastAPI()\n\ndata = {\n    \"isbn-9781529046137\": \"The Hitchhiker's Guide to the Galaxy\",\n    \"imdb-tt0371724\": \"The Hitchhiker's Guide to the Galaxy\",\n    \"isbn-9781439512982\": \"Isaac Asimov: The Complete Stories, Vol. 2\",\n}\n\n\ndef check_valid_id(id: str):\n    if not id.startswith((\"isbn-\", \"imdb-\")):\n        raise ValueError('Invalid ID format, it must start with \"isbn-\" or \"imdb-\"')\n    return id\n\n\n@app.get(\"/items/\")\nasync def read_items(\n    id: Annotated[Union[str, None], AfterValidator(check_valid_id)] = None,\n):\n    if id:\n        item = data.get(id)\n    else:\n        id, item = random.choice(list(data.items()))\n    return {\"id\": id, \"name\": item}\n", "metadata": {"license": "MIT", "len_tokens": 223}}
{"id": "fastapi:docs_src/query_params_str_validations/tutorial015_an_py310.py", "language": "python", "code": "import random\nfrom typing import Annotated\n\nfrom fastapi import FastAPI\nfrom pydantic import AfterValidator\n\napp = FastAPI()\n\ndata = {\n    \"isbn-9781529046137\": \"The Hitchhiker's Guide to the Galaxy\",\n    \"imdb-tt0371724\": \"The Hitchhiker's Guide to the Galaxy\",\n    \"isbn-9781439512982\": \"Isaac Asimov: The Complete Stories, Vol. 2\",\n}\n\n\ndef check_valid_id(id: str):\n    if not id.startswith((\"isbn-\", \"imdb-\")):\n        raise ValueError('Invalid ID format, it must start with \"isbn-\" or \"imdb-\"')\n    return id\n\n\n@app.get(\"/items/\")\nasync def read_items(\n    id: Annotated[str | None, AfterValidator(check_valid_id)] = None,\n):\n    if id:\n        item = data.get(id)\n    else:\n        id, item = random.choice(list(data.items()))\n    return {\"id\": id, \"name\": item}\n", "metadata": {"license": "MIT", "len_tokens": 215}}
{"id": "fastapi:docs_src/body_updates/tutorial002.py", "language": "python", "code": "from typing import List, Union\n\nfrom fastapi import FastAPI\nfrom fastapi.encoders import jsonable_encoder\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    name: Union[str, None] = None\n    description: Union[str, None] = None\n    price: Union[float, None] = None\n    tax: float = 10.5\n    tags: List[str] = []\n\n\nitems = {\n    \"foo\": {\"name\": \"Foo\", \"price\": 50.2},\n    \"bar\": {\"name\": \"Bar\", \"description\": \"The bartenders\", \"price\": 62, \"tax\": 20.2},\n    \"baz\": {\"name\": \"Baz\", \"description\": None, \"price\": 50.2, \"tax\": 10.5, \"tags\": []},\n}\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\nasync def read_item(item_id: str):\n    return items[item_id]\n\n\n@app.patch(\"/items/{item_id}\", response_model=Item)\nasync def update_item(item_id: str, item: Item):\n    stored_item_data = items[item_id]\n    stored_item_model = Item(**stored_item_data)\n    update_data = item.dict(exclude_unset=True)\n    updated_item = stored_item_model.copy(update=update_data)\n    items[item_id] = jsonable_encoder(updated_item)\n    return updated_item\n", "metadata": {"license": "MIT", "len_tokens": 301}}
{"id": "fastapi:docs_src/body_updates/tutorial001_py310.py", "language": "python", "code": "from fastapi import FastAPI\nfrom fastapi.encoders import jsonable_encoder\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    name: str | None = None\n    description: str | None = None\n    price: float | None = None\n    tax: float = 10.5\n    tags: list[str] = []\n\n\nitems = {\n    \"foo\": {\"name\": \"Foo\", \"price\": 50.2},\n    \"bar\": {\"name\": \"Bar\", \"description\": \"The bartenders\", \"price\": 62, \"tax\": 20.2},\n    \"baz\": {\"name\": \"Baz\", \"description\": None, \"price\": 50.2, \"tax\": 10.5, \"tags\": []},\n}\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\nasync def read_item(item_id: str):\n    return items[item_id]\n\n\n@app.put(\"/items/{item_id}\", response_model=Item)\nasync def update_item(item_id: str, item: Item):\n    update_item_encoded = jsonable_encoder(item)\n    items[item_id] = update_item_encoded\n    return update_item_encoded\n", "metadata": {"license": "MIT", "len_tokens": 251}}
{"id": "fastapi:docs_src/body_updates/tutorial002_py39.py", "language": "python", "code": "from typing import Union\n\nfrom fastapi import FastAPI\nfrom fastapi.encoders import jsonable_encoder\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    name: Union[str, None] = None\n    description: Union[str, None] = None\n    price: Union[float, None] = None\n    tax: float = 10.5\n    tags: list[str] = []\n\n\nitems = {\n    \"foo\": {\"name\": \"Foo\", \"price\": 50.2},\n    \"bar\": {\"name\": \"Bar\", \"description\": \"The bartenders\", \"price\": 62, \"tax\": 20.2},\n    \"baz\": {\"name\": \"Baz\", \"description\": None, \"price\": 50.2, \"tax\": 10.5, \"tags\": []},\n}\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\nasync def read_item(item_id: str):\n    return items[item_id]\n\n\n@app.patch(\"/items/{item_id}\", response_model=Item)\nasync def update_item(item_id: str, item: Item):\n    stored_item_data = items[item_id]\n    stored_item_model = Item(**stored_item_data)\n    update_data = item.dict(exclude_unset=True)\n    updated_item = stored_item_model.copy(update=update_data)\n    items[item_id] = jsonable_encoder(updated_item)\n    return updated_item\n", "metadata": {"license": "MIT", "len_tokens": 299}}
{"id": "fastapi:docs_src/body_updates/tutorial002_py310.py", "language": "python", "code": "from fastapi import FastAPI\nfrom fastapi.encoders import jsonable_encoder\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    name: str | None = None\n    description: str | None = None\n    price: float | None = None\n    tax: float = 10.5\n    tags: list[str] = []\n\n\nitems = {\n    \"foo\": {\"name\": \"Foo\", \"price\": 50.2},\n    \"bar\": {\"name\": \"Bar\", \"description\": \"The bartenders\", \"price\": 62, \"tax\": 20.2},\n    \"baz\": {\"name\": \"Baz\", \"description\": None, \"price\": 50.2, \"tax\": 10.5, \"tags\": []},\n}\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\nasync def read_item(item_id: str):\n    return items[item_id]\n\n\n@app.patch(\"/items/{item_id}\", response_model=Item)\nasync def update_item(item_id: str, item: Item):\n    stored_item_data = items[item_id]\n    stored_item_model = Item(**stored_item_data)\n    update_data = item.dict(exclude_unset=True)\n    updated_item = stored_item_model.copy(update=update_data)\n    items[item_id] = jsonable_encoder(updated_item)\n    return updated_item\n", "metadata": {"license": "MIT", "len_tokens": 287}}
{"id": "fastapi:docs_src/body_updates/tutorial001_py39.py", "language": "python", "code": "from typing import Union\n\nfrom fastapi import FastAPI\nfrom fastapi.encoders import jsonable_encoder\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    name: Union[str, None] = None\n    description: Union[str, None] = None\n    price: Union[float, None] = None\n    tax: float = 10.5\n    tags: list[str] = []\n\n\nitems = {\n    \"foo\": {\"name\": \"Foo\", \"price\": 50.2},\n    \"bar\": {\"name\": \"Bar\", \"description\": \"The bartenders\", \"price\": 62, \"tax\": 20.2},\n    \"baz\": {\"name\": \"Baz\", \"description\": None, \"price\": 50.2, \"tax\": 10.5, \"tags\": []},\n}\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\nasync def read_item(item_id: str):\n    return items[item_id]\n\n\n@app.put(\"/items/{item_id}\", response_model=Item)\nasync def update_item(item_id: str, item: Item):\n    update_item_encoded = jsonable_encoder(item)\n    items[item_id] = update_item_encoded\n    return update_item_encoded\n", "metadata": {"license": "MIT", "len_tokens": 263}}
{"id": "fastapi:docs_src/body_updates/tutorial001.py", "language": "python", "code": "from typing import List, Union\n\nfrom fastapi import FastAPI\nfrom fastapi.encoders import jsonable_encoder\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    name: Union[str, None] = None\n    description: Union[str, None] = None\n    price: Union[float, None] = None\n    tax: float = 10.5\n    tags: List[str] = []\n\n\nitems = {\n    \"foo\": {\"name\": \"Foo\", \"price\": 50.2},\n    \"bar\": {\"name\": \"Bar\", \"description\": \"The bartenders\", \"price\": 62, \"tax\": 20.2},\n    \"baz\": {\"name\": \"Baz\", \"description\": None, \"price\": 50.2, \"tax\": 10.5, \"tags\": []},\n}\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\nasync def read_item(item_id: str):\n    return items[item_id]\n\n\n@app.put(\"/items/{item_id}\", response_model=Item)\nasync def update_item(item_id: str, item: Item):\n    update_item_encoded = jsonable_encoder(item)\n    items[item_id] = update_item_encoded\n    return update_item_encoded\n", "metadata": {"license": "MIT", "len_tokens": 265}}
{"id": "fastapi:docs_src/path_operation_advanced_configuration/tutorial006.py", "language": "python", "code": "from fastapi import FastAPI, Request\n\napp = FastAPI()\n\n\ndef magic_data_reader(raw_body: bytes):\n    return {\n        \"size\": len(raw_body),\n        \"content\": {\n            \"name\": \"Maaaagic\",\n            \"price\": 42,\n            \"description\": \"Just kiddin', no magic here. \",\n        },\n    }\n\n\n@app.post(\n    \"/items/\",\n    openapi_extra={\n        \"requestBody\": {\n            \"content\": {\n                \"application/json\": {\n                    \"schema\": {\n                        \"required\": [\"name\", \"price\"],\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"name\": {\"type\": \"string\"},\n                            \"price\": {\"type\": \"number\"},\n                            \"description\": {\"type\": \"string\"},\n                        },\n                    }\n                }\n            },\n            \"required\": True,\n        },\n    },\n)\nasync def create_item(request: Request):\n    raw_body = await request.body()\n    data = magic_data_reader(raw_body)\n    return data\n", "metadata": {"license": "MIT", "len_tokens": 209}}
{"id": "fastapi:docs_src/generate_clients/tutorial003.py", "language": "python", "code": "from typing import List\n\nfrom fastapi import FastAPI\nfrom fastapi.routing import APIRoute\nfrom pydantic import BaseModel\n\n\ndef custom_generate_unique_id(route: APIRoute):\n    return f\"{route.tags[0]}-{route.name}\"\n\n\napp = FastAPI(generate_unique_id_function=custom_generate_unique_id)\n\n\nclass Item(BaseModel):\n    name: str\n    price: float\n\n\nclass ResponseMessage(BaseModel):\n    message: str\n\n\nclass User(BaseModel):\n    username: str\n    email: str\n\n\n@app.post(\"/items/\", response_model=ResponseMessage, tags=[\"items\"])\nasync def create_item(item: Item):\n    return {\"message\": \"Item received\"}\n\n\n@app.get(\"/items/\", response_model=List[Item], tags=[\"items\"])\nasync def get_items():\n    return [\n        {\"name\": \"Plumbus\", \"price\": 3},\n        {\"name\": \"Portal Gun\", \"price\": 9001},\n    ]\n\n\n@app.post(\"/users/\", response_model=ResponseMessage, tags=[\"users\"])\nasync def create_user(user: User):\n    return {\"message\": \"User received\"}\n", "metadata": {"license": "MIT", "len_tokens": 230}}
{"id": "fastapi:docs_src/generate_clients/tutorial003_py39.py", "language": "python", "code": "from fastapi import FastAPI\nfrom fastapi.routing import APIRoute\nfrom pydantic import BaseModel\n\n\ndef custom_generate_unique_id(route: APIRoute):\n    return f\"{route.tags[0]}-{route.name}\"\n\n\napp = FastAPI(generate_unique_id_function=custom_generate_unique_id)\n\n\nclass Item(BaseModel):\n    name: str\n    price: float\n\n\nclass ResponseMessage(BaseModel):\n    message: str\n\n\nclass User(BaseModel):\n    username: str\n    email: str\n\n\n@app.post(\"/items/\", response_model=ResponseMessage, tags=[\"items\"])\nasync def create_item(item: Item):\n    return {\"message\": \"Item received\"}\n\n\n@app.get(\"/items/\", response_model=list[Item], tags=[\"items\"])\nasync def get_items():\n    return [\n        {\"name\": \"Plumbus\", \"price\": 3},\n        {\"name\": \"Portal Gun\", \"price\": 9001},\n    ]\n\n\n@app.post(\"/users/\", response_model=ResponseMessage, tags=[\"users\"])\nasync def create_user(user: User):\n    return {\"message\": \"User received\"}\n", "metadata": {"license": "MIT", "len_tokens": 224}}
{"id": "fastapi:docs_src/dependencies/tutorial014_an_py310.py", "language": "python", "code": "import time\nfrom typing import Annotated\n\nfrom fastapi import Depends, FastAPI, HTTPException\nfrom fastapi.responses import StreamingResponse\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\nengine = create_engine(\"postgresql+psycopg://postgres:postgres@localhost/db\")\n\n\nclass User(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n\n\napp = FastAPI()\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\ndef get_user(user_id: int, session: Annotated[Session, Depends(get_session)]):\n    user = session.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=403, detail=\"Not authorized\")\n    session.close()\n\n\ndef generate_stream(query: str):\n    for ch in query:\n        yield ch\n        time.sleep(0.1)\n\n\n@app.get(\"/generate\", dependencies=[Depends(get_user)])\ndef generate(query: str):\n    return StreamingResponse(content=generate_stream(query))\n", "metadata": {"license": "MIT", "len_tokens": 217}}
{"id": "fastapi:docs_src/dependencies/tutorial013_an_py310.py", "language": "python", "code": "import time\nfrom typing import Annotated\n\nfrom fastapi import Depends, FastAPI, HTTPException\nfrom fastapi.responses import StreamingResponse\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\nengine = create_engine(\"postgresql+psycopg://postgres:postgres@localhost/db\")\n\n\nclass User(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n\n\napp = FastAPI()\n\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\ndef get_user(user_id: int, session: Annotated[Session, Depends(get_session)]):\n    user = session.get(User, user_id)\n    if not user:\n        raise HTTPException(status_code=403, detail=\"Not authorized\")\n\n\ndef generate_stream(query: str):\n    for ch in query:\n        yield ch\n        time.sleep(0.1)\n\n\n@app.get(\"/generate\", dependencies=[Depends(get_user)])\ndef generate(query: str):\n    return StreamingResponse(content=generate_stream(query))\n", "metadata": {"license": "MIT", "len_tokens": 213}}
{"id": "fastapi:docs_src/websockets/tutorial002_an_py310.py", "language": "python", "code": "from typing import Annotated\n\nfrom fastapi import (\n    Cookie,\n    Depends,\n    FastAPI,\n    Query,\n    WebSocket,\n    WebSocketException,\n    status,\n)\nfrom fastapi.responses import HTMLResponse\n\napp = FastAPI()\n\nhtml = \"\"\"\n<!DOCTYPE html>\n<html>\n    <head>\n        <title>Chat</title>\n    </head>\n    <body>\n        <h1>WebSocket Chat</h1>\n        <form action=\"\" onsubmit=\"sendMessage(event)\">\n            <label>Item ID: <input type=\"text\" id=\"itemId\" autocomplete=\"off\" value=\"foo\"/></label>\n            <label>Token: <input type=\"text\" id=\"token\" autocomplete=\"off\" value=\"some-key-token\"/></label>\n            <button onclick=\"connect(event)\">Connect</button>\n            <hr>\n            <label>Message: <input type=\"text\" id=\"messageText\" autocomplete=\"off\"/></label>\n            <button>Send</button>\n        </form>\n        <ul id='messages'>\n        </ul>\n        <script>\n        var ws = null;\n            function connect(event) {\n                var itemId = document.getElementById(\"itemId\")\n                var token = document.getElementById(\"token\")\n                ws = new WebSocket(\"ws://localhost:8000/items/\" + itemId.value + \"/ws?token=\" + token.value);\n                ws.onmessage = function(event) {\n                    var messages = document.getElementById('messages')\n                    var message = document.createElement('li')\n                    var content = document.createTextNode(event.data)\n                    message.appendChild(content)\n                    messages.appendChild(message)\n                };\n                event.preventDefault()\n            }\n            function sendMessage(event) {\n                var input = document.getElementById(\"messageText\")\n                ws.send(input.value)\n                input.value = ''\n                event.preventDefault()\n            }\n        </script>\n    </body>\n</html>\n\"\"\"\n\n\n@app.get(\"/\")\nasync def get():\n    return HTMLResponse(html)\n\n\nasync def get_cookie_or_token(\n    websocket: WebSocket,\n    session: Annotated[str | None, Cookie()] = None,\n    token: Annotated[str | None, Query()] = None,\n):\n    if session is None and token is None:\n        raise WebSocketException(code=status.WS_1008_POLICY_VIOLATION)\n    return session or token\n\n\n@app.websocket(\"/items/{item_id}/ws\")\nasync def websocket_endpoint(\n    *,\n    websocket: WebSocket,\n    item_id: str,\n    q: int | None = None,\n    cookie_or_token: Annotated[str, Depends(get_cookie_or_token)],\n):\n    await websocket.accept()\n    while True:\n        data = await websocket.receive_text()\n        await websocket.send_text(\n            f\"Session cookie or query token value is: {cookie_or_token}\"\n        )\n        if q is not None:\n            await websocket.send_text(f\"Query parameter q is: {q}\")\n        await websocket.send_text(f\"Message text was: {data}, for item ID: {item_id}\")\n", "metadata": {"license": "MIT", "len_tokens": 633}}
{"id": "fastapi:docs_src/websockets/tutorial002.py", "language": "python", "code": "from typing import Union\n\nfrom fastapi import (\n    Cookie,\n    Depends,\n    FastAPI,\n    Query,\n    WebSocket,\n    WebSocketException,\n    status,\n)\nfrom fastapi.responses import HTMLResponse\n\napp = FastAPI()\n\nhtml = \"\"\"\n<!DOCTYPE html>\n<html>\n    <head>\n        <title>Chat</title>\n    </head>\n    <body>\n        <h1>WebSocket Chat</h1>\n        <form action=\"\" onsubmit=\"sendMessage(event)\">\n            <label>Item ID: <input type=\"text\" id=\"itemId\" autocomplete=\"off\" value=\"foo\"/></label>\n            <label>Token: <input type=\"text\" id=\"token\" autocomplete=\"off\" value=\"some-key-token\"/></label>\n            <button onclick=\"connect(event)\">Connect</button>\n            <hr>\n            <label>Message: <input type=\"text\" id=\"messageText\" autocomplete=\"off\"/></label>\n            <button>Send</button>\n        </form>\n        <ul id='messages'>\n        </ul>\n        <script>\n        var ws = null;\n            function connect(event) {\n                var itemId = document.getElementById(\"itemId\")\n                var token = document.getElementById(\"token\")\n                ws = new WebSocket(\"ws://localhost:8000/items/\" + itemId.value + \"/ws?token=\" + token.value);\n                ws.onmessage = function(event) {\n                    var messages = document.getElementById('messages')\n                    var message = document.createElement('li')\n                    var content = document.createTextNode(event.data)\n                    message.appendChild(content)\n                    messages.appendChild(message)\n                };\n                event.preventDefault()\n            }\n            function sendMessage(event) {\n                var input = document.getElementById(\"messageText\")\n                ws.send(input.value)\n                input.value = ''\n                event.preventDefault()\n            }\n        </script>\n    </body>\n</html>\n\"\"\"\n\n\n@app.get(\"/\")\nasync def get():\n    return HTMLResponse(html)\n\n\nasync def get_cookie_or_token(\n    websocket: WebSocket,\n    session: Union[str, None] = Cookie(default=None),\n    token: Union[str, None] = Query(default=None),\n):\n    if session is None and token is None:\n        raise WebSocketException(code=status.WS_1008_POLICY_VIOLATION)\n    return session or token\n\n\n@app.websocket(\"/items/{item_id}/ws\")\nasync def websocket_endpoint(\n    websocket: WebSocket,\n    item_id: str,\n    q: Union[int, None] = None,\n    cookie_or_token: str = Depends(get_cookie_or_token),\n):\n    await websocket.accept()\n    while True:\n        data = await websocket.receive_text()\n        await websocket.send_text(\n            f\"Session cookie or query token value is: {cookie_or_token}\"\n        )\n        if q is not None:\n            await websocket.send_text(f\"Query parameter q is: {q}\")\n        await websocket.send_text(f\"Message text was: {data}, for item ID: {item_id}\")\n", "metadata": {"license": "MIT", "len_tokens": 628}}
{"id": "fastapi:docs_src/websockets/tutorial003.py", "language": "python", "code": "from typing import List\n\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect\nfrom fastapi.responses import HTMLResponse\n\napp = FastAPI()\n\nhtml = \"\"\"\n<!DOCTYPE html>\n<html>\n    <head>\n        <title>Chat</title>\n    </head>\n    <body>\n        <h1>WebSocket Chat</h1>\n        <h2>Your ID: <span id=\"ws-id\"></span></h2>\n        <form action=\"\" onsubmit=\"sendMessage(event)\">\n            <input type=\"text\" id=\"messageText\" autocomplete=\"off\"/>\n            <button>Send</button>\n        </form>\n        <ul id='messages'>\n        </ul>\n        <script>\n            var client_id = Date.now()\n            document.querySelector(\"#ws-id\").textContent = client_id;\n            var ws = new WebSocket(`ws://localhost:8000/ws/${client_id}`);\n            ws.onmessage = function(event) {\n                var messages = document.getElementById('messages')\n                var message = document.createElement('li')\n                var content = document.createTextNode(event.data)\n                message.appendChild(content)\n                messages.appendChild(message)\n            };\n            function sendMessage(event) {\n                var input = document.getElementById(\"messageText\")\n                ws.send(input.value)\n                input.value = ''\n                event.preventDefault()\n            }\n        </script>\n    </body>\n</html>\n\"\"\"\n\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: List[WebSocket] = []\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        self.active_connections.append(websocket)\n\n    def disconnect(self, websocket: WebSocket):\n        self.active_connections.remove(websocket)\n\n    async def send_personal_message(self, message: str, websocket: WebSocket):\n        await websocket.send_text(message)\n\n    async def broadcast(self, message: str):\n        for connection in self.active_connections:\n            await connection.send_text(message)\n\n\nmanager = ConnectionManager()\n\n\n@app.get(\"/\")\nasync def get():\n    return HTMLResponse(html)\n\n\n@app.websocket(\"/ws/{client_id}\")\nasync def websocket_endpoint(websocket: WebSocket, client_id: int):\n    await manager.connect(websocket)\n    try:\n        while True:\n            data = await websocket.receive_text()\n            await manager.send_personal_message(f\"You wrote: {data}\", websocket)\n            await manager.broadcast(f\"Client #{client_id} says: {data}\")\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n        await manager.broadcast(f\"Client #{client_id} left the chat\")\n", "metadata": {"license": "MIT", "len_tokens": 531}}
{"id": "fastapi:docs_src/websockets/tutorial002_an.py", "language": "python", "code": "from typing import Union\n\nfrom fastapi import (\n    Cookie,\n    Depends,\n    FastAPI,\n    Query,\n    WebSocket,\n    WebSocketException,\n    status,\n)\nfrom fastapi.responses import HTMLResponse\nfrom typing_extensions import Annotated\n\napp = FastAPI()\n\nhtml = \"\"\"\n<!DOCTYPE html>\n<html>\n    <head>\n        <title>Chat</title>\n    </head>\n    <body>\n        <h1>WebSocket Chat</h1>\n        <form action=\"\" onsubmit=\"sendMessage(event)\">\n            <label>Item ID: <input type=\"text\" id=\"itemId\" autocomplete=\"off\" value=\"foo\"/></label>\n            <label>Token: <input type=\"text\" id=\"token\" autocomplete=\"off\" value=\"some-key-token\"/></label>\n            <button onclick=\"connect(event)\">Connect</button>\n            <hr>\n            <label>Message: <input type=\"text\" id=\"messageText\" autocomplete=\"off\"/></label>\n            <button>Send</button>\n        </form>\n        <ul id='messages'>\n        </ul>\n        <script>\n        var ws = null;\n            function connect(event) {\n                var itemId = document.getElementById(\"itemId\")\n                var token = document.getElementById(\"token\")\n                ws = new WebSocket(\"ws://localhost:8000/items/\" + itemId.value + \"/ws?token=\" + token.value);\n                ws.onmessage = function(event) {\n                    var messages = document.getElementById('messages')\n                    var message = document.createElement('li')\n                    var content = document.createTextNode(event.data)\n                    message.appendChild(content)\n                    messages.appendChild(message)\n                };\n                event.preventDefault()\n            }\n            function sendMessage(event) {\n                var input = document.getElementById(\"messageText\")\n                ws.send(input.value)\n                input.value = ''\n                event.preventDefault()\n            }\n        </script>\n    </body>\n</html>\n\"\"\"\n\n\n@app.get(\"/\")\nasync def get():\n    return HTMLResponse(html)\n\n\nasync def get_cookie_or_token(\n    websocket: WebSocket,\n    session: Annotated[Union[str, None], Cookie()] = None,\n    token: Annotated[Union[str, None], Query()] = None,\n):\n    if session is None and token is None:\n        raise WebSocketException(code=status.WS_1008_POLICY_VIOLATION)\n    return session or token\n\n\n@app.websocket(\"/items/{item_id}/ws\")\nasync def websocket_endpoint(\n    *,\n    websocket: WebSocket,\n    item_id: str,\n    q: Union[int, None] = None,\n    cookie_or_token: Annotated[str, Depends(get_cookie_or_token)],\n):\n    await websocket.accept()\n    while True:\n        data = await websocket.receive_text()\n        await websocket.send_text(\n            f\"Session cookie or query token value is: {cookie_or_token}\"\n        )\n        if q is not None:\n            await websocket.send_text(f\"Query parameter q is: {q}\")\n        await websocket.send_text(f\"Message text was: {data}, for item ID: {item_id}\")\n", "metadata": {"license": "MIT", "len_tokens": 645}}
{"id": "fastapi:docs_src/websockets/tutorial003_py39.py", "language": "python", "code": "from fastapi import FastAPI, WebSocket, WebSocketDisconnect\nfrom fastapi.responses import HTMLResponse\n\napp = FastAPI()\n\nhtml = \"\"\"\n<!DOCTYPE html>\n<html>\n    <head>\n        <title>Chat</title>\n    </head>\n    <body>\n        <h1>WebSocket Chat</h1>\n        <h2>Your ID: <span id=\"ws-id\"></span></h2>\n        <form action=\"\" onsubmit=\"sendMessage(event)\">\n            <input type=\"text\" id=\"messageText\" autocomplete=\"off\"/>\n            <button>Send</button>\n        </form>\n        <ul id='messages'>\n        </ul>\n        <script>\n            var client_id = Date.now()\n            document.querySelector(\"#ws-id\").textContent = client_id;\n            var ws = new WebSocket(`ws://localhost:8000/ws/${client_id}`);\n            ws.onmessage = function(event) {\n                var messages = document.getElementById('messages')\n                var message = document.createElement('li')\n                var content = document.createTextNode(event.data)\n                message.appendChild(content)\n                messages.appendChild(message)\n            };\n            function sendMessage(event) {\n                var input = document.getElementById(\"messageText\")\n                ws.send(input.value)\n                input.value = ''\n                event.preventDefault()\n            }\n        </script>\n    </body>\n</html>\n\"\"\"\n\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: list[WebSocket] = []\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        self.active_connections.append(websocket)\n\n    def disconnect(self, websocket: WebSocket):\n        self.active_connections.remove(websocket)\n\n    async def send_personal_message(self, message: str, websocket: WebSocket):\n        await websocket.send_text(message)\n\n    async def broadcast(self, message: str):\n        for connection in self.active_connections:\n            await connection.send_text(message)\n\n\nmanager = ConnectionManager()\n\n\n@app.get(\"/\")\nasync def get():\n    return HTMLResponse(html)\n\n\n@app.websocket(\"/ws/{client_id}\")\nasync def websocket_endpoint(websocket: WebSocket, client_id: int):\n    await manager.connect(websocket)\n    try:\n        while True:\n            data = await websocket.receive_text()\n            await manager.send_personal_message(f\"You wrote: {data}\", websocket)\n            await manager.broadcast(f\"Client #{client_id} says: {data}\")\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n        await manager.broadcast(f\"Client #{client_id} left the chat\")\n", "metadata": {"license": "MIT", "len_tokens": 526}}
{"id": "fastapi:docs_src/websockets/tutorial002_py310.py", "language": "python", "code": "from fastapi import (\n    Cookie,\n    Depends,\n    FastAPI,\n    Query,\n    WebSocket,\n    WebSocketException,\n    status,\n)\nfrom fastapi.responses import HTMLResponse\n\napp = FastAPI()\n\nhtml = \"\"\"\n<!DOCTYPE html>\n<html>\n    <head>\n        <title>Chat</title>\n    </head>\n    <body>\n        <h1>WebSocket Chat</h1>\n        <form action=\"\" onsubmit=\"sendMessage(event)\">\n            <label>Item ID: <input type=\"text\" id=\"itemId\" autocomplete=\"off\" value=\"foo\"/></label>\n            <label>Token: <input type=\"text\" id=\"token\" autocomplete=\"off\" value=\"some-key-token\"/></label>\n            <button onclick=\"connect(event)\">Connect</button>\n            <hr>\n            <label>Message: <input type=\"text\" id=\"messageText\" autocomplete=\"off\"/></label>\n            <button>Send</button>\n        </form>\n        <ul id='messages'>\n        </ul>\n        <script>\n        var ws = null;\n            function connect(event) {\n                var itemId = document.getElementById(\"itemId\")\n                var token = document.getElementById(\"token\")\n                ws = new WebSocket(\"ws://localhost:8000/items/\" + itemId.value + \"/ws?token=\" + token.value);\n                ws.onmessage = function(event) {\n                    var messages = document.getElementById('messages')\n                    var message = document.createElement('li')\n                    var content = document.createTextNode(event.data)\n                    message.appendChild(content)\n                    messages.appendChild(message)\n                };\n                event.preventDefault()\n            }\n            function sendMessage(event) {\n                var input = document.getElementById(\"messageText\")\n                ws.send(input.value)\n                input.value = ''\n                event.preventDefault()\n            }\n        </script>\n    </body>\n</html>\n\"\"\"\n\n\n@app.get(\"/\")\nasync def get():\n    return HTMLResponse(html)\n\n\nasync def get_cookie_or_token(\n    websocket: WebSocket,\n    session: str | None = Cookie(default=None),\n    token: str | None = Query(default=None),\n):\n    if session is None and token is None:\n        raise WebSocketException(code=status.WS_1008_POLICY_VIOLATION)\n    return session or token\n\n\n@app.websocket(\"/items/{item_id}/ws\")\nasync def websocket_endpoint(\n    websocket: WebSocket,\n    item_id: str,\n    q: int | None = None,\n    cookie_or_token: str = Depends(get_cookie_or_token),\n):\n    await websocket.accept()\n    while True:\n        data = await websocket.receive_text()\n        await websocket.send_text(\n            f\"Session cookie or query token value is: {cookie_or_token}\"\n        )\n        if q is not None:\n            await websocket.send_text(f\"Query parameter q is: {q}\")\n        await websocket.send_text(f\"Message text was: {data}, for item ID: {item_id}\")\n", "metadata": {"license": "MIT", "len_tokens": 617}}
{"id": "fastapi:docs_src/websockets/tutorial002_an_py39.py", "language": "python", "code": "from typing import Annotated, Union\n\nfrom fastapi import (\n    Cookie,\n    Depends,\n    FastAPI,\n    Query,\n    WebSocket,\n    WebSocketException,\n    status,\n)\nfrom fastapi.responses import HTMLResponse\n\napp = FastAPI()\n\nhtml = \"\"\"\n<!DOCTYPE html>\n<html>\n    <head>\n        <title>Chat</title>\n    </head>\n    <body>\n        <h1>WebSocket Chat</h1>\n        <form action=\"\" onsubmit=\"sendMessage(event)\">\n            <label>Item ID: <input type=\"text\" id=\"itemId\" autocomplete=\"off\" value=\"foo\"/></label>\n            <label>Token: <input type=\"text\" id=\"token\" autocomplete=\"off\" value=\"some-key-token\"/></label>\n            <button onclick=\"connect(event)\">Connect</button>\n            <hr>\n            <label>Message: <input type=\"text\" id=\"messageText\" autocomplete=\"off\"/></label>\n            <button>Send</button>\n        </form>\n        <ul id='messages'>\n        </ul>\n        <script>\n        var ws = null;\n            function connect(event) {\n                var itemId = document.getElementById(\"itemId\")\n                var token = document.getElementById(\"token\")\n                ws = new WebSocket(\"ws://localhost:8000/items/\" + itemId.value + \"/ws?token=\" + token.value);\n                ws.onmessage = function(event) {\n                    var messages = document.getElementById('messages')\n                    var message = document.createElement('li')\n                    var content = document.createTextNode(event.data)\n                    message.appendChild(content)\n                    messages.appendChild(message)\n                };\n                event.preventDefault()\n            }\n            function sendMessage(event) {\n                var input = document.getElementById(\"messageText\")\n                ws.send(input.value)\n                input.value = ''\n                event.preventDefault()\n            }\n        </script>\n    </body>\n</html>\n\"\"\"\n\n\n@app.get(\"/\")\nasync def get():\n    return HTMLResponse(html)\n\n\nasync def get_cookie_or_token(\n    websocket: WebSocket,\n    session: Annotated[Union[str, None], Cookie()] = None,\n    token: Annotated[Union[str, None], Query()] = None,\n):\n    if session is None and token is None:\n        raise WebSocketException(code=status.WS_1008_POLICY_VIOLATION)\n    return session or token\n\n\n@app.websocket(\"/items/{item_id}/ws\")\nasync def websocket_endpoint(\n    *,\n    websocket: WebSocket,\n    item_id: str,\n    q: Union[int, None] = None,\n    cookie_or_token: Annotated[str, Depends(get_cookie_or_token)],\n):\n    await websocket.accept()\n    while True:\n        data = await websocket.receive_text()\n        await websocket.send_text(\n            f\"Session cookie or query token value is: {cookie_or_token}\"\n        )\n        if q is not None:\n            await websocket.send_text(f\"Query parameter q is: {q}\")\n        await websocket.send_text(f\"Message text was: {data}, for item ID: {item_id}\")\n", "metadata": {"license": "MIT", "len_tokens": 641}}
{"id": "fastapi:docs_src/websockets/tutorial001.py", "language": "python", "code": "from fastapi import FastAPI, WebSocket\nfrom fastapi.responses import HTMLResponse\n\napp = FastAPI()\n\nhtml = \"\"\"\n<!DOCTYPE html>\n<html>\n    <head>\n        <title>Chat</title>\n    </head>\n    <body>\n        <h1>WebSocket Chat</h1>\n        <form action=\"\" onsubmit=\"sendMessage(event)\">\n            <input type=\"text\" id=\"messageText\" autocomplete=\"off\"/>\n            <button>Send</button>\n        </form>\n        <ul id='messages'>\n        </ul>\n        <script>\n            var ws = new WebSocket(\"ws://localhost:8000/ws\");\n            ws.onmessage = function(event) {\n                var messages = document.getElementById('messages')\n                var message = document.createElement('li')\n                var content = document.createTextNode(event.data)\n                message.appendChild(content)\n                messages.appendChild(message)\n            };\n            function sendMessage(event) {\n                var input = document.getElementById(\"messageText\")\n                ws.send(input.value)\n                input.value = ''\n                event.preventDefault()\n            }\n        </script>\n    </body>\n</html>\n\"\"\"\n\n\n@app.get(\"/\")\nasync def get():\n    return HTMLResponse(html)\n\n\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    while True:\n        data = await websocket.receive_text()\n        await websocket.send_text(f\"Message text was: {data}\")\n", "metadata": {"license": "MIT", "len_tokens": 300}}
{"id": "fastapi:docs_src/additional_responses/tutorial003.py", "language": "python", "code": "from fastapi import FastAPI\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    id: str\n    value: str\n\n\nclass Message(BaseModel):\n    message: str\n\n\napp = FastAPI()\n\n\n@app.get(\n    \"/items/{item_id}\",\n    response_model=Item,\n    responses={\n        404: {\"model\": Message, \"description\": \"The item was not found\"},\n        200: {\n            \"description\": \"Item requested by ID\",\n            \"content\": {\n                \"application/json\": {\n                    \"example\": {\"id\": \"bar\", \"value\": \"The bar tenders\"}\n                }\n            },\n        },\n    },\n)\nasync def read_item(item_id: str):\n    if item_id == \"foo\":\n        return {\"id\": \"foo\", \"value\": \"there goes my hero\"}\n    else:\n        return JSONResponse(status_code=404, content={\"message\": \"Item not found\"})\n", "metadata": {"license": "MIT", "len_tokens": 200}}
{"id": "fastapi:docs_src/handling_errors/tutorial004.py", "language": "python", "code": "from fastapi import FastAPI, HTTPException\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.responses import PlainTextResponse\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\n\napp = FastAPI()\n\n\n@app.exception_handler(StarletteHTTPException)\nasync def http_exception_handler(request, exc):\n    return PlainTextResponse(str(exc.detail), status_code=exc.status_code)\n\n\n@app.exception_handler(RequestValidationError)\nasync def validation_exception_handler(request, exc: RequestValidationError):\n    message = \"Validation errors:\"\n    for error in exc.errors():\n        message += f\"\\nField: {error['loc']}, Error: {error['msg']}\"\n    return PlainTextResponse(message, status_code=400)\n\n\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int):\n    if item_id == 3:\n        raise HTTPException(status_code=418, detail=\"Nope! I don't like 3.\")\n    return {\"item_id\": item_id}\n", "metadata": {"license": "MIT", "len_tokens": 202}}
{"id": "fastapi:docs_src/custom_request_and_route/tutorial003.py", "language": "python", "code": "import time\nfrom typing import Callable\n\nfrom fastapi import APIRouter, FastAPI, Request, Response\nfrom fastapi.routing import APIRoute\n\n\nclass TimedRoute(APIRoute):\n    def get_route_handler(self) -> Callable:\n        original_route_handler = super().get_route_handler()\n\n        async def custom_route_handler(request: Request) -> Response:\n            before = time.time()\n            response: Response = await original_route_handler(request)\n            duration = time.time() - before\n            response.headers[\"X-Response-Time\"] = str(duration)\n            print(f\"route duration: {duration}\")\n            print(f\"route response: {response}\")\n            print(f\"route response headers: {response.headers}\")\n            return response\n\n        return custom_route_handler\n\n\napp = FastAPI()\nrouter = APIRouter(route_class=TimedRoute)\n\n\n@app.get(\"/\")\nasync def not_timed():\n    return {\"message\": \"Not timed\"}\n\n\n@router.get(\"/timed\")\nasync def timed():\n    return {\"message\": \"It's the time of my life\"}\n\n\napp.include_router(router)\n", "metadata": {"license": "MIT", "len_tokens": 220}}
{"id": "fastapi:docs_src/custom_request_and_route/tutorial001_an_py39.py", "language": "python", "code": "import gzip\nfrom typing import Annotated, Callable\n\nfrom fastapi import Body, FastAPI, Request, Response\nfrom fastapi.routing import APIRoute\n\n\nclass GzipRequest(Request):\n    async def body(self) -> bytes:\n        if not hasattr(self, \"_body\"):\n            body = await super().body()\n            if \"gzip\" in self.headers.getlist(\"Content-Encoding\"):\n                body = gzip.decompress(body)\n            self._body = body\n        return self._body\n\n\nclass GzipRoute(APIRoute):\n    def get_route_handler(self) -> Callable:\n        original_route_handler = super().get_route_handler()\n\n        async def custom_route_handler(request: Request) -> Response:\n            request = GzipRequest(request.scope, request.receive)\n            return await original_route_handler(request)\n\n        return custom_route_handler\n\n\napp = FastAPI()\napp.router.route_class = GzipRoute\n\n\n@app.post(\"/sum\")\nasync def sum_numbers(numbers: Annotated[list[int], Body()]):\n    return {\"sum\": sum(numbers)}\n", "metadata": {"license": "MIT", "len_tokens": 209}}
{"id": "fastapi:docs_src/custom_request_and_route/tutorial001_py310.py", "language": "python", "code": "import gzip\nfrom collections.abc import Callable\n\nfrom fastapi import Body, FastAPI, Request, Response\nfrom fastapi.routing import APIRoute\n\n\nclass GzipRequest(Request):\n    async def body(self) -> bytes:\n        if not hasattr(self, \"_body\"):\n            body = await super().body()\n            if \"gzip\" in self.headers.getlist(\"Content-Encoding\"):\n                body = gzip.decompress(body)\n            self._body = body\n        return self._body\n\n\nclass GzipRoute(APIRoute):\n    def get_route_handler(self) -> Callable:\n        original_route_handler = super().get_route_handler()\n\n        async def custom_route_handler(request: Request) -> Response:\n            request = GzipRequest(request.scope, request.receive)\n            return await original_route_handler(request)\n\n        return custom_route_handler\n\n\napp = FastAPI()\napp.router.route_class = GzipRoute\n\n\n@app.post(\"/sum\")\nasync def sum_numbers(numbers: list[int] = Body()):\n    return {\"sum\": sum(numbers)}\n", "metadata": {"license": "MIT", "len_tokens": 206}}
{"id": "fastapi:docs_src/custom_request_and_route/tutorial001_an_py310.py", "language": "python", "code": "import gzip\nfrom collections.abc import Callable\nfrom typing import Annotated\n\nfrom fastapi import Body, FastAPI, Request, Response\nfrom fastapi.routing import APIRoute\n\n\nclass GzipRequest(Request):\n    async def body(self) -> bytes:\n        if not hasattr(self, \"_body\"):\n            body = await super().body()\n            if \"gzip\" in self.headers.getlist(\"Content-Encoding\"):\n                body = gzip.decompress(body)\n            self._body = body\n        return self._body\n\n\nclass GzipRoute(APIRoute):\n    def get_route_handler(self) -> Callable:\n        original_route_handler = super().get_route_handler()\n\n        async def custom_route_handler(request: Request) -> Response:\n            request = GzipRequest(request.scope, request.receive)\n            return await original_route_handler(request)\n\n        return custom_route_handler\n\n\napp = FastAPI()\napp.router.route_class = GzipRoute\n\n\n@app.post(\"/sum\")\nasync def sum_numbers(numbers: Annotated[list[int], Body()]):\n    return {\"sum\": sum(numbers)}\n", "metadata": {"license": "MIT", "len_tokens": 214}}
{"id": "fastapi:docs_src/custom_request_and_route/tutorial003_py310.py", "language": "python", "code": "import time\nfrom collections.abc import Callable\n\nfrom fastapi import APIRouter, FastAPI, Request, Response\nfrom fastapi.routing import APIRoute\n\n\nclass TimedRoute(APIRoute):\n    def get_route_handler(self) -> Callable:\n        original_route_handler = super().get_route_handler()\n\n        async def custom_route_handler(request: Request) -> Response:\n            before = time.time()\n            response: Response = await original_route_handler(request)\n            duration = time.time() - before\n            response.headers[\"X-Response-Time\"] = str(duration)\n            print(f\"route duration: {duration}\")\n            print(f\"route response: {response}\")\n            print(f\"route response headers: {response.headers}\")\n            return response\n\n        return custom_route_handler\n\n\napp = FastAPI()\nrouter = APIRouter(route_class=TimedRoute)\n\n\n@app.get(\"/\")\nasync def not_timed():\n    return {\"message\": \"Not timed\"}\n\n\n@router.get(\"/timed\")\nasync def timed():\n    return {\"message\": \"It's the time of my life\"}\n\n\napp.include_router(router)\n", "metadata": {"license": "MIT", "len_tokens": 222}}
{"id": "fastapi:docs_src/custom_request_and_route/tutorial001_py39.py", "language": "python", "code": "import gzip\nfrom typing import Callable\n\nfrom fastapi import Body, FastAPI, Request, Response\nfrom fastapi.routing import APIRoute\n\n\nclass GzipRequest(Request):\n    async def body(self) -> bytes:\n        if not hasattr(self, \"_body\"):\n            body = await super().body()\n            if \"gzip\" in self.headers.getlist(\"Content-Encoding\"):\n                body = gzip.decompress(body)\n            self._body = body\n        return self._body\n\n\nclass GzipRoute(APIRoute):\n    def get_route_handler(self) -> Callable:\n        original_route_handler = super().get_route_handler()\n\n        async def custom_route_handler(request: Request) -> Response:\n            request = GzipRequest(request.scope, request.receive)\n            return await original_route_handler(request)\n\n        return custom_route_handler\n\n\napp = FastAPI()\napp.router.route_class = GzipRoute\n\n\n@app.post(\"/sum\")\nasync def sum_numbers(numbers: list[int] = Body()):\n    return {\"sum\": sum(numbers)}\n", "metadata": {"license": "MIT", "len_tokens": 204}}
{"id": "fastapi:docs_src/custom_request_and_route/tutorial001_an.py", "language": "python", "code": "import gzip\nfrom typing import Callable, List\n\nfrom fastapi import Body, FastAPI, Request, Response\nfrom fastapi.routing import APIRoute\nfrom typing_extensions import Annotated\n\n\nclass GzipRequest(Request):\n    async def body(self) -> bytes:\n        if not hasattr(self, \"_body\"):\n            body = await super().body()\n            if \"gzip\" in self.headers.getlist(\"Content-Encoding\"):\n                body = gzip.decompress(body)\n            self._body = body\n        return self._body\n\n\nclass GzipRoute(APIRoute):\n    def get_route_handler(self) -> Callable:\n        original_route_handler = super().get_route_handler()\n\n        async def custom_route_handler(request: Request) -> Response:\n            request = GzipRequest(request.scope, request.receive)\n            return await original_route_handler(request)\n\n        return custom_route_handler\n\n\napp = FastAPI()\napp.router.route_class = GzipRoute\n\n\n@app.post(\"/sum\")\nasync def sum_numbers(numbers: Annotated[List[int], Body()]):\n    return {\"sum\": sum(numbers)}\n", "metadata": {"license": "MIT", "len_tokens": 215}}
{"id": "fastapi:docs_src/custom_request_and_route/tutorial001.py", "language": "python", "code": "import gzip\nfrom typing import Callable, List\n\nfrom fastapi import Body, FastAPI, Request, Response\nfrom fastapi.routing import APIRoute\n\n\nclass GzipRequest(Request):\n    async def body(self) -> bytes:\n        if not hasattr(self, \"_body\"):\n            body = await super().body()\n            if \"gzip\" in self.headers.getlist(\"Content-Encoding\"):\n                body = gzip.decompress(body)\n            self._body = body\n        return self._body\n\n\nclass GzipRoute(APIRoute):\n    def get_route_handler(self) -> Callable:\n        original_route_handler = super().get_route_handler()\n\n        async def custom_route_handler(request: Request) -> Response:\n            request = GzipRequest(request.scope, request.receive)\n            return await original_route_handler(request)\n\n        return custom_route_handler\n\n\napp = FastAPI()\napp.router.route_class = GzipRoute\n\n\n@app.post(\"/sum\")\nasync def sum_numbers(numbers: List[int] = Body()):\n    return {\"sum\": sum(numbers)}\n", "metadata": {"license": "MIT", "len_tokens": 206}}
{"id": "fastapi:docs_src/extra_models/tutorial002.py", "language": "python", "code": "from typing import Union\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel, EmailStr\n\napp = FastAPI()\n\n\nclass UserBase(BaseModel):\n    username: str\n    email: EmailStr\n    full_name: Union[str, None] = None\n\n\nclass UserIn(UserBase):\n    password: str\n\n\nclass UserOut(UserBase):\n    pass\n\n\nclass UserInDB(UserBase):\n    hashed_password: str\n\n\ndef fake_password_hasher(raw_password: str):\n    return \"supersecret\" + raw_password\n\n\ndef fake_save_user(user_in: UserIn):\n    hashed_password = fake_password_hasher(user_in.password)\n    user_in_db = UserInDB(**user_in.dict(), hashed_password=hashed_password)\n    print(\"User saved! ..not really\")\n    return user_in_db\n\n\n@app.post(\"/user/\", response_model=UserOut)\nasync def create_user(user_in: UserIn):\n    user_saved = fake_save_user(user_in)\n    return user_saved\n", "metadata": {"license": "MIT", "len_tokens": 203}}
{"id": "fastapi:docs_src/extra_models/tutorial001_py310.py", "language": "python", "code": "from fastapi import FastAPI\nfrom pydantic import BaseModel, EmailStr\n\napp = FastAPI()\n\n\nclass UserIn(BaseModel):\n    username: str\n    password: str\n    email: EmailStr\n    full_name: str | None = None\n\n\nclass UserOut(BaseModel):\n    username: str\n    email: EmailStr\n    full_name: str | None = None\n\n\nclass UserInDB(BaseModel):\n    username: str\n    hashed_password: str\n    email: EmailStr\n    full_name: str | None = None\n\n\ndef fake_password_hasher(raw_password: str):\n    return \"supersecret\" + raw_password\n\n\ndef fake_save_user(user_in: UserIn):\n    hashed_password = fake_password_hasher(user_in.password)\n    user_in_db = UserInDB(**user_in.dict(), hashed_password=hashed_password)\n    print(\"User saved! ..not really\")\n    return user_in_db\n\n\n@app.post(\"/user/\", response_model=UserOut)\nasync def create_user(user_in: UserIn):\n    user_saved = fake_save_user(user_in)\n    return user_saved\n", "metadata": {"license": "MIT", "len_tokens": 229}}
{"id": "fastapi:docs_src/extra_models/tutorial001.py", "language": "python", "code": "from typing import Union\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel, EmailStr\n\napp = FastAPI()\n\n\nclass UserIn(BaseModel):\n    username: str\n    password: str\n    email: EmailStr\n    full_name: Union[str, None] = None\n\n\nclass UserOut(BaseModel):\n    username: str\n    email: EmailStr\n    full_name: Union[str, None] = None\n\n\nclass UserInDB(BaseModel):\n    username: str\n    hashed_password: str\n    email: EmailStr\n    full_name: Union[str, None] = None\n\n\ndef fake_password_hasher(raw_password: str):\n    return \"supersecret\" + raw_password\n\n\ndef fake_save_user(user_in: UserIn):\n    hashed_password = fake_password_hasher(user_in.password)\n    user_in_db = UserInDB(**user_in.dict(), hashed_password=hashed_password)\n    print(\"User saved! ..not really\")\n    return user_in_db\n\n\n@app.post(\"/user/\", response_model=UserOut)\nasync def create_user(user_in: UserIn):\n    user_saved = fake_save_user(user_in)\n    return user_saved\n", "metadata": {"license": "MIT", "len_tokens": 240}}
{"id": "fastapi:docs_src/custom_docs_ui/tutorial002.py", "language": "python", "code": "from fastapi import FastAPI\nfrom fastapi.openapi.docs import (\n    get_redoc_html,\n    get_swagger_ui_html,\n    get_swagger_ui_oauth2_redirect_html,\n)\nfrom fastapi.staticfiles import StaticFiles\n\napp = FastAPI(docs_url=None, redoc_url=None)\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n\n@app.get(\"/docs\", include_in_schema=False)\nasync def custom_swagger_ui_html():\n    return get_swagger_ui_html(\n        openapi_url=app.openapi_url,\n        title=app.title + \" - Swagger UI\",\n        oauth2_redirect_url=app.swagger_ui_oauth2_redirect_url,\n        swagger_js_url=\"/static/swagger-ui-bundle.js\",\n        swagger_css_url=\"/static/swagger-ui.css\",\n    )\n\n\n@app.get(app.swagger_ui_oauth2_redirect_url, include_in_schema=False)\nasync def swagger_ui_redirect():\n    return get_swagger_ui_oauth2_redirect_html()\n\n\n@app.get(\"/redoc\", include_in_schema=False)\nasync def redoc_html():\n    return get_redoc_html(\n        openapi_url=app.openapi_url,\n        title=app.title + \" - ReDoc\",\n        redoc_js_url=\"/static/redoc.standalone.js\",\n    )\n\n\n@app.get(\"/users/{username}\")\nasync def read_user(username: str):\n    return {\"message\": f\"Hello {username}\"}\n", "metadata": {"license": "MIT", "len_tokens": 283}}
{"id": "fastapi:docs_src/custom_docs_ui/tutorial001.py", "language": "python", "code": "from fastapi import FastAPI\nfrom fastapi.openapi.docs import (\n    get_redoc_html,\n    get_swagger_ui_html,\n    get_swagger_ui_oauth2_redirect_html,\n)\n\napp = FastAPI(docs_url=None, redoc_url=None)\n\n\n@app.get(\"/docs\", include_in_schema=False)\nasync def custom_swagger_ui_html():\n    return get_swagger_ui_html(\n        openapi_url=app.openapi_url,\n        title=app.title + \" - Swagger UI\",\n        oauth2_redirect_url=app.swagger_ui_oauth2_redirect_url,\n        swagger_js_url=\"https://unpkg.com/swagger-ui-dist@5/swagger-ui-bundle.js\",\n        swagger_css_url=\"https://unpkg.com/swagger-ui-dist@5/swagger-ui.css\",\n    )\n\n\n@app.get(app.swagger_ui_oauth2_redirect_url, include_in_schema=False)\nasync def swagger_ui_redirect():\n    return get_swagger_ui_oauth2_redirect_html()\n\n\n@app.get(\"/redoc\", include_in_schema=False)\nasync def redoc_html():\n    return get_redoc_html(\n        openapi_url=app.openapi_url,\n        title=app.title + \" - ReDoc\",\n        redoc_js_url=\"https://unpkg.com/redoc@2/bundles/redoc.standalone.js\",\n    )\n\n\n@app.get(\"/users/{username}\")\nasync def read_user(username: str):\n    return {\"message\": f\"Hello {username}\"}\n", "metadata": {"license": "MIT", "len_tokens": 288}}
{"id": "fastapi:docs_src/metadata/tutorial001_1.py", "language": "python", "code": "from fastapi import FastAPI\n\ndescription = \"\"\"\nChimichangApp API helps you do awesome stuff. \n\n## Items\n\nYou can **read items**.\n\n## Users\n\nYou will be able to:\n\n* **Create users** (_not implemented_).\n* **Read users** (_not implemented_).\n\"\"\"\n\napp = FastAPI(\n    title=\"ChimichangApp\",\n    description=description,\n    summary=\"Deadpool's favorite app. Nuff said.\",\n    version=\"0.0.1\",\n    terms_of_service=\"http://example.com/terms/\",\n    contact={\n        \"name\": \"Deadpoolio the Amazing\",\n        \"url\": \"http://x-force.example.com/contact/\",\n        \"email\": \"dp@x-force.example.com\",\n    },\n    license_info={\n        \"name\": \"Apache 2.0\",\n        \"identifier\": \"MIT\",\n    },\n)\n\n\n@app.get(\"/items/\")\nasync def read_items():\n    return [{\"name\": \"Katana\"}]\n", "metadata": {"license": "MIT", "len_tokens": 204}}
{"id": "fastapi:docs_src/metadata/tutorial001.py", "language": "python", "code": "from fastapi import FastAPI\n\ndescription = \"\"\"\nChimichangApp API helps you do awesome stuff. \n\n## Items\n\nYou can **read items**.\n\n## Users\n\nYou will be able to:\n\n* **Create users** (_not implemented_).\n* **Read users** (_not implemented_).\n\"\"\"\n\napp = FastAPI(\n    title=\"ChimichangApp\",\n    description=description,\n    summary=\"Deadpool's favorite app. Nuff said.\",\n    version=\"0.0.1\",\n    terms_of_service=\"http://example.com/terms/\",\n    contact={\n        \"name\": \"Deadpoolio the Amazing\",\n        \"url\": \"http://x-force.example.com/contact/\",\n        \"email\": \"dp@x-force.example.com\",\n    },\n    license_info={\n        \"name\": \"Apache 2.0\",\n        \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\",\n    },\n)\n\n\n@app.get(\"/items/\")\nasync def read_items():\n    return [{\"name\": \"Katana\"}]\n", "metadata": {"license": "MIT", "len_tokens": 215}}
{"id": "fastapi:docs_src/dataclasses/tutorial003.py", "language": "python", "code": "from dataclasses import field  # (1)\nfrom typing import List, Union\n\nfrom fastapi import FastAPI\nfrom pydantic.dataclasses import dataclass  # (2)\n\n\n@dataclass\nclass Item:\n    name: str\n    description: Union[str, None] = None\n\n\n@dataclass\nclass Author:\n    name: str\n    items: List[Item] = field(default_factory=list)  # (3)\n\n\napp = FastAPI()\n\n\n@app.post(\"/authors/{author_id}/items/\", response_model=Author)  # (4)\nasync def create_author_items(author_id: str, items: List[Item]):  # (5)\n    return {\"name\": author_id, \"items\": items}  # (6)\n\n\n@app.get(\"/authors/\", response_model=List[Author])  # (7)\ndef get_authors():  # (8)\n    return [  # (9)\n        {\n            \"name\": \"Breaters\",\n            \"items\": [\n                {\n                    \"name\": \"Island In The Moon\",\n                    \"description\": \"A place to be playin' and havin' fun\",\n                },\n                {\"name\": \"Holy Buddies\"},\n            ],\n        },\n        {\n            \"name\": \"System of an Up\",\n            \"items\": [\n                {\n                    \"name\": \"Salt\",\n                    \"description\": \"The kombucha mushroom people's favorite\",\n                },\n                {\"name\": \"Pad Thai\"},\n                {\n                    \"name\": \"Lonely Night\",\n                    \"description\": \"The mostests lonliest nightiest of allest\",\n                },\n            ],\n        },\n    ]\n", "metadata": {"license": "MIT", "len_tokens": 332}}
{"id": "fastapi:docs_src/dataclasses/tutorial003_py39.py", "language": "python", "code": "from dataclasses import field  # (1)\nfrom typing import Union\n\nfrom fastapi import FastAPI\nfrom pydantic.dataclasses import dataclass  # (2)\n\n\n@dataclass\nclass Item:\n    name: str\n    description: Union[str, None] = None\n\n\n@dataclass\nclass Author:\n    name: str\n    items: list[Item] = field(default_factory=list)  # (3)\n\n\napp = FastAPI()\n\n\n@app.post(\"/authors/{author_id}/items/\", response_model=Author)  # (4)\nasync def create_author_items(author_id: str, items: list[Item]):  # (5)\n    return {\"name\": author_id, \"items\": items}  # (6)\n\n\n@app.get(\"/authors/\", response_model=list[Author])  # (7)\ndef get_authors():  # (8)\n    return [  # (9)\n        {\n            \"name\": \"Breaters\",\n            \"items\": [\n                {\n                    \"name\": \"Island In The Moon\",\n                    \"description\": \"A place to be playin' and havin' fun\",\n                },\n                {\"name\": \"Holy Buddies\"},\n            ],\n        },\n        {\n            \"name\": \"System of an Up\",\n            \"items\": [\n                {\n                    \"name\": \"Salt\",\n                    \"description\": \"The kombucha mushroom people's favorite\",\n                },\n                {\"name\": \"Pad Thai\"},\n                {\n                    \"name\": \"Lonely Night\",\n                    \"description\": \"The mostests lonliest nightiest of allest\",\n                },\n            ],\n        },\n    ]\n", "metadata": {"license": "MIT", "len_tokens": 329}}
{"id": "fastapi:docs_src/dataclasses/tutorial003_py310.py", "language": "python", "code": "from dataclasses import field  # (1)\n\nfrom fastapi import FastAPI\nfrom pydantic.dataclasses import dataclass  # (2)\n\n\n@dataclass\nclass Item:\n    name: str\n    description: str | None = None\n\n\n@dataclass\nclass Author:\n    name: str\n    items: list[Item] = field(default_factory=list)  # (3)\n\n\napp = FastAPI()\n\n\n@app.post(\"/authors/{author_id}/items/\", response_model=Author)  # (4)\nasync def create_author_items(author_id: str, items: list[Item]):  # (5)\n    return {\"name\": author_id, \"items\": items}  # (6)\n\n\n@app.get(\"/authors/\", response_model=list[Author])  # (7)\ndef get_authors():  # (8)\n    return [  # (9)\n        {\n            \"name\": \"Breaters\",\n            \"items\": [\n                {\n                    \"name\": \"Island In The Moon\",\n                    \"description\": \"A place to be playin' and havin' fun\",\n                },\n                {\"name\": \"Holy Buddies\"},\n            ],\n        },\n        {\n            \"name\": \"System of an Up\",\n            \"items\": [\n                {\n                    \"name\": \"Salt\",\n                    \"description\": \"The kombucha mushroom people's favorite\",\n                },\n                {\"name\": \"Pad Thai\"},\n                {\n                    \"name\": \"Lonely Night\",\n                    \"description\": \"The mostests lonliest nightiest of allest\",\n                },\n            ],\n        },\n    ]\n", "metadata": {"license": "MIT", "len_tokens": 322}}
{"id": "fastapi:docs_src/bigger_applications/app_an_py39/routers/items.py", "language": "python", "code": "from fastapi import APIRouter, Depends, HTTPException\n\nfrom ..dependencies import get_token_header\n\nrouter = APIRouter(\n    prefix=\"/items\",\n    tags=[\"items\"],\n    dependencies=[Depends(get_token_header)],\n    responses={404: {\"description\": \"Not found\"}},\n)\n\n\nfake_items_db = {\"plumbus\": {\"name\": \"Plumbus\"}, \"gun\": {\"name\": \"Portal Gun\"}}\n\n\n@router.get(\"/\")\nasync def read_items():\n    return fake_items_db\n\n\n@router.get(\"/{item_id}\")\nasync def read_item(item_id: str):\n    if item_id not in fake_items_db:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return {\"name\": fake_items_db[item_id][\"name\"], \"item_id\": item_id}\n\n\n@router.put(\n    \"/{item_id}\",\n    tags=[\"custom\"],\n    responses={403: {\"description\": \"Operation forbidden\"}},\n)\nasync def update_item(item_id: str):\n    if item_id != \"plumbus\":\n        raise HTTPException(\n            status_code=403, detail=\"You can only update the item: plumbus\"\n        )\n    return {\"item_id\": item_id, \"name\": \"The great Plumbus\"}\n", "metadata": {"license": "MIT", "len_tokens": 249}}
{"id": "fastapi:docs_src/bigger_applications/app_an/routers/items.py", "language": "python", "code": "from fastapi import APIRouter, Depends, HTTPException\n\nfrom ..dependencies import get_token_header\n\nrouter = APIRouter(\n    prefix=\"/items\",\n    tags=[\"items\"],\n    dependencies=[Depends(get_token_header)],\n    responses={404: {\"description\": \"Not found\"}},\n)\n\n\nfake_items_db = {\"plumbus\": {\"name\": \"Plumbus\"}, \"gun\": {\"name\": \"Portal Gun\"}}\n\n\n@router.get(\"/\")\nasync def read_items():\n    return fake_items_db\n\n\n@router.get(\"/{item_id}\")\nasync def read_item(item_id: str):\n    if item_id not in fake_items_db:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return {\"name\": fake_items_db[item_id][\"name\"], \"item_id\": item_id}\n\n\n@router.put(\n    \"/{item_id}\",\n    tags=[\"custom\"],\n    responses={403: {\"description\": \"Operation forbidden\"}},\n)\nasync def update_item(item_id: str):\n    if item_id != \"plumbus\":\n        raise HTTPException(\n            status_code=403, detail=\"You can only update the item: plumbus\"\n        )\n    return {\"item_id\": item_id, \"name\": \"The great Plumbus\"}\n", "metadata": {"license": "MIT", "len_tokens": 249}}
{"id": "fastapi:docs_src/bigger_applications/app/routers/items.py", "language": "python", "code": "from fastapi import APIRouter, Depends, HTTPException\n\nfrom ..dependencies import get_token_header\n\nrouter = APIRouter(\n    prefix=\"/items\",\n    tags=[\"items\"],\n    dependencies=[Depends(get_token_header)],\n    responses={404: {\"description\": \"Not found\"}},\n)\n\n\nfake_items_db = {\"plumbus\": {\"name\": \"Plumbus\"}, \"gun\": {\"name\": \"Portal Gun\"}}\n\n\n@router.get(\"/\")\nasync def read_items():\n    return fake_items_db\n\n\n@router.get(\"/{item_id}\")\nasync def read_item(item_id: str):\n    if item_id not in fake_items_db:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return {\"name\": fake_items_db[item_id][\"name\"], \"item_id\": item_id}\n\n\n@router.put(\n    \"/{item_id}\",\n    tags=[\"custom\"],\n    responses={403: {\"description\": \"Operation forbidden\"}},\n)\nasync def update_item(item_id: str):\n    if item_id != \"plumbus\":\n        raise HTTPException(\n            status_code=403, detail=\"You can only update the item: plumbus\"\n        )\n    return {\"item_id\": item_id, \"name\": \"The great Plumbus\"}\n", "metadata": {"license": "MIT", "len_tokens": 249}}
{"id": "fastapi:scripts/playwright/cookie_param_models/image01.py", "language": "python", "code": "import subprocess\nimport time\n\nimport httpx\nfrom playwright.sync_api import Playwright, sync_playwright\n\n\n# Run playwright codegen to generate the code below, copy paste the sections in run()\ndef run(playwright: Playwright) -> None:\n    browser = playwright.chromium.launch(headless=False)\n    # Update the viewport manually\n    context = browser.new_context(viewport={\"width\": 960, \"height\": 1080})\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"http://localhost:8000/docs\")\n    page.get_by_role(\"link\", name=\"/items/\").click()\n    # Manually add the screenshot\n    page.screenshot(path=\"docs/en/docs/img/tutorial/cookie-param-models/image01.png\")\n\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nprocess = subprocess.Popen(\n    [\"fastapi\", \"run\", \"docs_src/cookie_param_models/tutorial001.py\"]\n)\ntry:\n    for _ in range(3):\n        try:\n            response = httpx.get(\"http://localhost:8000/docs\")\n        except httpx.ConnectError:\n            time.sleep(1)\n            break\n    with sync_playwright() as playwright:\n        run(playwright)\nfinally:\n    process.terminate()\n", "metadata": {"license": "MIT", "len_tokens": 272}}
{"id": "fastapi:scripts/playwright/sql_databases/image01.py", "language": "python", "code": "import subprocess\nimport time\n\nimport httpx\nfrom playwright.sync_api import Playwright, sync_playwright\n\n\n# Run playwright codegen to generate the code below, copy paste the sections in run()\ndef run(playwright: Playwright) -> None:\n    browser = playwright.chromium.launch(headless=False)\n    # Update the viewport manually\n    context = browser.new_context(viewport={\"width\": 960, \"height\": 1080})\n    page = context.new_page()\n    page.goto(\"http://localhost:8000/docs\")\n    page.get_by_label(\"post /heroes/\").click()\n    # Manually add the screenshot\n    page.screenshot(path=\"docs/en/docs/img/tutorial/sql-databases/image01.png\")\n\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nprocess = subprocess.Popen(\n    [\"fastapi\", \"run\", \"docs_src/sql_databases/tutorial001.py\"],\n)\ntry:\n    for _ in range(3):\n        try:\n            response = httpx.get(\"http://localhost:8000/docs\")\n        except httpx.ConnectError:\n            time.sleep(1)\n            break\n    with sync_playwright() as playwright:\n        run(playwright)\nfinally:\n    process.terminate()\n", "metadata": {"license": "MIT", "len_tokens": 250}}
{"id": "fastapi:scripts/playwright/sql_databases/image02.py", "language": "python", "code": "import subprocess\nimport time\n\nimport httpx\nfrom playwright.sync_api import Playwright, sync_playwright\n\n\n# Run playwright codegen to generate the code below, copy paste the sections in run()\ndef run(playwright: Playwright) -> None:\n    browser = playwright.chromium.launch(headless=False)\n    # Update the viewport manually\n    context = browser.new_context(viewport={\"width\": 960, \"height\": 1080})\n    page = context.new_page()\n    page.goto(\"http://localhost:8000/docs\")\n    page.get_by_label(\"post /heroes/\").click()\n    # Manually add the screenshot\n    page.screenshot(path=\"docs/en/docs/img/tutorial/sql-databases/image02.png\")\n\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nprocess = subprocess.Popen(\n    [\"fastapi\", \"run\", \"docs_src/sql_databases/tutorial002.py\"],\n)\ntry:\n    for _ in range(3):\n        try:\n            response = httpx.get(\"http://localhost:8000/docs\")\n        except httpx.ConnectError:\n            time.sleep(1)\n            break\n    with sync_playwright() as playwright:\n        run(playwright)\nfinally:\n    process.terminate()\n", "metadata": {"license": "MIT", "len_tokens": 250}}
{"id": "fastapi:scripts/playwright/query_param_models/image01.py", "language": "python", "code": "import subprocess\nimport time\n\nimport httpx\nfrom playwright.sync_api import Playwright, sync_playwright\n\n\n# Run playwright codegen to generate the code below, copy paste the sections in run()\ndef run(playwright: Playwright) -> None:\n    browser = playwright.chromium.launch(headless=False)\n    # Update the viewport manually\n    context = browser.new_context(viewport={\"width\": 960, \"height\": 1080})\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n    page = context.new_page()\n    page.goto(\"http://localhost:8000/docs\")\n    page.get_by_role(\"button\", name=\"GET /items/ Read Items\").click()\n    page.get_by_role(\"button\", name=\"Try it out\").click()\n    page.get_by_role(\"heading\", name=\"Servers\").click()\n    # Manually add the screenshot\n    page.screenshot(path=\"docs/en/docs/img/tutorial/query-param-models/image01.png\")\n\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nprocess = subprocess.Popen(\n    [\"fastapi\", \"run\", \"docs_src/query_param_models/tutorial001.py\"]\n)\ntry:\n    for _ in range(3):\n        try:\n            response = httpx.get(\"http://localhost:8000/docs\")\n        except httpx.ConnectError:\n            time.sleep(1)\n            break\n    with sync_playwright() as playwright:\n        run(playwright)\nfinally:\n    process.terminate()\n", "metadata": {"license": "MIT", "len_tokens": 304}}
{"id": "fastapi:scripts/playwright/header_param_models/image01.py", "language": "python", "code": "import subprocess\nimport time\n\nimport httpx\nfrom playwright.sync_api import Playwright, sync_playwright\n\n\n# Run playwright codegen to generate the code below, copy paste the sections in run()\ndef run(playwright: Playwright) -> None:\n    browser = playwright.chromium.launch(headless=False)\n    # Update the viewport manually\n    context = browser.new_context(viewport={\"width\": 960, \"height\": 1080})\n    page = context.new_page()\n    page.goto(\"http://localhost:8000/docs\")\n    page.get_by_role(\"button\", name=\"GET /items/ Read Items\").click()\n    page.get_by_role(\"button\", name=\"Try it out\").click()\n    # Manually add the screenshot\n    page.screenshot(path=\"docs/en/docs/img/tutorial/header-param-models/image01.png\")\n\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nprocess = subprocess.Popen(\n    [\"fastapi\", \"run\", \"docs_src/header_param_models/tutorial001.py\"]\n)\ntry:\n    for _ in range(3):\n        try:\n            response = httpx.get(\"http://localhost:8000/docs\")\n        except httpx.ConnectError:\n            time.sleep(1)\n            break\n    with sync_playwright() as playwright:\n        run(playwright)\nfinally:\n    process.terminate()\n", "metadata": {"license": "MIT", "len_tokens": 272}}
{"id": "fastapi:scripts/playwright/separate_openapi_schemas/image01.py", "language": "python", "code": "import subprocess\n\nfrom playwright.sync_api import Playwright, sync_playwright\n\n\n# Run playwright codegen to generate the code below, copy paste the sections in run()\ndef run(playwright: Playwright) -> None:\n    browser = playwright.chromium.launch(headless=False)\n    # Update the viewport manually\n    context = browser.new_context(viewport={\"width\": 960, \"height\": 1080})\n    page = context.new_page()\n    page.goto(\"http://localhost:8000/docs\")\n    page.get_by_text(\"POST/items/Create Item\").click()\n    page.get_by_role(\"tab\", name=\"Schema\").first.click()\n    # Manually add the screenshot\n    page.screenshot(\n        path=\"docs/en/docs/img/tutorial/separate-openapi-schemas/image01.png\"\n    )\n\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nprocess = subprocess.Popen(\n    [\"uvicorn\", \"docs_src.separate_openapi_schemas.tutorial001:app\"]\n)\ntry:\n    with sync_playwright() as playwright:\n        run(playwright)\nfinally:\n    process.terminate()\n", "metadata": {"license": "MIT", "len_tokens": 224}}
{"id": "fastapi:scripts/playwright/separate_openapi_schemas/image05.py", "language": "python", "code": "import subprocess\n\nfrom playwright.sync_api import Playwright, sync_playwright\n\n\n# Run playwright codegen to generate the code below, copy paste the sections in run()\ndef run(playwright: Playwright) -> None:\n    browser = playwright.chromium.launch(headless=False)\n    # Update the viewport manually\n    context = browser.new_context(viewport={\"width\": 960, \"height\": 1080})\n    page = context.new_page()\n    page.goto(\"http://localhost:8000/docs\")\n    page.get_by_role(\"button\", name=\"Item\", exact=True).click()\n    page.set_viewport_size({\"width\": 960, \"height\": 700})\n    # Manually add the screenshot\n    page.screenshot(\n        path=\"docs/en/docs/img/tutorial/separate-openapi-schemas/image05.png\"\n    )\n\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nprocess = subprocess.Popen(\n    [\"uvicorn\", \"docs_src.separate_openapi_schemas.tutorial002:app\"]\n)\ntry:\n    with sync_playwright() as playwright:\n        run(playwright)\nfinally:\n    process.terminate()\n", "metadata": {"license": "MIT", "len_tokens": 231}}
{"id": "fastapi:scripts/playwright/separate_openapi_schemas/image04.py", "language": "python", "code": "import subprocess\n\nfrom playwright.sync_api import Playwright, sync_playwright\n\n\n# Run playwright codegen to generate the code below, copy paste the sections in run()\ndef run(playwright: Playwright) -> None:\n    browser = playwright.chromium.launch(headless=False)\n    # Update the viewport manually\n    context = browser.new_context(viewport={\"width\": 960, \"height\": 1080})\n    page = context.new_page()\n    page.goto(\"http://localhost:8000/docs\")\n    page.get_by_role(\"button\", name=\"Item-Input\").click()\n    page.get_by_role(\"button\", name=\"Item-Output\").click()\n    page.set_viewport_size({\"width\": 960, \"height\": 820})\n    # Manually add the screenshot\n    page.screenshot(\n        path=\"docs/en/docs/img/tutorial/separate-openapi-schemas/image04.png\"\n    )\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nprocess = subprocess.Popen(\n    [\"uvicorn\", \"docs_src.separate_openapi_schemas.tutorial001:app\"]\n)\ntry:\n    with sync_playwright() as playwright:\n        run(playwright)\nfinally:\n    process.terminate()\n", "metadata": {"license": "MIT", "len_tokens": 246}}
{"id": "fastapi:scripts/playwright/separate_openapi_schemas/image03.py", "language": "python", "code": "import subprocess\n\nfrom playwright.sync_api import Playwright, sync_playwright\n\n\n# Run playwright codegen to generate the code below, copy paste the sections in run()\ndef run(playwright: Playwright) -> None:\n    browser = playwright.chromium.launch(headless=False)\n    # Update the viewport manually\n    context = browser.new_context(viewport={\"width\": 960, \"height\": 1080})\n    page = context.new_page()\n    page.goto(\"http://localhost:8000/docs\")\n    page.get_by_text(\"GET/items/Read Items\").click()\n    page.get_by_role(\"tab\", name=\"Schema\").click()\n    page.get_by_label(\"Schema\").get_by_role(\"button\", name=\"Expand all\").click()\n    # Manually add the screenshot\n    page.screenshot(\n        path=\"docs/en/docs/img/tutorial/separate-openapi-schemas/image03.png\"\n    )\n\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nprocess = subprocess.Popen(\n    [\"uvicorn\", \"docs_src.separate_openapi_schemas.tutorial001:app\"]\n)\ntry:\n    with sync_playwright() as playwright:\n        run(playwright)\nfinally:\n    process.terminate()\n", "metadata": {"license": "MIT", "len_tokens": 245}}
{"id": "fastapi:scripts/playwright/separate_openapi_schemas/image02.py", "language": "python", "code": "import subprocess\n\nfrom playwright.sync_api import Playwright, sync_playwright\n\n\n# Run playwright codegen to generate the code below, copy paste the sections in run()\ndef run(playwright: Playwright) -> None:\n    browser = playwright.chromium.launch(headless=False)\n    # Update the viewport manually\n    context = browser.new_context(viewport={\"width\": 960, \"height\": 1080})\n    page = context.new_page()\n    page.goto(\"http://localhost:8000/docs\")\n    page.get_by_text(\"GET/items/Read Items\").click()\n    page.get_by_role(\"button\", name=\"Try it out\").click()\n    page.get_by_role(\"button\", name=\"Execute\").click()\n    # Manually add the screenshot\n    page.screenshot(\n        path=\"docs/en/docs/img/tutorial/separate-openapi-schemas/image02.png\"\n    )\n\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nprocess = subprocess.Popen(\n    [\"uvicorn\", \"docs_src.separate_openapi_schemas.tutorial001:app\"]\n)\ntry:\n    with sync_playwright() as playwright:\n        run(playwright)\nfinally:\n    process.terminate()\n", "metadata": {"license": "MIT", "len_tokens": 240}}
{"id": "fastapi:scripts/playwright/request_form_models/image01.py", "language": "python", "code": "import subprocess\nimport time\n\nimport httpx\nfrom playwright.sync_api import Playwright, sync_playwright\n\n\n# Run playwright codegen to generate the code below, copy paste the sections in run()\ndef run(playwright: Playwright) -> None:\n    browser = playwright.chromium.launch(headless=False)\n    # Update the viewport manually\n    context = browser.new_context(viewport={\"width\": 960, \"height\": 1080})\n    page = context.new_page()\n    page.goto(\"http://localhost:8000/docs\")\n    page.get_by_role(\"button\", name=\"POST /login/ Login\").click()\n    page.get_by_role(\"button\", name=\"Try it out\").click()\n    # Manually add the screenshot\n    page.screenshot(path=\"docs/en/docs/img/tutorial/request-form-models/image01.png\")\n\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nprocess = subprocess.Popen(\n    [\"fastapi\", \"run\", \"docs_src/request_form_models/tutorial001.py\"]\n)\ntry:\n    for _ in range(3):\n        try:\n            response = httpx.get(\"http://localhost:8000/docs\")\n        except httpx.ConnectError:\n            time.sleep(1)\n            break\n    with sync_playwright() as playwright:\n        run(playwright)\nfinally:\n    process.terminate()\n", "metadata": {"license": "MIT", "len_tokens": 271}}
{"id": "fastapi:fastapi/security/open_id_connect_url.py", "language": "python", "code": "from typing import Optional\n\nfrom annotated_doc import Doc\nfrom fastapi.openapi.models import OpenIdConnect as OpenIdConnectModel\nfrom fastapi.security.base import SecurityBase\nfrom starlette.exceptions import HTTPException\nfrom starlette.requests import Request\nfrom starlette.status import HTTP_401_UNAUTHORIZED\nfrom typing_extensions import Annotated\n\n\nclass OpenIdConnect(SecurityBase):\n    \"\"\"\n    OpenID Connect authentication class. An instance of it would be used as a\n    dependency.\n\n    **Warning**: this is only a stub to connect the components with OpenAPI in FastAPI,\n    but it doesn't implement the full OpenIdConnect scheme, for example, it doesn't use\n    the OpenIDConnect URL. You would need to to subclass it and implement it in your\n    code.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        openIdConnectUrl: Annotated[\n            str,\n            Doc(\n                \"\"\"\n            The OpenID Connect URL.\n            \"\"\"\n            ),\n        ],\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if no HTTP Authorization header is provided, required for\n                OpenID Connect authentication, it will automatically cancel the request\n                and send the client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Authorization header\n                is not available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, with OpenID\n                Connect or in a cookie).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        self.model = OpenIdConnectModel(\n            openIdConnectUrl=openIdConnectUrl, description=description\n        )\n        self.scheme_name = scheme_name or self.__class__.__name__\n        self.auto_error = auto_error\n\n    def make_not_authenticated_error(self) -> HTTPException:\n        return HTTPException(\n            status_code=HTTP_401_UNAUTHORIZED,\n            detail=\"Not authenticated\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n\n    async def __call__(self, request: Request) -> Optional[str]:\n        authorization = request.headers.get(\"Authorization\")\n        if not authorization:\n            if self.auto_error:\n                raise self.make_not_authenticated_error()\n            else:\n                return None\n        return authorization\n", "metadata": {"license": "MIT", "len_tokens": 599}}
{"id": "fastapi:fastapi/security/open_id_connect_url.py", "language": "python", "code": "class OpenIdConnect(SecurityBase):\n    \"\"\"\n    OpenID Connect authentication class. An instance of it would be used as a\n    dependency.\n\n    **Warning**: this is only a stub to connect the components with OpenAPI in FastAPI,\n    but it doesn't implement the full OpenIdConnect scheme, for example, it doesn't use\n    the OpenIDConnect URL. You would need to to subclass it and implement it in your\n    code.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        openIdConnectUrl: Annotated[\n            str,\n            Doc(\n                \"\"\"\n            The OpenID Connect URL.\n            \"\"\"\n            ),\n        ],\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if no HTTP Authorization header is provided, required for\n                OpenID Connect authentication, it will automatically cancel the request\n                and send the client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Authorization header\n                is not available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, with OpenID\n                Connect or in a cookie).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        self.model = OpenIdConnectModel(\n            openIdConnectUrl=openIdConnectUrl, description=description\n        )\n        self.scheme_name = scheme_name or self.__class__.__name__\n        self.auto_error = auto_error\n\n    def make_not_authenticated_error(self) -> HTTPException:\n        return HTTPException(\n            status_code=HTTP_401_UNAUTHORIZED,\n            detail=\"Not authenticated\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n\n    async def __call__(self, request: Request) -> Optional[str]:\n        authorization = request.headers.get(\"Authorization\")\n        if not authorization:\n            if self.auto_error:\n                raise self.make_not_authenticated_error()\n            else:\n                return None\n        return authorization", "metadata": {"license": "MIT", "len_tokens": 530}}
{"id": "fastapi:fastapi/security/open_id_connect_url.py", "language": "python", "code": "def __init__(\n        self,\n        *,\n        openIdConnectUrl: Annotated[\n            str,\n            Doc(\n                \"\"\"\n            The OpenID Connect URL.\n            \"\"\"\n            ),\n        ],\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if no HTTP Authorization header is provided, required for\n                OpenID Connect authentication, it will automatically cancel the request\n                and send the client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Authorization header\n                is not available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, with OpenID\n                Connect or in a cookie).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        self.model = OpenIdConnectModel(\n            openIdConnectUrl=openIdConnectUrl, description=description\n        )\n        self.scheme_name = scheme_name or self.__class__.__name__\n        self.auto_error = auto_error", "metadata": {"license": "MIT", "len_tokens": 335}}
{"id": "fastapi:fastapi/security/oauth2.py", "language": "python", "code": "class OAuth2(SecurityBase):\n    \"\"\"\n    This is the base class for OAuth2 authentication, an instance of it would be used\n    as a dependency. All other OAuth2 classes inherit from it and customize it for\n    each OAuth2 flow.\n\n    You normally would not create a new class inheriting from it but use one of the\n    existing subclasses, and maybe compose them if you want to support multiple flows.\n\n    Read more about it in the\n    [FastAPI docs for Security](https://fastapi.tiangolo.com/tutorial/security/).\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        flows: Annotated[\n            Union[OAuthFlowsModel, Dict[str, Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                The dictionary of OAuth2 flows.\n                \"\"\"\n            ),\n        ] = OAuthFlowsModel(),\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if no HTTP Authorization header is provided, required for\n                OAuth2 authentication, it will automatically cancel the request and\n                send the client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Authorization header\n                is not available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, with OAuth2\n                or in a cookie).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        self.model = OAuth2Model(\n            flows=cast(OAuthFlowsModel, flows), description=description\n        )\n        self.scheme_name = scheme_name or self.__class__.__name__\n        self.auto_error = auto_error\n\n    def make_not_authenticated_error(self) -> HTTPException:\n        \"\"\"\n        The OAuth 2 specification doesn't define the challenge that should be used,\n        because a `Bearer` token is not really the only option to authenticate.\n\n        But declaring any other authentication challenge would be application-specific\n        as it's not defined in the specification.\n\n        For practical reasons, this method uses the `Bearer` challenge by default, as\n        it's probably the most common one.\n\n        If you are implementing an OAuth2 authentication scheme other than the provided\n        ones in FastAPI (based on bearer tokens), you might want to override this.\n\n        Ref: https://datatracker.ietf.org/doc/html/rfc6749\n        \"\"\"\n        return HTTPException(\n            status_code=HTTP_401_UNAUTHORIZED,\n            detail=\"Not authenticated\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n\n    async def __call__(self, request: Request) -> Optional[str]:\n        authorization = request.headers.get(\"Authorization\")\n        if not authorization:\n            if self.auto_error:\n                raise self.make_not_authenticated_error()\n            else:\n                return None\n        return authorization", "metadata": {"license": "MIT", "len_tokens": 699}}
{"id": "fastapi:fastapi/security/oauth2.py", "language": "python", "code": "class OAuth2PasswordBearer(OAuth2):\n    \"\"\"\n    OAuth2 flow for authentication using a bearer token obtained with a password.\n    An instance of it would be used as a dependency.\n\n    Read more about it in the\n    [FastAPI docs for Simple OAuth2 with Password and Bearer](https://fastapi.tiangolo.com/tutorial/security/simple-oauth2/).\n    \"\"\"\n\n    def __init__(\n        self,\n        tokenUrl: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL to obtain the OAuth2 token. This would be the *path operation*\n                that has `OAuth2PasswordRequestForm` as a dependency.\n                \"\"\"\n            ),\n        ],\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        scopes: Annotated[\n            Optional[Dict[str, str]],\n            Doc(\n                \"\"\"\n                The OAuth2 scopes that would be required by the *path operations* that\n                use this dependency.\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if no HTTP Authorization header is provided, required for\n                OAuth2 authentication, it will automatically cancel the request and\n                send the client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Authorization header\n                is not available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, with OAuth2\n                or in a cookie).\n                \"\"\"\n            ),\n        ] = True,\n        refreshUrl: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                The URL to refresh the token and obtain a new one.\n                \"\"\"\n            ),\n        ] = None,\n    ):\n        if not scopes:\n            scopes = {}\n        flows = OAuthFlowsModel(\n            password=cast(\n                Any,\n                {\n                    \"tokenUrl\": tokenUrl,\n                    \"refreshUrl\": refreshUrl,\n                    \"scopes\": scopes,\n                },\n            )\n        )\n        super().__init__(\n            flows=flows,\n            scheme_name=scheme_name,\n            description=description,\n            auto_error=auto_error,\n        )\n\n    async def __call__(self, request: Request) -> Optional[str]:\n        authorization = request.headers.get(\"Authorization\")\n        scheme, param = get_authorization_scheme_param(authorization)\n        if not authorization or scheme.lower() != \"bearer\":\n            if self.auto_error:\n                raise self.make_not_authenticated_error()\n            else:\n                return None\n        return param", "metadata": {"license": "MIT", "len_tokens": 637}}
{"id": "fastapi:fastapi/security/oauth2.py", "language": "python", "code": "class OAuth2AuthorizationCodeBearer(OAuth2):\n    \"\"\"\n    OAuth2 flow for authentication using a bearer token obtained with an OAuth2 code\n    flow. An instance of it would be used as a dependency.\n    \"\"\"\n\n    def __init__(\n        self,\n        authorizationUrl: str,\n        tokenUrl: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL to obtain the OAuth2 token.\n                \"\"\"\n            ),\n        ],\n        refreshUrl: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                The URL to refresh the token and obtain a new one.\n                \"\"\"\n            ),\n        ] = None,\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        scopes: Annotated[\n            Optional[Dict[str, str]],\n            Doc(\n                \"\"\"\n                The OAuth2 scopes that would be required by the *path operations* that\n                use this dependency.\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if no HTTP Authorization header is provided, required for\n                OAuth2 authentication, it will automatically cancel the request and\n                send the client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Authorization header\n                is not available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, with OAuth2\n                or in a cookie).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        if not scopes:\n            scopes = {}\n        flows = OAuthFlowsModel(\n            authorizationCode=cast(\n                Any,\n                {\n                    \"authorizationUrl\": authorizationUrl,\n                    \"tokenUrl\": tokenUrl,\n                    \"refreshUrl\": refreshUrl,\n                    \"scopes\": scopes,\n                },\n            )\n        )\n        super().__init__(\n            flows=flows,\n            scheme_name=scheme_name,\n            description=description,\n            auto_error=auto_error,\n        )\n\n    async def __call__(self, request: Request) -> Optional[str]:\n        authorization = request.headers.get(\"Authorization\")\n        scheme, param = get_authorization_scheme_param(authorization)\n        if not authorization or scheme.lower() != \"bearer\":\n            if self.auto_error:\n                raise self.make_not_authenticated_error()\n            else:\n                return None  # pragma: nocover\n        return param", "metadata": {"license": "MIT", "len_tokens": 602}}
{"id": "fastapi:fastapi/security/oauth2.py", "language": "python", "code": "class SecurityScopes:\n    \"\"\"\n    This is a special class that you can define in a parameter in a dependency to\n    obtain the OAuth2 scopes required by all the dependencies in the same chain.\n\n    This way, multiple dependencies can have different scopes, even when used in the\n    same *path operation*. And with this, you can access all the scopes required in\n    all those dependencies in a single place.\n\n    Read more about it in the\n    [FastAPI docs for OAuth2 scopes](https://fastapi.tiangolo.com/advanced/security/oauth2-scopes/).\n    \"\"\"\n\n    def __init__(\n        self,\n        scopes: Annotated[\n            Optional[List[str]],\n            Doc(\n                \"\"\"\n                This will be filled by FastAPI.\n                \"\"\"\n            ),\n        ] = None,\n    ):\n        self.scopes: Annotated[\n            List[str],\n            Doc(\n                \"\"\"\n                The list of all the scopes required by dependencies.\n                \"\"\"\n            ),\n        ] = scopes or []\n        self.scope_str: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                All the scopes required by all the dependencies in a single string\n                separated by spaces, as defined in the OAuth2 specification.\n                \"\"\"\n            ),\n        ] = \" \".join(self.scopes)", "metadata": {"license": "MIT", "len_tokens": 261}}
{"id": "fastapi:fastapi/security/oauth2.py", "language": "python", "code": "def __init__(\n        self,\n        *,\n        grant_type: Annotated[\n            Union[str, None],\n            Form(pattern=\"^password$\"),\n            Doc(\n                \"\"\"\n                The OAuth2 spec says it is required and MUST be the fixed string\n                \"password\". Nevertheless, this dependency class is permissive and\n                allows not passing it. If you want to enforce it, use instead the\n                `OAuth2PasswordRequestFormStrict` dependency.\n                \"\"\"\n            ),\n        ] = None,\n        username: Annotated[\n            str,\n            Form(),\n            Doc(\n                \"\"\"\n                `username` string. The OAuth2 spec requires the exact field name\n                `username`.\n                \"\"\"\n            ),\n        ],\n        password: Annotated[\n            str,\n            Form(json_schema_extra={\"format\": \"password\"}),\n            Doc(\n                \"\"\"\n                `password` string. The OAuth2 spec requires the exact field name\n                `password`.\n                \"\"\"\n            ),\n        ],\n        scope: Annotated[\n            str,\n            Form(),\n            Doc(\n                \"\"\"\n                A single string with actually several scopes separated by spaces. Each\n                scope is also a string.\n\n                For example, a single string with:\n\n                ```python\n                \"items:read items:write users:read profile openid\"\n                ````\n\n                would represent the scopes:\n\n                * `items:read`\n                * `items:write`\n                * `users:read`\n                * `profile`\n                * `openid`\n                \"\"\"\n            ),\n        ] = \"\",\n        client_id: Annotated[\n            Union[str, None],\n            Form(),\n            Doc(\n                \"\"\"\n                If there's a `client_id`, it can be sent as part of the form fields.\n                But the OAuth2 specification recommends sending the `client_id` and\n                `client_secret` (if any) using HTTP Basic auth.\n                \"\"\"\n            ),\n        ] = None,\n        client_secret: Annotated[\n            Union[str, None],\n            Form(json_schema_extra={\"format\": \"password\"}),\n            Doc(\n                \"\"\"\n                If there's a `client_password` (and a `client_id`), they can be sent\n                as part of the form fields. But the OAuth2 specification recommends\n                sending the `client_id` and `client_secret` (if any) using HTTP Basic\n                auth.\n                \"\"\"\n            ),\n        ] = None,\n    ):\n        self.grant_type = grant_type\n        self.username = username\n        self.password = password\n        self.scopes = scope.split()\n        self.client_id = client_id\n        self.client_secret = client_secret", "metadata": {"license": "MIT", "len_tokens": 532}}
{"id": "fastapi:fastapi/security/oauth2.py", "language": "python", "code": "def __init__(\n        self,\n        grant_type: Annotated[\n            str,\n            Form(pattern=\"^password$\"),\n            Doc(\n                \"\"\"\n                The OAuth2 spec says it is required and MUST be the fixed string\n                \"password\". This dependency is strict about it. If you want to be\n                permissive, use instead the `OAuth2PasswordRequestForm` dependency\n                class.\n                \"\"\"\n            ),\n        ],\n        username: Annotated[\n            str,\n            Form(),\n            Doc(\n                \"\"\"\n                `username` string. The OAuth2 spec requires the exact field name\n                `username`.\n                \"\"\"\n            ),\n        ],\n        password: Annotated[\n            str,\n            Form(),\n            Doc(\n                \"\"\"\n                `password` string. The OAuth2 spec requires the exact field name\n                `password`.\n                \"\"\"\n            ),\n        ],\n        scope: Annotated[\n            str,\n            Form(),\n            Doc(\n                \"\"\"\n                A single string with actually several scopes separated by spaces. Each\n                scope is also a string.\n\n                For example, a single string with:\n\n                ```python\n                \"items:read items:write users:read profile openid\"\n                ````\n\n                would represent the scopes:\n\n                * `items:read`\n                * `items:write`\n                * `users:read`\n                * `profile`\n                * `openid`\n                \"\"\"\n            ),\n        ] = \"\",\n        client_id: Annotated[\n            Union[str, None],\n            Form(),\n            Doc(\n                \"\"\"\n                If there's a `client_id`, it can be sent as part of the form fields.\n                But the OAuth2 specification recommends sending the `client_id` and\n                `client_secret` (if any) using HTTP Basic auth.\n                \"\"\"\n            ),\n        ] = None,\n        client_secret: Annotated[\n            Union[str, None],\n            Form(),\n            Doc(\n                \"\"\"\n                If there's a `client_password` (and a `client_id`), they can be sent\n                as part of the form fields. But the OAuth2 specification recommends\n                sending the `client_id` and `client_secret` (if any) using HTTP Basic\n                auth.\n                \"\"\"\n            ),\n        ] = None,\n    ):\n        super().__init__(\n            grant_type=grant_type,\n            username=username,\n            password=password,\n            scope=scope,\n            client_id=client_id,\n            client_secret=client_secret,\n        )", "metadata": {"license": "MIT", "len_tokens": 499}}
{"id": "fastapi:fastapi/security/oauth2.py", "language": "python", "code": "def __init__(\n        self,\n        *,\n        flows: Annotated[\n            Union[OAuthFlowsModel, Dict[str, Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                The dictionary of OAuth2 flows.\n                \"\"\"\n            ),\n        ] = OAuthFlowsModel(),\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if no HTTP Authorization header is provided, required for\n                OAuth2 authentication, it will automatically cancel the request and\n                send the client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Authorization header\n                is not available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, with OAuth2\n                or in a cookie).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        self.model = OAuth2Model(\n            flows=cast(OAuthFlowsModel, flows), description=description\n        )\n        self.scheme_name = scheme_name or self.__class__.__name__\n        self.auto_error = auto_error", "metadata": {"license": "MIT", "len_tokens": 349}}
{"id": "fastapi:fastapi/security/oauth2.py", "language": "python", "code": "def __init__(\n        self,\n        tokenUrl: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL to obtain the OAuth2 token. This would be the *path operation*\n                that has `OAuth2PasswordRequestForm` as a dependency.\n                \"\"\"\n            ),\n        ],\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        scopes: Annotated[\n            Optional[Dict[str, str]],\n            Doc(\n                \"\"\"\n                The OAuth2 scopes that would be required by the *path operations* that\n                use this dependency.\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if no HTTP Authorization header is provided, required for\n                OAuth2 authentication, it will automatically cancel the request and\n                send the client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Authorization header\n                is not available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, with OAuth2\n                or in a cookie).\n                \"\"\"\n            ),\n        ] = True,\n        refreshUrl: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                The URL to refresh the token and obtain a new one.\n                \"\"\"\n            ),\n        ] = None,\n    ):\n        if not scopes:\n            scopes = {}\n        flows = OAuthFlowsModel(\n            password=cast(\n                Any,\n                {\n                    \"tokenUrl\": tokenUrl,\n                    \"refreshUrl\": refreshUrl,\n                    \"scopes\": scopes,\n                },\n            )\n        )\n        super().__init__(\n            flows=flows,\n            scheme_name=scheme_name,\n            description=description,\n            auto_error=auto_error,\n        )", "metadata": {"license": "MIT", "len_tokens": 482}}
{"id": "fastapi:fastapi/security/oauth2.py", "language": "python", "code": "def __init__(\n        self,\n        authorizationUrl: str,\n        tokenUrl: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL to obtain the OAuth2 token.\n                \"\"\"\n            ),\n        ],\n        refreshUrl: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                The URL to refresh the token and obtain a new one.\n                \"\"\"\n            ),\n        ] = None,\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        scopes: Annotated[\n            Optional[Dict[str, str]],\n            Doc(\n                \"\"\"\n                The OAuth2 scopes that would be required by the *path operations* that\n                use this dependency.\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if no HTTP Authorization header is provided, required for\n                OAuth2 authentication, it will automatically cancel the request and\n                send the client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Authorization header\n                is not available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, with OAuth2\n                or in a cookie).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        if not scopes:\n            scopes = {}\n        flows = OAuthFlowsModel(\n            authorizationCode=cast(\n                Any,\n                {\n                    \"authorizationUrl\": authorizationUrl,\n                    \"tokenUrl\": tokenUrl,\n                    \"refreshUrl\": refreshUrl,\n                    \"scopes\": scopes,\n                },\n            )\n        )\n        super().__init__(\n            flows=flows,\n            scheme_name=scheme_name,\n            description=description,\n            auto_error=auto_error,\n        )", "metadata": {"license": "MIT", "len_tokens": 475}}
{"id": "fastapi:fastapi/security/api_key.py", "language": "python", "code": "class APIKeyBase(SecurityBase):\n    def __init__(\n        self,\n        location: APIKeyIn,\n        name: str,\n        description: Union[str, None],\n        scheme_name: Union[str, None],\n        auto_error: bool,\n    ):\n        self.auto_error = auto_error\n\n        self.model: APIKey = APIKey(\n            **{\"in\": location},\n            name=name,\n            description=description,\n        )\n        self.scheme_name = scheme_name or self.__class__.__name__\n\n    def make_not_authenticated_error(self) -> HTTPException:\n        \"\"\"\n        The WWW-Authenticate header is not standardized for API Key authentication but\n        the HTTP specification requires that an error of 401 \"Unauthorized\" must\n        include a WWW-Authenticate header.\n\n        Ref: https://datatracker.ietf.org/doc/html/rfc9110#name-401-unauthorized\n\n        For this, this method sends a custom challenge `APIKey`.\n        \"\"\"\n        return HTTPException(\n            status_code=HTTP_401_UNAUTHORIZED,\n            detail=\"Not authenticated\",\n            headers={\"WWW-Authenticate\": \"APIKey\"},\n        )\n\n    def check_api_key(self, api_key: Optional[str]) -> Optional[str]:\n        if not api_key:\n            if self.auto_error:\n                raise self.make_not_authenticated_error()\n            return None\n        return api_key", "metadata": {"license": "MIT", "len_tokens": 275}}
{"id": "fastapi:fastapi/security/api_key.py", "language": "python", "code": "class APIKeyQuery(APIKeyBase):\n    \"\"\"\n    API key authentication using a query parameter.\n\n    This defines the name of the query parameter that should be provided in the request\n    with the API key and integrates that into the OpenAPI documentation. It extracts\n    the key value sent in the query parameter automatically and provides it as the\n    dependency result. But it doesn't define how to send that API key to the client.\n\n    ## Usage\n\n    Create an instance object and use that object as the dependency in `Depends()`.\n\n    The dependency result will be a string containing the key value.\n\n    ## Example\n\n    ```python\n    from fastapi import Depends, FastAPI\n    from fastapi.security import APIKeyQuery\n\n    app = FastAPI()\n\n    query_scheme = APIKeyQuery(name=\"api_key\")\n\n\n    @app.get(\"/items/\")\n    async def read_items(api_key: str = Depends(query_scheme)):\n        return {\"api_key\": api_key}\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        name: Annotated[\n            str,\n            Doc(\"Query parameter name.\"),\n        ],\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if the query parameter is not provided, `APIKeyQuery` will\n                automatically cancel the request and send the client an error.\n\n                If `auto_error` is set to `False`, when the query parameter is not\n                available, instead of erroring out, the dependency result will be\n                `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, in a query\n                parameter or in an HTTP Bearer token).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        super().__init__(\n            location=APIKeyIn.query,\n            name=name,\n            scheme_name=scheme_name,\n            description=description,\n            auto_error=auto_error,\n        )\n\n    async def __call__(self, request: Request) -> Optional[str]:\n        api_key = request.query_params.get(self.model.name)\n        return self.check_api_key(api_key)", "metadata": {"license": "MIT", "len_tokens": 551}}
{"id": "fastapi:fastapi/security/api_key.py", "language": "python", "code": "class APIKeyHeader(APIKeyBase):\n    \"\"\"\n    API key authentication using a header.\n\n    This defines the name of the header that should be provided in the request with\n    the API key and integrates that into the OpenAPI documentation. It extracts\n    the key value sent in the header automatically and provides it as the dependency\n    result. But it doesn't define how to send that key to the client.\n\n    ## Usage\n\n    Create an instance object and use that object as the dependency in `Depends()`.\n\n    The dependency result will be a string containing the key value.\n\n    ## Example\n\n    ```python\n    from fastapi import Depends, FastAPI\n    from fastapi.security import APIKeyHeader\n\n    app = FastAPI()\n\n    header_scheme = APIKeyHeader(name=\"x-key\")\n\n\n    @app.get(\"/items/\")\n    async def read_items(key: str = Depends(header_scheme)):\n        return {\"key\": key}\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        name: Annotated[str, Doc(\"Header name.\")],\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if the header is not provided, `APIKeyHeader` will\n                automatically cancel the request and send the client an error.\n\n                If `auto_error` is set to `False`, when the header is not available,\n                instead of erroring out, the dependency result will be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, in a header or\n                in an HTTP Bearer token).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        super().__init__(\n            location=APIKeyIn.header,\n            name=name,\n            scheme_name=scheme_name,\n            description=description,\n            auto_error=auto_error,\n        )\n\n    async def __call__(self, request: Request) -> Optional[str]:\n        api_key = request.headers.get(self.model.name)\n        return self.check_api_key(api_key)", "metadata": {"license": "MIT", "len_tokens": 532}}
{"id": "fastapi:fastapi/security/api_key.py", "language": "python", "code": "class APIKeyCookie(APIKeyBase):\n    \"\"\"\n    API key authentication using a cookie.\n\n    This defines the name of the cookie that should be provided in the request with\n    the API key and integrates that into the OpenAPI documentation. It extracts\n    the key value sent in the cookie automatically and provides it as the dependency\n    result. But it doesn't define how to set that cookie.\n\n    ## Usage\n\n    Create an instance object and use that object as the dependency in `Depends()`.\n\n    The dependency result will be a string containing the key value.\n\n    ## Example\n\n    ```python\n    from fastapi import Depends, FastAPI\n    from fastapi.security import APIKeyCookie\n\n    app = FastAPI()\n\n    cookie_scheme = APIKeyCookie(name=\"session\")\n\n\n    @app.get(\"/items/\")\n    async def read_items(session: str = Depends(cookie_scheme)):\n        return {\"session\": session}\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        name: Annotated[str, Doc(\"Cookie name.\")],\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if the cookie is not provided, `APIKeyCookie` will\n                automatically cancel the request and send the client an error.\n\n                If `auto_error` is set to `False`, when the cookie is not available,\n                instead of erroring out, the dependency result will be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, in a cookie or\n                in an HTTP Bearer token).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        super().__init__(\n            location=APIKeyIn.cookie,\n            name=name,\n            scheme_name=scheme_name,\n            description=description,\n            auto_error=auto_error,\n        )\n\n    async def __call__(self, request: Request) -> Optional[str]:\n        api_key = request.cookies.get(self.model.name)\n        return self.check_api_key(api_key)", "metadata": {"license": "MIT", "len_tokens": 528}}
{"id": "fastapi:fastapi/security/api_key.py", "language": "python", "code": "def __init__(\n        self,\n        *,\n        name: Annotated[\n            str,\n            Doc(\"Query parameter name.\"),\n        ],\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if the query parameter is not provided, `APIKeyQuery` will\n                automatically cancel the request and send the client an error.\n\n                If `auto_error` is set to `False`, when the query parameter is not\n                available, instead of erroring out, the dependency result will be\n                `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, in a query\n                parameter or in an HTTP Bearer token).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        super().__init__(\n            location=APIKeyIn.query,\n            name=name,\n            scheme_name=scheme_name,\n            description=description,\n            auto_error=auto_error,\n        )", "metadata": {"license": "MIT", "len_tokens": 312}}
{"id": "fastapi:fastapi/security/api_key.py", "language": "python", "code": "def __init__(\n        self,\n        *,\n        name: Annotated[str, Doc(\"Header name.\")],\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if the header is not provided, `APIKeyHeader` will\n                automatically cancel the request and send the client an error.\n\n                If `auto_error` is set to `False`, when the header is not available,\n                instead of erroring out, the dependency result will be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, in a header or\n                in an HTTP Bearer token).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        super().__init__(\n            location=APIKeyIn.header,\n            name=name,\n            scheme_name=scheme_name,\n            description=description,\n            auto_error=auto_error,\n        )", "metadata": {"license": "MIT", "len_tokens": 301}}
{"id": "fastapi:fastapi/security/api_key.py", "language": "python", "code": "def __init__(\n        self,\n        *,\n        name: Annotated[str, Doc(\"Cookie name.\")],\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if the cookie is not provided, `APIKeyCookie` will\n                automatically cancel the request and send the client an error.\n\n                If `auto_error` is set to `False`, when the cookie is not available,\n                instead of erroring out, the dependency result will be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, in a cookie or\n                in an HTTP Bearer token).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        super().__init__(\n            location=APIKeyIn.cookie,\n            name=name,\n            scheme_name=scheme_name,\n            description=description,\n            auto_error=auto_error,\n        )", "metadata": {"license": "MIT", "len_tokens": 301}}
{"id": "fastapi:fastapi/security/http.py", "language": "python", "code": "class HTTPBase(SecurityBase):\n    def __init__(\n        self,\n        *,\n        scheme: str,\n        scheme_name: Optional[str] = None,\n        description: Optional[str] = None,\n        auto_error: bool = True,\n    ):\n        self.model: HTTPBaseModel = HTTPBaseModel(\n            scheme=scheme, description=description\n        )\n        self.scheme_name = scheme_name or self.__class__.__name__\n        self.auto_error = auto_error\n\n    def make_authenticate_headers(self) -> Dict[str, str]:\n        return {\"WWW-Authenticate\": f\"{self.model.scheme.title()}\"}\n\n    def make_not_authenticated_error(self) -> HTTPException:\n        return HTTPException(\n            status_code=HTTP_401_UNAUTHORIZED,\n            detail=\"Not authenticated\",\n            headers=self.make_authenticate_headers(),\n        )\n\n    async def __call__(\n        self, request: Request\n    ) -> Optional[HTTPAuthorizationCredentials]:\n        authorization = request.headers.get(\"Authorization\")\n        scheme, credentials = get_authorization_scheme_param(authorization)\n        if not (authorization and scheme and credentials):\n            if self.auto_error:\n                raise self.make_not_authenticated_error()\n            else:\n                return None\n        return HTTPAuthorizationCredentials(scheme=scheme, credentials=credentials)", "metadata": {"license": "MIT", "len_tokens": 257}}
{"id": "fastapi:fastapi/security/http.py", "language": "python", "code": "class HTTPBasic(HTTPBase):\n    \"\"\"\n    HTTP Basic authentication.\n\n    Ref: https://datatracker.ietf.org/doc/html/rfc7617\n\n    ## Usage\n\n    Create an instance object and use that object as the dependency in `Depends()`.\n\n    The dependency result will be an `HTTPBasicCredentials` object containing the\n    `username` and the `password`.\n\n    Read more about it in the\n    [FastAPI docs for HTTP Basic Auth](https://fastapi.tiangolo.com/advanced/security/http-basic-auth/).\n\n    ## Example\n\n    ```python\n    from typing import Annotated\n\n    from fastapi import Depends, FastAPI\n    from fastapi.security import HTTPBasic, HTTPBasicCredentials\n\n    app = FastAPI()\n\n    security = HTTPBasic()\n\n\n    @app.get(\"/users/me\")\n    def read_current_user(credentials: Annotated[HTTPBasicCredentials, Depends(security)]):\n        return {\"username\": credentials.username, \"password\": credentials.password}\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        realm: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                HTTP Basic authentication realm.\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if the HTTP Basic authentication is not provided (a\n                header), `HTTPBasic` will automatically cancel the request and send the\n                client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Basic authentication\n                is not available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, in HTTP Basic\n                authentication or in an HTTP Bearer token).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        self.model = HTTPBaseModel(scheme=\"basic\", description=description)\n        self.scheme_name = scheme_name or self.__class__.__name__\n        self.realm = realm\n        self.auto_error = auto_error\n\n    def make_authenticate_headers(self) -> Dict[str, str]:\n        if self.realm:\n            return {\"WWW-Authenticate\": f'Basic realm=\"{self.realm}\"'}\n        return {\"WWW-Authenticate\": \"Basic\"}\n\n    async def __call__(  # type: ignore\n        self, request: Request\n    ) -> Optional[HTTPBasicCredentials]:\n        authorization = request.headers.get(\"Authorization\")\n        scheme, param = get_authorization_scheme_param(authorization)\n        if not authorization or scheme.lower() != \"basic\":\n            if self.auto_error:\n                raise self.make_not_authenticated_error()\n            else:\n                return None\n        try:\n            data = b64decode(param).decode(\"ascii\")\n        except (ValueError, UnicodeDecodeError, binascii.Error) as e:\n            raise self.make_not_authenticated_error() from e\n        username, separator, password = data.partition(\":\")\n        if not separator:\n            raise self.make_not_authenticated_error()\n        return HTTPBasicCredentials(username=username, password=password)", "metadata": {"license": "MIT", "len_tokens": 750}}
{"id": "fastapi:fastapi/security/http.py", "language": "python", "code": "class HTTPBearer(HTTPBase):\n    \"\"\"\n    HTTP Bearer token authentication.\n\n    ## Usage\n\n    Create an instance object and use that object as the dependency in `Depends()`.\n\n    The dependency result will be an `HTTPAuthorizationCredentials` object containing\n    the `scheme` and the `credentials`.\n\n    ## Example\n\n    ```python\n    from typing import Annotated\n\n    from fastapi import Depends, FastAPI\n    from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer\n\n    app = FastAPI()\n\n    security = HTTPBearer()\n\n\n    @app.get(\"/users/me\")\n    def read_current_user(\n        credentials: Annotated[HTTPAuthorizationCredentials, Depends(security)]\n    ):\n        return {\"scheme\": credentials.scheme, \"credentials\": credentials.credentials}\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        bearerFormat: Annotated[Optional[str], Doc(\"Bearer token format.\")] = None,\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if the HTTP Bearer token is not provided (in an\n                `Authorization` header), `HTTPBearer` will automatically cancel the\n                request and send the client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Bearer token\n                is not available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, in an HTTP\n                Bearer token or in a cookie).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        self.model = HTTPBearerModel(bearerFormat=bearerFormat, description=description)\n        self.scheme_name = scheme_name or self.__class__.__name__\n        self.auto_error = auto_error\n\n    async def __call__(\n        self, request: Request\n    ) -> Optional[HTTPAuthorizationCredentials]:\n        authorization = request.headers.get(\"Authorization\")\n        scheme, credentials = get_authorization_scheme_param(authorization)\n        if not (authorization and scheme and credentials):\n            if self.auto_error:\n                raise self.make_not_authenticated_error()\n            else:\n                return None\n        if scheme.lower() != \"bearer\":\n            if self.auto_error:\n                raise self.make_not_authenticated_error()\n            else:\n                return None\n        return HTTPAuthorizationCredentials(scheme=scheme, credentials=credentials)", "metadata": {"license": "MIT", "len_tokens": 610}}
{"id": "fastapi:fastapi/security/http.py", "language": "python", "code": "class HTTPDigest(HTTPBase):\n    \"\"\"\n    HTTP Digest authentication.\n\n    **Warning**: this is only a stub to connect the components with OpenAPI in FastAPI,\n    but it doesn't implement the full Digest scheme, you would need to to subclass it\n    and implement it in your code.\n\n    Ref: https://datatracker.ietf.org/doc/html/rfc7616\n\n    ## Usage\n\n    Create an instance object and use that object as the dependency in `Depends()`.\n\n    The dependency result will be an `HTTPAuthorizationCredentials` object containing\n    the `scheme` and the `credentials`.\n\n    ## Example\n\n    ```python\n    from typing import Annotated\n\n    from fastapi import Depends, FastAPI\n    from fastapi.security import HTTPAuthorizationCredentials, HTTPDigest\n\n    app = FastAPI()\n\n    security = HTTPDigest()\n\n\n    @app.get(\"/users/me\")\n    def read_current_user(\n        credentials: Annotated[HTTPAuthorizationCredentials, Depends(security)]\n    ):\n        return {\"scheme\": credentials.scheme, \"credentials\": credentials.credentials}\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if the HTTP Digest is not provided, `HTTPDigest` will\n                automatically cancel the request and send the client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Digest is not\n                available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, in HTTP\n                Digest or in a cookie).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        self.model = HTTPBaseModel(scheme=\"digest\", description=description)\n        self.scheme_name = scheme_name or self.__class__.__name__\n        self.auto_error = auto_error\n\n    async def __call__(\n        self, request: Request\n    ) -> Optional[HTTPAuthorizationCredentials]:\n        authorization = request.headers.get(\"Authorization\")\n        scheme, credentials = get_authorization_scheme_param(authorization)\n        if not (authorization and scheme and credentials):\n            if self.auto_error:\n                raise self.make_not_authenticated_error()\n            else:\n                return None\n        if scheme.lower() != \"digest\":\n            if self.auto_error:\n                raise self.make_not_authenticated_error()\n            else:\n                return None\n        return HTTPAuthorizationCredentials(scheme=scheme, credentials=credentials)", "metadata": {"license": "MIT", "len_tokens": 632}}
{"id": "fastapi:fastapi/security/http.py", "language": "python", "code": "def __init__(\n        self,\n        *,\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        realm: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                HTTP Basic authentication realm.\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if the HTTP Basic authentication is not provided (a\n                header), `HTTPBasic` will automatically cancel the request and send the\n                client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Basic authentication\n                is not available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, in HTTP Basic\n                authentication or in an HTTP Bearer token).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        self.model = HTTPBaseModel(scheme=\"basic\", description=description)\n        self.scheme_name = scheme_name or self.__class__.__name__\n        self.realm = realm\n        self.auto_error = auto_error", "metadata": {"license": "MIT", "len_tokens": 335}}
{"id": "fastapi:fastapi/security/http.py", "language": "python", "code": "def __init__(\n        self,\n        *,\n        bearerFormat: Annotated[Optional[str], Doc(\"Bearer token format.\")] = None,\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if the HTTP Bearer token is not provided (in an\n                `Authorization` header), `HTTPBearer` will automatically cancel the\n                request and send the client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Bearer token\n                is not available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, in an HTTP\n                Bearer token or in a cookie).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        self.model = HTTPBearerModel(bearerFormat=bearerFormat, description=description)\n        self.scheme_name = scheme_name or self.__class__.__name__\n        self.auto_error = auto_error", "metadata": {"license": "MIT", "len_tokens": 328}}
{"id": "fastapi:fastapi/security/http.py", "language": "python", "code": "def __init__(\n        self,\n        *,\n        scheme_name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme name.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Security scheme description.\n\n                It will be included in the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        auto_error: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                By default, if the HTTP Digest is not provided, `HTTPDigest` will\n                automatically cancel the request and send the client an error.\n\n                If `auto_error` is set to `False`, when the HTTP Digest is not\n                available, instead of erroring out, the dependency result will\n                be `None`.\n\n                This is useful when you want to have optional authentication.\n\n                It is also useful when you want to have authentication that can be\n                provided in one of multiple optional ways (for example, in HTTP\n                Digest or in a cookie).\n                \"\"\"\n            ),\n        ] = True,\n    ):\n        self.model = HTTPBaseModel(scheme=\"digest\", description=description)\n        self.scheme_name = scheme_name or self.__class__.__name__\n        self.auto_error = auto_error", "metadata": {"license": "MIT", "len_tokens": 288}}
{"id": "fastapi:fastapi/dependencies/models.py", "language": "python", "code": "def is_gen_callable(self) -> bool:\n        if self.call is None:\n            return False  # pragma: no cover\n        if inspect.isgeneratorfunction(\n            _impartial(self.call)\n        ) or inspect.isgeneratorfunction(_unwrapped_call(self.call)):\n            return True\n        if inspect.isclass(_unwrapped_call(self.call)):\n            return False\n        dunder_call = getattr(_impartial(self.call), \"__call__\", None)  # noqa: B004\n        if dunder_call is None:\n            return False  # pragma: no cover\n        if inspect.isgeneratorfunction(\n            _impartial(dunder_call)\n        ) or inspect.isgeneratorfunction(_unwrapped_call(dunder_call)):\n            return True\n        dunder_unwrapped_call = getattr(_unwrapped_call(self.call), \"__call__\", None)  # noqa: B004\n        if dunder_unwrapped_call is None:\n            return False  # pragma: no cover\n        if inspect.isgeneratorfunction(\n            _impartial(dunder_unwrapped_call)\n        ) or inspect.isgeneratorfunction(_unwrapped_call(dunder_unwrapped_call)):\n            return True\n        return False", "metadata": {"license": "MIT", "len_tokens": 242}}
{"id": "fastapi:fastapi/dependencies/models.py", "language": "python", "code": "def is_async_gen_callable(self) -> bool:\n        if self.call is None:\n            return False  # pragma: no cover\n        if inspect.isasyncgenfunction(\n            _impartial(self.call)\n        ) or inspect.isasyncgenfunction(_unwrapped_call(self.call)):\n            return True\n        if inspect.isclass(_unwrapped_call(self.call)):\n            return False\n        dunder_call = getattr(_impartial(self.call), \"__call__\", None)  # noqa: B004\n        if dunder_call is None:\n            return False  # pragma: no cover\n        if inspect.isasyncgenfunction(\n            _impartial(dunder_call)\n        ) or inspect.isasyncgenfunction(_unwrapped_call(dunder_call)):\n            return True\n        dunder_unwrapped_call = getattr(_unwrapped_call(self.call), \"__call__\", None)  # noqa: B004\n        if dunder_unwrapped_call is None:\n            return False  # pragma: no cover\n        if inspect.isasyncgenfunction(\n            _impartial(dunder_unwrapped_call)\n        ) or inspect.isasyncgenfunction(_unwrapped_call(dunder_unwrapped_call)):\n            return True\n        return False", "metadata": {"license": "MIT", "len_tokens": 249}}
{"id": "fastapi:fastapi/dependencies/models.py", "language": "python", "code": "def is_coroutine_callable(self) -> bool:\n        if self.call is None:\n            return False  # pragma: no cover\n        if inspect.isroutine(_impartial(self.call)) and iscoroutinefunction(\n            _impartial(self.call)\n        ):\n            return True\n        if inspect.isroutine(_unwrapped_call(self.call)) and iscoroutinefunction(\n            _unwrapped_call(self.call)\n        ):\n            return True\n        if inspect.isclass(_unwrapped_call(self.call)):\n            return False\n        dunder_call = getattr(_impartial(self.call), \"__call__\", None)  # noqa: B004\n        if dunder_call is None:\n            return False  # pragma: no cover\n        if iscoroutinefunction(_impartial(dunder_call)) or iscoroutinefunction(\n            _unwrapped_call(dunder_call)\n        ):\n            return True\n        dunder_unwrapped_call = getattr(_unwrapped_call(self.call), \"__call__\", None)  # noqa: B004\n        if dunder_unwrapped_call is None:\n            return False  # pragma: no cover\n        if iscoroutinefunction(\n            _impartial(dunder_unwrapped_call)\n        ) or iscoroutinefunction(_unwrapped_call(dunder_unwrapped_call)):\n            return True\n        return False", "metadata": {"license": "MIT", "len_tokens": 273}}
{"id": "fastapi:fastapi/dependencies/utils.py", "language": "python", "code": "def ensure_multipart_is_installed() -> None:\n    try:\n        from python_multipart import __version__\n\n        # Import an attribute that can be mocked/deleted in testing\n        assert __version__ > \"0.0.12\"\n    except (ImportError, AssertionError):\n        try:\n            # __version__ is available in both multiparts, and can be mocked\n            from multipart import __version__  # type: ignore[no-redef,import-untyped]\n\n            assert __version__\n            try:\n                # parse_options_header is only available in the right multipart\n                from multipart.multipart import (  # type: ignore[import-untyped]\n                    parse_options_header,\n                )\n\n                assert parse_options_header\n            except ImportError:\n                logger.error(multipart_incorrect_install_error)\n                raise RuntimeError(multipart_incorrect_install_error) from None\n        except ImportError:\n            logger.error(multipart_not_installed_error)\n            raise RuntimeError(multipart_not_installed_error) from None", "metadata": {"license": "MIT", "len_tokens": 207}}
{"id": "fastapi:fastapi/dependencies/utils.py", "language": "python", "code": "def get_flat_dependant(\n    dependant: Dependant,\n    *,\n    skip_repeats: bool = False,\n    visited: Optional[List[DependencyCacheKey]] = None,\n    parent_oauth_scopes: Optional[List[str]] = None,\n) -> Dependant:\n    if visited is None:\n        visited = []\n    visited.append(dependant.cache_key)\n    use_parent_oauth_scopes = (parent_oauth_scopes or []) + (\n        dependant.oauth_scopes or []\n    )\n\n    flat_dependant = Dependant(\n        path_params=dependant.path_params.copy(),\n        query_params=dependant.query_params.copy(),\n        header_params=dependant.header_params.copy(),\n        cookie_params=dependant.cookie_params.copy(),\n        body_params=dependant.body_params.copy(),\n        name=dependant.name,\n        call=dependant.call,\n        request_param_name=dependant.request_param_name,\n        websocket_param_name=dependant.websocket_param_name,\n        http_connection_param_name=dependant.http_connection_param_name,\n        response_param_name=dependant.response_param_name,\n        background_tasks_param_name=dependant.background_tasks_param_name,\n        security_scopes_param_name=dependant.security_scopes_param_name,\n        own_oauth_scopes=dependant.own_oauth_scopes,\n        parent_oauth_scopes=use_parent_oauth_scopes,\n        use_cache=dependant.use_cache,\n        path=dependant.path,\n        scope=dependant.scope,\n    )\n    for sub_dependant in dependant.dependencies:\n        if skip_repeats and sub_dependant.cache_key in visited:\n            continue\n        flat_sub = get_flat_dependant(\n            sub_dependant,\n            skip_repeats=skip_repeats,\n            visited=visited,\n            parent_oauth_scopes=flat_dependant.oauth_scopes,\n        )\n        flat_dependant.dependencies.append(flat_sub)\n        flat_dependant.path_params.extend(flat_sub.path_params)\n        flat_dependant.query_params.extend(flat_sub.query_params)\n        flat_dependant.header_params.extend(flat_sub.header_params)\n        flat_dependant.cookie_params.extend(flat_sub.cookie_params)\n        flat_dependant.body_params.extend(flat_sub.body_params)\n        flat_dependant.dependencies.extend(flat_sub.dependencies)\n\n    return flat_dependant", "metadata": {"license": "MIT", "len_tokens": 460}}
{"id": "fastapi:fastapi/dependencies/utils.py", "language": "python", "code": "def get_dependant(\n    *,\n    path: str,\n    call: Callable[..., Any],\n    name: Optional[str] = None,\n    own_oauth_scopes: Optional[List[str]] = None,\n    parent_oauth_scopes: Optional[List[str]] = None,\n    use_cache: bool = True,\n    scope: Union[Literal[\"function\", \"request\"], None] = None,\n) -> Dependant:\n    dependant = Dependant(\n        call=call,\n        name=name,\n        path=path,\n        use_cache=use_cache,\n        scope=scope,\n        own_oauth_scopes=own_oauth_scopes,\n        parent_oauth_scopes=parent_oauth_scopes,\n    )\n    current_scopes = (parent_oauth_scopes or []) + (own_oauth_scopes or [])\n    path_param_names = get_path_param_names(path)\n    endpoint_signature = get_typed_signature(call)\n    signature_params = endpoint_signature.parameters\n    for param_name, param in signature_params.items():\n        is_path_param = param_name in path_param_names\n        param_details = analyze_param(\n            param_name=param_name,\n            annotation=param.annotation,\n            value=param.default,\n            is_path_param=is_path_param,\n        )\n        if param_details.depends is not None:\n            assert param_details.depends.dependency\n            if (\n                (dependant.is_gen_callable or dependant.is_async_gen_callable)\n                and dependant.computed_scope == \"request\"\n                and param_details.depends.scope == \"function\"\n            ):\n                assert dependant.call\n                raise DependencyScopeError(\n                    f'The dependency \"{dependant.call.__name__}\" has a scope of '\n                    '\"request\", it cannot depend on dependencies with scope \"function\".'\n                )\n            sub_own_oauth_scopes: List[str] = []\n            if isinstance(param_details.depends, params.Security):\n                if param_details.depends.scopes:\n                    sub_own_oauth_scopes = list(param_details.depends.scopes)\n            sub_dependant = get_dependant(\n                path=path,\n                call=param_details.depends.dependency,\n                name=param_name,\n                own_oauth_scopes=sub_own_oauth_scopes,\n                parent_oauth_scopes=current_scopes,\n                use_cache=param_details.depends.use_cache,\n                scope=param_details.depends.scope,\n            )\n            dependant.dependencies.append(sub_dependant)\n            continue\n        if add_non_field_param_to_dependency(\n            param_name=param_name,\n            type_annotation=param_details.type_annotation,\n            dependant=dependant,\n        ):\n            assert param_details.field is None, (\n                f\"Cannot specify multiple FastAPI annotations for {param_name!r}\"\n            )\n            continue\n        assert param_details.field is not None\n        if isinstance(\n            param_details.field.field_info, (params.Body, temp_pydantic_v1_params.Body)\n        ):\n            dependant.body_params.append(param_details.field)\n        else:\n            add_param_to_fields(field=param_details.field, dependant=dependant)\n    return dependant", "metadata": {"license": "MIT", "len_tokens": 628}}
{"id": "fastapi:fastapi/dependencies/utils.py", "language": "python", "code": "def add_non_field_param_to_dependency(\n    *, param_name: str, type_annotation: Any, dependant: Dependant\n) -> Optional[bool]:\n    if lenient_issubclass(type_annotation, Request):\n        dependant.request_param_name = param_name\n        return True\n    elif lenient_issubclass(type_annotation, WebSocket):\n        dependant.websocket_param_name = param_name\n        return True\n    elif lenient_issubclass(type_annotation, HTTPConnection):\n        dependant.http_connection_param_name = param_name\n        return True\n    elif lenient_issubclass(type_annotation, Response):\n        dependant.response_param_name = param_name\n        return True\n    elif lenient_issubclass(type_annotation, StarletteBackgroundTasks):\n        dependant.background_tasks_param_name = param_name\n        return True\n    elif lenient_issubclass(type_annotation, SecurityScopes):\n        dependant.security_scopes_param_name = param_name\n        return True\n    return None", "metadata": {"license": "MIT", "len_tokens": 200}}
{"id": "fastapi:fastapi/dependencies/utils.py", "language": "python", "code": "def request_params_to_args(\n    fields: Sequence[ModelField],\n    received_params: Union[Mapping[str, Any], QueryParams, Headers],\n) -> Tuple[Dict[str, Any], List[Any]]:\n    values: Dict[str, Any] = {}\n    errors: List[Dict[str, Any]] = []\n\n    if not fields:\n        return values, errors\n\n    first_field = fields[0]\n    fields_to_extract = fields\n    single_not_embedded_field = False\n    default_convert_underscores = True\n    if len(fields) == 1 and lenient_issubclass(first_field.type_, BaseModel):\n        fields_to_extract = get_cached_model_fields(first_field.type_)\n        single_not_embedded_field = True\n        # If headers are in a Pydantic model, the way to disable convert_underscores\n        # would be with Header(convert_underscores=False) at the Pydantic model level\n        default_convert_underscores = getattr(\n            first_field.field_info, \"convert_underscores\", True\n        )\n\n    params_to_process: Dict[str, Any] = {}\n\n    processed_keys = set()\n\n    for field in fields_to_extract:\n        alias = None\n        if isinstance(received_params, Headers):\n            # Handle fields extracted from a Pydantic Model for a header, each field\n            # doesn't have a FieldInfo of type Header with the default convert_underscores=True\n            convert_underscores = getattr(\n                field.field_info, \"convert_underscores\", default_convert_underscores\n            )\n            if convert_underscores:\n                alias = get_validation_alias(field)\n                if alias == field.name:\n                    alias = alias.replace(\"_\", \"-\")\n        value = _get_multidict_value(field, received_params, alias=alias)\n        if value is not None:\n            params_to_process[get_validation_alias(field)] = value\n        processed_keys.add(alias or get_validation_alias(field))\n\n    for key in received_params.keys():\n        if key not in processed_keys:\n            if hasattr(received_params, \"getlist\"):\n                value = received_params.getlist(key)\n                if isinstance(value, list) and (len(value) == 1):\n                    params_to_process[key] = value[0]\n                else:\n                    params_to_process[key] = value\n            else:\n                params_to_process[key] = received_params.get(key)\n\n    if single_not_embedded_field:\n        field_info = first_field.field_info\n        assert isinstance(field_info, (params.Param, temp_pydantic_v1_params.Param)), (\n            \"Params must be subclasses of Param\"\n        )\n        loc: Tuple[str, ...] = (field_info.in_.value,)\n        v_, errors_ = _validate_value_with_model_field(\n            field=first_field, value=params_to_process, values=values, loc=loc\n        )\n        return {first_field.name: v_}, errors_\n\n    for field in fields:\n        value = _get_multidict_value(field, received_params)\n        field_info = field.field_info\n        assert isinstance(field_info, (params.Param, temp_pydantic_v1_params.Param)), (\n            \"Params must be subclasses of Param\"\n        )\n        loc = (field_info.in_.value, get_validation_alias(field))\n        v_, errors_ = _validate_value_with_model_field(\n            field=field, value=value, values=values, loc=loc\n        )\n        if errors_:\n            errors.extend(errors_)\n        else:\n            values[field.name] = v_\n    return values, errors", "metadata": {"license": "MIT", "len_tokens": 732}}
{"id": "fastapi:fastapi/dependencies/utils.py", "language": "python", "code": "def _should_embed_body_fields(fields: List[ModelField]) -> bool:\n    if not fields:\n        return False\n    # More than one dependency could have the same field, it would show up as multiple\n    # fields but it's the same one, so count them by name\n    body_param_names_set = {field.name for field in fields}\n    # A top level field has to be a single field, not multiple\n    if len(body_param_names_set) > 1:\n        return True\n    first_field = fields[0]\n    # If it explicitly specifies it is embedded, it has to be embedded\n    if getattr(first_field.field_info, \"embed\", None):\n        return True\n    # If it's a Form (or File) field, it has to be a BaseModel (or a union of BaseModels) to be top level\n    # otherwise it has to be embedded, so that the key value pair can be extracted\n    if (\n        isinstance(first_field.field_info, (params.Form, temp_pydantic_v1_params.Form))\n        and not _is_model_class(first_field.type_)\n        and not is_union_of_base_models(first_field.type_)\n    ):\n        return True\n    return False", "metadata": {"license": "MIT", "len_tokens": 252}}
{"id": "fastapi:fastapi/dependencies/utils.py", "language": "python", "code": "async def _extract_form_body(\n    body_fields: List[ModelField],\n    received_body: FormData,\n) -> Dict[str, Any]:\n    values = {}\n\n    for field in body_fields:\n        value = _get_multidict_value(field, received_body)\n        field_info = field.field_info\n        if (\n            isinstance(field_info, (params.File, temp_pydantic_v1_params.File))\n            and is_bytes_field(field)\n            and isinstance(value, UploadFile)\n        ):\n            value = await value.read()\n        elif (\n            is_bytes_sequence_field(field)\n            and isinstance(field_info, (params.File, temp_pydantic_v1_params.File))\n            and value_is_sequence(value)\n        ):\n            # For types\n            assert isinstance(value, sequence_types)  # type: ignore[arg-type]\n            results: List[Union[bytes, str]] = []\n\n            async def process_fn(\n                fn: Callable[[], Coroutine[Any, Any, Any]],\n            ) -> None:\n                result = await fn()\n                results.append(result)  # noqa: B023\n\n            async with anyio.create_task_group() as tg:\n                for sub_value in value:\n                    tg.start_soon(process_fn, sub_value.read)\n            value = serialize_sequence_value(field=field, value=results)\n        if value is not None:\n            values[get_validation_alias(field)] = value\n    field_aliases = {get_validation_alias(field) for field in body_fields}\n    for key in received_body.keys():\n        if key not in field_aliases:\n            param_values = received_body.getlist(key)\n            if len(param_values) == 1:\n                values[key] = param_values[0]\n            else:\n                values[key] = param_values\n    return values", "metadata": {"license": "MIT", "len_tokens": 362}}
{"id": "fastapi:fastapi/dependencies/utils.py", "language": "python", "code": "async def request_body_to_args(\n    body_fields: List[ModelField],\n    received_body: Optional[Union[Dict[str, Any], FormData]],\n    embed_body_fields: bool,\n) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:\n    values: Dict[str, Any] = {}\n    errors: List[Dict[str, Any]] = []\n    assert body_fields, \"request_body_to_args() should be called with fields\"\n    single_not_embedded_field = len(body_fields) == 1 and not embed_body_fields\n    first_field = body_fields[0]\n    body_to_process = received_body\n\n    fields_to_extract: List[ModelField] = body_fields\n\n    if (\n        single_not_embedded_field\n        and _is_model_class(first_field.type_)\n        and isinstance(received_body, FormData)\n    ):\n        fields_to_extract = get_cached_model_fields(first_field.type_)\n\n    if isinstance(received_body, FormData):\n        body_to_process = await _extract_form_body(fields_to_extract, received_body)\n\n    if single_not_embedded_field:\n        loc: Tuple[str, ...] = (\"body\",)\n        v_, errors_ = _validate_value_with_model_field(\n            field=first_field, value=body_to_process, values=values, loc=loc\n        )\n        return {first_field.name: v_}, errors_\n    for field in body_fields:\n        loc = (\"body\", get_validation_alias(field))\n        value: Optional[Any] = None\n        if body_to_process is not None:\n            try:\n                value = body_to_process.get(get_validation_alias(field))\n            # If the received body is a list, not a dict\n            except AttributeError:\n                errors.append(get_missing_field_error(loc))\n                continue\n        v_, errors_ = _validate_value_with_model_field(\n            field=field, value=value, values=values, loc=loc\n        )\n        if errors_:\n            errors.extend(errors_)\n        else:\n            values[field.name] = v_\n    return values, errors", "metadata": {"license": "MIT", "len_tokens": 426}}
{"id": "fastapi:fastapi/dependencies/utils.py", "language": "python", "code": "def get_body_field(\n    *, flat_dependant: Dependant, name: str, embed_body_fields: bool\n) -> Optional[ModelField]:\n    \"\"\"\n    Get a ModelField representing the request body for a path operation, combining\n    all body parameters into a single field if necessary.\n\n    Used to check if it's form data (with `isinstance(body_field, params.Form)`)\n    or JSON and to generate the JSON Schema for a request body.\n\n    This is **not** used to validate/parse the request body, that's done with each\n    individual body parameter.\n    \"\"\"\n    if not flat_dependant.body_params:\n        return None\n    first_param = flat_dependant.body_params[0]\n    if not embed_body_fields:\n        return first_param\n    model_name = \"Body_\" + name\n    BodyModel = create_body_model(\n        fields=flat_dependant.body_params, model_name=model_name\n    )\n    required = any(True for f in flat_dependant.body_params if f.required)\n    BodyFieldInfo_kwargs: Dict[str, Any] = {\n        \"annotation\": BodyModel,\n        \"alias\": \"body\",\n    }\n    if not required:\n        BodyFieldInfo_kwargs[\"default\"] = None\n    if any(isinstance(f.field_info, params.File) for f in flat_dependant.body_params):\n        BodyFieldInfo: Type[params.Body] = params.File\n    elif any(\n        isinstance(f.field_info, temp_pydantic_v1_params.File)\n        for f in flat_dependant.body_params\n    ):\n        BodyFieldInfo: Type[temp_pydantic_v1_params.Body] = temp_pydantic_v1_params.File  # type: ignore[no-redef]\n    elif any(isinstance(f.field_info, params.Form) for f in flat_dependant.body_params):\n        BodyFieldInfo = params.Form\n    elif any(\n        isinstance(f.field_info, temp_pydantic_v1_params.Form)\n        for f in flat_dependant.body_params\n    ):\n        BodyFieldInfo = temp_pydantic_v1_params.Form  # type: ignore[assignment]\n    else:\n        if annotation_is_pydantic_v1(BodyModel):\n            BodyFieldInfo = temp_pydantic_v1_params.Body  # type: ignore[assignment]\n        else:\n            BodyFieldInfo = params.Body\n\n        body_param_media_types = [\n            f.field_info.media_type\n            for f in flat_dependant.body_params\n            if isinstance(f.field_info, (params.Body, temp_pydantic_v1_params.Body))\n        ]\n        if len(set(body_param_media_types)) == 1:\n            BodyFieldInfo_kwargs[\"media_type\"] = body_param_media_types[0]\n    final_field = create_model_field(\n        name=\"body\",\n        type_=BodyModel,\n        required=required,\n        alias=\"body\",\n        field_info=BodyFieldInfo(**BodyFieldInfo_kwargs),\n    )\n    return final_field", "metadata": {"license": "MIT", "len_tokens": 607}}
{"id": "fastapi:fastapi/openapi/models.py", "language": "python", "code": "class EmailStr(str):  # type: ignore\n        @classmethod\n        def __get_validators__(cls) -> Iterable[Callable[..., Any]]:\n            yield cls.validate\n\n        @classmethod\n        def validate(cls, v: Any) -> str:\n            logger.warning(\n                \"email-validator not installed, email fields will be treated as str.\\n\"\n                \"To install, run: pip install email-validator\"\n            )\n            return str(v)\n\n        @classmethod\n        def _validate(cls, __input_value: Any, _: Any) -> str:\n            logger.warning(\n                \"email-validator not installed, email fields will be treated as str.\\n\"\n                \"To install, run: pip install email-validator\"\n            )\n            return str(__input_value)\n\n        @classmethod\n        def __get_pydantic_json_schema__(\n            cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n        ) -> JsonSchemaValue:\n            return {\"type\": \"string\", \"format\": \"email\"}\n\n        @classmethod\n        def __get_pydantic_core_schema__(\n            cls, source: Type[Any], handler: Callable[[Any], CoreSchema]\n        ) -> CoreSchema:\n            return with_info_plain_validator_function(cls._validate)", "metadata": {"license": "MIT", "len_tokens": 255}}
{"id": "fastapi:fastapi/openapi/docs.py", "language": "python", "code": "def get_redoc_html(\n    *,\n    openapi_url: Annotated[\n        str,\n        Doc(\n            \"\"\"\n            The OpenAPI URL that ReDoc should load and use.\n\n            This is normally done automatically by FastAPI using the default URL\n            `/openapi.json`.\n            \"\"\"\n        ),\n    ],\n    title: Annotated[\n        str,\n        Doc(\n            \"\"\"\n            The HTML `<title>` content, normally shown in the browser tab.\n            \"\"\"\n        ),\n    ],\n    redoc_js_url: Annotated[\n        str,\n        Doc(\n            \"\"\"\n            The URL to use to load the ReDoc JavaScript.\n\n            It is normally set to a CDN URL.\n            \"\"\"\n        ),\n    ] = \"https://cdn.jsdelivr.net/npm/redoc@2/bundles/redoc.standalone.js\",\n    redoc_favicon_url: Annotated[\n        str,\n        Doc(\n            \"\"\"\n            The URL of the favicon to use. It is normally shown in the browser tab.\n            \"\"\"\n        ),\n    ] = \"https://fastapi.tiangolo.com/img/favicon.png\",\n    with_google_fonts: Annotated[\n        bool,\n        Doc(\n            \"\"\"\n            Load and use Google Fonts.\n            \"\"\"\n        ),\n    ] = True,\n) -> HTMLResponse:\n    \"\"\"\n    Generate and return the HTML response that loads ReDoc for the alternative\n    API docs (normally served at `/redoc`).\n\n    You would only call this function yourself if you needed to override some parts,\n    for example the URLs to use to load ReDoc's JavaScript and CSS.\n\n    Read more about it in the\n    [FastAPI docs for Custom Docs UI Static Assets (Self-Hosting)](https://fastapi.tiangolo.com/how-to/custom-docs-ui-assets/).\n    \"\"\"\n    html = f\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n    <title>{title}</title>\n    <!-- needed for adaptive design -->\n    <meta charset=\"utf-8\"/>\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    \"\"\"\n    if with_google_fonts:\n        html += \"\"\"\n    <link href=\"https://fonts.googleapis.com/css?family=Montserrat:300,400,700|Roboto:300,400,700\" rel=\"stylesheet\">\n    \"\"\"\n    html += f\"\"\"\n    <link rel=\"shortcut icon\" href=\"{redoc_favicon_url}\">\n    <!--\n    ReDoc doesn't change outer page styles\n    -->\n    <style>\n      body {{\n        margin: 0;\n        padding: 0;\n      }}\n    </style>\n    </head>\n    <body>\n    <noscript>\n        ReDoc requires Javascript to function. Please enable it to browse the documentation.\n    </noscript>\n    <redoc spec-url=\"{openapi_url}\"></redoc>\n    <script src=\"{redoc_js_url}\"> </script>\n    </body>\n    </html>\n    \"\"\"\n    return HTMLResponse(html)", "metadata": {"license": "MIT", "len_tokens": 611}}
{"id": "fastapi:fastapi/openapi/docs.py", "language": "python", "code": "def get_swagger_ui_oauth2_redirect_html() -> HTMLResponse:\n    \"\"\"\n    Generate the HTML response with the OAuth2 redirection for Swagger UI.\n\n    You normally don't need to use or change this.\n    \"\"\"\n    # copied from https://github.com/swagger-api/swagger-ui/blob/v4.14.0/dist/oauth2-redirect.html\n    html = \"\"\"\n    <!doctype html>\n    <html lang=\"en-US\">\n    <head>\n        <title>Swagger UI: OAuth2 Redirect</title>\n    </head>\n    <body>\n    <script>\n        'use strict';\n        function run () {\n            var oauth2 = window.opener.swaggerUIRedirectOauth2;\n            var sentState = oauth2.state;\n            var redirectUrl = oauth2.redirectUrl;\n            var isValid, qp, arr;\n\n            if (/code|token|error/.test(window.location.hash)) {\n                qp = window.location.hash.substring(1).replace('?', '&');\n            } else {\n                qp = location.search.substring(1);\n            }\n\n            arr = qp.split(\"&\");\n            arr.forEach(function (v,i,_arr) { _arr[i] = '\"' + v.replace('=', '\":\"') + '\"';});\n            qp = qp ? JSON.parse('{' + arr.join() + '}',\n                    function (key, value) {\n                        return key === \"\" ? value : decodeURIComponent(value);\n                    }\n            ) : {};\n\n            isValid = qp.state === sentState;\n\n            if ((\n              oauth2.auth.schema.get(\"flow\") === \"accessCode\" ||\n              oauth2.auth.schema.get(\"flow\") === \"authorizationCode\" ||\n              oauth2.auth.schema.get(\"flow\") === \"authorization_code\"\n            ) && !oauth2.auth.code) {\n                if (!isValid) {\n                    oauth2.errCb({\n                        authId: oauth2.auth.name,\n                        source: \"auth\",\n                        level: \"warning\",\n                        message: \"Authorization may be unsafe, passed state was changed in server. The passed state wasn't returned from auth server.\"\n                    });\n                }\n\n                if (qp.code) {\n                    delete oauth2.state;\n                    oauth2.auth.code = qp.code;\n                    oauth2.callback({auth: oauth2.auth, redirectUrl: redirectUrl});\n                } else {\n                    let oauthErrorMsg;\n                    if (qp.error) {\n                        oauthErrorMsg = \"[\"+qp.error+\"]: \" +\n                            (qp.error_description ? qp.error_description+ \". \" : \"no accessCode received from the server. \") +\n                            (qp.error_uri ? \"More info: \"+qp.error_uri : \"\");\n                    }\n\n                    oauth2.errCb({\n                        authId: oauth2.auth.name,\n                        source: \"auth\",\n                        level: \"error\",\n                        message: oauthErrorMsg || \"[Authorization failed]: no accessCode received from the server.\"\n                    });\n                }\n            } else {\n                oauth2.callback({auth: oauth2.auth, token: qp, isValid: isValid, redirectUrl: redirectUrl});\n            }\n            window.close();\n        }\n\n        if (document.readyState !== 'loading') {\n            run();\n        } else {\n            document.addEventListener('DOMContentLoaded', function () {\n                run();\n            });\n        }\n    </script>\n    </body>\n    </html>\n        \"\"\"\n    return HTMLResponse(content=html)", "metadata": {"license": "MIT", "len_tokens": 683}}
{"id": "fastapi:fastapi/openapi/utils.py", "language": "python", "code": "def get_openapi_security_definitions(\n    flat_dependant: Dependant,\n) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:\n    security_definitions = {}\n    # Use a dict to merge scopes for same security scheme\n    operation_security_dict: Dict[str, List[str]] = {}\n    for security_dependency in flat_dependant._security_dependencies:\n        security_definition = jsonable_encoder(\n            security_dependency._security_scheme.model,\n            by_alias=True,\n            exclude_none=True,\n        )\n        security_name = security_dependency._security_scheme.scheme_name\n        security_definitions[security_name] = security_definition\n        # Merge scopes for the same security scheme\n        if security_name not in operation_security_dict:\n            operation_security_dict[security_name] = []\n        for scope in security_dependency.oauth_scopes or []:\n            if scope not in operation_security_dict[security_name]:\n                operation_security_dict[security_name].append(scope)\n    operation_security = [\n        {name: scopes} for name, scopes in operation_security_dict.items()\n    ]\n    return security_definitions, operation_security", "metadata": {"license": "MIT", "len_tokens": 225}}
{"id": "fastapi:fastapi/openapi/utils.py", "language": "python", "code": "def _get_openapi_operation_parameters(\n    *,\n    dependant: Dependant,\n    model_name_map: ModelNameMap,\n    field_mapping: Dict[\n        Tuple[ModelField, Literal[\"validation\", \"serialization\"]], JsonSchemaValue\n    ],\n    separate_input_output_schemas: bool = True,\n) -> List[Dict[str, Any]]:\n    parameters = []\n    flat_dependant = get_flat_dependant(dependant, skip_repeats=True)\n    path_params = _get_flat_fields_from_params(flat_dependant.path_params)\n    query_params = _get_flat_fields_from_params(flat_dependant.query_params)\n    header_params = _get_flat_fields_from_params(flat_dependant.header_params)\n    cookie_params = _get_flat_fields_from_params(flat_dependant.cookie_params)\n    parameter_groups = [\n        (ParamTypes.path, path_params),\n        (ParamTypes.query, query_params),\n        (ParamTypes.header, header_params),\n        (ParamTypes.cookie, cookie_params),\n    ]\n    default_convert_underscores = True\n    if len(flat_dependant.header_params) == 1:\n        first_field = flat_dependant.header_params[0]\n        if lenient_issubclass(first_field.type_, BaseModel):\n            default_convert_underscores = getattr(\n                first_field.field_info, \"convert_underscores\", True\n            )\n    for param_type, param_group in parameter_groups:\n        for param in param_group:\n            field_info = param.field_info\n            # field_info = cast(Param, field_info)\n            if not getattr(field_info, \"include_in_schema\", True):\n                continue\n            param_schema = get_schema_from_model_field(\n                field=param,\n                model_name_map=model_name_map,\n                field_mapping=field_mapping,\n                separate_input_output_schemas=separate_input_output_schemas,\n            )\n            name = get_validation_alias(param)\n            convert_underscores = getattr(\n                param.field_info,\n                \"convert_underscores\",\n                default_convert_underscores,\n            )\n            if (\n                param_type == ParamTypes.header\n                and name == param.name\n                and convert_underscores\n            ):\n                name = param.name.replace(\"_\", \"-\")\n\n            parameter = {\n                \"name\": name,\n                \"in\": param_type.value,\n                \"required\": param.required,\n                \"schema\": param_schema,\n            }\n            if field_info.description:\n                parameter[\"description\"] = field_info.description\n            openapi_examples = getattr(field_info, \"openapi_examples\", None)\n            example = getattr(field_info, \"example\", None)\n            if openapi_examples:\n                parameter[\"examples\"] = jsonable_encoder(openapi_examples)\n            elif example != Undefined:\n                parameter[\"example\"] = jsonable_encoder(example)\n            if getattr(field_info, \"deprecated\", None):\n                parameter[\"deprecated\"] = True\n            parameters.append(parameter)\n    return parameters", "metadata": {"license": "MIT", "len_tokens": 591}}
{"id": "fastapi:fastapi/openapi/utils.py", "language": "python", "code": "def get_openapi_operation_request_body(\n    *,\n    body_field: Optional[ModelField],\n    model_name_map: ModelNameMap,\n    field_mapping: Dict[\n        Tuple[ModelField, Literal[\"validation\", \"serialization\"]], JsonSchemaValue\n    ],\n    separate_input_output_schemas: bool = True,\n) -> Optional[Dict[str, Any]]:\n    if not body_field:\n        return None\n    assert _is_model_field(body_field)\n    body_schema = get_schema_from_model_field(\n        field=body_field,\n        model_name_map=model_name_map,\n        field_mapping=field_mapping,\n        separate_input_output_schemas=separate_input_output_schemas,\n    )\n    field_info = cast(Body, body_field.field_info)\n    request_media_type = field_info.media_type\n    required = body_field.required\n    request_body_oai: Dict[str, Any] = {}\n    if required:\n        request_body_oai[\"required\"] = required\n    request_media_content: Dict[str, Any] = {\"schema\": body_schema}\n    if field_info.openapi_examples:\n        request_media_content[\"examples\"] = jsonable_encoder(\n            field_info.openapi_examples\n        )\n    elif field_info.example != Undefined:\n        request_media_content[\"example\"] = jsonable_encoder(field_info.example)\n    request_body_oai[\"content\"] = {request_media_type: request_media_content}\n    return request_body_oai", "metadata": {"license": "MIT", "len_tokens": 289}}
{"id": "fastapi:fastapi/openapi/utils.py", "language": "python", "code": "def get_openapi_operation_metadata(\n    *, route: routing.APIRoute, method: str, operation_ids: Set[str]\n) -> Dict[str, Any]:\n    operation: Dict[str, Any] = {}\n    if route.tags:\n        operation[\"tags\"] = route.tags\n    operation[\"summary\"] = generate_operation_summary(route=route, method=method)\n    if route.description:\n        operation[\"description\"] = route.description\n    operation_id = route.operation_id or route.unique_id\n    if operation_id in operation_ids:\n        message = (\n            f\"Duplicate Operation ID {operation_id} for function \"\n            + f\"{route.endpoint.__name__}\"\n        )\n        file_name = getattr(route.endpoint, \"__globals__\", {}).get(\"__file__\")\n        if file_name:\n            message += f\" at {file_name}\"\n        warnings.warn(message, stacklevel=1)\n    operation_ids.add(operation_id)\n    operation[\"operationId\"] = operation_id\n    if route.deprecated:\n        operation[\"deprecated\"] = route.deprecated\n    return operation", "metadata": {"license": "MIT", "len_tokens": 215}}
{"id": "fastapi:fastapi/openapi/utils.py", "language": "python", "code": "def get_fields_from_routes(\n    routes: Sequence[BaseRoute],\n) -> List[ModelField]:\n    body_fields_from_routes: List[ModelField] = []\n    responses_from_routes: List[ModelField] = []\n    request_fields_from_routes: List[ModelField] = []\n    callback_flat_models: List[ModelField] = []\n    for route in routes:\n        if getattr(route, \"include_in_schema\", None) and isinstance(\n            route, routing.APIRoute\n        ):\n            if route.body_field:\n                assert _is_model_field(route.body_field), (\n                    \"A request body must be a Pydantic Field\"\n                )\n                body_fields_from_routes.append(route.body_field)\n            if route.response_field:\n                responses_from_routes.append(route.response_field)\n            if route.response_fields:\n                responses_from_routes.extend(route.response_fields.values())\n            if route.callbacks:\n                callback_flat_models.extend(get_fields_from_routes(route.callbacks))\n            params = get_flat_params(route.dependant)\n            request_fields_from_routes.extend(params)\n\n    flat_models = callback_flat_models + list(\n        body_fields_from_routes + responses_from_routes + request_fields_from_routes\n    )\n    return flat_models", "metadata": {"license": "MIT", "len_tokens": 242}}
{"id": "fastapi:fastapi/_compat/__init__.py", "language": "python", "code": "from .main import BaseConfig as BaseConfig\nfrom .main import PydanticSchemaGenerationError as PydanticSchemaGenerationError\nfrom .main import RequiredParam as RequiredParam\nfrom .main import Undefined as Undefined\nfrom .main import UndefinedType as UndefinedType\nfrom .main import Url as Url\nfrom .main import Validator as Validator\nfrom .main import _get_model_config as _get_model_config\nfrom .main import _is_error_wrapper as _is_error_wrapper\nfrom .main import _is_model_class as _is_model_class\nfrom .main import _is_model_field as _is_model_field\nfrom .main import _is_undefined as _is_undefined\nfrom .main import _model_dump as _model_dump\nfrom .main import _model_rebuild as _model_rebuild\nfrom .main import copy_field_info as copy_field_info\nfrom .main import create_body_model as create_body_model\nfrom .main import evaluate_forwardref as evaluate_forwardref\nfrom .main import get_annotation_from_field_info as get_annotation_from_field_info\nfrom .main import get_cached_model_fields as get_cached_model_fields\nfrom .main import get_compat_model_name_map as get_compat_model_name_map\nfrom .main import get_definitions as get_definitions\nfrom .main import get_missing_field_error as get_missing_field_error\nfrom .main import get_schema_from_model_field as get_schema_from_model_field\nfrom .main import is_bytes_field as is_bytes_field\nfrom .main import is_bytes_sequence_field as is_bytes_sequence_field\nfrom .main import is_scalar_field as is_scalar_field\nfrom .main import is_scalar_sequence_field as is_scalar_sequence_field\nfrom .main import is_sequence_field as is_sequence_field\nfrom .main import serialize_sequence_value as serialize_sequence_value\nfrom .main import (\n    with_info_plain_validator_function as with_info_plain_validator_function,\n)\nfrom .may_v1 import CoreSchema as CoreSchema\nfrom .may_v1 import GetJsonSchemaHandler as GetJsonSchemaHandler\nfrom .may_v1 import JsonSchemaValue as JsonSchemaValue\nfrom .may_v1 import _normalize_errors as _normalize_errors\nfrom .model_field import ModelField as ModelField\nfrom .shared import PYDANTIC_V2 as PYDANTIC_V2\nfrom .shared import PYDANTIC_VERSION_MINOR_TUPLE as PYDANTIC_VERSION_MINOR_TUPLE\nfrom .shared import annotation_is_pydantic_v1 as annotation_is_pydantic_v1\nfrom .shared import field_annotation_is_scalar as field_annotation_is_scalar\nfrom .shared import (\n    is_uploadfile_or_nonable_uploadfile_annotation as is_uploadfile_or_nonable_uploadfile_annotation,\n)\nfrom .shared import (\n    is_uploadfile_sequence_annotation as is_uploadfile_sequence_annotation,\n)\nfrom .shared import lenient_issubclass as lenient_issubclass\nfrom .shared import sequence_types as sequence_types\nfrom .shared import value_is_sequence as value_is_sequence\n", "metadata": {"license": "MIT", "len_tokens": 618}}
{"id": "fastapi:fastapi/_compat/may_v1.py", "language": "python", "code": "import sys\nfrom typing import Any, Dict, List, Literal, Sequence, Tuple, Type, Union\n\nfrom fastapi.types import ModelNameMap\n\nif sys.version_info >= (3, 14):\n\n    class AnyUrl:\n        pass\n\n    class BaseConfig:\n        pass\n\n    class BaseModel:\n        pass\n\n    class Color:\n        pass\n\n    class CoreSchema:\n        pass\n\n    class ErrorWrapper:\n        pass\n\n    class FieldInfo:\n        pass\n\n    class GetJsonSchemaHandler:\n        pass\n\n    class JsonSchemaValue:\n        pass\n\n    class ModelField:\n        pass\n\n    class NameEmail:\n        pass\n\n    class RequiredParam:\n        pass\n\n    class SecretBytes:\n        pass\n\n    class SecretStr:\n        pass\n\n    class Undefined:\n        pass\n\n    class UndefinedType:\n        pass\n\n    class Url:\n        pass\n\n    from .v2 import ValidationError, create_model\n\n    def get_definitions(\n        *,\n        fields: List[ModelField],\n        model_name_map: ModelNameMap,\n        separate_input_output_schemas: bool = True,\n    ) -> Tuple[\n        Dict[\n            Tuple[ModelField, Literal[\"validation\", \"serialization\"]], JsonSchemaValue\n        ],\n        Dict[str, Dict[str, Any]],\n    ]:\n        return {}, {}  # pragma: no cover\n\n\nelse:\n    from .v1 import AnyUrl as AnyUrl\n    from .v1 import BaseConfig as BaseConfig\n    from .v1 import BaseModel as BaseModel\n    from .v1 import Color as Color\n    from .v1 import CoreSchema as CoreSchema\n    from .v1 import ErrorWrapper as ErrorWrapper\n    from .v1 import FieldInfo as FieldInfo\n    from .v1 import GetJsonSchemaHandler as GetJsonSchemaHandler\n    from .v1 import JsonSchemaValue as JsonSchemaValue\n    from .v1 import ModelField as ModelField\n    from .v1 import NameEmail as NameEmail\n    from .v1 import RequiredParam as RequiredParam\n    from .v1 import SecretBytes as SecretBytes\n    from .v1 import SecretStr as SecretStr\n    from .v1 import Undefined as Undefined\n    from .v1 import UndefinedType as UndefinedType\n    from .v1 import Url as Url\n    from .v1 import ValidationError, create_model\n    from .v1 import get_definitions as get_definitions\n\n\nRequestErrorModel: Type[BaseModel] = create_model(\"Request\")\n\n\ndef _normalize_errors(errors: Sequence[Any]) -> List[Dict[str, Any]]:\n    use_errors: List[Any] = []\n    for error in errors:\n        if isinstance(error, ErrorWrapper):\n            new_errors = ValidationError(  # type: ignore[call-arg]\n                errors=[error], model=RequestErrorModel\n            ).errors()\n            use_errors.extend(new_errors)\n        elif isinstance(error, list):\n            use_errors.extend(_normalize_errors(error))\n        else:\n            use_errors.append(error)\n    return use_errors\n\n\ndef _regenerate_error_with_loc(\n    *, errors: Sequence[Any], loc_prefix: Tuple[Union[str, int], ...]\n) -> List[Dict[str, Any]]:\n    updated_loc_errors: List[Any] = [\n        {**err, \"loc\": loc_prefix + err.get(\"loc\", ())}\n        for err in _normalize_errors(errors)\n    ]\n\n    return updated_loc_errors\n", "metadata": {"license": "MIT", "len_tokens": 713}}
{"id": "fastapi:fastapi/_compat/model_field.py", "language": "python", "code": "from typing import (\n    Any,\n    Dict,\n    List,\n    Tuple,\n    Union,\n)\n\nfrom fastapi.types import IncEx\nfrom pydantic.fields import FieldInfo\nfrom typing_extensions import Literal, Protocol\n\n\nclass ModelField(Protocol):\n    field_info: \"FieldInfo\"\n    name: str\n    mode: Literal[\"validation\", \"serialization\"] = \"validation\"\n    _version: Literal[\"v1\", \"v2\"] = \"v1\"\n\n    @property\n    def alias(self) -> str: ...\n\n    @property\n    def required(self) -> bool: ...\n\n    @property\n    def default(self) -> Any: ...\n\n    @property\n    def type_(self) -> Any: ...\n\n    def get_default(self) -> Any: ...\n\n    def validate(\n        self,\n        value: Any,\n        values: Dict[str, Any] = {},  # noqa: B006\n        *,\n        loc: Tuple[Union[int, str], ...] = (),\n    ) -> Tuple[Any, Union[List[Dict[str, Any]], None]]: ...\n\n    def serialize(\n        self,\n        value: Any,\n        *,\n        mode: Literal[\"json\", \"python\"] = \"json\",\n        include: Union[IncEx, None] = None,\n        exclude: Union[IncEx, None] = None,\n        by_alias: bool = True,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n    ) -> Any: ...\n", "metadata": {"license": "MIT", "len_tokens": 315}}
{"id": "fastapi:fastapi/_compat/model_field.py", "language": "python", "code": "class ModelField(Protocol):\n    field_info: \"FieldInfo\"\n    name: str\n    mode: Literal[\"validation\", \"serialization\"] = \"validation\"\n    _version: Literal[\"v1\", \"v2\"] = \"v1\"\n\n    @property\n    def alias(self) -> str: ...\n\n    @property\n    def required(self) -> bool: ...\n\n    @property\n    def default(self) -> Any: ...\n\n    @property\n    def type_(self) -> Any: ...\n\n    def get_default(self) -> Any: ...\n\n    def validate(\n        self,\n        value: Any,\n        values: Dict[str, Any] = {},  # noqa: B006\n        *,\n        loc: Tuple[Union[int, str], ...] = (),\n    ) -> Tuple[Any, Union[List[Dict[str, Any]], None]]: ...\n\n    def serialize(\n        self,\n        value: Any,\n        *,\n        mode: Literal[\"json\", \"python\"] = \"json\",\n        include: Union[IncEx, None] = None,\n        exclude: Union[IncEx, None] = None,\n        by_alias: bool = True,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n    ) -> Any: ...", "metadata": {"license": "MIT", "len_tokens": 270}}
{"id": "fastapi:fastapi/_compat/main.py", "language": "python", "code": "def get_compat_model_name_map(fields: List[ModelField]) -> ModelNameMap:\n    v1_model_fields = [\n        field for field in fields if isinstance(field, may_v1.ModelField)\n    ]\n    if v1_model_fields:\n        from fastapi._compat import v1\n\n        v1_flat_models = v1.get_flat_models_from_fields(\n            v1_model_fields, known_models=set()\n        )\n        all_flat_models = v1_flat_models\n    else:\n        all_flat_models = set()\n    if PYDANTIC_V2:\n        from . import v2\n\n        v2_model_fields = [\n            field for field in fields if isinstance(field, v2.ModelField)\n        ]\n        v2_flat_models = v2.get_flat_models_from_fields(\n            v2_model_fields, known_models=set()\n        )\n        all_flat_models = all_flat_models.union(v2_flat_models)\n\n        model_name_map = v2.get_model_name_map(all_flat_models)\n        return model_name_map\n    from fastapi._compat import v1\n\n    model_name_map = v1.get_model_name_map(all_flat_models)\n    return model_name_map", "metadata": {"license": "MIT", "len_tokens": 238}}
{"id": "fastapi:fastapi/_compat/main.py", "language": "python", "code": "def get_definitions(\n    *,\n    fields: List[ModelField],\n    model_name_map: ModelNameMap,\n    separate_input_output_schemas: bool = True,\n) -> Tuple[\n    Dict[\n        Tuple[ModelField, Literal[\"validation\", \"serialization\"]],\n        may_v1.JsonSchemaValue,\n    ],\n    Dict[str, Dict[str, Any]],\n]:\n    if sys.version_info < (3, 14):\n        v1_fields = [field for field in fields if isinstance(field, may_v1.ModelField)]\n        v1_field_maps, v1_definitions = may_v1.get_definitions(\n            fields=v1_fields,\n            model_name_map=model_name_map,\n            separate_input_output_schemas=separate_input_output_schemas,\n        )\n        if not PYDANTIC_V2:\n            return v1_field_maps, v1_definitions\n        else:\n            from . import v2\n\n            v2_fields = [field for field in fields if isinstance(field, v2.ModelField)]\n            v2_field_maps, v2_definitions = v2.get_definitions(\n                fields=v2_fields,\n                model_name_map=model_name_map,\n                separate_input_output_schemas=separate_input_output_schemas,\n            )\n            all_definitions = {**v1_definitions, **v2_definitions}\n            all_field_maps = {**v1_field_maps, **v2_field_maps}\n            return all_field_maps, all_definitions\n\n    # Pydantic v1 is not supported since Python 3.14\n    else:\n        from . import v2\n\n        v2_fields = [field for field in fields if isinstance(field, v2.ModelField)]\n        v2_field_maps, v2_definitions = v2.get_definitions(\n            fields=v2_fields,\n            model_name_map=model_name_map,\n            separate_input_output_schemas=separate_input_output_schemas,\n        )\n        return v2_field_maps, v2_definitions", "metadata": {"license": "MIT", "len_tokens": 406}}
{"id": "fastapi:fastapi/_compat/main.py", "language": "python", "code": "def get_schema_from_model_field(\n    *,\n    field: ModelField,\n    model_name_map: ModelNameMap,\n    field_mapping: Dict[\n        Tuple[ModelField, Literal[\"validation\", \"serialization\"]],\n        may_v1.JsonSchemaValue,\n    ],\n    separate_input_output_schemas: bool = True,\n) -> Dict[str, Any]:\n    if isinstance(field, may_v1.ModelField):\n        from fastapi._compat import v1\n\n        return v1.get_schema_from_model_field(\n            field=field,\n            model_name_map=model_name_map,\n            field_mapping=field_mapping,\n            separate_input_output_schemas=separate_input_output_schemas,\n        )\n    else:\n        assert PYDANTIC_V2\n        from . import v2\n\n        return v2.get_schema_from_model_field(\n            field=field,  # type: ignore[arg-type]\n            model_name_map=model_name_map,\n            field_mapping=field_mapping,  # type: ignore[arg-type]\n            separate_input_output_schemas=separate_input_output_schemas,\n        )", "metadata": {"license": "MIT", "len_tokens": 219}}
{"id": "fastapi:fastapi/_compat/v2.py", "language": "python", "code": "def get_schema_from_model_field(\n    *,\n    field: ModelField,\n    model_name_map: ModelNameMap,\n    field_mapping: Dict[\n        Tuple[ModelField, Literal[\"validation\", \"serialization\"]], JsonSchemaValue\n    ],\n    separate_input_output_schemas: bool = True,\n) -> Dict[str, Any]:\n    override_mode: Union[Literal[\"validation\"], None] = (\n        None\n        if (separate_input_output_schemas or _has_computed_fields(field))\n        else \"validation\"\n    )\n    field_alias = (\n        (field.validation_alias or field.alias)\n        if field.mode == \"validation\"\n        else (field.serialization_alias or field.alias)\n    )\n\n    # This expects that GenerateJsonSchema was already used to generate the definitions\n    json_schema = field_mapping[(field, override_mode or field.mode)]\n    if \"$ref\" not in json_schema:\n        # TODO remove when deprecating Pydantic v1\n        # Ref: https://github.com/pydantic/pydantic/blob/d61792cc42c80b13b23e3ffa74bc37ec7c77f7d1/pydantic/schema.py#L207\n        json_schema[\"title\"] = field.field_info.title or field_alias.title().replace(\n            \"_\", \" \"\n        )\n    return json_schema", "metadata": {"license": "MIT", "len_tokens": 276}}
{"id": "fastapi:fastapi/_compat/v2.py", "language": "python", "code": "def get_definitions(\n    *,\n    fields: Sequence[ModelField],\n    model_name_map: ModelNameMap,\n    separate_input_output_schemas: bool = True,\n) -> Tuple[\n    Dict[Tuple[ModelField, Literal[\"validation\", \"serialization\"]], JsonSchemaValue],\n    Dict[str, Dict[str, Any]],\n]:\n    schema_generator = GenerateJsonSchema(ref_template=REF_TEMPLATE)\n    validation_fields = [field for field in fields if field.mode == \"validation\"]\n    serialization_fields = [field for field in fields if field.mode == \"serialization\"]\n    flat_validation_models = get_flat_models_from_fields(\n        validation_fields, known_models=set()\n    )\n    flat_serialization_models = get_flat_models_from_fields(\n        serialization_fields, known_models=set()\n    )\n    flat_validation_model_fields = [\n        ModelField(\n            field_info=FieldInfo(annotation=model),\n            name=model.__name__,\n            mode=\"validation\",\n        )\n        for model in flat_validation_models\n    ]\n    flat_serialization_model_fields = [\n        ModelField(\n            field_info=FieldInfo(annotation=model),\n            name=model.__name__,\n            mode=\"serialization\",\n        )\n        for model in flat_serialization_models\n    ]\n    flat_model_fields = flat_validation_model_fields + flat_serialization_model_fields\n    input_types = {f.type_ for f in fields}\n    unique_flat_model_fields = {\n        f for f in flat_model_fields if f.type_ not in input_types\n    }\n    inputs = [\n        (\n            field,\n            (\n                field.mode\n                if (separate_input_output_schemas or _has_computed_fields(field))\n                else \"validation\"\n            ),\n            field._type_adapter.core_schema,\n        )\n        for field in list(fields) + list(unique_flat_model_fields)\n    ]\n    field_mapping, definitions = schema_generator.generate_definitions(inputs=inputs)\n    for item_def in cast(Dict[str, Dict[str, Any]], definitions).values():\n        if \"description\" in item_def:\n            item_description = cast(str, item_def[\"description\"]).split(\"\\f\")[0]\n            item_def[\"description\"] = item_description\n    new_mapping, new_definitions = _remap_definitions_and_field_mappings(\n        model_name_map=model_name_map,\n        definitions=definitions,  # type: ignore[arg-type]\n        field_mapping=field_mapping,\n    )\n    return new_mapping, new_definitions", "metadata": {"license": "MIT", "len_tokens": 494}}
{"id": "fastapi:fastapi/_compat/v2.py", "language": "python", "code": "def _replace_refs(\n    *,\n    schema: Dict[str, Any],\n    old_name_to_new_name_map: Dict[str, str],\n) -> Dict[str, Any]:\n    new_schema = deepcopy(schema)\n    for key, value in new_schema.items():\n        if key == \"$ref\":\n            value = schema[\"$ref\"]\n            if isinstance(value, str):\n                ref_name = schema[\"$ref\"].split(\"/\")[-1]\n                if ref_name in old_name_to_new_name_map:\n                    new_name = old_name_to_new_name_map[ref_name]\n                    new_schema[\"$ref\"] = REF_TEMPLATE.format(model=new_name)\n            continue\n        if isinstance(value, dict):\n            new_schema[key] = _replace_refs(\n                schema=value,\n                old_name_to_new_name_map=old_name_to_new_name_map,\n            )\n        elif isinstance(value, list):\n            new_value = []\n            for item in value:\n                if isinstance(item, dict):\n                    new_item = _replace_refs(\n                        schema=item,\n                        old_name_to_new_name_map=old_name_to_new_name_map,\n                    )\n                    new_value.append(new_item)\n\n                else:\n                    new_value.append(item)\n            new_schema[key] = new_value\n    return new_schema", "metadata": {"license": "MIT", "len_tokens": 253}}
{"id": "fastapi:fastapi/_compat/v2.py", "language": "python", "code": "def _remap_definitions_and_field_mappings(\n    *,\n    model_name_map: ModelNameMap,\n    definitions: Dict[str, Any],\n    field_mapping: Dict[\n        Tuple[ModelField, Literal[\"validation\", \"serialization\"]], JsonSchemaValue\n    ],\n) -> Tuple[\n    Dict[Tuple[ModelField, Literal[\"validation\", \"serialization\"]], JsonSchemaValue],\n    Dict[str, Any],\n]:\n    old_name_to_new_name_map = {}\n    for field_key, schema in field_mapping.items():\n        model = field_key[0].type_\n        if model not in model_name_map or \"$ref\" not in schema:\n            continue\n        new_name = model_name_map[model]\n        old_name = schema[\"$ref\"].split(\"/\")[-1]\n        if old_name in {f\"{new_name}-Input\", f\"{new_name}-Output\"}:\n            continue\n        old_name_to_new_name_map[old_name] = new_name\n\n    new_field_mapping: Dict[\n        Tuple[ModelField, Literal[\"validation\", \"serialization\"]], JsonSchemaValue\n    ] = {}\n    for field_key, schema in field_mapping.items():\n        new_schema = _replace_refs(\n            schema=schema,\n            old_name_to_new_name_map=old_name_to_new_name_map,\n        )\n        new_field_mapping[field_key] = new_schema\n\n    new_definitions = {}\n    for key, value in definitions.items():\n        if key in old_name_to_new_name_map:\n            new_key = old_name_to_new_name_map[key]\n        else:\n            new_key = key\n        new_value = _replace_refs(\n            schema=value,\n            old_name_to_new_name_map=old_name_to_new_name_map,\n        )\n        new_definitions[new_key] = new_value\n    return new_field_mapping, new_definitions", "metadata": {"license": "MIT", "len_tokens": 376}}
{"id": "fastapi:fastapi/_compat/v2.py", "language": "python", "code": "def __post_init__(self) -> None:\n        with warnings.catch_warnings():\n            # Pydantic >= 2.12.0 warns about field specific metadata that is unused\n            # (e.g. `TypeAdapter(Annotated[int, Field(alias='b')])`). In some cases, we\n            # end up building the type adapter from a model field annotation so we\n            # need to ignore the warning:\n            if shared.PYDANTIC_VERSION_MINOR_TUPLE >= (2, 12):\n                from pydantic.warnings import UnsupportedFieldAttributeWarning\n\n                warnings.simplefilter(\n                    \"ignore\", category=UnsupportedFieldAttributeWarning\n                )\n            # TODO: remove after dropping support for Python 3.8 and\n            # setting the min Pydantic to v2.12.3 that adds asdict()\n            field_dict = asdict(self.field_info)\n            annotated_args = (\n                field_dict[\"annotation\"],\n                *field_dict[\"metadata\"],\n                # this FieldInfo needs to be created again so that it doesn't include\n                # the old field info metadata and only the rest of the attributes\n                Field(**field_dict[\"attributes\"]),\n            )\n            self._type_adapter: TypeAdapter[Any] = TypeAdapter(\n                Annotated[annotated_args],\n                config=self.config,\n            )", "metadata": {"license": "MIT", "len_tokens": 272}}
